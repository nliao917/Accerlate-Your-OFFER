JT,JD,tool_list,skill_list,degree_list,clean,lda_list,newlist
DE,"**
Job Overview
Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, reference data or master data management, data architecture, data modeling, data governance, data analysis, business intelligence.
Skills & Requirements:
MUST BE HANDS ON.
Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge:
Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)
Must have significant hands-on experience with SQL, data profiling, and data discovery
Experience building business intelligence, analytics, or reporting solutions - either front-end consumption mechanisms (e.g., Microsoft, Tableau & Qlik) or supply of data for these purposes
Familiarity with data architecture principles/approaches, data environment infrastructure considerations, and data modeling principles/approaches
Ability to drive out technical requirements with business and IT stakeholders for implementations of data solutions
Hands-on experience with Agile delivery methodology
Prior professional experience in an IT management, management consulting, or client facing role is preferred
Knowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred
Demonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions
Tools and Technology:
Proficient at leveraging tools and technology to drive value for clients.
Examples include the following;
Database Management Tools:
Relational – e.g.
Oracle, MySQL, Microsoft SQL Server, PostgreSQL, DB, or similar
NoSQL – e.g.
MongoDB, Couchbase, DataStax, Redix, MarkLogic, or similar
Cloud – e.g.
AWS, Azure, xxx, xxx, xxx
ETL Tools - e.g.
Informatica, Talend, Microsoft SSIS, or similar
Data Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar
Industry Leading BI tools - e.g., Business Objects, Microsoft, Cognos, Tableau, OBIEE, Qlickview, or similar
General:
3+ years of professional experience working in a related role
Must be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude
Strong desire to work on interesting projects with smart and creative people
Willingness to travel up to 80% of the week (M-Th)
Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.
","['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'bi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'mysql', 'oracl']","['healthcar', 'data modeling', 'data warehousing', 'etl', 'commun']",2,"['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'powerbi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'etl', 'commun']","['analyt', 'azur', 'relat', 'infrastructur', 'bi', 'aw', 'etl', 'engin']","['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'powerbi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'etl', 'commun', 'analyt', 'azur', 'relat', 'infrastructur', 'bi', 'aw', 'etl', 'engin']"
DE,"Key Responsibilities
- Architect, build, and maintain efficient data pipelines, utilizing a variety of data access technologies (REST APIs, FTP, SQL, S3)
- Work with analytics team to help with automating reporting needs leveraging various solutions like REST APIs or BI tools
- End-to-end development, starting from requirements gathering with stakeholders to deployment to production systems
- Lead greenfield development of innovative applications, analysis, and systems
- Contribute to development of a rapidly growing, integrated datawarehouse that will provide a complete vision of the entire healthcare landscape
- Work with large, alternative datasets (100s of millions of data points) to extract insights and proactively monitor trends
Minimum Qualifications
· Advanced SQL Knowledge
· Proficient in object-oriented development (C#, Java, etc)
· Proficient in creating REST Web Services using C#/Python/Node
· Experience working with large data sets (50+ million rows)
· Ability to architect end-to-end ETL pipelines
· Comfortable writing unit/integration tests, implementing logging/monitoring, and maintaining reliability of internal processes
· Familiarity with software development best practices: Dependency Injection, ORMs, Test-Driven Development, Modular design, etc.
Preferred Skills
· Architect-level experience with development using Cloud tools (ideally Azure)
· Experience with distributed datastores such as Databricks, Spark/Hadoop, Snowflake NoSql, etc.
· Web development in a full-stack capacity (ideally Angular)
· Business Intelligence Experience (Power BI, Tableau, etc.)
· Experience working with EMR or Claims data
Benefits
· Unlimited meals
· Always-stocked kitchens/resource rooms
· Monthly happy hours
· Great work-life balance
","['sql', 'spark', 'azur', 'angular', 'node', 'hadoop', 'bi', 'snowflak', 'python', 's3', 'c', 'nosql', 'java', 'tableau', 'cloud', 'power bi']","['etl', 'pipelin', 'healthcar']",999,"['sql', 'spark', 'azur', 'angular', 'node', 'hadoop', 'powerbi', 'snowflak', 'python', 's3', 'c', 'nosql', 'java', 'tableau', 'cloud', 'etl', 'pipelin', 'healthcar']","['analyt', 'spark', 'pipelin', 'azur', 'set', 'power', 'hadoop', 'bi', 'python', 'etl', 'integr']","['sql', 'spark', 'azur', 'angular', 'node', 'hadoop', 'powerbi', 'snowflak', 'python', 's3', 'c', 'nosql', 'java', 'tableau', 'cloud', 'etl', 'pipelin', 'healthcar', 'analyt', 'spark', 'pipelin', 'azur', 'set', 'power', 'hadoop', 'bi', 'python', 'etl', 'integr']"
DE,"Overview
Position Overview:
Seeking a highly qualified Data Engineer with a track record of managing and architecting complex data platforms.
As a member of the Data Engineering and Operations Team, the Data Engineer will work on data management projects for a wide variety of companies including Cerberus portfolio companies.
This person will be responsible to creating programs for loading, transforming, and validating data.
As such, a successful candidate will have deep technical and management expertise across a variety of methodologies as well as have demonstrated management skills.
Responsibilities:
Design and build data management platforms
Design and build scalable data platforms to securely ingest, process, validate, analyze and publish data
Manage large & complex data and analytical projects: data transformation, exploration, performance evaluation & testing
Move existing ETL jobs from traditional data warehouse processing to the big data processing platform, ensure that the jobs are designed to scale
Manage and lead data engineering projects
Manage and execute the successful delivery of the data (ETL/ELT) pipelines, analytics platforms and reporting tools to meet business needs
Perform analyses of large structured and unstructured data to solve multiple and complex business problems utilizing advanced statistical techniques, and specialized expertise in the organization and/or industry
Assess data management platforms
Assess the complete landscape of a data refinery (data discovery, data loading, data transformation, data validation and data publish)
Business Knowledge/Technical Skills:
5+ years professional work experience as a data engineer
Deep experience in data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design, data modeling, reporting process metadata, and ETL processes
Proven experience in developing analytics strategy, roadmap and delivering major change initiatives
Proven experience in negotiating contracts, license optimization, service levels & managing vendors
Ability to set and manage priorities judiciously
Knowledge of: data processing platforms like Hadoop, Spark, Hive, AWS (EMR, Kinesis, lambda, glue) Azure (HDInsight, data factory); airflow; coding best practices (git/stash usage); python, R; Java/Scala; writing production and modular code
Other:
Bachelor's degree required
Master’s degree in in Computer Science, Statistics, Economics, Physics, Engineering, Mathematics, or other closely related field is preferred
This position will be based out of New York City and will require travel
As a member of the Data Engineering and Operations Team, the Data Engineer will work on data management projects for a wide variety of companies including Cerberus portfolio companies.
This person will be responsible to creating programs for loading, transforming, and validating data.
As such, a successful candidate will have deep technical and management expertise across a variety of methodologies as well as have demonstrated management skills.
?
","['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r']","['big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom']",1,"['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r', 'big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom']","['analyt', 'program', 'techniqu', 'spark', 'azur', 'relat', 'hadoop', 'optim', 'aw', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin', 'evalu']","['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r', 'big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom', 'analyt', 'program', 'techniqu', 'spark', 'azur', 'relat', 'hadoop', 'optim', 'aw', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin', 'evalu']"
DE,"Data Engineer
Job Details
Level
Experienced
New York (Home Office) - New York, NY
Position Type
Full Time
Education Level
4 Year Degree
Salary Range
Undisclosed
Travel Percentage
Undisclosed
Job Shift
Day
Job Category
Information Technology
Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor’s or Master’s degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills
","['sql', 'r', 'python', 'vba', 'excel']","['recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']",1,"['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']","['day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']","['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment', 'day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']"
DE,"Senior Data Engineer
Master’s degree in Information Technology, Computer Science with 2 years experience.
Build & maintain Analytical Data Platforms using appropriate SQL, NoSQL and NewSQL technologies like MapR, Spark, Hadoop, & Python.
Lead engineering processes to ensure data quality & meta data documentation using tools like Python, Amazon RedShift, Tableau & Confluence.
Perform quantitative analysis of customer data using tools like SaaS & machine learning.
Monitor tag transactions to correctly reward consumers based on merchant reward program offerings.
Skills: SQL, NoSQL, NewSQL, MapR, Spark, Hadoop, Python, Amazon RedShift, Tableau, Confluence, SaaS & machine learning.
","['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql']","['machine learning', 'information technology']",2,"['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql', 'machine learning', 'information technology']","['analyt', 'machin', 'program', 'learn', 'spark', 'hadoop', 'python', 'comput', 'quantit', 'engin']","['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql', 'machine learning', 'information technology', 'analyt', 'machin', 'program', 'learn', 'spark', 'hadoop', 'python', 'comput', 'quantit', 'engin']"
DE,"Responsibilities:
Driving innovation through product and platform development
Helping to facilitate bespoke custom basket trades for clients in a scalable infrastructure
Developing infrastructure and tools to administer basket rebalances for external clients and internal trading teams
Automation of corporate action adjustments and improvement of work-flow
Providing metrics for basket trades, to drive sales and trading decisions and to grow the business
Both independent and collaborative work, involving several sales/strat/trading teams globally
Requirements:
Expertise in Python
Strong SQL skills
Web scraping experience
Experience with Linux and Windows platform
Strong communication skills, both written and verbal
Exposure to non-relational databases
Exposure to web UI technologies
Data Warehousing and Modeling expertise
Financial knowledge
If you would like to be considered for the position of Data Engineer or wish to discuss the role further then please leave your details below.
Thank you
","['sql', 'linux', 'python']","['data warehousing', 'commun']",999,"['sql', 'linux', 'python', 'data warehousing', 'commun']","['corpor', 'infrastructur', 'python', 'action', 'relat', 'engin']","['sql', 'linux', 'python', 'data warehousing', 'commun', 'corpor', 'infrastructur', 'python', 'action', 'relat', 'engin']"
DE,"The ideal candidate is entrepreneurial, motivated to grow, and has a passion for Python development.
What you'll be doing
Developing and enhancing multiple ETL pipelines
Designing and implementing data storage structures and ETL pipelines, keeping long-term impacts in mind
Assemble large, complex data sets that meet functional / non-functional business requirements.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Identifying and improving upon current internal processes through automation and optimization
Who you are:
Bachelor's degree in Computer Science, Mathematics, or related field/equivalent experience
3+ years of experience with Python, Java, Scala, R, Go or similar language
3+ years of experience in working with cloud computing technology (AWS, Google Cloud Platform, etc.)
3+ years experience working on data warehouse systems such as Snowflake
Have developed systems based on key principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)
Comfortable using version control and working in a collaborative environment
Experienced with CI/CD tools for testing and deployment
Self-starter with the ability to work independently or in a team
Able to manage one's schedule and prioritize tasks independently
Why You Should Join
Wherever you are from, you will find a common ground here for continuing to push forward your career and make a difference in this industry.
The technology generates customer intent insights that lead to compelling content, increased traffic, and higher organic marketing ROI.
Customizable dashboards and workflows guide marketers through the content creation process, empowering them to measure, refine, and demonstrate the effectiveness of their SEO and content marketing efforts.
","['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud']","['pipelin', 'dashboard', 'optim', 'big data', 'etl']",1,"['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud', 'pipelin', 'dashboard', 'optim', 'big data', 'etl']","['stream', 'pipelin', 'set', 'relat', 'optim', 'aw', 'python', 'avail', 'comput', 'etl', 'common', 'algorithm']","['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud', 'pipelin', 'dashboard', 'optim', 'big data', 'etl', 'stream', 'pipelin', 'set', 'relat', 'optim', 'aw', 'python', 'avail', 'comput', 'etl', 'common', 'algorithm']"
DE,"By applying for this role, you could choose to work in the following locations:
New York City
San Francisco
DATA ENGINEER - HEALTH DATA ENGINEERING TEAM
Are you an engineer who’s passionate about defending online users against abuse, spam, and manipulation?
Will you be proud to work on a real-time, scalable pipelines that process terabytes of data to enable training and analysis of Machine Learning models, product analysis, and experimentation?
This team is also in charge of the best practices around building scalable, production-ready data processing solutions, as well as researching and implementing the most efficient mechanisms for data access.
Health Data Engineering team will be partnering closely with all the engineering teams in the Health org to understand and improve its data production and consumption needs.
What You’ll Do
Here are some examples of what you’ll find yourself doing daily:
Directly contribute to the design and code of the data pipelines operating on production data
Improve approaches to efficiently handle ever-increasing volumes of data
Maintain efficiency and reliability of production of the critical datasets
Evaluate and propose the best tooling and processes for data access and analysis
Provide design and review support to the engineering teams working on data processing
Continuously evaluate team’s processes to maintain a positive and efficient engineering culture
Who You Are
You have experience working in an environment that supports data analysis, experimentation, and Machine Learning modeling or its integration into a product.
You have a solid understanding of backend and distributed systems and strong experience working with MapReduce-based architectures.
You have experience in working with large volumes of data.
You have a broad knowledge of the data infrastructure ecosystem.
You are familiar with standard software engineering methodology, e.g.
unit testing, code reviews, design documentation.
You enjoy working in a collaborative environment and interact effectively with others.
You ground your decisions with data and reasoning and can adapt to new information to make informed choices.
You bring thoughtful perspectives, empathy, creativity, and a positive attitude to solve problems at scale.
Here’s all the legal good stuff:
",['mapreduc'],"['research', 'machine learning', 'pipelin']",999,"['mapreduc', 'research', 'machine learning', 'pipelin']","['machin', 'learn', 'pipelin', 'handl', 'infrastructur', 'provid', 'integr', 'engin', 'evalu']","['mapreduc', 'research', 'machine learning', 'pipelin', 'machin', 'learn', 'pipelin', 'handl', 'infrastructur', 'provid', 'integr', 'engin', 'evalu']"
DE,"Data Engineer
YOUR OPPORTUNITY:
Develop solutions that enable investment professionals to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
YOUR SKILLS & TALENTS:
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker']","['research', 'commun']",1,"['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun']","['program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun', 'program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']"
DE,"What you'll be doing:
Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives
Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way
Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)
Collaborate within a small team with diverse technology backgrounds
Provide mentorship and guidance to junior team members
Team Responsibilities:
Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS
Ingest, validate and process internal & third party data
Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time
Maintain and enhance framework for jobs(primarily aggregate jobs in Hive)
Create different consumers for data in Kafka using Spark Streaming for near time aggregation
Train Developers/Analysts on tools to pull data
Tool evaluation/selection/implementation
Backups/Retention/High Availability/Capacity Planning
24*7 On call rotation for Production support
Airflow - for job scheduling
Docker - Packaged container image with all dependencies
Graphite/Beacon - for monitoring data flows
Hive - SQL data warehouse layer for data in HDFS
Impala- faster SQL layer on top of Hive
Kafka- distributed commit log storage
Kubernetes - Distributed cluster resource manager
Presto - fast parallel data warehouse and data federation layer
Spark Streaming - Near time aggregation
SQL Server - Reliable OLTP RDBMS
Sqoop - Import/Export data to RDBMS
Required Skills:
BA/BS degree in Computer science or related field
5+ years of software engineering experience
Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus
Knowledge and exposure to Cloud migration is a plus
Proficiency in Linux
Fluency in Python, Experience in Scala/Java is a huge plus
Strong understanding of RDBMS, SQL;
Passion for engineering and computer science around data
Willingness to participate in 24x7 on-call rotation
401(k) Match and free access to a financial advisor
Generous paid vacation/company holidays
Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance
$2,000 annual training and development budget
Complimentary annual memberships to One Medical, NY Citi Bike and SF Ford GoBike
Monthly chair massages
Free fitness classes (spin, yoga, boxing)
Gym reimbursement, local gym membership discounts
Onsite flu shots, dental cleanings and vision exams
Paid parental leave and a lot of new parent perks
Emergency childcare credits
Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities
Team lunches, Sip & Social Thursdays, Game Nights, Movie Nights
Healthy snacks and drinks
And there's a lot more!
","['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker']","['optim', 'clean', 'healthcar', 'cluster']",1,"['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker', 'optim', 'clean', 'healthcar', 'cluster']","['stream', 'spark', 'credit', 'divers', 'hadoop', 'provid', 'optim', 'python', 'avail', 'packag', 'parti', 'scientist', 'comput', 'warehous', 'relat', 'engin', 'evalu']","['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker', 'optim', 'clean', 'healthcar', 'cluster', 'stream', 'spark', 'credit', 'divers', 'hadoop', 'provid', 'optim', 'python', 'avail', 'packag', 'parti', 'scientist', 'comput', 'warehous', 'relat', 'engin', 'evalu']"
DE,"70570
Fundamental Components: Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with data science team to transform data and integrate algorithms and models into automated processes.
Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support Data Science and other internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience: Strong problem solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams.5 or more years of progressively complex related experience.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Experience building data transformation and processing solutions.Has strong knowledge of large scale search applications and building high volume data pipelines.
Masters degree or PhD preferred.Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
from you via e-mail.
Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter.
#LI-DT1
","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",2,"['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"# 81883 - New York, New York, United States
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Assist with data-related technical issues and support their data infrastructure needs.
Qualifications for Data Engineer
Bachelor’s degree required, Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is preferred
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience supporting and working with cross-functional teams in a dynamic environment.
They should also have experience using the following software/tools:
Experience with relational SQL and NoSQL databases: MongoDB, Neo4j, etc
Experience with cloud services: GCP, AWS, etc
Experience with object-oriented/object function scripting languages: Python, Java, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with Data Flow, Data Pipeline and workflow management tools: Cloud Composer, Airflow, Luigi, etc
Equal Opportunity Employer
Are you interested in working for Colgate-Palmolive?
Applications received by e-mail are not considered in the selection process.
To learn more about Hill's and the Hill’s Food, Shelter & Love program please visit http://www.hillspet.com.
To learn more about Tom’s of Maine please visit http://www.tomsofmaine.com.
Please contact Application_Accommodation@colpal.com with the subject ""Accommodation Request"" should you require accommodation.
","['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka']","['optim', 'statist', 'pipelin', 'big data']",1,"['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'statist', 'pipelin', 'big data']","['analyt', 'program', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']","['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'statist', 'pipelin', 'big data', 'analyt', 'program', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']"
DE,"Auto req ID: 212077BR
As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly and a period of extraordinary disruption is now underway.
New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms to the creation of supply chain tools that enable speedy at-home delivery.
As it needs the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain.
Accountabilities:
Lead problems assessment of eCommerce challenges to lead the development and design of technology solutions across functions involving computer hardware and software
Lead technology project evaluations as well as proposal feasibility with the different eCommerce businesses
Act as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements
Build new technologies and algorithms to optimize any business process
Develop data set processes and projects requirements
Use large data sets to resolve major business and functional issues whisle improving data reliability, efficiency and quality
Optimize processes implementing new technology and automation across eCommerce businesses and eCommerce functions
Qualifications/Requirements
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job
Knowledge of machine-learning tools and techniques
Experience optimizing larger applications to increase speed, scalability, and extensibility
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology
Job Type: Regular
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents.
","['sql', 'mapreduc', 'spark', 'airflow', 'hadoop', 'aw', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'kafka']","['hardwar', 'data modeling', 'machine learning', 'optim', 'account']",1,"['sql', 'mapreduc', 'spark', 'airflow', 'hadoop', 'aw', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'kafka', 'hardwar', 'data modeling', 'machine learning', 'optim', 'account']","['analyt', 'digit', 'machin', 'techniqu', 'learn', 'spark', 'challeng', 'relat', 'hadoop', 'evalu', 'optim', 'aw', 'python', 'avail', 'comput', 'set', 'algorithm']","['sql', 'mapreduc', 'spark', 'airflow', 'hadoop', 'aw', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'kafka', 'hardwar', 'data modeling', 'machine learning', 'optim', 'account', 'analyt', 'digit', 'machin', 'techniqu', 'learn', 'spark', 'challeng', 'relat', 'hadoop', 'evalu', 'optim', 'aw', 'python', 'avail', 'comput', 'set', 'algorithm']"
DE,"What does the music landscape look like in 2023?
How will people listen to music in a world of voice controlled UI, autonomous cars and AR?
* Build large-scale batch and real-time data pipelines with data processing frameworks like Scio, Storm, or Spark on the Google Cloud Platform.
* Leverage best practices in continuous integration and delivery.
* Help drive optimization, testing, and tooling to improve data quality.
* Collaborate with other engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day.
* Work in cross functional agile teams to continuously experiment, iterate, and deliver on new product objectives.
* You understand the value of collaboration within teams, are excellent communicators, and can build relationships with a diverse set of stakeholders.
* Experience with data ingestion via API and/or web scraping/crawling (e.g.
Selenium, BeautifulSoup) at scale preferred.
","['spark', 'cloud', 'excel', 'google cloud']","['optim', 'commun', 'pipelin']",999,"['spark', 'cloud', 'excel', 'google cloud', 'optim', 'commun', 'pipelin']","['day', 'learn', 'spark', 'pipelin', 'ml', 'set', 'divers', 'optim', 'integr', 'engin']","['spark', 'cloud', 'excel', 'google cloud', 'optim', 'commun', 'pipelin', 'day', 'learn', 'spark', 'pipelin', 'ml', 'set', 'divers', 'optim', 'integr', 'engin']"
DE,"Publishers then have more time to do what they do best: create content.
Data Engineer Job Responsibilities:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Build dashboards that concisely and succinctly convey business metrics.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Data Engineer Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Education, Experience
Ample relevant knowledge and experience.
You either have a BS or MS degree in Computer Science or a related technical field, OR certification from a data science bootcamp + 2 years of experience in a role as a data engineer
Proficiency in Python and Java, Scala, or Go development experience
4+ years of SQL experience (Strong SQL required)
Familiarity with BI reporting tools like Tableau, Looker.
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience/knowledge of cloud computing platforms like AWS/GCP would be a plus
Excellent interpersonal and problem solving skills with the ability to communicate with team members to deliver actionable results
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
What you can expect in return:
Full-Time, Salaried Position
Medical, Dental, and Vision benefits
The opportunity to be part of something BIG
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
This role is not eligible for visa sponsorship
","['sql', 'gcp', 'looker', 'scala', 'bi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel']","['pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun']",1,"['sql', 'gcp', 'looker', 'scala', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel', 'pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun']","['analyt', 'digit', 'engin', 'pipelin', 'relat', 'bi', 'aw', 'avail', 'python', 'comput', 'action', 'big', 'integr', 'sourc', 'linear']","['sql', 'gcp', 'looker', 'scala', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel', 'pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun', 'analyt', 'digit', 'engin', 'pipelin', 'relat', 'bi', 'aw', 'avail', 'python', 'comput', 'action', 'big', 'integr', 'sourc', 'linear']"
DE,"Databases are the beating heart of every business in the world.
About the Role
You Will
Providing and maintaining the code for the collection and processing of data related to CockroachCloud and CockroachDB.
Managing ETL pipelines, workflows to collect, process, and load data into tools such as Segment, Snowflake, Looker, Salesforce, and Zendesk.
The Expectations
After 3 months, you'll be a fully-fledged member of the team.
You Have
A passion for working on complex technical products, have experience collecting and processing large amounts of data and determining its underlying meaning.
Comfort using programming languages like Go, C/C++, Java, and Python.
Experience building collaborative relationships with your colleagues.
You enjoy being part of the code review process and partnering with your teammates on challenging problems.
Comfort with SQL style query languages.
Experience with tools used to collect telemetry data.
5 or more years experience with data collection pipelines and schema design.
A BS in Computer Science or equivalent experience.
The Team
Reporting to Kendra Curtis - Director of Engineering, CockroachCloud
Kendra has 20+ years of experience in all levels of the software stack, from early days writing firmware for wireless networking products at Wi-LAN, to managing teams of developers building Web Applications at Google.
Kendra worked on Google's early data centers.
She was a member of the Management team responsible for the integration of DoubleClick (DCLK) into Google.
She was a Co-founder and CEO of Scout It Out, a listing service for Rehearsal Spaces.
Outside of work she enjoys skiing, acting, and walking in the park with her dog, Lady.
Isaac Wong - VP of Engineering
When not working he likes to draw, play the piano and search NYC for cannolis with his wife and kids.
100% health insurance option (for you and your dependents!)
Paid parental leave (with baby bucks)
Flex Fridays
Flexible time off & flexible hours
Learning and Development budget
","['sql', 'looker', 'salesforc', 'snowflak', 'java', 'python', 'c']","['etl', 'segment', 'pipelin']",1,"['sql', 'looker', 'salesforc', 'snowflak', 'java', 'python', 'c', 'etl', 'segment', 'pipelin']","['day', 'engin', 'pipelin', 'relat', 'amount', 'python', 'comput', 'etl', 'collect', 'integr']","['sql', 'looker', 'salesforc', 'snowflak', 'java', 'python', 'c', 'etl', 'segment', 'pipelin', 'day', 'engin', 'pipelin', 'relat', 'amount', 'python', 'comput', 'etl', 'collect', 'integr']"
DE,"Staff Data Engineer
New York, NY
DV provides media transparency and accountability to deliver the highest level of impression quality for maximum advertising performance.
Since 2008, DV has helped hundreds of Fortune 500 companies gain the most from their media spend by delivering best in class solutions across the digital ecosystem, helping to build a better industry.
Learn more at www.doubleverify.com.
The Role
As a Staff Data Engineer, you own new initiatives, designs and build world-class platforms in order to measure and optimize ad performance.
You ensure industry-leading scalability and reliability of mission-critical systems processing billions of real-time transactions a day.
What you'll do
Architect, design and build big data processing platforms handling tens of TBs/Day, serves thousands of clients and supports advanced analytic workloads
Mentor a great team of motivated engineers by constantly reviewing code, looking for design breaches, providing meaningful and relevant feedback to developers and stay up-to-date with system changes
Design, develop, and test data-driven products, features, and APIs that scale
Continuously improve the quality of deliverables and SDLC processes
Operate production environments, investigate issues, assess their impact, and come up with feasible solutions.
Understand business needs and work with product owners to establish priorities
Bridge the gap between Business / Product requirements and technical details
Work in multi-functional agile teams with end-to-end responsibility for product development and delivery
Who you are
Lead by example - design, develop and deliver quality solutions.
Love what you do and are passionate about crafting clean code and have a steady foundation with 8+ years of programming experience in coding, object-oriented design and/or functional programming
Deep understanding of distributed system technologies, standards, protocols, and have 3+ years of experience working in distributed systems like Hadoop, Big Query, Spark, Kafka Eco System ( Kafka Connect, Kafka Streams), and building data pipelines at scale.
Hands-on experience building low latency, high-throughput APIs, and are comfortable using external APIs from platforms.
Excellent SQL query writing abilities and data understanding
Care about agile software processes, data-driven development, reliability, and responsible experimentation
Genuine desire to automate decision making, processes, and workflows
Experience working with dependency management tools such as Luigi/Airflow
Experience with DevOps domain - working with build servers, docker and containers clusters (kubernetes)
Experience in Mentoring and growing a diverse team of talented data engineers
B.S./M.S.
in Computer Science or a related field
Excellent communication skills and a team player
Vertica or other columnar data stores
Google BigQuery
Spark Streaming or other live stream processing technology
Cloud environment, Google Cloud Platform
Container technologies - Docker / Kubernetes
Ad serving technologies and standards
Experience with Avro, Parquet, or ORC
Akka
","['sql', 'google cloud', 'spark', 'airflow', 'bigqueri', 'hadoop', 'cloud', 'kubernet', 'kafka', 'excel', 'docker']","['pipelin', 'clean', 'big data', 'commun', 'cluster', 'account']",1,"['sql', 'google cloud', 'spark', 'airflow', 'bigqueri', 'hadoop', 'cloud', 'kubernet', 'kafka', 'excel', 'docker', 'pipelin', 'clean', 'big data', 'commun', 'cluster', 'account']","['day', 'analyt', 'digit', 'program', 'stream', 'learn', 'spark', 'pipelin', 'divers', 'hadoop', 'comput', 'big', 'relat', 'engin']","['sql', 'google cloud', 'spark', 'airflow', 'bigqueri', 'hadoop', 'cloud', 'kubernet', 'kafka', 'excel', 'docker', 'pipelin', 'clean', 'big data', 'commun', 'cluster', 'account', 'day', 'analyt', 'digit', 'program', 'stream', 'learn', 'spark', 'pipelin', 'divers', 'hadoop', 'comput', 'big', 'relat', 'engin']"
DE,"OLAP, BI tools), and machine learning technologies* Experience operating systems in AWS* Excellent communication skills, including the ability to identify and communicate data-driven insightsFoursquare is proud to foster an inclusive environment that is free from discrimination.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.
","['aw', 'bi', 'excel']","['machine learning', 'commun']",999,"['aw', 'powerbi', 'excel', 'machine learning', 'commun']","['aw', 'bi', 'machin', 'learn']","['aw', 'powerbi', 'excel', 'machine learning', 'commun', 'aw', 'bi', 'machin', 'learn']"
DE,"Overview
Position Overview:
Seeking a highly qualified Data Engineer with a track record of managing and architecting complex data platforms.
As a member of the Data Engineering and Operations Team, the Data Engineer will work on data management projects for a wide variety of companies including Cerberus portfolio companies.
This person will be responsible to creating programs for loading, transforming, and validating data.
As such, a successful candidate will have deep technical and management expertise across a variety of methodologies as well as have demonstrated management skills.
Responsibilities:
Design and build data management platforms
Design and build scalable data platforms to securely ingest, process, validate, analyze and publish data
Manage large & complex data and analytical projects: data transformation, exploration, performance evaluation & testing
Move existing ETL jobs from traditional data warehouse processing to the big data processing platform, ensure that the jobs are designed to scale
Manage and lead data engineering projects
Manage and execute the successful delivery of the data (ETL/ELT) pipelines, analytics platforms and reporting tools to meet business needs
Perform analyses of large structured and unstructured data to solve multiple and complex business problems utilizing advanced statistical techniques, and specialized expertise in the organization and/or industry
Assess data management platforms
Assess the complete landscape of a data refinery (data discovery, data loading, data transformation, data validation and data publish)
Business Knowledge/Technical Skills:
5+ years professional work experience as a data engineer
Deep experience in data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design, data modeling, reporting process metadata, and ETL processes
Proven experience in developing analytics strategy, roadmap and delivering major change initiatives
Proven experience in negotiating contracts, license optimization, service levels & managing vendors
Ability to set and manage priorities judiciously
Knowledge of: data processing platforms like Hadoop, Spark, Hive, AWS (EMR, Kinesis, lambda, glue) Azure (HDInsight, data factory); airflow; coding best practices (git/stash usage); python, R; Java/Scala; writing production and modular code
Other:
Bachelor's degree required
Master’s degree in in Computer Science, Statistics, Economics, Physics, Engineering, Mathematics, or other closely related field is preferred
This position will be based out of New York City and will require travel
As a member of the Data Engineering and Operations Team, the Data Engineer will work on data management projects for a wide variety of companies including Cerberus portfolio companies.
This person will be responsible to creating programs for loading, transforming, and validating data.
As such, a successful candidate will have deep technical and management expertise across a variety of methodologies as well as have demonstrated management skills.
?
","['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r']","['big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom']",1,"['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r', 'big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom']","['analyt', 'program', 'techniqu', 'spark', 'azur', 'relat', 'hadoop', 'optim', 'aw', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin', 'evalu']","['spark', 'azur', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'hive', 'java', 'r', 'big data', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'etl', 'econom', 'analyt', 'program', 'techniqu', 'spark', 'azur', 'relat', 'hadoop', 'optim', 'aw', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin', 'evalu']"
DE,"Data Engineer
Job Details
Level
Experienced
New York (Home Office) - New York, NY
Position Type
Full Time
Education Level
4 Year Degree
Salary Range
Undisclosed
Travel Percentage
Undisclosed
Job Shift
Day
Job Category
Information Technology
Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor’s or Master’s degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills
","['sql', 'r', 'python', 'vba', 'excel']","['recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']",1,"['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']","['day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']","['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment', 'day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']"
DE,"The Data team uses data to create better customer experiences, smarter business decisions, and efficiencies company-wide.
",[None],[None],999,[],[None],[None]
DE,"The Data Engineer on the Enterprise Analytics team, is responsible for data acquisition strategies and integration scripting and tools, data migrations, conversions, purging and back-ups; fulfills data acquisition strategy requirements.
They work with product, financial control, analysts, users and other stakeholders to understand business requirements and supports data architecture to translate into data acquisition strategies.
The Data Engineer will author artifacts defining standards and definitions for storing, processing and moving data, including associated processes and business rules.
Additionally, the Data Engineer will map the details within these their artifacts to business processes, non-functional characteristics, QA criteria and technical enablement.
The role is responsible to be constantly thinking through the needs of the business to support efficient and error free processes.
The Data Engineer, Enterprise Analytics, will be responsible for finding trends in datasets and developing workflows and algorithms to help make raw data more useful to the enterprise.
He or she will also be responsible for creating data acquisition strategy and develops data set processes.
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning and data destruction (scripts, programs, automation, assisted by automation, etc).
Develop, construct, test and maintain architectures
Align architecture with business requirements and use programming language and tools
Identify ways to improve data reliability, efficiency and quality
Conduct research for industry and business questions
Deploy sophisticated analytics programs, machine learning and statistical methods
Prepare data for predictive and prescriptive modeling and find hidden patterns using data
Use data to discover tasks that can be automated
Create data monitoring capabilities for each business process and work with data consumers on updates
Minimum Qualifications:
Bachelor’s Degree in Computer Engineering or related field
7+ years’ experience in a data engineering
10+ years’ experience in data programing languages such as java or python
4+ years’ experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc.
3+ years’ experience working in a production cloud infrastructure
Preferred Qualifications:
Proven track record of success directing the efforts of data engineers and business analysts within a deadline-driven and fast-paced environment
Hands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment
Demonstrated experience working in an Agile environment as a Data Engineer
Hands on work with Amazon Web Services, including creating Redshift data structures, accessing them with Spectrum and storing data in S3
Knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval.
Proven results using an analytical perspective to identify engineering patterns within complex strategies and ideas, and break them down into engineered code components
Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry
Experience developing, prototyping, and testing engineered processes, products or services
Proven ability to work in distributed systems
Proficiency with relational, graph and noSQL databases required; expertise in SQL
Must be able to develop creative solutions to problems
Demonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes
Excellent interpersonal skills with proven ability to influence with impact across functions and disciplines
Ability to work independently and as part of a team
Ability to manage multiple projects/deadlines, identifying the necessary steps and moving forward through completion
Skilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio
EEO Law Poster and Supplement
]]>
","['sql', 'microsoft', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'powerpoint', 'excel', 'amazon web services']","['research', 'healthcar', 'graph', 'predict', 'machine learning', 'statist', 'big data']",1,"['sql', 'microsoft', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'powerpoint', 'excel', 'aw', 'research', 'healthcar', 'graph', 'predict', 'machine learning', 'statist', 'big data']","['analyt', 'machin', 'program', 'learn', 'set', 'relat', 'predict', 'infrastructur', 'provid', 'python', 'comput', 'statist', 'big', 'integr', 'engin', 'algorithm']","['sql', 'microsoft', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'powerpoint', 'excel', 'aw', 'research', 'healthcar', 'graph', 'predict', 'machine learning', 'statist', 'big data', 'analyt', 'machin', 'program', 'learn', 'set', 'relat', 'predict', 'infrastructur', 'provid', 'python', 'comput', 'statist', 'big', 'integr', 'engin', 'algorithm']"
DE,"This exceptional individual will be a member of a small team of Data Engineers, Data Scientists, and Data Analysts who play a vital role in ensuring the smooth day-to-day implementation of a large research infrastructure, and the live production trading of billions of dollars of capital across global capital markets, including equities, futures, options and other financial instruments.
Job Responsibilities
Building processes and technology tools to deliver data to and support data for discretionary portfolio managers and other teams.
Data onboarding project management for discretionary portfolio managers.
Monitoring and enhancing the automated data collection and cleansing infrastructure.
Researching new technologies for improved data management and efficient data retrieval.
Desirable Candidates
Ph.D. or Masters in computer science, mathematics, physics, statistics, or other discipline involving rigorous fundamental and/or quantitative analysis techniques.
Ideal candidate will have at least 1 year of experience as an Analyst for a discretionary portfolio manager or in a similar role.
Experience working with large data sets, including classification, regression, and distribution analysis.
Experience applying statistical tests to large data sets.
Programming skills in SQL, TSQL, SQL Server, or PL-SQL.
Programming skills in Python and at least one of C#, C++, or Java.
Web/GUI development experience using Microsoft technologies is a plus.
Experience dealing with intraday, tick and order book data is a plus.
Strong problem solving skills.
Intellectual curiosity and a love of learning.
Attention to detail and a love of process.
Strong oral and written communication skills.
","['sql', 'java', 'python', 'c', 'microsoft']","['research', 'classif', 'regress', 'problem solving', 'statist', 'commun']",2,"['sql', 'java', 'python', 'c', 'microsoft', 'research', 'classif', 'regress', 'problem solving', 'statist', 'commun']","['day', 'program', 'techniqu', 'engin', 'set', 'infrastructur', 'python', 'scientist', 'statist', 'comput', 'quantit', 'collect']","['sql', 'java', 'python', 'c', 'microsoft', 'research', 'classif', 'regress', 'problem solving', 'statist', 'commun', 'day', 'program', 'techniqu', 'engin', 'set', 'infrastructur', 'python', 'scientist', 'statist', 'comput', 'quantit', 'collect']"
DE,"Data Engineers operate within a distributed, agile, cross-functional squad.
The data squad has an organization-wide impact by providing the data to inform the user experience, product, editorial, growth, and financial decisions at Wirecutter.
Python and Apache Airflow for ETL pipelines
PostgreSQL database and S3 data lake in AWS RDS
BigQuery analytics data
Looker BI tool
In this role, you will:
Help drive the optimization, testing, and tooling to improve data quality.
Write and maintain database design and architecture documentation.
Collaborate with your squad leaders and stakeholders on the scoping, planning, prioritization, successful execution, and rollout of complex technical projects to provide the foundation for generating insights and address additional data for reporting needs.
Provide insight into changing database storage and utilization requirements.
Share knowledge and problem solving with other members of your squad and the engineering team.
Contribute to engineering initiatives and culture as a member of Wirecutter’s engineering team.
About You:
You have 3+ years in software or data engineering and scaling large data sets.
You can design & optimize queries, data sets, and data pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
You understand the challenges of reliable data replication, optimizing for a data warehouse, and maintaining the integrity of a data lake.
You have experience reliably integrating and handling data from multiple APIs.
You have experience building ETLs at scale on any major cloud provider (AWS, GCP: Cloud Composer, Kubernetes, etc.)
You are thoughtful, clear, and persuasive in writing and in person.
You have strong problem-solving skills and critical thinking abilities.
You have experience listening to analysts and other business users, and can translate their needs into actionable tasks.
You are excited to play a pivotal role in Wirecutter’s mission, innovation, and growth.
You are passionate and enthusiastic about what you do.
You have experience with version control, shell scripting, the Unix filesystem, and automating deployments.
Ideally, you have production experience with Python and Apache Airflow.
Ideally, you have experience with BI tools and managing data sets for BI tools.
Ideally, you have a basic understanding of statistics and sampling.
Ideally, you have experience working with Google Tag Manager and analytics data sets
Ideally, you’ve worked as a member of a distributed team.
About Wirecutter:
Founded five years ago by journalists fed up with the time and energy it takes to shop, Wirecutter developed a simpler approach to giving buying advice: just tell people exactly what to get in one single guide.
Through rigorous testing, research, reporting, and whatever means necessary, they create straightforward recommendations that save readers from unnecessary stress, time, and effort.
Locations:
Wirecutter’s main office is located in Long Island City, Queens, NY.
#LI-AM1
Achieving true diversity and inclusion is the right thing to do.
All applications will receive consideration for employment without regard to legally protected characteristics.
","['gcp', 'airflow', 'looker', 'bigqueri', 'unix', 'bi', 'aw', 'python', 's3', 'cloud', 'postgresql', 'kubernet']","['recommend', 'research', 'pipelin', 'optim', 'problem solving', 'statist', 'etl']",999,"['gcp', 'airflow', 'looker', 'bigqueri', 'unix', 'powerbi', 'aw', 'python', 's3', 'cloud', 'postgresql', 'kubernet', 'recommend', 'research', 'pipelin', 'optim', 'problem solving', 'statist', 'etl']","['analyt', 'engin', 'challeng', 'pipelin', 'set', 'divers', 'provid', 'optim', 'python', 'bi', 'statist', 'aw', 'action', 'warehous', 'etl', 'collect', 'integr']","['gcp', 'airflow', 'looker', 'bigqueri', 'unix', 'powerbi', 'aw', 'python', 's3', 'cloud', 'postgresql', 'kubernet', 'recommend', 'research', 'pipelin', 'optim', 'problem solving', 'statist', 'etl', 'analyt', 'engin', 'challeng', 'pipelin', 'set', 'divers', 'provid', 'optim', 'python', 'bi', 'statist', 'aw', 'action', 'warehous', 'etl', 'collect', 'integr']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"* Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data.
* Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.
* Design best practices for big data processing, data modeling and warehouse development throughout the company.Requirements* Familiar with at least one of the programming languages: Python, Java.
* Comfortable with Linux operating system and command line tools such as Bash.
* Familiar with REST for accessing cloud based services.
* Excellent knowledge about databases, such as PostgreSQL and Redshift.
* Has experiences with GIT, Github, JIRA and SCRUM.
* 2+ years in building a data warehouse and data pipelines.
Or, 3+ years in data intensive engineering roles.
* Experience with big data architectures and data modeling to efficiently process large volumes of data.
* Background in ETL and data processing, know how to transform data to meet business goals.
* Experience developing large data processing pipelines on Apache Spark.
* Experience with Python or Java programming languages.
* Strong understanding of SQL and working knowledge of using SQL(prefer PostgreSQL and Redshift) for various reporting and transformation needs.
* Excellent communication, adaptability and collaboration skills.
* Experience running Agile methodology and applying Agile to data engineering.
* Experience with Java, JDBC, AWS, SDKNice to have* Familiar with AWS ecosystem, including RDS, Glue, Athena, etc.
","['sql', 'linux', 'spark', 'jira', 'git', 'aw', 'cloud', 'python', 'redshift', 'java', 'postgresql', 'github', 'excel']","['pipelin', 'data modeling', 'big data', 'etl', 'commun']",999,"['sql', 'linux', 'spark', 'jira', 'git', 'aw', 'cloud', 'python', 'redshift', 'java', 'postgresql', 'github', 'excel', 'pipelin', 'data modeling', 'big data', 'etl', 'commun']","['program', 'spark', 'pipelin', 'line', 'aw', 'python', 'warehous', 'big', 'etl', 'engin']","['sql', 'linux', 'spark', 'jira', 'git', 'aw', 'cloud', 'python', 'redshift', 'java', 'postgresql', 'github', 'excel', 'pipelin', 'data modeling', 'big data', 'etl', 'commun', 'program', 'spark', 'pipelin', 'line', 'aw', 'python', 'warehous', 'big', 'etl', 'engin']"
DE,"Data profiling - Create physical Database design Big Data Implementing Change data capture Python Development Spark, Scala AWS
","['scala', 'aw', 'python', 'spark']",['big data'],999,"['scala', 'aw', 'python', 'spark', 'big data']","['big', 'aw', 'python', 'spark']","['scala', 'aw', 'python', 'spark', 'big data', 'big', 'aw', 'python', 'spark']"
DE,"Requisition no: 503035
Work type: Full Time
School/Department: Biomedical Informatics
Grade: Grade 105
Categories: Information Technology, Research (Lab and Non-Lab)
Position Summary
The ideal candidate will have experience with data pipelines and cloud environments.
The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture.
The ideal candidate will have professional experience in a number of programming languages, databases, and development environments.
The candidate should be able to contribute to improving the reliability and quality of data.
Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred.
The successful candidate will contribute to the development of open source solutions together with a community of international researchers.
Current available position is grant-funded.
Responsibilities
Software and system design, implementation, and testing (75%)
Application deployment and configuration (10%)
Communicate with technical individuals at various grant sites (10%)
Software requirements specification (5%)
Minimum Qualifications
Bachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.
Other Requirements
Great communication skills; Experience with one or more compiled programming languages (e.g.
Java, Scala, C#, C++, etc.)
and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)
Working knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.
Equal Opportunity Employer / Disability / Veteran
Applications open: Sep 12 2019 Eastern Daylight Time Applications close:
","['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql']","['research', 'pipelin', 'visual', 'big data', 'information technology', 'etl', 'commun']",1,"['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql', 'research', 'pipelin', 'visual', 'big data', 'information technology', 'etl', 'commun']","['big', 'program', 'pipelin', 'relat', 'visual', 'python', 'avail', 'interpret', 'comput', 'clinic', 'etl', 'sourc', 'school']","['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql', 'research', 'pipelin', 'visual', 'big data', 'information technology', 'etl', 'commun', 'big', 'program', 'pipelin', 'relat', 'visual', 'python', 'avail', 'interpret', 'comput', 'clinic', 'etl', 'sourc', 'school']"
DE,"Country:
United States
Cities:
Boston, Hartford, New York City, Philadelphia, Providence
Area of expertise:
Analytics
14-time winner of Microsoft Partner of the Year
24,000+ certifications in Microsoft technology
90+ Microsoft partner awards
17 Gold Competencies
3,500 analytics professionals worldwide
1,000 data engineers
Implemented analytics systems for more than 550 clients
400 AI practitioners
300 cognitive service experts
About you:
You design data solutions that enable clients to see the whole picture and provide insightful and accurate analysis that helps to build successful businesses.
About the job:
As a Manager, Data Engineering, you use modern data engineering techniques and Advanced Analytics methods to give your clients the information they need.
You collect, aggregate, store and reconcile data from various sources, helping to design and build data pipelines, streams, reporting tools, data generators and a whole range of tools to provide information and insight.
You know how to read the patterns and trends that influence business outcomes.
Day-to-day, you will:
Give colleagues and clients the tools to find and use data for routine and non-routine analysis
Use your sound eye for business to translate business requirements into technical solutions
Analyze current business practices, processes and procedures to spot future opportunities
Assess client needs to build bespoke data design services
Build the building blocks for transforming enterprise data solutions
Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)
Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools
Implement effective metrics and monitoring
Be comfortable to make your own decisions and guide your colleagues
Travel as needed to various client locations
Your skills and business experience include:
Transforming business needs into technical solutions
Mapping data and analytics
Data profiling, cataloguing and mapping to enable the design and build of technical data flows
Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration
Knowledge of multiple Azure data applications including Azure Databricks
Experience in preparing data for and building pipelines and architecture
You probably have a Bachelors or Master’s degree in a quantitative field such as computer science, applied mathematics, statistics or machine learning – or an equivalent combination of education and experience.
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['microsoft', 'azur']","['pipelin', 'visual', 'machine learning', 'statist', 'analyz']",1,"['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz']","['day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']","['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz', 'day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']"
DE,"As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale.
About the Job
As a data engineer, you will:
Run and support a production enterprise data platform
Design and develop data models
Work with languages like Java, Python, Go, Bash, and SQL
Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub
Develop processes for automating, testing, and deploying your work
About You
You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists.
You are own and shape your technical domain area and move the related business goals forward.
You are eager to resolve upstream data issues at the source instead of applying workarounds.
Benefits and Perks:
Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.
#LI-AM1
Achieving true diversity and inclusion is the right thing to do.
All applications will receive consideration for employment without regard to legally protected characteristics.
","['sql', 'spark', 'airflow', 'bigqueri', 'java', 'python']","['etl', 'pipelin', 'data modeling']",999,"['sql', 'spark', 'airflow', 'bigqueri', 'java', 'python', 'etl', 'pipelin', 'data modeling']","['stream', 'spark', 'pipelin', 'relat', 'divers', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'bigqueri', 'java', 'python', 'etl', 'pipelin', 'data modeling', 'stream', 'spark', 'pipelin', 'relat', 'divers', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin']"
DE,"Posted: Jun 18, 2020
Weekly Hours: 40
Role Number:
200166439
This team is more than a group of engineers — it’s a group of music lovers.
And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love.
Areas of work: macOS/iOS Engineering, Full-Stack Engineering, Front-End Engineering, Back-End Engineering, Quality Engineering, Machine Learning Engineering, Data Science, Data Engineering, Site Reliability Engineering, Commerce Engineering, and Engineering Project Management.
Key Qualifications
Experience in designing, implementing and supporting highly scalable data systems and services in Java and/or Scala
Experience with Hadoop-ecosystem technologies in particular MapReduce, Spark / Spark-SQL / Spark Streaming, Hive, YARN/MR2
Experience building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Experience in data-modeling and data-architecture optimized for big data patterns, ie.
warehousing concepts; efficient storage and query on HDFS; data security and privacy techniques)
Knowledgable about distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores and traditional relational databases is desired
You will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability.
High-throughput and reliability are essential.
This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users!
Education & Experience
Bachelors degree in Computer Science, or equivalent experience.
","['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka']","['pipelin', 'machine learning', 'optim', 'big data', 'cluster']",1,"['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'cluster']","['essenti', 'techniqu', 'stream', 'learn', 'machin', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'comput', 'big', 'relat', 'sourc', 'engin']","['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'cluster', 'essenti', 'techniqu', 'stream', 'learn', 'machin', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'comput', 'big', 'relat', 'sourc', 'engin']"
DE,"Overview:
As a Data Engineer, you are responsible for delivering and maintaining highly available computing platforms and creating data integration services.
You will own the systems that collect and transform data from a number of sources, storing data in highly optimized data warehouses.
You will also maintain ongoing reliability, performance and support of the infrastructure.
This includes monitoring the computing environments and providing solutions based on application needs and anticipated growth.
A successful candidate will combine strong technical skills, a passion for creative problem solving, and an intense curiosity.
Data Warehousing with Snowflake, AWS Aurora, Redshift
AWS for serving infrastructure
Python, JavaScript, and TypeScript
DBT and Luigi for ETLs
Fivetran & Stitch
Segment
Looker
Docker
6 Month Expectations:
Collaborate with team on best practices and overall business strategy
12 Month Expectations:
Deliver data engineering system-including data pipeline and data warehousing components-that is simple, reliant, and performant
Preferred Qualifications:
3 - 5 years of experience working in software development, data engineering, or related STEM fields
3+ years of working experience with various relational databases and data warehousing
Strong programming skills in Python and SQL
Practical experience in best practices for developing data pipelining frameworks
Experience in Linux
Ability to learn autonomously and quickly
Analytical, creative and commercial mindset
Extremely organized and detail-oriented with effective multitasking and prioritization skills
Highly motivated, willing to take ownership of work, drive to solve problems and work effectively under pressure
Excellent written and verbal communication skills, willing to proactively engage other team members in fostering a strong collaborative team-oriented environment
This policy applies to all aspects of employment, including hiring, promotion, demotion, compensation, training, working conditions, transfer, job assignment, benefits, layoff, and termination.
Global Job Applicant Privacy Policy
Last Updated: November 25, 2019
I. APPLICABILITY OF OTHER POLICIES
II.
Reference information and/or information received from background checks if you are offered a job (where applicable), including information provided by third parties such as past employers, educational institutions and references; and
You are entirely free to decide whether or not to provide such information and your application will not be affected either way.
III.
A ""cookie"" is a text file that websites send to a visitor's computer or other internet-connected device to uniquely identify the visitor's browser or to store information or settings in the browser.
A ""web beacon,"" also known as a pixel tag or clear GIF, is used to transmit information back to a web server.
IV.
Such uses include:
Assessment of your skills, qualifications, and suitability for the role;
Communication with you about the recruitment process;
Other uses with your consent, which you may withdraw at any time; and
V. WHO MAY HAVE ACCESS TO YOUR INFORMATION
IV.
DATA RETENTION
VII.
YOUR RIGHTS
You may have certain rights under U.S. and international privacy laws in relation to your personal information.
You may also have the right to object to and restrict certain processing of your data.
Certain information may be exempt from such requests pursuant to applicable data protection laws.
You can contact privacy@glossier.com to exercise your rights in relation to your personal information.
VIII.
CALIFORNIA RESIDENTS
IX.
EUROPEAN RESIDENTS
As necessary to evaluate and potentially enter into an employment relationship with you;
With your consent, which you may withdraw at any time;
Where necessary to protect your vital interests or those of others; and
Where the collection or processing of personal information is based on your consent, you may withdraw your consent at any time to the extent permitted by applicable law.
X.
INTERNATIONAL DATA TRANSFERS
You may request additional information concerning such safeguards from the Privacy team by contacting privacy@glossier.com.
XI.
SECURITY
XII.
DATA CONTROLLER
XIII.
Phase EU Limited
233 Spring Street 5 New Street Square
East 10th Floor London EC 4A 3TW
New York, NY 10012 United Kingdom
United States
Attn: Legal Attn: Legal
privacy@glossier.com priv
","['sql', 'linux', 'looker', 'javascript', 'snowflak', 'python', 'redshift', 'aw', 'excel', 'docker']","['pipelin', 'segment', 'optim', 'data warehousing', 'problem solving', 'etl', 'commun']",999,"['sql', 'linux', 'looker', 'javascript', 'snowflak', 'python', 'redshift', 'aw', 'excel', 'docker', 'pipelin', 'segment', 'optim', 'data warehousing', 'problem solving', 'etl', 'commun']","['analyt', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'provid', 'optim', 'python', 'avail', 'parti', 'aw', 'comput', 'warehous', 'etl', 'sourc', 'collect', 'integr']","['sql', 'linux', 'looker', 'javascript', 'snowflak', 'python', 'redshift', 'aw', 'excel', 'docker', 'pipelin', 'segment', 'optim', 'data warehousing', 'problem solving', 'etl', 'commun', 'analyt', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'provid', 'optim', 'python', 'avail', 'parti', 'aw', 'comput', 'warehous', 'etl', 'sourc', 'collect', 'integr']"
DE,"The MI Data – Sourcing function identifies, acquires, and onboards the data required to create MI Data products.
What you’ll do
As a Web Scraping focused Data Engineer, you will be responsible for extracting and ingesting data from websites using web crawling tools.
In this role you will own the creation process of these tools, services, and workflows to improve crawl/ scrape analysis, reports and data management.
You will own the process to identify and rectify any issues with breaks as well as scale scrapes as needed.
What’s required
Experience running large scale web scrapes
Solid Python knowledge
Familiarity with Linux/UNIX, HTTP, HTML, Javascript and Networking
Familiarity with techniques and tools for crawling, extracting and processing data (e.g.
Scrapy, pandas, mapreduce, SQL, BeautifulSoup, etc).
Experience with system monitoring/administration tools
Experience with version control, open source practices, and code review
Experience with applications designed to display archived web content
Great communication skills (written and Spoken in English)
Bachelor's Degree in Computer Science or a related field or the equivalent demonstrated experience
Fully-paid health care benefits
Generous parental and family leave policies
Mental and physical wellness programs
Tuition assistance
A 401(k) savings program with an employer match and more
For more information, visit www.Point72.com/working-here.
","['sql', 'mapreduc', 'linux', 'panda', 'unix', 'javascript', 'python']",['commun'],1,"['sql', 'mapreduc', 'linux', 'panda', 'unix', 'javascript', 'python', 'commun']","['program', 'techniqu', 'python', 'comput', 'relat', 'sourc', 'engin']","['sql', 'mapreduc', 'linux', 'panda', 'unix', 'javascript', 'python', 'commun', 'program', 'techniqu', 'python', 'comput', 'relat', 'sourc', 'engin']"
DE,"* Building and deploying web, app, api, and backend code to dev/qa/staging/production systems.
* Release based branching and Vendor based Branching.
* Tagging, Versioning, and Release Note maintenance.Desired Qualifications* 2-3 years' experience in System Administration, Build/Release Engineering, DevOps required.
* System Admin level experience with Atlassian tools such as Jira, Confluence, Fisheye/Crucible, Bitbucket, Bamboo.
* Experience with Build/Release Engineering and automation using Jenkins/Bamboo, Git, Nexus/Artifactory, Ansible/SaltStack.
* Experience with Docker, Kubernetes, and SaltStack for containerization.
* Knowledge of various programming languages such as Python, NodeJS, and Java.
* Scripting experience (Bash, Python).
* System Admin level experience with both Windows and Linux operating systems.
* Background in Dev-Tool integration, LDAP, Load Balancing is a plus.
* Excellent troubleshooting and analytical skills.
* Ability to work independently and on a team.
","['linux', 'git', 'jira', 'java', 'python', 'kubernet', 'excel', 'docker', 'nodej']",[None],999,"['linux', 'git', 'jira', 'java', 'python', 'kubernet', 'excel', 'docker', 'nodej']","['analyt', 'python', 'releas', 'integr', 'engin']","['linux', 'git', 'jira', 'java', 'python', 'kubernet', 'excel', 'docker', 'nodej', 'analyt', 'python', 'releas', 'integr', 'engin']"
DE,"You will build distributed services and large scale processing systems that will support various teams to work faster and smarter.
Hadoop, Yarn, Spark, MongoDB, Hive
AWS EMR/EC2/Lambda/kinesis/S3/Glue/DynamoDB/API Gateway, Redshift
ElasticSearch, Airflow, and Terraform.
Scala, Python
What you'll do:
Build core components of data platform which will serve various types of consumers including but not limited to data science, engineers, product, qa
Build various data ingestion and transformation job/s as and when they are needed
Build scalable data services to bridge the gap between analytics and application space
Develop an understanding of key product, user, and business questions
3+ years of professional experience working in data engineering
BS / MS in Computer Science, Engineering, Mathematics, or a related field
You have built large-scale data products and understand the tradeoffs made when building these features
You have a deep understanding of system design, data structures, and algorithms
Experience (or a strong interest in) working with Python or Scala
Experience with working with a cluster manager (YARN / Mesos / Kubernetes)
Experience with distributed computing and working with Spark, Hadoop, or MapReduce Framework
Experience working on a cloud platform such as AWS
Experience with ETL in general
Gold stars:
Experience working with Apache Airflow
Experience working with AWS Glue
Experience in Machine Learning and Information Retrieval
_________________________________________
Benefits & Perks:
Flexible Vacation
Family-Friendly Medical, Dental, and Vision Insurance Plans
401k
Learning & Development Stipend
Commuter Benefits and Flexible Spending Account (FSA)
Employee referral bonuses
Stocked fridges & kitchens and catered lunch on Fridays
Thursday happy hours
Team outings that do not involve trust falls...
Awards & Recognition:
Forbes Fintech 50 (2019)
LendIt Fintech Innovator of the Year (2019)
Built in NYC's Best Places to work (2019)
Built in NYC's Startups to Watch (2018)
Wall Street Journal's ""Top 25 Tech Companies To Watch"" (2018)
MarCom Awards Double Gold & Platinum Winner (2018)
Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017)
W3 Awards Winner for Best User Experience (2017)
**No recruiters, please.
","['mongodb', 'mapreduc', 'spark', 'airflow', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'hive', 'cloud', 'kubernet']","['machine learning', 'etl', 'cluster', 'account']",1,"['mongodb', 'mapreduc', 'spark', 'airflow', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'hive', 'cloud', 'kubernet', 'machine learning', 'etl', 'cluster', 'account']","['analyt', 'machin', 'learn', 'spark', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'etl', 'engin', 'algorithm']","['mongodb', 'mapreduc', 'spark', 'airflow', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'hive', 'cloud', 'kubernet', 'machine learning', 'etl', 'cluster', 'account', 'analyt', 'machin', 'learn', 'spark', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'etl', 'engin', 'algorithm']"
DE,"You have extensive experience architecting complex, highly available and optimized real-time data pipelines, know the pros and cons of different tools, and when to build your own vs use an off-the-shelf solution.
What you can expect to work on
Evangelize best data practices among the engineering team, help internal education
BA/BS degree in Computer Science or a related technical field or equivalent practical experience
Knowledge and at least a year of production experience working with streaming platforms such as AWS Kinesis, Kafka, and/or Kafka Connect
Experience working with both MySQL and Microsoft SQL Server databases
Data warehouse design and implementation experience
Good knowledge of at least one scripting language
Excellent communication skills, verbal and written
Experience in a fast paced, agile startup environment
Opportunities to learn and grow personally and professionally
Building mission critical and socially responsible software to enable first responders to better serve their communities
A workplace dedicated to supporting and bettering law enforcement, first responders, and other government agencies via mission critical software products
Working towards a worthwhile mission with a team of friendly and intelligent coworkers
","['sql', 'aw', 'microsoft', 'mysql', 'kafka', 'excel']","['optim', 'commun', 'pipelin']",1,"['sql', 'aw', 'microsoft', 'kafka', 'excel', 'optim', 'commun', 'pipelin']","['pipelin', 'aw', 'optim', 'avail', 'comput', 'warehous', 'relat', 'engin']","['sql', 'aw', 'microsoft', 'kafka', 'excel', 'optim', 'commun', 'pipelin', 'pipelin', 'aw', 'optim', 'avail', 'comput', 'warehous', 'relat', 'engin']"
DE,"As a Data Engineer, you will
Collaborate with operational teams including sales, marketing, and customer success.
Write SQL queries and reusable views that enable various analyses including funnel, retention, and performance reporting.
Build rules-based models and statistical machine learning models in Python using packages like scikit-learn.
You'd be a good fit if
You are fluent in SQL and Python.
You have experience building and using data infrastructure, including systems like Postgres and Redshift.
You've used reporting tools like Metabase, Tableau, or Looker in the past.
You know that no dataset is ever pristine, but love to interrogate, structure, and clean data.
You've contributed to extract-transform-load pipelines to collect data from disparate sources and centralize them in a data warehouse.
You feel comfortable managing your time and deciding amongst competing priorities.
You have worked with non-engineering teams and are comfortable explaining technical solutions to them.
You are passionate about the future of work.
You enjoy learning and teaching.
You have strong written and verbal communication skills in English.
Don't fear
Some candidates may see this list and feel discouraged because they don't match all the items.
Please provide:
A pointer to your CV, resume, LinkedIn profile, or any other summary of your career so far.
Some informal text introducing yourself and what you are excited about.
If you have a profile on websites like GitHub or other repositories of open source software, you can provide that as well.
","['sql', 'github', 'looker', 'tableau', 'python', 'redshift', 'postgr', 'scikit']","['pipelin', 'machine learning', 'clean', 'statist', 'commun']",999,"['sql', 'github', 'looker', 'tableau', 'python', 'redshift', 'scikit', 'pipelin', 'machine learning', 'clean', 'statist', 'commun']","['machin', 'learn', 'pipelin', 'infrastructur', 'python', 'packag', 'statist', 'warehous', 'sourc', 'engin']","['sql', 'github', 'looker', 'tableau', 'python', 'redshift', 'scikit', 'pipelin', 'machine learning', 'clean', 'statist', 'commun', 'machin', 'learn', 'pipelin', 'infrastructur', 'python', 'packag', 'statist', 'warehous', 'sourc', 'engin']"
DE,"You will collaborate with a team of engineers, data specialists, and business analysts to build scalable and performant data pipelines.
You've turned your passion for all things data (file formats, storage, databases, and DAGs) into your calling by combining it with deep computer science fundamentals.
You will architect data pipelines that ingest, process, store, cleanse, transform, and route massive amounts of financial data to support multiple business initiatives.
Data Engineer Characteristics:
You have ~5 years of data engineering experience focused on delivering highly scalable data pipelines.
Your code has handled the full ETL process used for large scale analytics across huge datasets.
Your pipelines have ingested data from multiple data sources (proprietary and vendor datasets).
You have a strong command over object-oriented design patterns, data structures, and algorithms.
You communicate technical ideas with ease and always look to collaborate to deliver high quality products.
You grasp product specifications and effectively map them to technical requirements.
You thoughtfully and successfully incorporate these requirements into your system design and implementation.
You are a collaborator by nature who works effectively with product teams to understand the scope, cost, and requirements of new product feature development.
DataOps Team Stack: Python, PostgreSQL , Snowflake, Airflow, Redis, Celery/Dask, Kubernetes, Apache Arrow, Pandas, Sklearn, Kafka, Docker
The opportunity to join a small and growing team of good people, where you can make a difference
A new, high-quality code base with little technical debt and room to build new services and features
An environment that embraces the utility of a DevOps oriented culture and combines it with a focus on CI/CD methodology
A meritocratic philosophy that champions collaboration
Competitive compensation, benefits, and perks
","['airflow', 'dask', 'panda', 'sklearn', 'snowflak', 'python', 'postgresql', 'kubernet', 'kafka', 'docker']","['cleans', 'etl', 'pipelin']",999,"['airflow', 'dask', 'panda', 'sklearn', 'snowflak', 'python', 'postgresql', 'kubernet', 'kafka', 'docker', 'cleans', 'etl', 'pipelin']","['analyt', 'pipelin', 'handl', 'amount', 'python', 'comput', 'etl', 'sourc', 'engin']","['airflow', 'dask', 'panda', 'sklearn', 'snowflak', 'python', 'postgresql', 'kubernet', 'kafka', 'docker', 'cleans', 'etl', 'pipelin', 'analyt', 'pipelin', 'handl', 'amount', 'python', 'comput', 'etl', 'sourc', 'engin']"
DE,"Data Engineer
New York (Headquarters)
Smart access isnt about locking doors, its about opening up new possibilities.
Latch is the worlds first fully integrated hardware and software system dedicated to bringing seamless access to every door in a modern building.
Responsibilities
Understand the data gathered across the entire Latch organization
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Transform raw data into meaningful sets that are query-able and visualizable.
Work closely with Data Analysts and Data Scientists to implement production ready systems
Be a helping hand with tools used by other teams such as Sales CRMs, Ops Customer Success tools, Marketing automation or Finance ERP.
Requirements
BS in Computer Science, Math, related technical field or equivalent practical experience
3+ years of general software programming experience in Java or similar languages
Excellent grasp of data structures and algorithms
Solid level of understanding in SQL
Knowledge of database technology, schema design, and query optimization techniques
Experience in ETL pipelines and data transformations.
Excellent communication skills
Preferred Qualifications
MS in Computer Science, Mathematics, or related technical field
Experience with Map-Reduce technologies such as Spark or Hadoop.
Understanding of basic data science concepts
Experiencing in productionizing machine learning models.
Acute sense of data analysis: being able to make sense out of many seemingly unrelated data sets.
Founded in 2014, Latch is a venture-backed, high-growth organization that's on a mission to change the way people open, manage, and share their spaces.
","['sql', 'erp', 'spark', 'hadoop', 'java', 'excel']","['pipelin', 'hardwar', 'machine learning', 'optim', 'financ', 'etl', 'commun', 'math']",1,"['sql', 'erp', 'spark', 'hadoop', 'java', 'excel', 'pipelin', 'hardwar', 'machine learning', 'optim', 'financ', 'etl', 'commun', 'math']","['machin', 'techniqu', 'learn', 'spark', 'pipelin', 'set', 'relat', 'integr', 'hadoop', 'optim', 'comput', 'scientist', 'etl', 'engin', 'algorithm']","['sql', 'erp', 'spark', 'hadoop', 'java', 'excel', 'pipelin', 'hardwar', 'machine learning', 'optim', 'financ', 'etl', 'commun', 'math', 'machin', 'techniqu', 'learn', 'spark', 'pipelin', 'set', 'relat', 'integr', 'hadoop', 'optim', 'comput', 'scientist', 'etl', 'engin', 'algorithm']"
DE,"The role requires both a broad knowledge of existing data modeling and processing along with the creativity to invent and customize when necessary using programming and technology platforms.
You will work with data scientists, engineers & product managers hand in hand to build insightful and efficient reporting solutions & data analysis.
What you'll do
Work with product, engineering & business teams to deliver complex data analysis requests
Visualize datasets across multiple databases & warehouses using tools such as Tableau, D3, Looker, etc.
Build financial models & growth projections for new products and business initiatives
Build ETL pipelines for regular reporting on business and operational KPIs
Help business understand key trends by executing complex analysis via Tableau or ad-hoc SQL queries
Coordinate within cross-functional teams such as engineering, product, marketing, customer experience for various data analysis needs
Proactively build data and event-driven dashboard for real-time business operations and consumer insights
What you'll bring
Bachelors in CS, Statistics, Economics or Engineering, Masters preferred
3+ years of hands-on SQL experience
2+ years of experience in using data visualization tools such as Tableau, Looker, PowerBI
2+ years of experience in building financial models, growth projections & ETL data pipelines
experience either in R or Python and working with data warehousing solution such as AWS Redshift or Google BigQuery
What you'll get
Comprehensive Healthcare, Dental, and Vision
Generous 401(k) Matching
Stock options
Unlimited PTO
Pre-Tax Flexible healthcare spending account (FSA), Dependent Care FSA and Commuter Benefits
Paid Family Leave
Stocked fridges, coffee, soda, and lots of treats
","['sql', 'looker', 'bigqueri', 'd3', 'aw', 'tableau', 'python', 'redshift', 'r', 'powerbi']","['healthcar', 'pipelin', 'kpi', 'visual', 'data modeling', 'data warehousing', 'statist', 'etl', 'econom', 'account']",1,"['sql', 'looker', 'bigqueri', 'd3', 'aw', 'tableau', 'python', 'redshift', 'r', 'powerbi', 'healthcar', 'pipelin', 'kpi', 'visual', 'data modeling', 'data warehousing', 'statist', 'etl', 'econom', 'account']","['program', 'pipelin', 'visual', 'aw', 'python', 'scientist', 'statist', 'etl', 'engin']","['sql', 'looker', 'bigqueri', 'd3', 'aw', 'tableau', 'python', 'redshift', 'r', 'powerbi', 'healthcar', 'pipelin', 'kpi', 'visual', 'data modeling', 'data warehousing', 'statist', 'etl', 'econom', 'account', 'program', 'pipelin', 'visual', 'aw', 'python', 'scientist', 'statist', 'etl', 'engin']"
DE,"SunIRef:it
This is an opportunity to design and maintain a robust, scalable and sustainable enterprise data platform with other members of the Business Intelligence team.
The ideal candidate will enjoy working on a variety of projects at once, with lots of different software services and data sources, all orchestrated with Python code.
You will help define data access and discoverability requirements and work to create infrastructure and services that provide access to event streams.
What you'll do:
Work closely with other BI engineers, BI analysts, and business stakeholders to understand and plan technical requirements for BI projects
Provide decision analysis and decision support to business stakeholders using BI and other data
Contribute software designs, code, tooling, testing, and operational support to a multi-terabyte BI data platform
Collaborative work: iterative development, design and code review sessions
Skills and knowledge you should possess:
3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Proficiency in SQL
Proficiency in Python
Proficiency with modern source control systems, especially Git
Experience working with non-technical business stakeholders on technical projects
Bonus Points (Nice Skills to Have, but Not Needed):
Airflow, Celery, or other Python-based task processing systems
Cloud-based devops: AWS or Google Cloud Platform
Relational database design
Kafka, Kinesis, PubSub and other durable, scalable messaging systems
Spark
Pandas or R
Redshift, Vertica, Snowflake, CitusDB, or other distributed columnar-store databases
Experience with Mixpanel, , Snowplow, or similar event tracking systems
Experience with any distributed map/reduce framework, in Hadoop or otherwise
Basic Linux/Unix system administration skills
Learn more at .
","['sql', 'google cloud', 'linux', 'spark', 'airflow', 'panda', 'unix', 'git', 'hadoop', 'bi', 'snowflak', 'python', 'redshift', 'aw', 'r', 'cloud', 'kafka']",['etl'],999,"['sql', 'google cloud', 'linux', 'spark', 'airflow', 'panda', 'unix', 'git', 'hadoop', 'powerbi', 'snowflak', 'python', 'redshift', 'aw', 'r', 'cloud', 'kafka', 'etl']","['learn', 'spark', 'relat', 'hadoop', 'infrastructur', 'provid', 'bi', 'python', 'aw', 'etl', 'sourc', 'engin']","['sql', 'google cloud', 'linux', 'spark', 'airflow', 'panda', 'unix', 'git', 'hadoop', 'powerbi', 'snowflak', 'python', 'redshift', 'aw', 'r', 'cloud', 'kafka', 'etl', 'learn', 'spark', 'relat', 'hadoop', 'infrastructur', 'provid', 'bi', 'python', 'aw', 'etl', 'sourc', 'engin']"
DE,"You will join a fast-growing startup which offers exciting opportunities to those that are willing to put in the work.
This is a salaried position with full benefits.
Visa sponsorship is offered.
Qualifications
Passionate about Web Scraping
Expertise in Python
Experience with Selenium, BeautifulSoup
Experience with HTML, Javascript, CSS
Understanding of the DOM, ORMs
Additional Information
All your information will be kept confidential according to EEO guidelines.
Perks and Benefits
Dental & health coverage
Fully stocked kitchen, weekly team breakfast outings
Transitcheck
","['javascript', 'python']",[None],999,"['javascript', 'python']",['python'],"['javascript', 'python', 'python']"
DE,"The AWS Well-Architected Tool team is hiring Data Engineers!!
Imagine if you could help shape the future of architecture, and go on a journey where few have tread before.
AWS is one of Amazons fastest growing businesses.
More than a million active customers, from Airbnb to SAP, use AWS Cloud solutions to deliver flexibility, scalability, and reliability.
· Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
· Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
· Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
· Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency
· Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
· Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Basic Qualifications
· 2+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
· Demonstrated strength in data modeling, ETL development, and data warehousing
· Experience using big data technologies (Hadoop, Hive, Hbase, Spark etc.)
· Knowledge of data management fundamentals and data storage principles
· Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos etc.)
Preferred Qualifications
· Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
· Experience working with AWS big data technologies (Redshift, S3, EMR)
· Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
· Experience working with distributed systems as it pertains to data storage and computing
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
","['sql', 'spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'cloud']","['research', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'big data', 'etl']",1,"['sql', 'spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'cloud', 'research', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'big data', 'etl']","['analyt', 'spark', 'pipelin', 'set', 'relat', 'visual', 'power', 'hadoop', 'infrastructur', 'aw', 'avail', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'cloud', 'research', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'big data', 'etl', 'analyt', 'spark', 'pipelin', 'set', 'relat', 'visual', 'power', 'hadoop', 'infrastructur', 'aw', 'avail', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"multi-asset automated stat arb trading.
a complex suite of research, data, and trading systems.
build tools, automated testing, and deployment setup.
You'll also
lead
You'll evaluate and implement new tools
level.
platform, developer experience, test environment, and config
management.
This is a front office role.
You will work directly within an
extremely experienced team of about twenty
quants, engineers, and portfolio managers.
and hands-on builders.
to manage and improve complex
software systems.
This is an ideal role for an experienced developer interested in
quant finance, for a financial developer
looking for systems and infrastructure responsibilities, or a
strong operational engineer looking to
transition to software and infrastructure engineering.
Qualifications
all of them.
Python development skills, not just scripting.
Familiarity with Python tools and data science ecosystem.
Experience with test and CI tools (any).
Solid understanding of git and git workflows.
Basic familiarity with Linux, storage, and networking.
If you are not experienced with Python but have demonstrated
are open to discussion.
be able to acquire them as needed.
Experience on AWS and/or GCP.
Experience with Ansible, Puppet, Chef, or other config
management systems.
Knowledge of testing best practice.
Software and infrastructure security skills.
Experience with relational and/or NoSQL databases.
Familiarity with Docker or another container technolog
","['linux', 'gcp', 'git', 'aw', 'python', 'nosql', 'docker']","['research', 'financ']",999,"['linux', 'gcp', 'git', 'aw', 'python', 'nosql', 'docker', 'research', 'financ']","['asset', 'infrastructur', 'aw', 'python', 'relat', 'engin', 'evalu']","['linux', 'gcp', 'git', 'aw', 'python', 'nosql', 'docker', 'research', 'financ', 'asset', 'infrastructur', 'aw', 'python', 'relat', 'engin', 'evalu']"
DE,"Senior Data Engineer
Master’s degree in Information Technology, Computer Science with 2 years experience.
Build & maintain Analytical Data Platforms using appropriate SQL, NoSQL and NewSQL technologies like MapR, Spark, Hadoop, & Python.
Lead engineering processes to ensure data quality & meta data documentation using tools like Python, Amazon RedShift, Tableau & Confluence.
Perform quantitative analysis of customer data using tools like SaaS & machine learning.
Monitor tag transactions to correctly reward consumers based on merchant reward program offerings.
Skills: SQL, NoSQL, NewSQL, MapR, Spark, Hadoop, Python, Amazon RedShift, Tableau, Confluence, SaaS & machine learning.
","['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql']","['machine learning', 'information technology']",2,"['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql', 'machine learning', 'information technology']","['analyt', 'machin', 'program', 'learn', 'spark', 'hadoop', 'python', 'comput', 'quantit', 'engin']","['sql', 'spark', 'hadoop', 'tableau', 'python', 'redshift', 'nosql', 'machine learning', 'information technology', 'analyt', 'machin', 'program', 'learn', 'spark', 'hadoop', 'python', 'comput', 'quantit', 'engin']"
DE,"Responsibilities:
Driving innovation through product and platform development
Helping to facilitate bespoke custom basket trades for clients in a scalable infrastructure
Developing infrastructure and tools to administer basket rebalances for external clients and internal trading teams
Automation of corporate action adjustments and improvement of work-flow
Providing metrics for basket trades, to drive sales and trading decisions and to grow the business
Both independent and collaborative work, involving several sales/strat/trading teams globally
Requirements:
Expertise in Python
Strong SQL skills
Web scraping experience
Experience with Linux and Windows platform
Strong communication skills, both written and verbal
Exposure to non-relational databases
Exposure to web UI technologies
Data Warehousing and Modeling expertise
Financial knowledge
If you would like to be considered for the position of Data Engineer or wish to discuss the role further then please leave your details below.
Thank you
","['sql', 'linux', 'python']","['data warehousing', 'commun']",999,"['sql', 'linux', 'python', 'data warehousing', 'commun']","['corpor', 'infrastructur', 'python', 'action', 'relat', 'engin']","['sql', 'linux', 'python', 'data warehousing', 'commun', 'corpor', 'infrastructur', 'python', 'action', 'relat', 'engin']"
DE,"This is an amazing opportunity for someone looking to join in the early stages of a rapidly growing technology startup solving a meaningful problem in food and the broader world of personalization in nutrition and consumer products.
What you will be doing
You’ll make key decisions on the architecture and implementation of scalable data processing and analytics structure
You’ll help build data processes and pipelines, and craft the tools to make them efficient
You’ll use your skills to help solve real-word problems, while learning about the food, nutrition, and consumer product goods (CPG) ecosystem
Qualifications
4+ years of software engineering and/or Big Data Engineering experience
Degree in Computer Science, Computer Engineering, or Statistics (or commensurate experience)
Experience working with large, complex data sets from a variety of sources
Proven track record of building and shipping large-scale engineering products
3+ years experience with Node.js and the related ecosystem
Experience with MongoDB, GraphQL, Redis, REST APIs in general
Ability to collaborate cross-team with engineers, data analysts, product managers and business analysts
You are a strong communicator.
Explaining complex technical concepts to designers, support, and other engineers is no problem for you.
Self-awareness and a desire to continually learn and improve
Bonus points for
An open GitHub profile to showcase some of your work (or code samples)
Have built a deployment pipeline
Have built a full stack application
Experience with or strong interest in data science and/or machine learning
Additional Information
Current retail partners include Whole Foods, Kroger, and more.
See some examples searches:
-- Vegan meat substitutes: https://bit.ly/2m14P0P
-- Paleo snack bars: https://bit.ly/2lCOcYS
-- Gluten-free cereals that are also kosher: https://bit.ly/2m1510
All your information will be kept confidential according to EEO guidelines.
","['mongodb', 'github']","['pipelin', 'machine learning', 'statist', 'big data', 'commun']",1,"['mongodb', 'github', 'pipelin', 'machine learning', 'statist', 'big data', 'commun']","['analyt', 'machin', 'pipelin', 'relat', 'comput', 'statist', 'big', 'set', 'sourc', 'engin']","['mongodb', 'github', 'pipelin', 'machine learning', 'statist', 'big data', 'commun', 'analyt', 'machin', 'pipelin', 'relat', 'comput', 'statist', 'big', 'set', 'sourc', 'engin']"
DE,"Data Engineer
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
In this role you will build and maintain an efficient data pipeline architecture, while assembling large, complex data sets that meet functional / non-functional business requirements.
You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Principal Responsibilities
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Collaborate with partners from both the business and technology to assist with data-related technical issues and support their data infrastructure needs.
Qualifications/Skills Required
5 + years of experience with:
Object-oriented/object-function languages: Python, Java, Scala, etc.
Hadoop, Spark, Presto, Kafka, etc.
Relational SQL and NoSQL databases, including SQL Server, MySQL and Cassandra.
Data pipeline and workflow management tools: Airflow, Luigi, etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
Savvy with data science stack (Pandas, NumPy, SciPy)
Data Science/Analysis background; Proficient at working with large datasets
Unix/Linux command-line experience
Experience working with AWS, GCP or Azure
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
","['sql', 'linux', 'gcp', 'python', 'redshift', 'java', 'nosql', 'scipi', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'scala', 'panda', 'ec2', 'aw', 'mysql', 'spark', 'airflow', 'numpi', 'cloud']","['optim', 'pipelin', 'big data']",999,"['sql', 'linux', 'gcp', 'python', 'redshift', 'java', 'nosql', 'scipi', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'scala', 'panda', 'ec2', 'aw', 'spark', 'airflow', 'numpi', 'cloud', 'optim', 'pipelin', 'big data']","['analyt', 'spark', 'pipelin', 'azur', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'linux', 'gcp', 'python', 'redshift', 'java', 'nosql', 'scipi', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'scala', 'panda', 'ec2', 'aw', 'spark', 'airflow', 'numpi', 'cloud', 'optim', 'pipelin', 'big data', 'analyt', 'spark', 'pipelin', 'azur', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"What's in it for you?
Who are you?
An autonomous, creative engineer who loves technology, is endlessly curious, aggressively inquisitive, and an advocate for best practices
You're a systems thinker who can see the big picture and understand how to break a larger problem down into smaller pieces that can be solved independently
You can build a network and find new opportunities for improvement
Evangelize technical solutions and innovations among teams for further application
Leverage your development experience to build systems that are complete and robust enough to weave the data between automation and human expertise
Help build systems that utilize Machine Learning techniques to learn from human SME knowledge and actions
You'll need to have:
4+ years experience programming and scripting in Python within a production environment
2+ years experience with data modeling within SQL and NoSQL databases
2+ years experience working with restful APIs
Deep understanding of large-scale, distributed systems with an ability to understand large systems with minimal documentation
Legal authorization to work full-time in the United States and will not require visa sponsorship
Systems thinkers an interest or a natural preference to think about system architecture and how things work together
","['sql', 'python', 'nosql']","['machine learning', 'data modeling']",999,"['sql', 'python', 'nosql', 'machine learning', 'data modeling']","['machin', 'program', 'techniqu', 'learn', 'human', 'python', 'action', 'big', 'engin']","['sql', 'python', 'nosql', 'machine learning', 'data modeling', 'machin', 'program', 'techniqu', 'learn', 'human', 'python', 'action', 'big', 'engin']"
DE,"Work from Home (East Coast) or NYC
As the creator of the world's first electronic stock market, its technology powers more than 100 marketplaces in 50 countries, and 1 in 10 of the world's securities transactions.
To learn more, visit: http://www.nasdaq.com
The Team
The financial risk management team utilizes automated processes to manage workflow, risk monitoring, risk evaluation, and risk alerting functions.
Business Unit
Your role and responsibilities
You will be managing the data management platform for counterparty risk, which consists of an automated system of data flows with external and internal data source transformations which occur on a daily basis, and form the backbone of the team.
You will need the following skills and experience
Data integration software experience such as Informatica, Talend, or Adeptia
Extensive database experience, especially MS SQL
Project management
Good communication skills and time management
And it would be great if you have experience with
Big data platforms such as Apache Spark
Unstructured and specialty databases such as MongoDB & InfluxDB.
AWS resources for data management such as Redshift & Glue.
Business intelligence software such as Tableau, Power BI, or Grafana.
Sounds like you?
","['mongodb', 'sql', 'spark', 'bi', 'aw', 'redshift', 'tableau', 'power bi']","['commun', 'risk', 'big data']",999,"['mongodb', 'sql', 'spark', 'powerbi', 'aw', 'redshift', 'tableau', 'commun', 'risk', 'big data']","['basi', 'spark', 'power', 'bi', 'aw', 'big', 'integr', 'sourc', 'evalu']","['mongodb', 'sql', 'spark', 'powerbi', 'aw', 'redshift', 'tableau', 'commun', 'risk', 'big data', 'basi', 'spark', 'power', 'bi', 'aw', 'big', 'integr', 'sourc', 'evalu']"
DE,"The IT department services over 10 internal groups and the Data Engineer will be a seasoned Technologist comfortable with a variety of data technologies.
Data Engineering Group handles a Datawarehouse that sources data out of over 15 sources and services over 10 internal groups.
The current data technology stack consists primarily of Data warehouses using SQL Technologies, Neo4J based Graph DBs along with the use of a variety of NoSQL and AI/ML frameworks and languages like R, Python etc.
Assists in the ongoing support of applications.
The individual needs to have a consistent track record of successfully delivering value for their organization in a fast-paced environment along with successful management of customer expectations.
A passionate engineer who strives for automation would be ideal for this position.
You will coordinate with external and internal resources to be a part of a Data Engineering practice.
You must be comfortable switching between multiple projects, contributing as an individual and working with both business teams and technology teams to translate business requirements into finished products.
You possess strategic vision and tactical mastery and combine it with an entrepreneurial spirit to get it done.
This position reports into the Head of Data Engineering, ITMajor Responsibility:* Understands business needs and develop solutions that delight consumers and customers* Understands Agile artifacts and develops applications based upon business priority.
* Collaborate with project partners to ensure all requirements are met.
* Handles relationships with end-user communities.
Interacts regularly with users to gather feedback, listen to their issues and concerns, recommend solutions.
* Provide insights during application design and development for highly complex or critical machine learning projects across numerous lines of business and shared technology.
* Ensure alignment to enterprise architecture and usage of enterprise platforms when delivering projects* Continuously improve the quality of deliverables and SDLC processesRequired Skills/Knowledge:* Master's Degree in Computer Science, Engineering, or Management of Info Systems/Technology preferred* Advanced Education in Statistics or Mathematics would be a plus 3+ year of experience in developing ETL and ELT pipelines using SQL and MSFT SSIS 3+ years of experience in developing BigData and/or machine learning solutions 3+ years of experience in a highly regulated industry* 1+ Years of experience defining and/or designing data architectures* 1+ Years of experience leading and/or managing product engineering teams* Experience with the MS Cloud stack (Azure) or AWS* Experience with SQL, NoSQL, BigData and Graph Technologies along with Programming languages like R, Python, Kafka, Storm etc.
* Experience building microservices* Background in agile SW development and Scaled Agile Frameworks* A true believer in measuring success based on working software and in quick prototyping* Someone who is a passionate coder and can spin up a snippet of code quickly* Strategic thinker with the ability to build and execute innovative digital product, combined with tactical ability to execute simultaneously against multiple contending priorities* Someone with an iterative approach, drive to move fast and think big* Experience working with and/or managing internal and external teams at the same time, working with multiple brands and digital properties of varying maturities* Demonstrated ability to partner and communicate effectively with non-technical team members, resolving contending or contradictory objectives, and unifying disparate ideas into a homogenized solution* Ability to be versatile and handle multiple projects and re-prioritizations* Possess the ability to influence others, implement change, and standardize processes in a complex business environment* A passion for data and growing in your current role* Ability to effectively and appropriately interview technical candidates* Passion for Automation and Hunger for Acceleration* Keen knowledge of Devops as well as RPA is a big plus* Experience with Architecting Applications (e.g.
Design Patterns, distributed applications etc.)
with the aim of reuse would be a big plus* Superb communication skills (both written and verbal)* Great teammate - should be ready to go beyond to help immediate team and do not be averse to not shy away from asking for help if needed.
* Ability to translate ideas into solutions based on user and business needs* Open Eagerness to learn new technologies and bring new ideas to the tableEducation:Bachelors Degree or equivalent.
","['sql', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'kafka']","['graph', 'pipelin', 'machine learning', 'statist', 'etl', 'commun']",1,"['sql', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'kafka', 'graph', 'pipelin', 'machine learning', 'statist', 'etl', 'commun']","['program', 'handl', 'provid', 'python', 'etl', 'sourc', 'ml', 'azur', 'line', 'statist', 'learn', 'aw', 'warehous', 'big', 'engin', 'digit', 'machin', 'pipelin', 'comput']","['sql', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'kafka', 'graph', 'pipelin', 'machine learning', 'statist', 'etl', 'commun', 'program', 'handl', 'provid', 'python', 'etl', 'sourc', 'ml', 'azur', 'line', 'statist', 'learn', 'aw', 'warehous', 'big', 'engin', 'digit', 'machin', 'pipelin', 'comput']"
DE,"Role Background:
If you are a self-starter, excited about building a culture around data-driven decisions, motivated by making an impact, and pushing the boundaries of your knowledge, you will excel here and do great things!
Design, implement, and maintain an ever-growing ETL pipeline using state-of-the-art technology
Use best practices and standards for managing large collections of data for analytics
Discover and integrate new heterogeneous data sources
Work closely with data analysts, data scientists, and product managers enabling them to provide insight into key performance metrics of the business
Help to improve data reliability, efficiency, and quality
Your Qualifications:
4+ years of experience designing and developing a data warehouse on a distributed database platforms, such as Snowflake or Redshift
Experience designing, developing, and maintaining high-throughput and low-latency ETL pipelines
Experience with data modeling, data access, and data storage techniques
Experience with big data tools such as Apache Spark
Proficient in SQL and Python
Proficient with at least one RDBMS (MySQL or Postgres preferred)
Successfully implemented data pipelines in the public cloud, especially Amazon Web Services
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Benefits & Perks
Robust health benefits packages including access to a 401k and various medical, dental and vision plans, and $100/month fitness reimbursement
Full support for remote work during COVID-19
Daily lunch delivery credit and other goodies sent to home
Regular company-wide social events (even virtually!)
Generous annual education stipend toward job-related external learning opportunities
An extremely enthusiastic team that appreciates collaboration
Values like Integrity First, Listening & Cultivating Discussion, and Default to Action.
","['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'postgr', 'mysql', 'amazon web services']","['etl', 'data modeling', 'pipelin', 'big data']",999,"['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'aw', 'etl', 'data modeling', 'pipelin', 'big data']","['analyt', 'techniqu', 'learn', 'spark', 'pipelin', 'relat', 'credit', 'public', 'python', 'warehous', 'packag', 'scientist', 'action', 'big', 'etl', 'sourc', 'collect', 'integr']","['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'aw', 'etl', 'data modeling', 'pipelin', 'big data', 'analyt', 'techniqu', 'learn', 'spark', 'pipelin', 'relat', 'credit', 'public', 'python', 'warehous', 'packag', 'scientist', 'action', 'big', 'etl', 'sourc', 'collect', 'integr']"
DE,"Technical Lead, Data Engineering
It's the result of a commitment to excellence, intelligent planning, passionate teamwork, and focused effort.
Role:
Along with your manager, you'll ensure the team's projects and processes are focused, on track, high quality and that partnerships with other teams are effective and coordinated.
Some things you may be responsible for include:
Scaling up your impact through team leadership and technical project management
Acting as a mentor to other engineers on the team
Shepherding technical decisions for the team
Balancing technical debt and ensuring high code quality
Monitoring and maintaining the team's products in production
Collaborating with product stakeholders to scope and refine projects
Working with the team to improve processes and keep things running smoothly
Qualities for a successful candidate
Strong grasp of advanced SQL, including windowing functions, nested queries, regular expressions, and a solid understanding of how relational databases and cloud data warehouses work.
Experience with data warehouse design and development, including dimensional modeling and ETL/ELT pipelines.
Experience with backend web development practices and tools, including CI/CD, SQL databases, APIs, git/GitHub, etc.
Experience with building, scaling, and monitoring resilient systems.
Enjoys working collaboratively with peers across business and engineering teams.
Comfortable with mentoring teammates, communicating, and teaching your learnings.
At least three years of experience as a data engineer.
Strong communications skills, including the ability to speak clearly to technical and nontechnical audiences and to write clearly.
Extra Credit for
Experience with Ed-Tech or ecommerce marketplaces.
Experience as a Business/Data Analyst or related role.
Relevant education, such as a degree or substantial coursework in a field such as Computer Science.
Open source contributor or active in your professional community.
","['sql', 'github', 'cloud', 'git']","['etl', 'commun']",1,"['sql', 'github', 'cloud', 'git', 'etl', 'commun']","['learn', 'relat', 'credit', 'comput', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'github', 'cloud', 'git', 'etl', 'commun', 'learn', 'relat', 'credit', 'comput', 'warehous', 'etl', 'sourc', 'engin']"
DE,"The team:
You will:
Collect data from a wide range of sources: AWS S3, Redshift, PostgreSQL, and various APIs
Build data ETL pipelines using Spark, Luigi and other open-source technologies, with programming languages like Scala, Python, and SQL
Tune Spark jobs to improve performance
Work with Data Analysts to build the right analytics reports
Join a tightly knit team solving hard problems the right way
Requirements:
You are fluent in several programming languages such as Python, R, or Scala
You have 2+ years of work experience in building ETL pipelines in production
You value code simplicity and performance
You have work experience with data storage such as AWS S3, Redshift or similar.
Being a SQL expert is a minimum for this position
You are fluent with command line
You enjoy wrangling huge amounts of data and exploring new data sets
You have a natural curiosity and investigative mindset - driven to know “why”.
You can explain complex datasets in very clear ways
You want to work in a fast, high-growth startup environment and thrive on autonomy
Bonus points:
You are familiar with Spark and/or Hadoop
Experience with AWS Redshift and S3
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'redshift', 's3', 'r', 'postgresql']","['tune', 'etl', 'pipelin']",999,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'redshift', 's3', 'r', 'postgresql', 'tune', 'etl', 'pipelin']","['analyt', 'spark', 'pipelin', 'set', 'line', 'hadoop', 'amount', 'aw', 'python', 'etl', 'sourc']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'redshift', 's3', 'r', 'postgresql', 'tune', 'etl', 'pipelin', 'analyt', 'spark', 'pipelin', 'set', 'line', 'hadoop', 'amount', 'aw', 'python', 'etl', 'sourc']"
DE,"Data Engineer (Python)
The Fixed Income & Commodities Data team is looking for a Data Engineer that will work on the acquisition, accessibility, normalizing, and cataloging of large sets of data.
Responsibilities:
Create and maintain optimal data pipeline architecture.
Extraction, transformation, and loading of data from a wide variety of data sources using Python, SQL and AWS technologies.
Providing users access to datasets using REST and Python APIs
Categorizing, cataloging, cleansing and normalizing of datasets
Communicating with business users and technology stakeholders.
Required skills/experience:
Python and data analysis libraries (Pandas, NumPy, SciPy)
Relational SQL database development
Unix/Linux command-line experience
3+ years experience with Masters or 5+ years with Bachelors degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Desirable skills/experience:
Object-oriented languages: Java, C++, etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, Lambda, etc.
RDBMS: SQL Server and PostgreSQL
Big data tools: Hadoop, Spark, Presto, Kafka, etc.
Other: Elastic stack, RESTful APIs, Node.js
Broad understanding of fixed income, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills.
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Quick learner
","['sql', 'linux', 'spark', 'numpi', 'panda', 'unix', 'hadoop', 'ec2', 'aw', 'lambda', 'python', 'redshift', 'java', 'scipi', 'postgresql', 'cloud', 'kafka', 'excel']","['pipelin', 'cleans', 'normal', 'optim', 'statist', 'big data', 'commun']",1,"['sql', 'linux', 'spark', 'numpi', 'panda', 'unix', 'hadoop', 'ec2', 'aw', 'lambda', 'python', 'redshift', 'java', 'scipi', 'postgresql', 'cloud', 'kafka', 'excel', 'pipelin', 'cleans', 'normal', 'optim', 'statist', 'big data', 'commun']","['spark', 'pipelin', 'set', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']","['sql', 'linux', 'spark', 'numpi', 'panda', 'unix', 'hadoop', 'ec2', 'aw', 'lambda', 'python', 'redshift', 'java', 'scipi', 'postgresql', 'cloud', 'kafka', 'excel', 'pipelin', 'cleans', 'normal', 'optim', 'statist', 'big data', 'commun', 'spark', 'pipelin', 'set', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']"
DE,"BigQuery
SQL
R
Python
Partykit
ETL
DataStage
API
Skill mapping
Primary Skills - Python, R, SQL, Datastage, Linux scripting, API knowledge, SSIS
Platform Knowledge - GCP Big Query
Supporting skills - Azure platform knowledge and Partykit
","['sql', 'linux', 'gcp', 'azur', 'bigqueri', 'python', 'r']",['etl'],999,"['sql', 'linux', 'gcp', 'azur', 'bigqueri', 'python', 'r', 'etl']","['azur', 'primari', 'python', 'big', 'etl']","['sql', 'linux', 'gcp', 'azur', 'bigqueri', 'python', 'r', 'etl', 'azur', 'primari', 'python', 'big', 'etl']"
DE,"If you have an interest in being responsible for the dynamics of a fast-paced environment, this is the right role for you.
You will be working on many projects at a time, but also focused on the details while finding creative ways to pursue big picture challenges.
You will leverage not just technical skills, but strong emphasis on program management, technical leadership, and communication.
In this role, you will work closely with your direct data science counterparts, and other analytic teams around Instagram to support delivering comprehensive, accurate, and holistic data artifacts.
Instagram is organized into 3 main product groups: Community, Sharing Experiences, and Interests.
However, there are important questions that fall out of the scope of a single product group or that simply fall through the cracks.
These are the questions that the Ecosystems team is tasked with answering.
Some examples of projects are: common engagement metrics, session level metrics like time spent watching video, understanding relation between production and consumption, understanding how metric trade-off between each other and bringing conformance and standards to the way product teams measure their goals.
Responsibilities:
Craft and own the optimal data processing architecture and systems for new data and ETL pipelines
Build canonical datasets as well as scalable and fault-tolerant pipelines
Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage
Define and own the data engineering roadmap for Ecosystems
Collaborate with Software Engineers and Data Scientists to design technical specification for logging and add logging to production code to generate metrics both online as well as offline
Work with different cross functional partners - Data Scientists, Infra Engineering, Logging Framework Infra Teams, Product Managers
Build visualizations to provide insights into the data & metrics generated
Work with data infrastructure teams to suggest improvements and influence their roadmap
Immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions
Recommend improvements and modifications to existing data and ETL pipelines
Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership
Drive internal process improvements and automating manual processes for data quality and SLA management
Provide ongoing proactive communication and collaboration throughout the organization
Mininum Qualifications:
4+ years' experience in the data warehouse space
4+ years' experience working with either a MapReduce or an MPP system
7+ years' experience in writing complex SQL and ETL processes
4+ years' experience with object-oriented programming languages
7+ years' experience with schema design and dimensional data modeling
Preferred Qualifications:
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Actively mentored team members in their careers
Experience effectively collaborating and communicating complex technical concepts to a broad variety of audiences
Consulting or Strategy experience in technical implementations or management consulting
","['sql', 'mapreduc', 'python', 'java']","['pipelin', 'anomali', 'visual', 'data modeling', 'optim', 'etl', 'commun']",1,"['sql', 'mapreduc', 'python', 'java', 'pipelin', 'anomali', 'visual', 'data modeling', 'optim', 'etl', 'commun']","['analyt', 'program', 'pipelin', 'relat', 'visual', 'infrastructur', 'provid', 'optim', 'python', 'warehous', 'scientist', 'comput', 'big', 'etl', 'common', 'engin']","['sql', 'mapreduc', 'python', 'java', 'pipelin', 'anomali', 'visual', 'data modeling', 'optim', 'etl', 'commun', 'analyt', 'program', 'pipelin', 'relat', 'visual', 'infrastructur', 'provid', 'optim', 'python', 'warehous', 'scientist', 'comput', 'big', 'etl', 'common', 'engin']"
DE,"The Skills:
Strong programming experience in Python
Demonstrated ability to work with data
Track record of working successfully in a collaborative environment
Top-notch communication skills
The Profile:
You have a minimum of 2-3 years of experience working in data infrastructure
Bachelor's degree in computer science, math or a related field
You enjoy being part of an amazing team but don't mind working alone on a difficult problem
You can analyze and fix problems quickly
You really like to work with people who motivate you and make you better
In your spare time you: code, tinker, read, explore, break things, and have an insatiable curiosity for all things computer related
Culture:
Seem like something you might be interested in?
",['python'],"['commun', 'math']",1,"['python', 'commun', 'math']","['infrastructur', 'relat', 'python', 'comput']","['python', 'commun', 'math', 'infrastructur', 'relat', 'python', 'comput']"
DE,"The work will support core business decisions and models that serve as the basis for core product and growth strategy.
Candidates should be able to choose the right tool for the job and learn how to use it if they don't know how to already.
Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.
JOB REQUIRED SKILLS: Python, PostgreSQL, Redis, AWS, Periscope, Looker
","['aw', 'python', 'postgresql', 'looker']",[None],999,"['aw', 'python', 'postgresql', 'looker']","['basi', 'aw', 'python']","['aw', 'python', 'postgresql', 'looker', 'basi', 'aw', 'python']"
DE,"To be part of a growing and successful business.
To reach your full potential, whatever your specialty.
Above all, to make a difference in the world by helping people achieve financial security.
It’s a career journey you can be proud of, and you’ll find plenty of support along the way.
Position Summary
The Senior Associate is a member of the Risk Insights Team within the Underwriting Department of Strategic Capabilities.
Their primary focus is to support Underwriting by providing necessary data and reporting elements as needed.
The Senior Associate is expected to design reporting queries and reports to inform management decisions.
The individual is required to have a strong skillset in data extraction as well as in analytical tools such as Spotfire or Tableau.
This individual will work independently with other members of the Risk Insights team and Underwriting at large and may serve as a liaison with management or other business units as needed.
He/she will frequently work closely with adjacent teams (e.g.
Data Solutions and Governance) and functions (e.g.
Technology).
Responsibilities
• Write and optimize data extraction queries in order to provide data and/or reports that inform business decisions
• Provide technical expertise in the development of cross-functional or cross-team business activities, specifically around the availability and optimal utilization of available underwriting/application data
• Navigate independently through large amounts of data/data sources and develop reports for key strategic initiatives and Underwriting management
• Identify and interpret patterns and trends, assess data quality and eliminate irrelevant data
• Clearly and concisely articulating in written and verbal communication the findings of the analysis and investigations performed
• Consult with internal customers to develop analyses that lead to actionable insights
• Lead or participate in special projects as assigned related to underwriting/application data
• Gain a thorough understanding of the business, to make recommendations for additional reviews where there may be gaps
Experience
• Experience extracting data from various data repository systems.
• Strong SQL optimization skills
• Ability to work collaboratively with technical and business experts to develop reporting elements to assist in business decision making process.
• Strong management skills to prioritize and implement multiple medium to large scale complex initiatives simultaneously.
• Ability to work with little supervision.
• Advanced Excel knowledge.
Spotfire or Tableau knowledge also preferred, but not required.
• Must have the ability to multitask under tight deadlines.
• Prior business experience with Underwriting, Inforce Service, or Agency is preferred.
Qualifications
• Bachelor’s Degree preferred
• 5+ years’ experience required
• 5+ years’ experience with SQL or other data extraction
#LI-JP1
EOE M/F/D/V
","['sql', 'tableau', 'excel']","['recommend', 'risk', 'optim', 'supervis', 'commun']",1,"['sql', 'tableau', 'excel', 'recommend', 'risk', 'optim', 'supervis', 'commun']","['analyt', 'primari', 'provid', 'optim', 'avail', 'amount', 'action', 'relat', 'sourc']","['sql', 'tableau', 'excel', 'recommend', 'risk', 'optim', 'supervis', 'commun', 'analyt', 'primari', 'provid', 'optim', 'avail', 'amount', 'action', 'relat', 'sourc']"
DE,"In this role, you will:
Build and maintain data pipelines that ingest and process large and complex data.
Curate and prep datasets for other teams to use.
Work with various teams to ensure data is collected properly and accurately.
Reinforce best practices for ETL processes and data storage.
On your resume:
Experience working with large-scale data sets.
Experience working with a cloud-based data warehouse such as Snowflake or Redshift.
Experience working with data pipeline tools such as an in-house Airflow pipeline or commercial applications like Segment.
Fluency with SQL and a scripting language like Python, Shell, or Java.
Excellent understanding of data structures and algorithms.
A passion for improving education.
Experience working with Looker, AWS, Snowflake, and Google Analytics.
Strong organizational skills.
BS, BA, MS, MA, or PhD in Computer Science, Mathematics, or a related technical field.
3+ years of work experience.
Familiarity with basic data science concepts and statistics.
Monthly Wellness Activities
Catered Wednesday community lunches
ClassPass Corporate Membership
Learning & Development Opportunities
Seasonal In-Office Activities
","['sql', 'airflow', 'looker', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'excel']","['pipelin', 'segment', 'reinforc', 'statist', 'etl', 'commun']",1,"['sql', 'airflow', 'looker', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'excel', 'pipelin', 'segment', 'reinforc', 'statist', 'etl', 'commun']","['analyt', 'learn', 'pipelin', 'corpor', 'set', 'relat', 'aw', 'python', 'comput', 'statist', 'warehous', 'etl', 'collect']","['sql', 'airflow', 'looker', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'excel', 'pipelin', 'segment', 'reinforc', 'statist', 'etl', 'commun', 'analyt', 'learn', 'pipelin', 'corpor', 'set', 'relat', 'aw', 'python', 'comput', 'statist', 'warehous', 'etl', 'collect']"
DE,"Would you believe that more than 60% of millennials don’t own a credit card and over 57% of Americans have less than $1,000 in savings?
Believe it.
This is a unique opportunity to build the data strategy and architecture at a data driven startup that’s changing the world of finance, shoulder to shoulder with a tight team that has your back.
You dream in code:
SQL.
Python/Scala/C# You’ve coded before, you know the importance of efficient, clean code
You thrive in any data environment:
Someone who understands complex data and the challenges of accessing it
A real bottom-line person, not someone who throws terms like “big data” around because it’s popular
Traditional/relational databases, Lakes, or Pub/Subs make no difference to you
You know databases inside and out:
Database concepts – indexes, execution engines, etc
Database Administration experience (Azure DWH, SqlServer, Postgresql)
You understand that databases are an integral part of being a Data Engineer
You enjoy looking at and solving big picture problems:
You like to ask questions and devise a complete solution
You want to understand the data (not only the pipes) and you can definitely perform some analytics and build dashboards because you like it.
You love learning new things:
You know that you don’t know enough, and it bothers you that there isn’t enough time in the day to learn about the next topic.
If you like being thrown in the deep end of the pool, this team’s for you.
You believe and want to participate in a blameless culture which focuses on process and technology
You don’t sleep well at night when you leave work with a question unanswered
You feel accountable for everything you do and that sense of urgency has been driving you your entire life
You like to have a good time while getting things done
You have your team’s back.
And the team has yours.
Sense of humor is hugely preferred
","['sql', 'azur', 'scala', 'c', 'python', 'postgresql']","['dashboard', 'financ', 'clean', 'big data', 'account']",999,"['sql', 'azur', 'scala', 'c', 'python', 'postgresql', 'dashboard', 'financ', 'clean', 'big data', 'account']","['day', 'analyt', 'challeng', 'azur', 'relat', 'line', 'credit', 'python', 'big', 'integr', 'engin']","['sql', 'azur', 'scala', 'c', 'python', 'postgresql', 'dashboard', 'financ', 'clean', 'big data', 'account', 'day', 'analyt', 'challeng', 'azur', 'relat', 'line', 'credit', 'python', 'big', 'integr', 'engin']"
DE,"Job Summary:
Other office locations also include the SoMo area of San Francisco, CA and several international locations.
The data engineering team is responsible for collecting, analyzing and distributing data using public cloud and open source technologies and offers transparency into customer behavior and business performance.
Responsibilities:
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions
Design and build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably
Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices
Build and maintain dimensional data warehouses in support of business intelligence tools
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics
Drive and maintain a culture of quality, innovation and experimentation
Coach data engineers best practices and technical concepts of building large scale data platforms
Basic Qualifications:
3-5 years of experience developing in object oriented Python
Experience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis
Experience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques
Familiar with metadata management, data lineage, and principles of data governance
Experience loading and querying cloud-hosted databases such as Redshift and Snowflake
Building streaming data pipelines using Kafka, Spark, or Flink
Preferred Qualifications:
Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift
Experience deploying data notebook and analytic environments such as Jupyter and Databricks
Knowledge of the Python data ecosystem using pandas and numpy
Experience building and deploying ML pipelines: training models, feature development, regression testing
Experience with graph-based data workflows using Apache Airflow
Required Education
Bachelor's degree in Computer Science or related field or equivalent work experience
Applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.
","['spark', 'airflow', 'numpi', 'panda', 'jupyt', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka']","['regress', 'graph', 'pipelin']",1,"['spark', 'airflow', 'numpi', 'panda', 'jupyt', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka', 'regress', 'graph', 'pipelin']","['analyt', 'techniqu', 'stream', 'spark', 'pipelin', 'ml', 'relat', 'public', 'provid', 'aw', 'python', 'warehous', 'scientist', 'comput', 'big', 'integr', 'sourc', 'engin']","['spark', 'airflow', 'numpi', 'panda', 'jupyt', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka', 'regress', 'graph', 'pipelin', 'analyt', 'techniqu', 'stream', 'spark', 'pipelin', 'ml', 'relat', 'public', 'provid', 'aw', 'python', 'warehous', 'scientist', 'comput', 'big', 'integr', 'sourc', 'engin']"
DE,"The Role:
In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.
Responsibilities
Develop Cloud-enabled Data and Analytics solutions
Participate in the development of Cloud-based and hybrid data warehouses & business intelligence platforms
Build Data Pipelines to ingest structured and unstructured Data.
Gain hands-on experience with new data platforms and programming languages
Requirements
2+ years of experience working in Data Engineering or Data Warehousing
Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, or Google
Experience building data warehousing, data ingestion, and data profiling solutions
SQL & Python skills
Strong aptitude for learning new technologies and analytics techniques
Highly self-motivated and able to work independently as well as in a team environment
Experience building and migrating complex ETL pipelines
Familiarity with or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)
Bachelor’s degree in Business Analytics, Computer Science or a closely related field required
Benefits
Profit Sharing
Health/Medical Benefits
Dental & Vision benefits
Highly competitive PTO/Holiday package
401-k
And more
","['sql', 'azur', 'aw', 'cloud', 'python']","['pipelin', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'etl', 'regress']",1,"['sql', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'etl', 'regress']","['analyt', 'machin', 'techniqu', 'pipelin', 'azur', 'relat', 'etl', 'predict', 'optim', 'aw', 'python', 'comput', 'warehous', 'quantit', 'engin', 'particip']","['sql', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'etl', 'regress', 'analyt', 'machin', 'techniqu', 'pipelin', 'azur', 'relat', 'etl', 'predict', 'optim', 'aw', 'python', 'comput', 'warehous', 'quantit', 'engin', 'particip']"
DE,"Key Responsibilities:
· Develop solutions that enable internal analysts to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, bespoke processes), transformations (Python, Perl) and interface (API, schema design, events, etc.)
· Partner with internal analysts, quants and data scientists to design, develop, test and deploy solutions that answer fundamental questions about financial markets.
· Take on an entrepreneurial mentality by building and selling your own ideas.
Required Skills
· A deep passion for working with data and developing software to address data processing challenges
· Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
·Proficiency within one or more programming languages like Java, Python, Perl or JavaScript.
· Proficiency with RDBMS, or NoSQL
· Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning or Software Architecture
· Experience with any of the following systems: Apache Airflow, AWS/GCP/Azure, Jupyter, Kafka, Docker, Nomad/Kubernetes
· Strong written and verbal communications skills
· Ability to manage multiple tasks and thrive in a fast-paced team environment
","['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['machine learning', 'natural language processing', 'commun']",1,"['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'machine learning', 'nlp', 'commun']","['machin', 'program', 'learn', 'challeng', 'azur', 'aw', 'python', 'scientist', 'comput']","['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'machine learning', 'nlp', 'commun', 'machin', 'program', 'learn', 'challeng', 'azur', 'aw', 'python', 'scientist', 'comput']"
DE,"Position Summary
The ideal candidate will have experience with data pipelines and cloud environments.
The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture.
The ideal candidate will have professional experience in a number of programming languages, databases, and development environments.
The candidate should be able to contribute to improving the reliability and quality of data.
Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred.
The successful candidate will contribute to the development of open source solutions together with a community of international researchers.
Current available position is grant-funded.
Responsibilities
1.
Software and system design, implementation, and testing (75%)
2.
Application deployment and configuration (10%)
3.
Communicate with technical individuals at various grant sites (10%)
4.
Software requirements specification (5%)
Minimum Qualifications
Bachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.
Other Requirements
Great communication skills; Experience with one or more compiled programming languages (e.g.
Java, Scala, C#, C++, etc.)
and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)
Working knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.
Equal Opportunity Employer / Disability / Veteran
","['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql']","['research', 'pipelin', 'visual', 'big data', 'etl', 'commun']",1,"['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql', 'research', 'pipelin', 'visual', 'big data', 'etl', 'commun']","['big', 'program', 'pipelin', 'relat', 'visual', 'python', 'avail', 'interpret', 'comput', 'clinic', 'etl', 'sourc']","['sql', 'perl', 'scala', 'javascript', 'cloud', 'python', 'java', 'c', 'nosql', 'research', 'pipelin', 'visual', 'big data', 'etl', 'commun', 'big', 'program', 'pipelin', 'relat', 'visual', 'python', 'avail', 'interpret', 'comput', 'clinic', 'etl', 'sourc']"
DE,"You will be working to design and create low latency data processing pipelines.
Required
Scala, Spark, Hadoop
Kafka, Redis, NoSQL, Akka
Java or Python
ETL pipeline development
High throughput data volume
Why work here?
Generous salary and bonus package
Unlimited PTO
401K
Free snacks, breakfasts, coffee
Medical dental vision benefits
","['spark', 'scala', 'hadoop', 'java', 'python', 'nosql', 'kafka']","['etl', 'pipelin']",999,"['spark', 'scala', 'hadoop', 'java', 'python', 'nosql', 'kafka', 'etl', 'pipelin']","['spark', 'pipelin', 'hadoop', 'python', 'packag', 'etl']","['spark', 'scala', 'hadoop', 'java', 'python', 'nosql', 'kafka', 'etl', 'pipelin', 'spark', 'pipelin', 'hadoop', 'python', 'packag', 'etl']"
DE,"Above all, your work will impact the way the world experiences music.The Insights Platform teams within Data and Insights have a wide range of responsibilities.
Additionally, the teams build tools to support Data Scientists in their end to end workflow, querying data in BigQuery, exploring data in Jupyter notebook, and visualizing data with Tableau or QlikSense.
* Design intelligent solutions that include data and back-end systems.
* Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio, Luigi, Styx and Docker.
* Be a valued member of an autonomous, cross-functional agile team.
* You'll use industry standard, cloud native tech, which means easily transferable skills and a focus on your professional development.
* Hack on what you want during regular hack days and bi-annual hack weeks.
* You know how to write distributed, well designed services in Java.
* You know how to work with large scale data systems.
* Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google's Cloud Platform.
* You are experienced with deploying and operating services on Linux.
* You have a deep understanding of system design, data structures, and algorithms.
* You care about quality and you know what it means to ship high quality code.
* Google Cloud Platform experience is a bonus.
","['google cloud', 'linux', 'scala', 'jupyt', 'bigqueri', 'bi', 'tableau', 'java', 'cloud', 'docker']",[None],999,"['google cloud', 'linux', 'scala', 'jupyt', 'bigqueri', 'powerbi', 'tableau', 'java', 'cloud', 'docker']","['day', 'bi', 'scientist', 'algorithm']","['google cloud', 'linux', 'scala', 'jupyt', 'bigqueri', 'powerbi', 'tableau', 'java', 'cloud', 'docker', 'day', 'bi', 'scientist', 'algorithm']"
DE,"71482
Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with data science team to transform data and integrate algorithms and models into automated processes.
Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support Data Science and other internal customers.
ntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience:
Strong problem solving skills and critical thinking ability.
Strong collaboration and communication skills within and across teams.
5 or more years of progressively complex related experience.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Ability to understand complex systems and solve challenging analytical problems.
Experience with bash shell scripts, UNIX utilities & UNIX Commands.
Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.
Experience building data transformation and processing solutions.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Masters degree or PhD preferred.
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
from you via e-mail.
Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter.
","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",1,"['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"The role
Day-to-day responsibilities:
Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects
Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes
Always maintain a high level of data security and privacy
The team
Youll work in either the NY or DC office.
Top things were looking for
Good foundational understanding of statistical analysis
Extensive experience working with SQL databases in an analytics or business intelligence context
Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools
Experience with ETL/ELT tools, processes, and best practices
Strong Python experience:
Python should be your go-to tool for solving problems.
Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus
Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks
Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:
Designing data warehouse schemas for cross-channel marketing analytics
Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data
Building and running automated tasks within the GCP environment - e.g.
Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler
Comfortable managing GCP IAM policies across projects and teams
Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if youve extended Google Sheets using Google Apps Script
Familiarity with Git and maintains good habits around code maintenance
Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)
Good at teaching others what you know.
With clients including the AARP, Google, UNICEF, JDRF, and Colgate.
","['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python']","['dashboard', 'etl', 'statist', 'pipelin']",999,"['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python', 'dashboard', 'etl', 'statist', 'pipelin']","['day', 'analyt', 'pipelin', 'line', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'common']","['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python', 'dashboard', 'etl', 'statist', 'pipelin', 'day', 'analyt', 'pipelin', 'line', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'common']"
DE,"270 on the 2012 Inc. 500 List 2012,2016 and 2017 NJ 50 Fastest Growing Companies Global Investment Banking is looking for a Big Data Engineer for its team in New York.Long Term Contract Please contact Veena.Mahesh(at)Atyeti.com609-480-1642 Role Big Data Engineer I AM LOOKING FOR SENIOR ENGINEER FROM INVESTMENT BANKING BACKGROUND.
10+ YRS OF EXPERIENCE Responsibilities Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications.
Create optimal data pipeline architecture and systems using Apache Airflow Assemble large, complex data sets that meet functional and non-functional business requirements.
Analyze, debug and correct issues with data pipelines Operate on or build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and Azure technologies.
Required Skills 8+ years of experience building high performance scalable enterprise analytics or data centric solutions At least 5 years of experience implementing complex ETL pipelines preferably in connection with Hadoop or Spark Experience working with Spark data pipeline and or streaming experience Exceptional coding and design skills in JavaScala or C. Expert in Python, or can demonstrate ability to readily learn new languages and affinity for them.
Understand the nature of distributed systems design.
Hands-on experience with Azure.
Strong understanding usage of algorithms and data structures.
Work well in a team environment, and are a self-starter.
Ability to lead, or have led teams before.
Expertise building and managing python libraries Nice to have Skills Azure Data Lake experience Machine Learning expertise Experience with Apache Airflow Experience with C .NET Core Experience with traditional data martswarehouses Education BS in Computer Science Software Engineering or equivalent experience.
","['sql', 'spark', 'airflow', 'azur', 'hadoop', 'python', 'c']","['big data', 'pipelin', 'machine learning', 'optim', 'analyz', 'etl']",1,"['sql', 'spark', 'airflow', 'azur', 'hadoop', 'python', 'c', 'big data', 'pipelin', 'machine learning', 'optim', 'analyz', 'etl']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'set', 'hadoop', 'infrastructur', 'optim', 'python', 'comput', 'big', 'etl', 'sourc', 'engin', 'algorithm']","['sql', 'spark', 'airflow', 'azur', 'hadoop', 'python', 'c', 'big data', 'pipelin', 'machine learning', 'optim', 'analyz', 'etl', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'set', 'hadoop', 'infrastructur', 'optim', 'python', 'comput', 'big', 'etl', 'sourc', 'engin', 'algorithm']"
DE,"As the creator of the world's first electronic stock market, its technology powers more than 100 marketplaces in 50 countries, and 1 in 10 of the world's securities transactions.
To learn more, visit: http://www.nasdaq.com
The Team
This role will provide AWS, SCALA, SPARK and SQL expertise, and source data management support to the Index Administration Team.
Your role and responsibilities
Big Data Engineering on Cloud Platform
Good understanding of Big data technologies – Spark SQL
Programming SPARK in Scala
Proficiency in SQL
Strong data analysis and troubleshooting skills
Regularly interacts with Business ,Data Vendors and Cloud DevOps Team
Domain knowledge of Capital Market is plus
Knowledge of shell scripts and other languages including Python, R, Java is plus
Unit testing/verification of new development
Work with QA Test analyst to ensure test coverage (Including Integration & Regression testing)
Develops new program logic and/or assembles standard logic modules to create new applications
You will need the following skills and experience
At least 2 years of hands on experience on Big Data Engineering on AWS
Minimum 1 year experience with Spark, Scala
Must be able to write complex sql queries
Experience with RDB and sql server databases (Could be MS or Oracle…)
Experience with Large data sets
Excellent teamwork/collaboration skills
Excellent communication skills (Written & Oral)
Good knowledge of linux OS, shell scripting
Experience working on complex distributed information systems
Experience with version control systems, preferable SVN, GIT.
Strong work ethic in a mission-critical 24x7 diverse environment with multiple vendor/customer relationships.
And it would be great if you have experience with
AWS Certification is plus
Preferred – Experience with Financial Markets (options a plus)
Preferred – Working knowledge of various Financial Market data feeds
Beneficial having experience with log aggregation tools such as Elasticsearch, Splunk, Datadog, etc
Must be able to work Saturdays and rotational schedule
Sounds like you?
Job Type: Full-time
Pay: $125,000.00 - $130,000.00 per year
Benefits:
401(k)
Dental Insurance
Health Insurance
Paid Time Off
Tuition Reimbursement
Vision Insurance
Schedule:
Monday to Friday
Experience:
SQL: 1 year (Preferred)
Big Data Engineering on AWS: 2 years (Required)
Spark and Scala: 1 year (Required)
New York, NY 10036 (Required)
Visa Sponsorship Potentially Available:
No: Not providing sponsorship for this job
Work Remotely:
Temporarily due to COVID-19
","['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'excel', 'git', 'aw', 'cloud', 'python', 'java', 'r', 'splunk']","['big data', 'commun', 'regress']",2,"['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'excel', 'git', 'aw', 'cloud', 'python', 'java', 'r', 'splunk', 'big data', 'commun', 'regress']","['program', 'spark', 'set', 'divers', 'power', 'aw', 'python', 'avail', 'big', 'integr', 'sourc', 'engin']","['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'excel', 'git', 'aw', 'cloud', 'python', 'java', 'r', 'splunk', 'big data', 'commun', 'regress', 'program', 'spark', 'set', 'divers', 'power', 'aw', 'python', 'avail', 'big', 'integr', 'sourc', 'engin']"
DE,"Job DescriptionDesign, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software.
Build a system that builds and evaluates machine learning models at scale.
Work with technologies such as SQL, Azure and/or Kubernetes to build a data platform for projects.
Work with data scientists to design and implement advanced statistical models and machine learning pipelines.
Build automated tools to help answer questions about impact and design and manage queries from stakeholders.
Translate end user requirements into Power BI reports and dashboards and analyze impact of product offerings using Power BI.
Collect data from sources such as API, internal data source, and third party data source.
Investigate data interactions and dependencies across complex data pipelines and transformation to validate assumptions and find sources of problems.QualificationsRequires: Bachelor's degree in Computer Science or Electrical or Electronic Engineering and 5 years' experience as above or as an EDI Administrator or Programmer Analyst using similar skills as above.Not available to persons needing sponsorship for employment.Employment TypeFull Time
","['sql', 'azur', 'bi', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz']",1,"['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz']","['machin', 'learn', 'engin', 'pipelin', 'azur', 'power', 'bi', 'avail', 'parti', 'statist', 'scientist', 'comput', 'sourc', 'collect']","['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz', 'machin', 'learn', 'engin', 'pipelin', 'azur', 'power', 'bi', 'avail', 'parti', 'statist', 'scientist', 'comput', 'sourc', 'collect']"
DE,"The ideal candidate is entrepreneurial, motivated to grow, and has a passion for Python development.
What you'll be doing
Developing and enhancing multiple ETL pipelines
Designing and implementing data storage structures and ETL pipelines, keeping long-term impacts in mind
Assemble large, complex data sets that meet functional / non-functional business requirements.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Identifying and improving upon current internal processes through automation and optimization
Who you are:
Bachelor's degree in Computer Science, Mathematics, or related field/equivalent experience
3+ years of experience with Python, Java, Scala, R, Go or similar language
3+ years of experience in working with cloud computing technology (AWS, Google Cloud Platform, etc.)
3+ years experience working on data warehouse systems such as Snowflake
Have developed systems based on key principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)
Comfortable using version control and working in a collaborative environment
Experienced with CI/CD tools for testing and deployment
Self-starter with the ability to work independently or in a team
Able to manage one's schedule and prioritize tasks independently
Why You Should Join
Wherever you are from, you will find a common ground here for continuing to push forward your career and make a difference in this industry.
The technology generates customer intent insights that lead to compelling content, increased traffic, and higher organic marketing ROI.
Customizable dashboards and workflows guide marketers through the content creation process, empowering them to measure, refine, and demonstrate the effectiveness of their SEO and content marketing efforts.
","['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud']","['pipelin', 'dashboard', 'optim', 'big data', 'etl']",1,"['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud', 'pipelin', 'dashboard', 'optim', 'big data', 'etl']","['stream', 'pipelin', 'set', 'relat', 'optim', 'aw', 'python', 'avail', 'comput', 'etl', 'common', 'algorithm']","['google cloud', 'scala', 'aw', 'snowflak', 'java', 'python', 'r', 'cloud', 'pipelin', 'dashboard', 'optim', 'big data', 'etl', 'stream', 'pipelin', 'set', 'relat', 'optim', 'aw', 'python', 'avail', 'comput', 'etl', 'common', 'algorithm']"
DE,"By applying for this role, you could choose to work in the following locations:
New York City
San Francisco
DATA ENGINEER - HEALTH DATA ENGINEERING TEAM
Are you an engineer who’s passionate about defending online users against abuse, spam, and manipulation?
Will you be proud to work on a real-time, scalable pipelines that process terabytes of data to enable training and analysis of Machine Learning models, product analysis, and experimentation?
This team is also in charge of the best practices around building scalable, production-ready data processing solutions, as well as researching and implementing the most efficient mechanisms for data access.
Health Data Engineering team will be partnering closely with all the engineering teams in the Health org to understand and improve its data production and consumption needs.
What You’ll Do
Here are some examples of what you’ll find yourself doing daily:
Directly contribute to the design and code of the data pipelines operating on production data
Improve approaches to efficiently handle ever-increasing volumes of data
Maintain efficiency and reliability of production of the critical datasets
Evaluate and propose the best tooling and processes for data access and analysis
Provide design and review support to the engineering teams working on data processing
Continuously evaluate team’s processes to maintain a positive and efficient engineering culture
Who You Are
You have experience working in an environment that supports data analysis, experimentation, and Machine Learning modeling or its integration into a product.
You have a solid understanding of backend and distributed systems and strong experience working with MapReduce-based architectures.
You have experience in working with large volumes of data.
You have a broad knowledge of the data infrastructure ecosystem.
You are familiar with standard software engineering methodology, e.g.
unit testing, code reviews, design documentation.
You enjoy working in a collaborative environment and interact effectively with others.
You ground your decisions with data and reasoning and can adapt to new information to make informed choices.
You bring thoughtful perspectives, empathy, creativity, and a positive attitude to solve problems at scale.
Here’s all the legal good stuff:
",['mapreduc'],"['research', 'machine learning', 'pipelin']",999,"['mapreduc', 'research', 'machine learning', 'pipelin']","['machin', 'learn', 'pipelin', 'handl', 'infrastructur', 'provid', 'integr', 'engin', 'evalu']","['mapreduc', 'research', 'machine learning', 'pipelin', 'machin', 'learn', 'pipelin', 'handl', 'infrastructur', 'provid', 'integr', 'engin', 'evalu']"
DE,"Make your mark.
See the difference.
Be recognized.
Work with aligned engaged team members.
Ours is a story of being big enough to deliver and maintain large-scale operations but being nimble enough to make things happen.
About the role
Design, implement and support an analytical data infrastructure.
• Managing AWS resources including EC2, S3, Glue, Redshift, etc.
• Working with AWS services including Lambda, Athena, and API Gateway
• Working with the Elastic Stack
• Interface with engineering teams to extract, transform, and load data from network data sources using SQL and AWS big data technologies
• Explore and learn the latest data management technologies to provide new capabilities and increase efficiency2
• Collaborate with Data Scientists, Network Engineers and Product Managers to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis,validation, and documentation
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
About you
Bachelor’s degree in Data Science, Computer Science, Engineering, Mathematics, or related technical discipline.
• 3 years of industry experience in a data engineering or data science role (development,data engineering, data science, or related field with experience in manipulating, processing,and extracting value from large datasets).
• Experience with management of relational databases such as PostgreSQL, MySQL or Microsoft SQL Server (Experience in data modeling, ETL development, and datawarehousing).
• Experience with AWS S3, EC2 and Lambda.
• Experience with shell scripting and working in an UNIX environment.
• Expertise in Python.
• Familiarity with engagement with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
• Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Preferred Qualifications / Bonus Points
• 5 years of industry experience in a data engineering or data science role.
• Master’s degree in Data Science, Computer Science, Engineering, Mathematics, or related technical discipline.
• Experience with analysis or engineering of time series datasets.
• Experience with the TICK Stack (InfluxDB, Chronograph).
• Experience with AWS big data technologies (Redshift, EMR).
","['sql', 'unix', 'postgresql', 'ec2', 'aw', 'lambda', 'python', 'redshift', 's3', 'microsoft', 'mysql']","['data modeling', 'etl', 'time series', 'big data']",1,"['sql', 'unix', 'postgresql', 'ec2', 'aw', 'lambda', 'python', 'redshift', 's3', 'microsoft', 'data modeling', 'etl', 'time series', 'big data']","['analyt', 'relat', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']","['sql', 'unix', 'postgresql', 'ec2', 'aw', 'lambda', 'python', 'redshift', 's3', 'microsoft', 'data modeling', 'etl', 'time series', 'big data', 'analyt', 'relat', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"Team descriptions:
You will be exposed to both client-facing financial data modeling, calculations and infrastructure of data intensive systems.
As a Software Engineer on Data Tooling, you will convert human-centric workflows to automated workflows.
This will entail designing and building the next generation of internal tools, ETL pipelines, and distributed architecture.
You might even learn to build basic UIs to help configure the tools you've built.
In addition to those goals, this team is in charge of the entire data pipeline.
Requirements
B.S., M.S., or Ph.D. in Computer Science or similar technical field of study (or equivalent practical experience.).
3+ years' experience as a professional software engineer and experience with any object-oriented programming language.
Experience building large-scale distributed systems and system integrations; modeling financial data is a plus
Proven ability to collaborate with and mentor other engineers.
It provides asset owners and advisors a clearer financial picture at every level, allowing them to make more informed and timely investment decisions.
",[None],"['etl', 'pipelin', 'data modeling']",1,"['etl', 'pipelin', 'data modeling']","['asset', 'program', 'pipelin', 'human', 'infrastructur', 'comput', 'etl', 'engin', 'integr']","['etl', 'pipelin', 'data modeling', 'asset', 'program', 'pipelin', 'human', 'infrastructur', 'comput', 'etl', 'engin', 'integr']"
DE,"Data EngineerNew York, NYData Engineer New York, NY
*This is remote to start then onsite.
*
This key role is closely aligned with companys software developers, database architects, data analysts and data scientists, and is responsible for ensuring that optimal data delivery architecture is consistent throughout ongoing projects.
The successful candidate must be independent, self-directed and comfortable supporting the data needs of multiple teams, systems and products.
He/she will be responsible for expanding and optimizing the data and data pipeline architecture (in addition to optimizing data flow and collection for cross functional teams).
Required experience:
5+ years of experience expanding and optimizing analytics including the use of Oracle Utilities Analytics, Work Management Analytics, EBS Analytics and Enterprise Data Analytics platforms (EDAP).
5+ years of experience creating and maintaining optimal data pipeline architecture.
Demonstrated ability to assemble large, complex data sets that meet functional / non-functional business requirements.
Experience with the identification, design, and implementation of internal process improvements: automation of manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Experience driving the analysis of business processes, functions and procedures to identify opportunities, defining business requirements, and design of solutions to improve business efficiency.
5+ years experience with system/data integration, development or implementation of enterprise and/or cloud software (Oracle Cloud or AWS).
5+ years experience ETL development
5+ years SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Knowledge of web services/ REST
Data modeling experience to support analytics (ie dimensional modeling)
Bachelor's Degree Computer Science, Information Technology or Related discipline
","['sql', 'aw', 'oracl', 'cloud']","['pipelin', 'data modeling', 'optim', 'information technology', 'etl']",1,"['sql', 'aw', 'oracl', 'cloud', 'pipelin', 'data modeling', 'optim', 'information technology', 'etl']","['analyt', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'aw', 'comput', 'scientist', 'etl', 'collect', 'integr']","['sql', 'aw', 'oracl', 'cloud', 'pipelin', 'data modeling', 'optim', 'information technology', 'etl', 'analyt', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'aw', 'comput', 'scientist', 'etl', 'collect', 'integr']"
DE,"Data EngineerWe are looking for a Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.In this role you will build and maintain an efficient data pipeline architecture, while assembling large, complex data sets that meet functional / non-functional business requirements.
You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.Principal Responsibilities* Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
* Hadoop, Spark, Presto, Kafka, etc.
* Relational SQL and NoSQL databases, including SQL Server, MySQL and Cassandra.
* Data pipeline and workflow management tools: Airflow, Luigi, etc.
* AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
* Savvy with data science stack (Pandas, NumPy, SciPy)* Data Science/Analysis background; Proficient at working with large datasets* Unix/Linux command-line experience* Experience working with AWS, GCP or Azure* Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
","['sql', 'linux', 'gcp', 'redshift', 'scipi', 'nosql', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'panda', 'ec2', 'aw', 'mysql', 'spark', 'airflow', 'numpi', 'cloud']","['optim', 'big data', 'pipelin', 'analyz']",999,"['sql', 'linux', 'gcp', 'redshift', 'scipi', 'nosql', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'panda', 'ec2', 'aw', 'spark', 'airflow', 'numpi', 'cloud', 'optim', 'big data', 'pipelin', 'analyz']","['analyt', 'spark', 'pipelin', 'azur', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'action', 'set', 'sourc', 'engin']","['sql', 'linux', 'gcp', 'redshift', 'scipi', 'nosql', 'kafka', 'azur', 'unix', 'hadoop', 'cassandra', 'panda', 'ec2', 'aw', 'spark', 'airflow', 'numpi', 'cloud', 'optim', 'big data', 'pipelin', 'analyz', 'analyt', 'spark', 'pipelin', 'azur', 'relat', 'line', 'hadoop', 'infrastructur', 'optim', 'aw', 'action', 'set', 'sourc', 'engin']"
DE,"Responsibilities
These individuals demonstrate ease with quantitative analysis, can work well as part of small multi-disciplinary teams, have a keen interest in software engineering and are passionate about developing leading edge analytical business applications.
You can expect the following responsibilities:
Collaborate with Product Management and other engineers on the team to make product improvements and develop new features
Hands-on SQL Server development (Stored Procedures, Functions etc)
Migrate complex data processing from SQL Server to Big Data using Spark, Scala in the near future.
Create and maintain detailed documentation and functional design specifications
Perform ongoing backend Database maintenance of existing applications
Provide technical information to assist in the development of client facing product documentation
Author and participate in software design and code reviews
Adhere to change management protocols and version control
Desired Skills& Expertise
Aspiring candidates should have the following background, skills and characteristics:
Bachelor’s degree, preferably in computer science, or engineering field
3+ years of experience in SQL Server development
1+ years of ETL experience using flat files and other RDBMS
Experience in troubleshooting and debugging SQL code
Familiarity with Tableau
Familiarity with the design, development and maintenance of best-in-class BI capabilities, including data warehouse data structures and data pipelines spanning Spark/Hadoop and RDBMS worlds
Expert-level database development experience in SQL Server, preferably for reporting data marts and business intelligence solutions, including writing stored procedures for complex business logic in T-SQL
Familiarity of architectural design patterns for micro-services leveraging relational and big data technologies is an added plus
Familiarity with Agile development process, SVN or other change management protocols would be a plus
Proven track record of academic and/or professional success
Exceptional analytical thinking ability, ease with quantitative analysis, and excellent problem-solving skills
Self–discipline and willingness to learn
Ability to work well with others in a high-pressure environment
Excellent verbal and written communication skills
All candidates must possess work authorization which does not (and will not in the future) require work sponsorship by an employer.
Please visitwww.novantas.com for more information.
","['sql', 'spark', 'scala', 'hadoop', 'bi', 'tableau', 'excel']","['etl', 'commun', 'pipelin', 'big data']",1,"['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'excel', 'etl', 'commun', 'pipelin', 'big data']","['analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'etl', 'provid', 'bi', 'comput', 'warehous', 'big', 'quantit', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'excel', 'etl', 'commun', 'pipelin', 'big data', 'analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'etl', 'provid', 'bi', 'comput', 'warehous', 'big', 'quantit', 'engin']"
DE,"You'll write Python, SQL, and R in shared notebooks that you and the data science team have ownership of.
As such, successful candidates must be highly capable in each of the following dimensions (among others): adaptability, curiosity, resourcefulness, analytical thinking/problem solving, pro-activity, collaboration, technological savvy, and operating in a dynamic environment.
Job Responsibilities
Deliver on detailed specifications for business intelligence and reporting needs
Preferred Qualifications:
MS or PhD in Computer Science, Mathematics, Statistics, Physics, Economics, or similar hard-science
3+ years hands-on experience in Data + Analytics at growing product-driven tech companies
Proficiency in cloud services and modern ETL workflows
Advanced capabilities across Python, R, and SQL
Understanding of Spark
Strong analytical and problem solving skills
Working knowledge of Python web frameworks like Flask
Software development background
","['sql', 'spark', 'cloud', 'python', 'r', 'flask']","['etl', 'problem solving', 'econom', 'statist']",2,"['sql', 'spark', 'cloud', 'python', 'r', 'flask', 'etl', 'problem solving', 'econom', 'statist']","['analyt', 'spark', 'python', 'comput', 'statist', 'etl']","['sql', 'spark', 'cloud', 'python', 'r', 'flask', 'etl', 'problem solving', 'econom', 'statist', 'analyt', 'spark', 'python', 'comput', 'statist', 'etl']"
DE,"Hello.
ROLE & IMPACT
Working on Data Engineering your role will be focused on creating access to critical data for a variety of teams, spread across the business and engineering groups.
You will use a variety of techniques, ranging from structuring data for Tableau and creating the perfect report for broad consumption or writing APIs for the engineering team to consume data in a structured way.
You will join other data engineers who are building out the practice and you’ll have the freedom to explore new ways of thinking about these challenges and have the potential to make a significant impact on the way the business operates using data.
WHO YOU ARE
You understand the importance of process and structure but also know how to get things done in a scrappy way.
You’ve worked with a variety of types of data and know how to make things come together to form a cohesive dataset.
You are ready to answer the big questions about what’s working and what’s not, but also have the patience and focus to dig in on why a certain piece of data may or may not be correct due to technical issues.
You are ready to work closely with a variety of stakeholders to get things done and can explain problems from outside people’s experience in a way that helps them understand and feel engaged.
RESPONSIBILITIES
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights
Requirements
2-5 years’ experience working in Data Engineering
A foundation in Computer Science, Engineering, Stats, or Math, through education and/or work
Advanced SQL skills
Strong Python or R, and Tableau skills
Experience with AWS or Azure clouds
Linux/Unix experience preferred
Prefer experience working in ecommerce (bonus points if you’ve worked with eBay, Amazon, or Tmall)
Experience with Airflow, DBT, or similar tools for managing ETL pipelines
Bonus for working on international commerce
Ability to make thoughtful trade-offs among a diverse set of teams and priorities, filling the gap between business requirements and engineering, and suggesting reasonable workarounds when necessary
Bonus points if you have a passion for the sneaker and streetwear industry
Benefits
Paid time off & work from home policy
Dynamic career growth opportunities
A fun, creative and mission-driven work environment
Team outings and afterwork events
LOCAL CANDIDATES ONLY
ABSOLUTELY NO RECRUITING AGENCIES PLEASE
","['sql', 'linux', 'airflow', 'azur', 'unix', 'aw', 'tableau', 'python', 'r']","['pipelin', 'optim', 'big data', 'etl', 'math']",999,"['sql', 'linux', 'airflow', 'azur', 'unix', 'aw', 'tableau', 'python', 'r', 'pipelin', 'optim', 'big data', 'etl', 'math']","['analyt', 'techniqu', 'challeng', 'azur', 'pipelin', 'set', 'divers', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'action', 'big', 'etl', 'sourc', 'engin']","['sql', 'linux', 'airflow', 'azur', 'unix', 'aw', 'tableau', 'python', 'r', 'pipelin', 'optim', 'big data', 'etl', 'math', 'analyt', 'techniqu', 'challeng', 'azur', 'pipelin', 'set', 'divers', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'action', 'big', 'etl', 'sourc', 'engin']"
DE,"You should be comfortable building complex yet performant SQL queries on large data sets.
Experience working with and tuning these for large scale workloads would be a plus.
What you get in return
Beyond working with such a great team?
An exciting environment with real growth
Contribute to exciting products used by a highly passionate user base
Personal learning and development opportunities
Flexible holiday allowance
Attractive health insurance premiums
",['sql'],[None],999,['sql'],"['set', 'learn']","['sql', 'set', 'learn']"
DE,"Title
Data Engineer, #3969
10-Jul-2020
Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software.
Build a system that builds and evaluates machine learning models at scale.
Work with technologies such as SQL, Azure and/or Kubernetes to build a data platform for projects.
Work with data scientists to design and implement advanced statistical models and machine learning pipelines.
Build automated tools to help answer questions about impact and design and manage queries from stakeholders.
Translate end user requirements into Power BI reports and dashboards and analyze impact of product offerings using Power BI.
Collect data from sources such as API, internal data source, and third party data source.
Investigate data interactions and dependencies across complex data pipelines and transformation to validate assumptions and find sources of problems.
Requisition Number
3969BR
Qualifications
Requires: Bachelor’s degree in Computer Science or Electrical or Electronic Engineering and 5 years’ experience as above or as an EDI Administrator or Programmer Analyst using similar skills as above.
Not available to persons needing sponsorship for employment.
State
New York
Employment Type
Full Time
City
New York
Country
United States of America
","['sql', 'azur', 'bi', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz']",1,"['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz']","['machin', 'learn', 'engin', 'pipelin', 'azur', 'power', 'bi', 'avail', 'parti', 'statist', 'scientist', 'comput', 'sourc', 'collect']","['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'machine learning', 'statist', 'analyz', 'machin', 'learn', 'engin', 'pipelin', 'azur', 'power', 'bi', 'avail', 'parti', 'statist', 'scientist', 'comput', 'sourc', 'collect']"
DE,"Come join the team responsible for the flow of data from ingestion to consumption.
This role is highly visible and works with a variety of stakeholders.
The ideal candidate would understand both RDBMS and newer technologies (like RedShift, BigQuery, etc.).
The right person will have ample opportunity to make an impact on the organization.
RESPONSIBILITIES
Practice and promote craftsmanship in data infrastructure components (monitoring, testing, code reviews, documentation, scalability, performance, etc.)
Own long-term impacts of key design decisions and balance technical debt with business needs
Break down requirements, estimate tasks, and assist in planning roadmap accurately
Help users get value from analytic data
Grow engineering teams through mentorship, recruiting, and interviewing
Focus on team over individual achievements
Lead definition of team objectives and drive team towards them
QUALIFICATIONS
Previous success in a Data Engineering role, working knowledge of data concepts (optimization, integrity, policies, etc.)
Strong SQL and Relational Skills
Working knowledge of Cloud Data Platforms (RedShift, BigQuery)
Ability to collaborate cross functionally with other engineering and non-engineering departments
Curious, self-motivated, and an empathetic drive to solve problems
BONUSES
Project/Product management experience
Application development experience
Previous experience working in a startup environment
","['sql', 'cloud', 'redshift', 'bigqueri']",[None],999,"['sql', 'cloud', 'redshift', 'bigqueri']","['analyt', 'relat', 'infrastructur', 'integr', 'engin']","['sql', 'cloud', 'redshift', 'bigqueri', 'analyt', 'relat', 'infrastructur', 'integr', 'engin']"
DE,"Do you want to use data to influence product decisions for products being used by hundreds of millions of people every day?
You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.
Responsibilities:
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Partner with Product and Engineering teams to solve problems and identify trends and opportunities.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
Mininum Qualifications:
BS/BA in Technical Field, Computer Science or Mathematics.
4+ years experience in the data warehouse space.
4+ years experience in custom ETL design, implementation and maintenance.
4+ years experience working with either a Map Reduce or an MPP system.
4+ years experience with schema design and dimensional data modeling.
4+ years experience in writing SQL statements.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Communication skills including the ability to identify and communicate data driven insights.
Ability in managing and communicating data warehouse plans to internal clients.
Preferred Qualifications:
4+ years experience using Python or Java
","['sql', 'java', 'python']","['etl', 'commun', 'data modeling']",1,"['sql', 'java', 'python', 'etl', 'commun', 'data modeling']","['day', 'challeng', 'set', 'infrastructur', 'python', 'comput', 'warehous', 'etl', 'engin']","['sql', 'java', 'python', 'etl', 'commun', 'data modeling', 'day', 'challeng', 'set', 'infrastructur', 'python', 'comput', 'warehous', 'etl', 'engin']"
DE,"With $23.5 trillion of assets under custody and ranked #1 for Global Investment Banking fees, the Corporate & Investment Bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world.
The Cross Product Change Services (CPCS) Team within Corporate and Investment Bank serves multiple internal and external clients in providing change services such as Business Analysis, Minimum Viable Product Assessment, Reference Data Strategy definition and implementation, Process assessment and re-engineering, Automation, Implementation support, Test strategy and delivery.
CPCS is currently looking for an experienced Data Engineer to join the organization and play an integral role in the build out of the instrumentation required for measuring and impacting the transformation of the Operations workforce collaborating with the Business and Technology.
Key Responsibilities:
Responsible for delivering insights and reporting to the management team and stakeholders, based on the data gathered from diverse applications , processes and manual activities
Enable the management team and stakeholders to make timely, well-informed and data-driven decisions
Continuous innovation - seek out and adopt cutting edge techniques to improve the storage, collection, visualization and analysis of data
Development of detailed data model, in line with detailed business requirements, to ensure that the business is able to meet their business objectives
Use your experience in engineering, optimizing and debugging high volume data pipelines to contribute to the creation of reporting platforms.
Strive for the highest standards of data quality and governance
The successful candidate will join a team focused on change delivery across products, help accelerate pace of delivery, provide metrics, KPIs to enable data based decision making and be responsible for supporting the strategy and execution of Cross Product Change Services objectives and initiatives.
Qualifications:
2+ years of experience working in Business Intelligence / MIS Reporting on an enterprise level
Understanding and related experience in the integration of data from multiple data sources and automation of data processes
Understanding of data warehousing with coding abilities in the field of data manipulation
Depicts strong knowledge and experience in querying, manipulating and summarizing large amount of data through SQL, in addition to experience in scripting languages
Knowledge and build experience in Alteryx, Xceptor and Tableau.
Excellent analytical skills and ability to understand and build complex data models
Strong understanding of SDLC in an enterprise environment and experience working as an agile developer
Solid experience of and enthusiasm for agile development methodologies
Thoughtful and comfortable communicator (in person or on paper) with both technical and business stakeholders, with the ability to facilitate discussions and conduct training
","['sql', 'tableau', 'excel']","['pipelin', 'visual', 'risk', 'optim', 'data warehousing', 'commun', 'kpi']",999,"['sql', 'tableau', 'excel', 'pipelin', 'visual', 'risk', 'optim', 'data warehousing', 'commun', 'kpi']","['analyt', 'asset', 'techniqu', 'engin', 'pipelin', 'corpor', 'relat', 'line', 'visual', 'divers', 'provid', 'optim', 'amount', 'integr', 'sourc', 'collect']","['sql', 'tableau', 'excel', 'pipelin', 'visual', 'risk', 'optim', 'data warehousing', 'commun', 'kpi', 'analyt', 'asset', 'techniqu', 'engin', 'pipelin', 'corpor', 'relat', 'line', 'visual', 'divers', 'provid', 'optim', 'amount', 'integr', 'sourc', 'collect']"
DE,"https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board
(Senior) Data Engineers at BCG Platinion are:
Iterative.
Collaborative.
Comfortable with ambiguity.
They know projects and businesses move fast.
Interdisciplinary.
They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need.
Working with a diverse set of clients across domains and industries
Implementing data orchestration pipelines, data sourcing, cleansing, and augmentation and quality
control processes
Deploying machine learning models in production
Supporting data architects in designing data architectures
Assisting in mentoring data engineers to further their personal and professional growth
Supporting project management operations of a project
Translating business needs into solutions
Contributing to overall solution, integration, and enterprise architecture
You’ll Bring:
2+ years of experience working on large scale, full lifecycle data implementation projects
BS/BA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience
A deep knowledge of performant SQL and understanding of relational database technology
Hands-on RDBMS experience (data modeling, analysis, programming, stored procedures)
A deep understanding of relational and warehousing database technology,
Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto
A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills
Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment
Strong interpersonal and analytical skills
Intellectual curiosity and an ability to execute projects
An understanding of “big picture” business requirements that drive architecture
and design decisions
DevOps and DataOps skills including “infrastructure as code” systems like
CloudFormation or Terraform
Data system performance tuning
Implementation of predictive analytics and machine learning models (MLlib,
scikit-learn, etc)
At
times, this role involves significant travel to client sites.
https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board
","['sql', 'spark', 'mllib', 'hive', 'scikit']","['pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data']",1,"['sql', 'spark', 'mllib', 'hive', 'scikit', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data']","['analyt', 'digit', 'machin', 'program', 'learn', 'spark', 'pipelin', 'set', 'relat', 'divers', 'predict', 'infrastructur', 'comput', 'appli', 'big', 'integr', 'engin', 'algorithm']","['sql', 'spark', 'mllib', 'hive', 'scikit', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'analyt', 'digit', 'machin', 'program', 'learn', 'spark', 'pipelin', 'set', 'relat', 'divers', 'predict', 'infrastructur', 'comput', 'appli', 'big', 'integr', 'engin', 'algorithm']"
DE,"Design and implement complex logic for calculating royalties from many products and hundreds of tailor-made license contracts
Design large scale data pipelines (billions and billions of data points) that can retrospectively correct metrics over a long period of time without reprocessing huge amounts of data
Continuously consume and produce massive amounts of data while optimising for speed, accuracy, and quality
Above all, working as a software engineer in Financial Engineering will challenge your design, quality and problem-solving skills to build robust, highly distributed and scalable data processing systems and pipelines.
What you'll do
Work closely with cross-functional teams of data and backend engineers, scientists, user researchers, product managers and designers
Gaining technical expertise in building a data platform at scale to solve business, product and technical use cases
Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio and Docker
Working hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies
Communicate insights and recommendations to key stakeholders, engineering and product partners
Who you are
Master's degree in Computer Science or Electrical Engineering
6+ years of professional software engineering and programming experience (Java, C++, Scala)
3+ years of architecture and design (patterns, reliability, scalability, quality) of complex systems
Advanced coding skills and practices (concurrency, distributed systems, functional principles, performance optimization)
Professional experience working in an agile environment
Strong analytical and problem solving ability
Strong written and verbal communication skills
Experience in operating and maintaining production grade software
Comfortable with tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions
Preferred Skills
Deep knowledge of software engineering best practices
Experience in mentoring and leading junior engineers
Experience in serving as the technical lead for complex software development projects
Experience with large scale distributed data technologies and tools
Strong coding skills for analytics and data engineering (Scala, Java and Python)
Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform
T-Shaped.
Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development
Understands how to translate business requirements to technical architectures and designs
Comfortable communicating with stakeholders (customers, product managers, C-level management)
","['google cloud', 'scala', 'bigqueri', 'cloud', 'java', 'python', 'c', 'docker']","['recommend', 'research', 'pipelin', 'optim', 'problem solving', 'commun']",2,"['google cloud', 'scala', 'bigqueri', 'cloud', 'java', 'python', 'c', 'docker', 'recommend', 'research', 'pipelin', 'optim', 'problem solving', 'commun']","['day', 'analyt', 'pipelin', 'primari', 'optim', 'amount', 'python', 'scientist', 'comput', 'engin']","['google cloud', 'scala', 'bigqueri', 'cloud', 'java', 'python', 'c', 'docker', 'recommend', 'research', 'pipelin', 'optim', 'problem solving', 'commun', 'day', 'analyt', 'pipelin', 'primari', 'optim', 'amount', 'python', 'scientist', 'comput', 'engin']"
DE,"Data Engineer, #3969
Entry level
Professional
Type of job
Full-time
Area of Expertise
IT/Cyber
Your job
Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software.
Build a system that builds and evaluates machine learning ...
Open job details
Published on 07/10/2020
","['cloud', 'microsoft', 'azur']","['machine learning', 'pipelin']",999,"['cloud', 'microsoft', 'azur', 'machine learning', 'pipelin']","['machin', 'engin', 'pipelin', 'azur']","['cloud', 'microsoft', 'azur', 'machine learning', 'pipelin', 'machin', 'engin', 'pipelin', 'azur']"
DE,"The work will support core business decisions and models that serve as the basis for core product and growth strategy.
Candidates should be able to choose the right tool for the job and learn how to use it if they don’t know how to already.
Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.
Requirements
You have at least 2 years in industry.
You are experienced with:
- SQL (Postgres in particular)
- Python
- Redis
- AWS
- Periscope or Looker
Optional Skills and Experience:
- Experience with Segment or another CDP
- Familiarity with Node
- Machine learning
- Data visualization, web and/or mobile (eg, D3.js)
- Spark, Hadoop, or Hive
","['sql', 'spark', 'looker', 'hadoop', 'node', 'aw', 'python', 'hive', 'postgr']","['machine learning', 'visual', 'segment']",999,"['sql', 'spark', 'looker', 'hadoop', 'node', 'aw', 'python', 'hive', 'machine learning', 'visual', 'segment']","['basi', 'machin', 'spark', 'visual', 'hadoop', 'aw', 'python']","['sql', 'spark', 'looker', 'hadoop', 'node', 'aw', 'python', 'hive', 'machine learning', 'visual', 'segment', 'basi', 'machin', 'spark', 'visual', 'hadoop', 'aw', 'python']"
DE,"You will be responsible for all aspects of the design, development and delivery of data and database solutions.
What you'll do...
Design, develop and implement data solutions.
This may include: architecture design, prototyping of concepts to proof of concept, development of standards, design and development of test plans, code and module design, development and testing, data solution debugging, design and implementation of a solution that follows efficient design techniques and development that meets and exceeds the intent of the design of the data solution.
Develop, construct, test and maintain optimal data pipeline architecture
Support and maintain data and database systems to meet business delivery specifications and needs.
Partner with a team of developers to effectively meet the deliverables and schedule of a data solution component within a larger application project.
Lend support to various business and technology teams as necessary to ensure solid, scalable, robust solutions.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using MySQL, Python and ETL Tools
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Prepare data for predictive analysis & use data to discover tasks that can be automated
Partner with internal stakeholders including the BI & Finance teams to assist with data-related technical issues and support their data infrastructure needs.
About you...
3-5 years of experience in a Data Engineer role
Degree in Computer Science, Statistics, Information Systems or equivalent field
Advanced working SQL knowledge and experience working with relational databases
Proven experience with Data Transformations & Integrity testing by using test-driven development methodologies.
Demonstrated experience with EXPLAIN, indexing and query optimizations
Prior experience with object-oriented/object function scripting languages: Python, Java, C++
Prior experience with Amazon AWS RDS & Aurora
Working knowledge of with Docker, Git & SDLC Methodologies
Knowledge of Salesforce Data APIs and SOQL
Experience building (or redesigning) and optimizing data pipelines, architectures and data sets.
Knowledge of non-SQL data sources: such as JSON, MongoDB, and consuming data from APIs
Prior experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Proven experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
Demonstrated experience with data security, encryption, HIPAA
Family friendly benefits:
Paid maternity and paternity
Fertility benefits (including egg freezing and IVF)
Emergency childcare program
Parent’s group
Paid vacation and summer flex time
Bonus program
401K Match
Monday breakfasts/ Friday lunches/ healthy snacks
Sit/ stand desks
","['mongodb', 'sql', 'git', 'salesforc', 'bi', 'aw', 'python', 'java', 'mysql', 'docker']","['pipelin', 'predict', 'optim', 'statist', 'financ', 'etl']",2,"['mongodb', 'sql', 'git', 'salesforc', 'powerbi', 'aw', 'python', 'java', 'docker', 'pipelin', 'predict', 'optim', 'statist', 'financ', 'etl']","['program', 'techniqu', 'pipelin', 'set', 'relat', 'predict', 'infrastructur', 'optim', 'bi', 'python', 'aw', 'statist', 'comput', 'etl', 'sourc', 'engin', 'integr']","['mongodb', 'sql', 'git', 'salesforc', 'powerbi', 'aw', 'python', 'java', 'docker', 'pipelin', 'predict', 'optim', 'statist', 'financ', 'etl', 'program', 'techniqu', 'pipelin', 'set', 'relat', 'predict', 'infrastructur', 'optim', 'bi', 'python', 'aw', 'statist', 'comput', 'etl', 'sourc', 'engin', 'integr']"
DE,"Â
Â
4+ years' work experience as a Data Engineer or similar role
2+ years of hands-on development experience in AWS
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Hands-on implementation experience in manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Experience with Dev/OPS architecture, implementation and operationÂ
3+ years software development experience with object-oriented programming languages, preferably Python
Knowledge of version control systems: Git, Azure Devops, Bitbucket
Knowledge of AWS cloud services: EC2, EMR, S3, RDS, Glue
Knowledge of data pipeline management tools: Airflow, Luigi, Tidal
Knowledge of data blending tools: Alteryx, Matillion, Attunity
Previous experience with the Snowflake database management system
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Jenkins + CI/CD, Kafka, Containerization (Docker), DBT
AWS certified
Â
Pavan Kumar | Sr IT Recruiter
E: pavan@conchtech.com | T: 9018081426
","['sql', 'spark', 'airflow', 'azur', 'ec2', 'git', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'cloud', 'kafka', 'docker']","['pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'azur', 'ec2', 'git', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'cloud', 'kafka', 'docker', 'pipelin', 'big data']","['analyt', 'program', 'stream', 'engin', 'spark', 'azur', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'â']","['sql', 'spark', 'airflow', 'azur', 'ec2', 'git', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'cloud', 'kafka', 'docker', 'pipelin', 'big data', 'analyt', 'program', 'stream', 'engin', 'spark', 'azur', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'â']"
DE,"OLAP, BI tools), and machine learning technologies* Experience operating systems in AWS* Excellent communication skills, including the ability to identify and communicate data-driven insightsFoursquare is proud to foster an inclusive environment that is free from discrimination.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.
","['aw', 'bi', 'excel']","['machine learning', 'commun']",999,"['aw', 'powerbi', 'excel', 'machine learning', 'commun']","['aw', 'bi', 'machin', 'learn']","['aw', 'powerbi', 'excel', 'machine learning', 'commun', 'aw', 'bi', 'machin', 'learn']"
DE,"The right candidate(s) will have one of or multiple skills in each category Programming (Java, Scala or Python) Computer Science Fundamentals (Algorithms and Data Structures) Distributed Computing (Spark or Parquet) Proficiency in DatabasesSQL, performance tuning, NoSQL, MongoElastic Web Application Architectures (HTTPHTMLCSS, JavaScript) FinancialFixed income Knowledge is a plus Spark, Scala and Functional Programming Event Driven Programming (Kafka, Akka, Flink or Spark Streaming) FrontEnd Development (Angular, Java Scripts, Reactive Programming) Kubernetes, Docker, AWS Infrastructure and Process Management What You Will Do Build complex data ingestion pipelines using Scala, Spark, Parquet and S3 Design scalable processes in event driven architecture to support Fixed Income applications Develop near real time streaming analytics using KafkaKinesis Act as Subject Matter Expert and help rest of the team in leveraging the platform and migrating applications to it Establish end to end data lineage and data catalogue.
Work with data governance team to setup data quality checks and metrics Create self-service notebook environment with ZeppelinJupyter for exploratory data analytics and rapid interactive development Troubleshoot any performance issues and ensure efficient data organization Build efficient web-based tools for monitoring and tracking What You Will Need Bachelorrsquos degree in computer science or related field required.
Masterrsquos degree preferred Knowledge of data structures, algorithms and functional programming Passion to learn new things, experiment with new ideas and build world class data platform 5+ years of experience in programming with Scala, Python or Java 2+ years of experience in Scala, Spark and functional programming.
Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation.
Strong experience with relational data bases, SQL, and query optimization.
Knowledge of data warehousing, dimensional data model and business intelligence is a plus.
Knowledgeexperience with event driven programming and Akka actor model
","['sql', 'spark', 'scala', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['exploratori', 'optim', 'data warehousing', 'pipelin']",1,"['sql', 'spark', 'scala', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'exploratori', 'optim', 'data warehousing', 'pipelin']","['analyt', 'program', 'stream', 'spark', 'pipelin', 'infrastructur', 'optim', 'aw', 'python', 'algorithm', 'comput', 'relat', 'evalu']","['sql', 'spark', 'scala', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'exploratori', 'optim', 'data warehousing', 'pipelin', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'infrastructur', 'optim', 'aw', 'python', 'algorithm', 'comput', 'relat', 'evalu']"
DE,"Position Summary
The data engineer will also be proactive in maintaining relationships with product, marketing, and analytics to ensure all data-related business needs are met.
Responsibilities
Gather requirements from business stakeholders (product, marketing, analytics) and effectively translate them into a data environment that supports business needs
Design and maintain data warehouse (PostgreSQL and BigQuery) and data lake (GCS)
Build batch and real-time data pipelines to ingest and process data from various sources such as product backends, third party platforms, etc.
Build internal tools and services to support data analytics and engineering
Help product, marketing, analytics, and engineering with diagnosing data-related issues
Document, profile, and manage data across multiple products and platforms for data governance (consistent naming conventions, data validation, data retention)
Maintain a line of communication between developers and data team to ensure all stakeholders are up to speed on changes to source database schema for analytics purposes
Work closely with other data engineering teams across Mosaic to ensure alignment of methodologies and best practices
Qualifications
2 - 4 years experience programming and scripting in Python within a production environment
Hands-on experience with relational databases (MySQL and PostgreSQL)
Familiarity with creating ETL processes for large-scale data warehouses
Must have familiarity with GCP and its offered services (BigQuery, CloudSQL, Dataprep, etc)
Knowledge of PubSub, or relevant messaging queue like Kafka, is a must
Distributed file systems (GCS, HDFS) knowledge is a must
Familiarity with Java, JavaScript, Airflow, Docker, Scala, and Spark are preferred
Familiarity with NoSQL is a plus
Prior experience with CI/CD is a plus
An amazing working environment with a lot of perks including but not limited to:
Unlimited PTO!
Matching 401k!
Fully stocked kitchen!
Environment where you can mentor and learn from others.
PRIVACY STATEMENT
","['gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'mysql', 'kafka', 'docker']","['etl', 'commun', 'pipelin']",999,"['gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'sql', 'kafka', 'docker', 'etl', 'commun', 'pipelin']","['analyt', 'spark', 'pipelin', 'relat', 'line', 'python', 'parti', 'warehous', 'etl', 'sourc', 'engin']","['gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'sql', 'kafka', 'docker', 'etl', 'commun', 'pipelin', 'analyt', 'spark', 'pipelin', 'relat', 'line', 'python', 'parti', 'warehous', 'etl', 'sourc', 'engin']"
DE,")* Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g.
bug fixes, backfills )* Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.
AWS), using DevOps tools and automating data pipelines.
* The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g.
git/github, secrets, configurations, yaml/json)* Experience with data modeling, design patterns, building highly scalable and secured solutions preferred* Min 2 years experience with job orchestration tools like Airflow, Luigi or similar* Experience with end-to-end testing of data pipelines and implementing Data Quality checks (e.g.
","['salesforc', 'aw', 'github', 'airflow']","['clean', 'pipelin', 'data modeling']",999,"['salesforc', 'aw', 'github', 'airflow', 'clean', 'pipelin', 'data modeling']","['aw', 'pipelin']","['salesforc', 'aw', 'github', 'airflow', 'clean', 'pipelin', 'data modeling', 'aw', 'pipelin']"
DE,"Are you interested in developing a large-scale data warehouse while implementing KPIs to measure the impact on end-user productivity?
Are you proficient in utilizing programming languages and have a strong data analytics experience?
Do you want to work for a highly skilled team of engineers that are doing both?
",[None],['kpi'],999,['kpi'],"['analyt', 'warehous', 'program', 'engin']","['kpi', 'analyt', 'warehous', 'program', 'engin']"
DE,"Using text, voice, or visual channels, they will have the ability to describe their content needs creatively and thoroughly.
Who you are:
As a Data Engineer on the Data Science team, you will have end-to-end autonomy and ownership of your projects, and will work closely with other business units to develop creative solutions to a variety of problems.
Your next challenge
You will join a team of highly-collaborative and curious Data Scientists and Data Analysts that are comfortable working with a diverse set of tools, and willing to take initiative on their ideas.
As a member of the team, you will have the chance to define the technical architecture that serves as the foundation for upstream analytical projects, and accelerate the delivery of a robust portfolio of Data Science models.
Your primary goal will be to catalyze the development and deployment of full-stack Machine Learning pipelines.
You will also have the opportunity to continuously evaluate and provide guidance on the use of new technologies.
What you’ll need:
You have prior experience working as a Data Engineer, preferably in a product or customer-focused organization.
You are extremely comfortable working with Python and have a working knowledge of Cloud services and Tools, as well as standard engineering tools such as Git, Linux and SQL.
You have experience building streaming and batch data pipelines and are comfortable working within a large-scale distributed environment with open source tools such as Hadoop, Hive, Airflow and Spark.
You understand, or have interest learning about, the real-world advantages and drawbacks of various Machine Learning techniques, and have applied those to a variety of datasets.
Nice to Have:
A M.S.
or PhD.
in computer science, statistics, economics/econometrics, natural science or any other equivalent quantitative project is preferred.
Previous experience in an analytical role, and experience working with teams of Data Scientists and Data Analysts.
Experience having managed or contributed to the use of Business Intelligence platforms such as Looker and Snowflake.
Experience with Marketing platforms such as Google Analytics and SA 360.
Due to the current conditions and impact of Covid-19, this key position will work from home and will require relocating to New York in the future.
All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
","['sql', 'linux', 'spark', 'airflow', 'looker', 'sa', 'git', 'hadoop', 'cloud', 'snowflak', 'python', 'hive']","['pipelin', 'visual', 'econometr', 'machine learning', 'statist']",2,"['sql', 'linux', 'spark', 'airflow', 'looker', 'sa', 'git', 'hadoop', 'cloud', 'snowflak', 'python', 'hive', 'pipelin', 'visual', 'econometr', 'machine learning', 'statist']","['techniqu', 'stream', 'visual', 'provid', 'python', 'quantit', 'sourc', 'evalu', 'analyt', 'challeng', 'sa', 'hadoop', 'appli', 'statist', 'learn', 'primari', 'set', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'scientist']","['sql', 'linux', 'spark', 'airflow', 'looker', 'sa', 'git', 'hadoop', 'cloud', 'snowflak', 'python', 'hive', 'pipelin', 'visual', 'econometr', 'machine learning', 'statist', 'techniqu', 'stream', 'visual', 'provid', 'python', 'quantit', 'sourc', 'evalu', 'analyt', 'challeng', 'sa', 'hadoop', 'appli', 'statist', 'learn', 'primari', 'set', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'scientist']"
DE,"If you love solving challenging problems and like to move fast in a growing and changing environment, this role is right fit for you.
Key Responsibilities Include:
· Design, develop, and maintain data pipelines to enable faster business analysis and reporting.
· Manage automated unit and integration test suites to ensure data correctness and consistency.
· Maintain source code repository of scripts (SQL, Python/R) and other products (dashboards, reports, etc.).
· Understand and document business processes and design a path to incorporate new initiatives into existing solutions (or build new ones where required)
Basic Qualifications
· 3+ years of experience as a Data Engineer or in a similar role
· Experience with data modeling, data warehousing, and building ETL pipelines
· Experience in SQL
· Bachelors degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
· Proficiency in one or more of the following languages - Python, Java, R or similar.
· A real passion for technology.
· Ability to communicate effectively and work independently with little supervision to deliver on time quality products
Preferred Qualifications
· Graduate degree in Computer Science, Engineering or related technical field.
· Understanding of Big Data technologies and solutions (Spark, EMR, Hive, S3, Redshift, etc.)
· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.
","['sql', 'spark', 'python', 's3', 'redshift', 'hive', 'java', 'r']","['pipelin', 'dashboard', 'data modeling', 'data warehousing', 'statist', 'big data', 'supervis', 'etl']",1,"['sql', 'spark', 'python', 's3', 'redshift', 'hive', 'java', 'r', 'pipelin', 'dashboard', 'data modeling', 'data warehousing', 'statist', 'big data', 'supervis', 'etl']","['analyt', 'spark', 'pipelin', 'relat', 'python', 'comput', 'statist', 'big', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'python', 's3', 'redshift', 'hive', 'java', 'r', 'pipelin', 'dashboard', 'data modeling', 'data warehousing', 'statist', 'big data', 'supervis', 'etl', 'analyt', 'spark', 'pipelin', 'relat', 'python', 'comput', 'statist', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"Small Teams; Big Data
Every contribution is important.
Google Cloud Platform (GCP), Google Dataflow/Beam, SQL, BigQuery
Scala, sbt, cats, http4s, fs2
Spark ML, TensorFlow, Kubeflow, Python, PyTorch
Airflow, Prometheus, Kubernetes
5+ years of experience making significant contributions in the form of code
Deep familiarity with one or more programming languages (Ex: Scala, Java, Python, etc.)
Interest in machine learning techniques to develop better predictive and clustering models
Experience working with high-scale systems
Proficiency in SQL
Interest in creating powerful machine learning tools that facilitate experimentation and productionalization at scale
Interest in data engineering and warehousing to develop ingestion engines, ETL pipelines, and organizing data to expose it in a consumable format
Passionate about helping mentor your teammates grow by providing insightful code reviews and feedback
Lead technical projects.
Taken responsibility for the planning, execution, and success of complex technical projects
Bonus Experience:
Experience with building systems for model training and serving using TensorFlow, Keras, PyTorch, Spark ML, or Kubeflow
Working with productionalization of Machine Learning research, and ML project lifecycle management
Understanding and genuine interest of AI and machine learning methods and algorithms, e.g., gradient based optimization and neural networks
Experience with functional programming
Strong understanding of concurrent and parallel programming
Solve complex problems building advanced software systems, while processing several petabytes of data
Adapt quickly to utilize software engineering best practices
Demonstrate the ability to deliver quality software collaboratively
Designing, implementing, and running big data pipelines that canvas over petabytes of data
Collaborate with your team of engineers, and add to product, account, and business development functions to create new products and features
Learn functional programming
Ensure that the software you create is testable and tested
Lead and drive technical projects.
Take responsibility for the planning, execution, and success of complex technical projects
Generous PTO - no accruing necessary
401k matching
), Coursera, LinkedIn learning, peer-lead professional development, and an abundance of resources to help you stay sharp
Unlimited snacks and beverages, collaboration catered lunches
Discounts on gym memberships
Are you up for the challenge?
","['sql', 'kera', 'gcp', 'google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'java', 'tensorflow', 'kubernet', 'pytorch', 'kubeflow']","['research', 'pipelin', 'neural network', 'predict', 'machine learning', 'optim', 'big data', 'etl', 'account']",999,"['sql', 'kera', 'gcp', 'google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'java', 'tensorflow', 'kubernet', 'pytorch', 'kubeflow', 'research', 'pipelin', 'nn', 'predict', 'machine learning', 'optim', 'big data', 'etl', 'account']","['machin', 'program', 'techniqu', 'learn', 'challeng', 'ml', 'spark', 'pipelin', 'power', 'predict', 'optim', 'python', 'big', 'etl', 'engin']","['sql', 'kera', 'gcp', 'google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'java', 'tensorflow', 'kubernet', 'pytorch', 'kubeflow', 'research', 'pipelin', 'nn', 'predict', 'machine learning', 'optim', 'big data', 'etl', 'account', 'machin', 'program', 'techniqu', 'learn', 'challeng', 'ml', 'spark', 'pipelin', 'power', 'predict', 'optim', 'python', 'big', 'etl', 'engin']"
DE,"Data Engineer
Job summary
You may be the one to join an awesome team and be part of iterative development, from compiling, vetting specifications, schema design through integration, testing and deployment If you are passionate about building a product that is solving a real-world problem that buyers and sellers will use every day.
Key Qualifications
Strong CS skills with 3+ years in professional data-oriented engineering experience.
Expert in at least of one or more of the following languages PHP/Ruby/Python/Java
Proven SQL and scripting skills.
Experience with AWS, EC2, Linux command-line.
Willingness to learn and try to technologies
Experience with non-relational and/or column-oriented data stores
Comfort working with large data sets.
Familiarity with Amazon Web Services (AWS)
Love data!
Charting, recording, exploring, and finding the underlying causes
Bonus
BA in Computer Science or related field
Experience with CSS Preprocessors (SaSS or less)
Familiarity with R
Previous startup experience
Perks
Benefits include Premium health benefits (including health,vision, dental), competitive salaries, equity, and a chance to do cutting-edge work with a great team
","['sql', 'linux', 'amazon web services', 'ec2', 'aw', 'java', 'python', 'php', 'r', 'rubi']",[None],1,"['sql', 'linux', 'aw', 'ec2', 'aw', 'java', 'python', 'php', 'r', 'rubi']","['day', 'set', 'relat', 'line', 'aw', 'python', 'comput', 'integr', 'engin']","['sql', 'linux', 'aw', 'ec2', 'aw', 'java', 'python', 'php', 'r', 'rubi', 'day', 'set', 'relat', 'line', 'aw', 'python', 'comput', 'integr', 'engin']"
DE,"In this role, you will…
Take complicated algorithms, code review, optimize for production and integrate into product features or process flows.
Design data architecture that is simple, fault tolerant and requires little overhead.
Design data pipelines utilizing ETL tools, event driven software or cloud functions and other streaming software.
This requires learning to speak the language of statisticians as well as software engineers.
Develop internal data science specific tooling for solutions such as A/B testing, learning sharing and analysis repositories and machine learning components.
Mentor data engineering team members and even data scientists in architecture and coding techniques.
You have 3-5+ years of experience as a software engineer or data engineer coding in Python and using SQL.
You are acquainted with designing custom machine learning pipelines that integrate into production environments that are customer facing.
You understand life is not all machine learning and simple pipelining is often extremely relevant to add business value.
You are obsessed with reducing lag, building scalable systems, optimizing performance, automating things and solving complex problems!
You have some awareness of machine learning concepts.
You have 3-5+ years of experience working in a consumer facing business.
You have experience working on product teams, but also collaborating with other data science team members.
You have a background in computer science or related.
You can communicate with a team and articulate ideas to both team members and non-technical stakeholders.
You have a drive to learn and master new technologies and techniques.
You have experience with relational cloud databases like Redshift, BigQuery, Snowflake, but also comfortable working with unstructured files and datasets.
You can expect...
Company-paid health, dental, vision, life & disability insurance
401(k) plan, FSA & commuter benefits
Generous PTO
Training, mentorship and coaching from leadership
Fun, diverse, open-minded coworkers
Dog companionship!!!
Technologies You Will Use
Python for data pipelining and automation.
Airflow for data pipelining.
Tableau for data visualization and consumer facing dashboards.
Many more to come!
Come join the team!
","['sql', 'airflow', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift', 'tableau']","['pipelin', 'dashboard', 'visual', 'machine learning', 'optim', 'etl']",999,"['sql', 'airflow', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift', 'tableau', 'pipelin', 'dashboard', 'visual', 'machine learning', 'optim', 'etl']","['machin', 'techniqu', 'learn', 'pipelin', 'relat', 'visual', 'divers', 'optim', 'python', 'algorithm', 'scientist', 'comput', 'etl', 'engin', 'integr']","['sql', 'airflow', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift', 'tableau', 'pipelin', 'dashboard', 'visual', 'machine learning', 'optim', 'etl', 'machin', 'techniqu', 'learn', 'pipelin', 'relat', 'visual', 'divers', 'optim', 'python', 'algorithm', 'scientist', 'comput', 'etl', 'engin', 'integr']"
DE,"These insights expedite trial timelines and reduce the costs of development, allowing safe and efficacious treatments to get to patients.
About the Opportunity
How often are you given the opportunity to build something from the ground up, with an abundance of resources at your disposal; to be part of a team of people accomplished in diverse scientific and engineering disciplines, focused on using the best of what lies at the forefront of technology to address complex, real-world problems that have a positive impact on potentially millions of peoples' lives?
This is that kind of opportunity.
Key Responsibilities
Create automation systems and tools to configure, monitor, and orchestrate data infrastructure and pipelines
Create data integration services to help onboard new customers as quickly as possible
Maintain ongoing reliability, performance, and support of the data infrastructure, providing solutions based on application needs and anticipated growth
Participate in creating and maintaining strict compliance, data privacy and security measures
Develop robust and production-level code to implement new product features in collaboration with other engineers and subject matter experts
Identify and resolve performance and scalability issues, troubleshoot problems, and improve product quality
Collaborate with the Front-End Development team to thread the right information through to forward-facing applications
Interface with the Development Operations colleagues to evaluate and implement methodologies and workflows to facilitate the frequent and continuous release of high-quality software
Work closely with Data Science colleagues to implement descriptive and predictive algorithms and models using the latest technologies
Keep up to date on emerging technology solutions, particularly those on AWS, for continuous improvements in data engineering
Help recruit highly capable engineers to the team from diverse backgrounds
Mentor and be mentored by engineers of varied experience levels and subject matter areas
Minimum Requirements
3+ years relevant experience with data engineering
Strong proficiency with Python (ideally PySpark) and SQL
Experience with AWS S3, EC2, EMR, or an equivalent cloud-hosted infrastructure
Experience with cloud-hosted database/data warehouse architecture (e.g.
Redshift, Snowflake, etc.)
Experience writing and productionizing complex data transformations in SQL and related frameworks
Interest in building distributed computing and orchestration frameworks (e.g.
Spark, Kubernetes, Airflow, etc.)
Experience working in an Agile software development environment
Exceptional written and verbal communication skills
Strong attention to detail and highly organized, with effective multi-tasking and prioritization skills
Proactive, self-motivated and self-directed, with the ability to learn quickly and autonomously
Comfortable with ambiguity
Superior problem-solving and troubleshooting skills
Ability to work as part of a collaborative cross-functional team in a fast-paced environment
Bachelors degree with strong academic performance in Computer Science, Software Engineering, Applied Science, or equivalent field
Preferred (Nice-to-have) Qualifications
Experience building and deploying large-scale data processing pipelines
Experience integrating data from disparate data sources
Experience with continuous integration and automation tools and processes (e.g.
Jenkins, Semaphore, etc.)
Experience with healthcare data, ideally clinical/operational clinical trial data
Knowledge of clinical data standards (e.g.
CDISC, FHIR, HL7, etc.)
Knowledge of e-clinical systems and technologies (e.g.
EDC, CTMS, IRT, etc.)
Employee Benefits
Competitive salary and equity compensation
Full medical, dental, and vision benefits
One Medical membership
401(k) plan
Flexible PTO policy
Generous parental leave
Great NYC office located in the heart of Times Square
Team events and outings
","['sql', 'pyspark', 'spark', 'airflow', 'ec2', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kubernet']","['healthcar', 'commun', 'pipelin', 'predict']",1,"['sql', 'pyspark', 'spark', 'airflow', 'ec2', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kubernet', 'healthcar', 'commun', 'pipelin', 'predict']","['predict', 'python', 'integr', 'sourc', 'algorithm', 'appli', 'releas', 'clinic', 'infrastructur', 'aw', 'warehous', 'employe', 'engin', 'particip', 'spark', 'pipelin', 'divers', 'comput', 'relat']","['sql', 'pyspark', 'spark', 'airflow', 'ec2', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kubernet', 'healthcar', 'commun', 'pipelin', 'predict', 'predict', 'python', 'integr', 'sourc', 'algorithm', 'appli', 'releas', 'clinic', 'infrastructur', 'aw', 'warehous', 'employe', 'engin', 'particip', 'spark', 'pipelin', 'divers', 'comput', 'relat']"
DE,"You would be welcomed to a friendly, inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.
The ideal candidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability and performance.
RESPONSIBILITIES
ETL Design and Development - Assist in the development of a big data platform in Hadoop using pipeline technologies such as Spark, Oozie, and more to support a variety of requirements and applications.
Warehouse Design and Development - Set the standards for warehouse and schema design in massively parallel processing engines such as Hadoop and Vertica while collaborating with analysts and data scientist in the creation of efficient data models.
Implement and support big data tools and frameworks such as HDFS, Hive, and Impala.
Implement and support streaming technologies such as Kafka and Spark.
Assist in the development of deployment automation and operational support strategies.
QUALIFICATIONS
7+ years of work experience with ETL, data modeling, and business intelligence big data architectures.
4+ years of experience with the Hadoop ecosystem (Map Reduce, Spark, Oozie, Impala, HBase, etc.)
and big data ecosystems (Kafka, Cassandra, etc.).
Expert in at least one SQL language such as T-SQL or PL/SQL.
Experience developing and managing data warehouses on a terabyte or petabyte scale.
Strong experience in massively parallel processing & columnar databases.
Experience working in a Linux environment.
Experience with Python and shell scripting.
Deep understanding of advanced data warehousing concepts and track record of applying these concepts on the job.
Ability to manage numerous requests concurrently and strategically, prioritizing when necessary.
Good communication skills.
Dynamic team player.
PREFERRED
Experience in real-time analytics applications.
Knowledge of the video game industry.
Experience with Python, Java or Scala programming languages.
Experience in implementing a machine learning pipeline.
Experience with Vertica.
Experience with Tableau administration.
","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'kafka']","['pipelin', 'data modeling', 'machine learning', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'kafka', 'pipelin', 'data modeling', 'machine learning', 'data warehousing', 'big data', 'etl', 'commun']","['analyt', 'machin', 'spark', 'pipelin', 'set', 'hadoop', 'python', 'warehous', 'scientist', 'big', 'etl', 'engin']","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'kafka', 'pipelin', 'data modeling', 'machine learning', 'data warehousing', 'big data', 'etl', 'commun', 'analyt', 'machin', 'spark', 'pipelin', 'set', 'hadoop', 'python', 'warehous', 'scientist', 'big', 'etl', 'engin']"
DE,"Java as a secondary language would be helpful.
Skills with technologies like Kafka, Airflow, Spark and Scala.
Heavy ETL experience is also very desirable.
Leadership experience is also preferable and they want the person to be able to mentor more junior resources and possibly lead them in the near future.
Candidates must have very strong communication and articulation skills
","['spark', 'airflow', 'scala', 'java', 'kafka']","['etl', 'commun']",999,"['spark', 'airflow', 'scala', 'java', 'kafka', 'etl', 'commun']","['etl', 'spark']","['spark', 'airflow', 'scala', 'java', 'kafka', 'etl', 'commun', 'etl', 'spark']"
DE,"Team members operate in a fast-paced, collaborative, service-oriented environment, where flexibility and ability to achieve results are valued.
Responsibilities will include:
• Assist in architecting, mapping, developing, and testing data movement to data warehouses (Redshift and SQL Server), with emphasis on the ETL process
• Develop, document, and execute SQL/stored procedures/server scripts as needed to support ETL code
• Identify and resolve data, technical issues and mediate business impact
• Collect requirements, design, build and test reports and dashboards across applications and programs
• Perform ad hoc analysis as required
Minimum Qual Requirements
1.
Graduation from an accredited college with a baccalaureate degree; or
Preferred Skills
The successful candidate should possess the following:
• Excellent writing and communication skills
• Knowledge and interest in computer systems and the latest technologies
• Familiar with AWS ecosystem including S3, Redshift databases, Lamda, EC2, Matillion and necessary supporting activities
• Ability to write complex procedures using SQL (i.e.
T-SQL, PL/pgSQL, etc.)
• Experience in generic object-oriented languages (i.e.
C#, Java, etc.)
• Experience in scripting languages (i.e.
Python, PowerShell, etc.)
• Experience in ETL tools (i.e.
Matillion, SSIS, Informatica, ets.)
• Excellent analytical, organization, presentation and facilitation skills; ability to handle multiple tasks under tight deadlines
AWS Redshift, SQL Server, Oracle, PostgreSQL).
• Familiarity with Data Warehouse concept (Kimball), fact tables, slowly changing dimensions types.
GitHub, GitLab or AWS CodeCommit).
• Analyze user requirements and convert requirements to design documents.
Additional Information
To request an accommodation, please contact MOCS Disability Service Facilitator at disabilityaffairs@mocs.nyc.gov or 646-872-0231.
STUDENT LOAN FORGIVENESS PROGRAM:
For additional information on the PSLFP, please visit https://studentaid.ed.gov/sa/repay-loans/forgiveness-cancellation/public-service.
External Applicants, please go to www.nyc.gov/jobs and search for Job ID#: 436136.
No phone calls, faxes, or personal inquiries permitted.
Only those candidates under consideration will be contacted.
Residency Requirement
","['sql', 'sa', 'ec2', 'postgresql', 'aw', 'python', 'redshift', 's3', 'c', 'java', 'github', 'excel', 'oracl']","['dashboard', 'etl', 'commun', 'analyz']",2,"['sql', 'sa', 'ec2', 'postgresql', 'aw', 'python', 'redshift', 's3', 'c', 'java', 'github', 'excel', 'oracl', 'dashboard', 'etl', 'commun', 'analyz']","['analyt', 'program', 'sa', 'public', 'aw', 'python', 'comput', 'warehous', 'etl', 'collect']","['sql', 'sa', 'ec2', 'postgresql', 'aw', 'python', 'redshift', 's3', 'c', 'java', 'github', 'excel', 'oracl', 'dashboard', 'etl', 'commun', 'analyz', 'analyt', 'program', 'sa', 'public', 'aw', 'python', 'comput', 'warehous', 'etl', 'collect']"
DE,"This team provides business reporting, tools, and analysis to senior business leaders, driving strategic decisions and profitability maximization throughout the organization.
Additionally, the team manages Bernstein’s commission sharing program (CSA), a client-facing function closely aligned with the sales and trading functions.
The group’s responsibilities include:
Design and development of all business metrics and performance reporting
Data Management
Bespoke client analysis
Modeling and scenario analysis
Development and implementation of business-process enhancements
Managerial accounting policy and implementation.
This role provides opportunities to build skills and responsibility.
You will interact directly with internal and external clients and work with managers across the Bernstein organization.
Desired Skills and Experience
Ideal candidates typically possess:
Excellent analytical and problem-solving skill
Strong Interpersonal skills and ability to collaborate across departments to resolve issues
Relentless intellectual curiosity and a drive to excel at the highest level
Facility with Python, SQL, and Excel is highly desired
Bachelor's degree in finance or related field with 2-3 years of experience in financial services is preferred.
Familiarity with the brokerage industry is also a plus.
Research has always been Bernstein's calling card.
New York City, New York
","['sql', 'python', 'excel']","['research', 'account', 'financ']",1,"['sql', 'python', 'excel', 'research', 'account', 'financ']","['analyt', 'relat', 'program', 'python']","['sql', 'python', 'excel', 'research', 'account', 'financ', 'analyt', 'relat', 'program', 'python']"
DE,"The Personalization team makes deciding what to play next easier and more enjoyable for every listener.
You will build data driven solutions to bring music and digital media experiences to hundreds of millions of active users and millions of creators by matching fans with creators in a personal and relevant way.
Above all, your work will impact the way the world experiences art.
What you'll do:
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are:
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You know how to write distributed, high-volume services in Java or Scala.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams.
","['google cloud', 'spark', 'cassandra', 'scala', 'bigtabl', 'hadoop', 'cloud', 'java']","['optim', 'pipelin', 'data modeling']",999,"['google cloud', 'spark', 'cassandra', 'scala', 'bigtabl', 'hadoop', 'cloud', 'java', 'optim', 'pipelin', 'data modeling']","['day', 'digit', 'techniqu', 'learn', 'spark', 'pipelin', 'ml', 'hadoop', 'optim', 'integr', 'engin']","['google cloud', 'spark', 'cassandra', 'scala', 'bigtabl', 'hadoop', 'cloud', 'java', 'optim', 'pipelin', 'data modeling', 'day', 'digit', 'techniqu', 'learn', 'spark', 'pipelin', 'ml', 'hadoop', 'optim', 'integr', 'engin']"
DE,"About TelTech
A member of the IAC family, TelTech builds innovative communication apps that help people better enjoy their mobile-connected lives.
Position Summary
This position will serve as the primary data engineer for TelTech.
A successful data engineer at Teltech will have significant data warehousing experience, Python development experience, and mastery of SQL.
The data engineer will also be proactive in maintaining relationships with product, marketing, and analytics to ensure all data-related business needs are met.
Responsibilities
Gather requirements from business stakeholders (product, marketing, analytics) and effectively translate them into a data environment that supports business needs
Design and maintain data warehouse (PostgreSQL and BigQuery) and data lake (GCS)
Build batch and real-time data pipelines to ingest and process data from various sources such as product backends, third party platforms, etc.
Build internal tools and services to support data analytics and engineering
Help product, marketing, analytics, and engineering with diagnosing data-related issues
Document, profile, and manage data across multiple products and platforms for data governance (consistent naming conventions, data validation, data retention)
Maintain a line of communication between developers and data team to ensure all stakeholders are up to speed on changes to source database schema for analytics purposes
Qualifications
2 - 4 years experience programming and scripting in Python within a production environment
Hands-on experience with relational databases (MySQL and PostgreSQL)
Familiarity with creating ETL processes for large-scale data warehouses
Must have familiarity with GCP and its offered services (BigQuery, CloudSQL, Dataprep, etc)
Knowledge of PubSub, or relevant messaging queue like Kafka, is a must
Distributed file systems (GCS, HDFS) knowledge is a must
Familiarity with Java, JavaScript, Airflow, Docker, Scala, and Spark are preferred
Familiarity with NoSQL is a plus
Prior experience with CI/CD is a plus
Why TelTech?
An amazing working environment with a lot of perks including but not limited to:
Unlimited PTO!
Matching 401k!
Fully stocked kitchen!
Environment where you can mentor and learn from others.
PRIVACY STATEMENT
","['sql', 'gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'mysql', 'kafka', 'docker']","['data warehousing', 'etl', 'commun', 'pipelin']",999,"['sql', 'gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'kafka', 'docker', 'data warehousing', 'etl', 'commun', 'pipelin']","['analyt', 'spark', 'pipelin', 'relat', 'line', 'primari', 'python', 'parti', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'gcp', 'spark', 'airflow', 'scala', 'bigqueri', 'javascript', 'python', 'java', 'nosql', 'postgresql', 'kafka', 'docker', 'data warehousing', 'etl', 'commun', 'pipelin', 'analyt', 'spark', 'pipelin', 'relat', 'line', 'primari', 'python', 'parti', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Applicants are considered for positions and are evaluated without regard to mental or physical disability, race, color, creed, religion, sex, gender, national origin, ancestry, age, genetic information, military or veteran status, sexual orientation, gender identity or expression, marital status, pregnancy or any other legally protected status under applicable law.
If you require assistance or reasonable accommodation during any aspect of the application process, please contact [the Human Resources Department or the hiring manager].We thank all applicants in advance for their interest in this position, however, only those selected for an interview will be contacted.Motivate LLC., does not offer sponsorship of any kind for this position.Other details* Pay Type Salary* New York, NY, USA* Washington, DC, USA
",[None],[None],999,[],"['human', 'evalu']","['human', 'evalu']"
DE,"You will assist in the design, implementation and maintenance of tools that extract and manipulate data from various sources, including in-house and external databases, for use in the research and development of Computational Pathology tools and algorithms.
You Are
A problem solver with the ability to think outside of the box, to find novel solutions to obstacles and setbacks.
A teammate with the ability to work well both independently and within a diverse team.
Detail and deadline oriented, with the ability to proofread, thoroughly test, and submit high quality work on time.
An effective communicator with strong interpersonal skills.
Willing to learn new skills and adaptable to fluctuating workloads and deadlines.
You Will
Create software and data pipelines that enable the ingestion, transformation and transfer of large quantities of structured and unstructured clinical data from various databases and filesystems sources, that are destined for the development of computation pathology applications and algorithms.
Build database logic to automatically fetch and store data in various forms.
Be responsible for server, application, and database development and the building and testing of high-performance, complex systems.
Produce required functional, technical, and user documentation (e.g., business requirements, functional and technical specifications, system architecture, data flows, end-users training requirements) on assigned projects.
Work and collaborate with scientists, engineers, IT operations and medical doctors to build tools manipulating data in order to build a new generation of artificial intelligence applications for cancer detection and treatment.
Learn the Pathology Department’s laboratory and diagnostic procedures as they pertain to the generation and flow of data in Digital and Computational Pathology.
Provide consultation and guidance to scientists, engineers, as well as other bioinformatics engineers and medical doctors, at the Center and partnering institutions.
Maintain and improve professional growth and development through participation in scientific and technical discussions, workshops, and seminars to keep current in the development of industry-grade software.
You Need
Bachelor’s degree in Computer Science, Information Systems, Biomedical Engineering or related field
4+ years of industry experience as a Data Engineer
Extensive experience in Python programming, or related language.
Extensive experience in the development of SQL database schema and query logic.
Familiarity with the design and architecture of cloud-based data warehouses and/or Data Lakes (Amazon Redshift, Snowflake).
Experience with the design, detailed testing, and documentation of complex systems.
Experience with version control standard methodologies.
Bonus
Extensive Expereice with design and architecture of cloud-based data warehouses and/or data lakes (Amazon Redshift, Snowflake)
Experience with modern DevOps practices & technologies (e.g.
Docker, Jenkins)
Experience with image processing software and techniques (e.g.
OpenCV) and familiarity with image file formats
#LI-POST
Closing
Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
","['sql', 'cloud', 'snowflak', 'python', 'redshift', 'docker']","['research', 'bioinformat', 'commun', 'pipelin']",1,"['sql', 'cloud', 'snowflak', 'python', 'redshift', 'docker', 'research', 'bioinformat', 'commun', 'pipelin']","['digit', 'program', 'techniqu', 'learn', 'pipelin', 'divers', 'provid', 'python', 'warehous', 'interpret', 'scientist', 'comput', 'clinic', 'relat', 'sourc', 'engin', 'particip', 'algorithm']","['sql', 'cloud', 'snowflak', 'python', 'redshift', 'docker', 'research', 'bioinformat', 'commun', 'pipelin', 'digit', 'program', 'techniqu', 'learn', 'pipelin', 'divers', 'provid', 'python', 'warehous', 'interpret', 'scientist', 'comput', 'clinic', 'relat', 'sourc', 'engin', 'particip', 'algorithm']"
DE,"The role
As a Data Engineer at Blue State, youll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations.
Youll design, build, and manage the systems and processes which form the underpinning of Blue States analytics work, supporting and working alongside data analysts and campaign strategists.
But youll also work directly with Blue States clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.
Day-to-day responsibilities:
Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects
Work with Blue States client organizations to solve difficult data migration, management, and integration challenges
Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes
Always maintain a high level of data security and privacy
The team
Youll work in either the NY or DC office.
Top things were looking for
Good foundational understanding of statistical analysis
Extensive experience working with SQL databases in an analytics or business intelligence context
Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools
Experience with ETL/ELT tools, processes, and best practices
Strong Python experience:
Python should be your go-to tool for solving problems.
Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus
Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks
Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:
Designing data warehouse schemas for cross-channel marketing analytics
Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data
Building and running automated tasks within the GCP environment - e.g.
Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler
Comfortable managing GCP IAM policies across projects and teams
Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if youve extended Google Sheets using Google Apps Script
Familiarity with Git and maintains good habits around code maintenance
Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)
Good at teaching others what you know.
Blue State is the purpose-driven creative and tech agency for brands and causes looking to inspire people to take action.
With clients including the AARP, Google, UNICEF, JDRF, and Colgate.
Blue State cultivates communities, builds platforms, and transforms how organizations engage their most important people.
","['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python']","['pipelin', 'dashboard', 'statist', 'etl', 'commun']",999,"['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python', 'pipelin', 'dashboard', 'statist', 'etl', 'commun']","['day', 'analyt', 'challeng', 'pipelin', 'line', 'python', 'warehous', 'statist', 'comput', 'action', 'big', 'etl', 'common', 'engin', 'integr']","['sql', 'google cloud', 'gcp', 'airflow', 'dask', 'bigqueri', 'git', 'cloud', 'python', 'pipelin', 'dashboard', 'statist', 'etl', 'commun', 'day', 'analyt', 'challeng', 'pipelin', 'line', 'python', 'warehous', 'statist', 'comput', 'action', 'big', 'etl', 'common', 'engin', 'integr']"
DE,"The right candidate(s) will have one of or multiple skills in each category:
Programming (Java, Scala or Python)
Computer Science Fundamentals (Algorithms and Data Structures)
Distributed Computing (Spark or Parquet)
Proficiency in Databases/SQL, performance tuning, NoSQL, Mongo/Elastic
Web Application Architectures (HTTP/HTML/CSS, JavaScript)
Financial/Fixed income Knowledge is a plus
Spark, Scala and Functional Programming
Event Driven Programming (Kafka, Akka, Flink or Spark Streaming)
FrontEnd Development (Angular, Java Scripts, Reactive Programming)
Kubernetes, Docker, AWS Infrastructure and Process Management
What You Will Do:
Build complex data ingestion pipelines using Scala, Spark, Parquet and S3
Design scalable processes in event driven architecture to support Fixed Income applications
Develop near real time streaming analytics using Kafka/Kinesis
Act as Subject Matter Expert and help rest of the team in leveraging the platform and migrating applications to it
Establish end to end data lineage and data catalogue.
Work with data governance team to setup data quality checks and metrics
Create self-service notebook environment with Zeppelin/Jupyter for exploratory data analytics and rapid interactive development
Troubleshoot any performance issues and ensure efficient data organization
Build efficient web-based tools for monitoring and tracking
What You Will Need:
Bachelors degree in computer science or related field required.
Masters degree preferred
Knowledge of data structures, algorithms and functional programming
Passion to learn new things, experiment with new ideas and build world class data platform
5+ years of experience in programming with Scala, Python or Java
2+ years of experience in Scala, Spark and functional programming.
Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation.
Strong experience with relational data bases, SQL, and query optimization.
Knowledge of data warehousing, dimensional data model and business intelligence is a plus.
Knowledge/experience with event driven programming and Akka actor model
","['sql', 'spark', 'scala', 'jupyt', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['exploratori', 'optim', 'data warehousing', 'pipelin']",1,"['sql', 'spark', 'scala', 'jupyt', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'exploratori', 'optim', 'data warehousing', 'pipelin']","['analyt', 'program', 'stream', 'spark', 'pipelin', 'infrastructur', 'optim', 'aw', 'python', 'algorithm', 'comput', 'relat', 'evalu']","['sql', 'spark', 'scala', 'jupyt', 'angular', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'exploratori', 'optim', 'data warehousing', 'pipelin', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'infrastructur', 'optim', 'aw', 'python', 'algorithm', 'comput', 'relat', 'evalu']"
DE,"The Role
You love designing creative solutions to complex problems and are always thinking about how you can automate processes.
You care deeply about your work and always seek to grow your knowledge and skills.
If you are excited about growing your technical career and want to be in an environment that fosters growth and mentorship this is the place for you.
What You'll Do
Collaborate closely with senior engineers to design and develop new product features
Identify solutions to client requests
Qualifications
Fluent in at least one programming language
Detailed-oriented and able to work across departments
Motivated self-starter
Strong technical communication skills
Experience with Python/Django a plus but not expected
Proficiency in SQL is a plus
Visa sponsorship for this role is currently not available.
Diversity
","['sql', 'django', 'python']",['commun'],999,"['sql', 'django', 'python', 'commun']","['program', 'divers', 'avail', 'python', 'engin']","['sql', 'django', 'python', 'commun', 'program', 'divers', 'avail', 'python', 'engin']"
DE,"What You Will Do
Build robust and scalable tooling in Python to aid in Diagnostics, Analysis and Inference which will lead to pipeline and platform optimizations
Work on a small team and take ownership of high impact projects
Work in a fast paced environment with continuous deployment and rapid product iteration following standard DevSecOps practices
Understand how the futures of media, entertainment and advertising are being defined by the innovative applications of technology and data
Learn about how a startup is built from an early stage
Self-driven individuals who take ownership of their work
Ability to build products quickly and efficiently
Strong understanding of software engineering practices and principles
Interpersonal and communication skills to work on a small team
Experience with Python, MySQL, background job frameworks (such as Airflow), and AWS is a plus
Benefits
Early-stage equity and competitive salary
Medical, dental, and vision insurance
Meals and snacks during work
Movie tickets, fitness discounts, commuter subsidies, and Apple hardware
","['mysql', 'aw', 'python', 'airflow']","['optim', 'hardwar', 'commun', 'pipelin']",999,"['sql', 'aw', 'python', 'airflow', 'optim', 'hardwar', 'commun', 'pipelin']","['learn', 'pipelin', 'aw', 'optim', 'python', 'engin']","['sql', 'aw', 'python', 'airflow', 'optim', 'hardwar', 'commun', 'pipelin', 'learn', 'pipelin', 'aw', 'optim', 'python', 'engin']"
DE,"You will build distributed services and large scale processing systems that will support various teams to work faster and smarter.
",[None],[None],999,[],[None],[None]
DE,"#mustlovedogs
Where You'll Come In
This individual is expected to have strong opinions on tech stacks, while not being married to any one tool.
How You'll Make An Impact
Own and maintain all data-related tools
5+ years of experience working in either a data or software engineering related field
3+ years of experience working with Data directly
At least 2 years of recent experience working with BigQuery & GCP
Ability to put together an entire ETL pipeline from scratch using documentation without assistance
Benefits
Brand new dog-friendly office in Greenwich Village (complete with free-roaming, friendly dogs)
Comprehensive Healthcare, Dental, and Vision
Flexible PTO and WFH policy
Discounted fresh food for your pup
Strict daily belly rub quota
","['gcp', 'bigqueri']","['etl', 'pipelin', 'healthcar']",999,"['gcp', 'bigqueri', 'etl', 'pipelin', 'healthcar']","['etl', 'engin', 'pipelin', 'relat']","['gcp', 'bigqueri', 'etl', 'pipelin', 'healthcar', 'etl', 'engin', 'pipelin', 'relat']"
DE,"Want to leverage your experience and development skills in the Healthcare industry as a Data Engineer?
It is an extraordinary time to be in business.
Be part of building one of the largest independent technology and business services firms in the world.
No unsolicited agency referrals please.
Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, or any other legally protected status or characteristics.
You will need to reference the requisition number of the position in which you are interested.
Your message will be routed to the appropriate recruiter who will assist you.
Emails for any other reason or those that do not include a requisition number will not be returned.
Your future duties and responsibilities
As a Data Engineer , this team member will have a superior software development background that will facilitate design and development of technical solutions for new and existing Business Intelligence applications.
Interacts with Lead Developers, System Analysts, Business Users, Architects, Test Analysts, Project Managers and peer developers to analyze system requirements, design and develop software applications.
This position is on a team that is responsible for providing technical Business Intelligence solutions.
If you are looking for a new challenge and want to make a difference in the Healthcare Industry, this role is for you.
Required qualifications to be successful in this role
• The teams scope is to ensure that all the data related Pharmacy is Integrated in the Clients Corporate Warehouse and available for Analytics and reporting capabilities supporting the Business areas across the Enterprise.
Data will also be persisted on Big Data HADOOP Platform.
Team will be focused on Data Analysis, Unified Data Design, Business and Test driven Development, Deployment with focuses on Continuous Improvement Continuous Delivery (CICD).
• Conduct unit testing and participate with system testing as required.
Knowledge with Test Driven Development is strongly preferred.
• Fill out required documentation such as installation instructions and follow Client standards and procedures.
• Anticipate customers analytic needs and proactively conceptualize and champion high-value Business Intelligence solutions.
• Actively support products by providing prompt responses to customer problems and inquiries.
Make the customers experience first priority in all.
• Provide on-call support duties for Application Production Support, which could include off hours and weekends.
• Keep the Production Support team management informed of any issues or concerns.
• Participate/Assist with deployments to all environments including Production (which could occur after hours).
New development and support task responsibilities as assigned
REQUIRED QUALIFICATIONS TO BE SUCCESSFUL IN THIS ROLE
This position requires:
Technical Skill set required:
Knowledge of Data Warehousing and Data Integration Concepts is MUST
Strong Data Analysis skills and I Love Data Data Mindset
Understanding Data Modeling concepts
3+ Years in Teradata or Oracle or related DB experience
3+ years writing Advanced SQL knowledge and ability to tune queries
1+ years in Python or strong coding background in 1-language is a must C/C++ or JAVA
Good to Have:
Experience with HADOOP / Big Data
Experience writing ETL
GIT Version control,
UNIX/LINUX and shell scripting is a Plus
Knowledge of Pharmacy and Healthcare experience is a Plus.
Knowledge of Cloud Computing is a Plus.
Soft Skill set required:
Positive can do attitude
Good Team player
Self-learner and strong problem solving skills
Work Collaboratively
","['sql', 'linux', 'unix', 'git', 'hadoop', 'cloud', 'db', 'java', 'python', 'c', 'oracl']","['healthcar', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']",999,"['sql', 'linux', 'unix', 'git', 'hadoop', 'cloud', 'db', 'java', 'python', 'c', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']","['analyt', 'challeng', 'corpor', 'set', 'relat', 'hadoop', 'provid', 'python', 'avail', 'warehous', 'comput', 'big', 'etl', 'engin', 'particip', 'integr']","['sql', 'linux', 'unix', 'git', 'hadoop', 'cloud', 'db', 'java', 'python', 'c', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl', 'analyt', 'challeng', 'corpor', 'set', 'relat', 'hadoop', 'provid', 'python', 'avail', 'warehous', 'comput', 'big', 'etl', 'engin', 'particip', 'integr']"
DE,"This position is full-time and 100% remote with the option of working on-site once it is safe to do so.
* You've built and maintained an ETL pipeline using a data warehouse like BigQuery or Redshift* Data engineering experience, including SQL and manipulating large structured or unstructured datasets for analysis* Practical knowledge of how to build efficient end-to-end ML workflows* Familiarity with Python machine learning tools like scikit-learn, pandas, etc.
","['sql', 'panda', 'bigqueri', 'redshift', 'python', 'scikit']","['machine learning', 'etl', 'pipelin']",999,"['sql', 'panda', 'bigqueri', 'redshift', 'python', 'scikit', 'machine learning', 'etl', 'pipelin']","['machin', 'learn', 'pipelin', 'ml', 'python', 'warehous', 'etl', 'engin']","['sql', 'panda', 'bigqueri', 'redshift', 'python', 'scikit', 'machine learning', 'etl', 'pipelin', 'machin', 'learn', 'pipelin', 'ml', 'python', 'warehous', 'etl', 'engin']"
DE,"Data Engineer– Performance Data Service
The Managed Data Services team supports the full life-cycle of data in T-REX from ingestion, to curation, normalization, and delivery.
Demonstrable experience working in data management for products in the Financial Services industry
Experience in Structured Finance or related field
Ability to develop, create, test, and maintain data processing pipelines
Experience in recommending and implementing ways to improve data reliability, efficiency and quality
Experience in managing and supporting the acquisition of client and 3rd party data
Experience in working with clients on problem resolution
Responsibilities:
Define mapping from source documents to T-REX dictionaries, normalizing and automating the process
Integrate python wrappers for API queries in the data pipeline
Expand on and automate the data ingestion pipeline
Establish and maintain excellent relationships with data sourcing and integration partners
Skills & Qualifications
Required:
3+ years of Data Engineering experience
Proficient in Python and Querying languages
Experience working with APIs
Experience working in Financial markets
Exceptional written and verbal communication skills
Preferred:
Experience working with Structured Finance products and/or in the Structured Finance industry or related field
Experience working with AWS Textract, Xtractor, IvyTools, PdfToText and similar toolsets
What it’s like working at T-REX:
T-REX - People Hiring People:
T-REX is a team of people who love to put in good work and have fun together.
","['aw', 'python', 'excel']","['normal', 'commun', 'pipelin', 'financ']",999,"['aw', 'python', 'excel', 'normal', 'commun', 'pipelin', 'financ']","['pipelin', 'relat', 'aw', 'python', 'parti', '3rd', 'integr', 'sourc', 'engin']","['aw', 'python', 'excel', 'normal', 'commun', 'pipelin', 'financ', 'pipelin', 'relat', 'aw', 'python', 'parti', '3rd', 'integr', 'sourc', 'engin']"
DE,"This person will create solutions that compile heterogeneous data in various formats from multiple sources into a normalized data model which is consumed by a processing engine, applies business rules and provides reporting capabilities.
The candidate should have excellent problem solving and communication skills.
They should be able to articulate and document designs and concepts that will be utilized to solve complex data architecture problems, working with key stakeholders to prioritize needs.
The ideal candidate will thrive in a fast-paced environment, driven by innovation, and the creation of tools that have already demonstrated disruption in the marketplace.
You will demonstrate a special aptitude for rapidly understanding and solving complex problems and thrive on quickly implementing your solutions into executable code.
If you enjoy being part of an agile team that develops solutions at the speed of business, creating new software that solves problems now, and getting recognition for your efforts, this job is for you.
Please review the following job requirements carefully.
During the interview process, you will be expected to demonstrate mastery of required skills.
Key objectives/Results:
Deliver database solutions as an active (hands-on) part of a small multi-disciplinary team
Develop and maintain databases which include: data repositories, reporting and analytical databases
Deliver analysis of data from multiple sources and provide analytical results to target the business issues and engineer the data to fit the client needs
Design and build robust and scalable solutions for managing structured and unstructured data using traditional databases
Conduct end-to-end analysis which includes data gathering and requirements specification, analysis, ongoing scaled deliverables and presentations
Coordinate, prepare and perform data analysis and transformations to align the data to business rules.
Use this data to provide actionable information and business insights
Work through early stages of software life cycle to profile data and to create conceptual, logical and physical data model designs with appropriate structure and relationships for optimal performance
Provide gap analysis of available data against requirements to design future state solutions
Deliver documentation including technical designs, data flows, ERDs, and mapping documents
Responsibilities:
Work with client data owners and functional designers to create technical specifications and execution methods
Translation of business and technical requirements into scalable database solutions
Participate in all aspects of the application lifecycle: requirements analysis & definition, system design, implementation, testing, deployment and sustainment
Collaborate with the team's application architect to improve upon database and application designs and performance
Work with Business SMEs to obtain a greater understanding of the needs of the business and the types of analysis that support business goals
Interface with internal teams to extract, transform and load data from a wide variety of data sources with frequency varying from batch to on demand
Provide technical expertise and capabilities in SQL query writing and use of DBMS technology
Work closely with web development team in the ongoing development and maintenance of databases, applications or tools
Desired Skills & Experience
7+ years of database design and development work experience
Detailed knowledge of development with MS SQL Server 2016 or later, including new database features and T-SQL enhancements
Experience in ETL concepts and tools, including SSIS
Extensive experience in SQL programming: Procedures, Functions, Triggers, and dynamic SQL
Strong Skills in Data Modelling such as Star Schema, Snow Flake Schema, Fact and dimensional tables and loading data from multiple data sources
Normalization/De-normalization concepts, database design methodology
Experience in data analysis, data profiling, data transformation, data mapping from source to target database schemas and data cleansing
Experience creating technical design documentation
Solid understanding of data architecture, efficient database design, writing complex queries and query tuning and troubleshooting
Business leaders know that transforming I.T.
into a Service is a shift that needs to happen.
But many don't know how to get there.
as a Service.
If I.T.
is keeping customers awake, it's not working.
Fundamental change can feel like stepping into the abyss.
Were obsessed with return on energy and deliver return on investment.
","['sql', 'excel']","['normal', 'tune', 'optim', 'problem solving', 'etl', 'commun']",999,"['sql', 'excel', 'normal', 'tune', 'optim', 'problem solving', 'etl', 'commun']","['analyt', 'program', 'provid', 'optim', 'avail', 'appli', 'action', 'etl', 'sourc', 'engin', 'particip']","['sql', 'excel', 'normal', 'tune', 'optim', 'problem solving', 'etl', 'commun', 'analyt', 'program', 'provid', 'optim', 'avail', 'appli', 'action', 'etl', 'sourc', 'engin', 'particip']"
DE,"Work with data governance team to setup data quality checks and metrics Create self-service notebook environment with ZeppelinJupyter for exploratory data analytics and rapid interactive development Troubleshoot any performance issues and ensure efficient data organization Build efficient web-based tools for monitoring and tracking Qualifications (Required) Bachelor's degree in computer science or related field required.
Master's degree preferred Knowledge of data structures, algorithms and functional programming Passion to learn new things, experiment with new ideas and build world class data platform 5+ years of experience in programming with Scala, Python or Java 2+ years of experience in Scala, Spark and functional programming.
Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation.
Strong experience with relational data bases, SQL, and query optimization.
Knowledge of data warehousing, dimensional data model and business intelligence is a plus.
Knowledgeexperience with event driven programming and Akka actor model Excellent verbal and written communication skills Qualifications (Desired) Experience with AWS infrastructure, docker, ECSEKS, EMR, KafkaKinesis.
Front-end development experience with Angular, Java Script, Reactive Programming.
Knowledge or desire to learn Investment Management, Fixed Income and Finance Familiarity with NoSQL and ElasticSearch
","['sql', 'spark', 'elasticsearch', 'scala', 'angular', 'aw', 'python', 'java', 'nosql', 'excel', 'docker']","['optim', 'data warehousing', 'financ', 'exploratori', 'commun']",1,"['sql', 'spark', 'elasticsearch', 'scala', 'angular', 'aw', 'python', 'java', 'nosql', 'excel', 'docker', 'optim', 'data warehousing', 'financ', 'exploratori', 'commun']","['analyt', 'program', 'spark', 'evalu', 'optim', 'infrastructur', 'python', 'aw', 'comput', 'relat', 'algorithm']","['sql', 'spark', 'elasticsearch', 'scala', 'angular', 'aw', 'python', 'java', 'nosql', 'excel', 'docker', 'optim', 'data warehousing', 'financ', 'exploratori', 'commun', 'analyt', 'program', 'spark', 'evalu', 'optim', 'infrastructur', 'python', 'aw', 'comput', 'relat', 'algorithm']"
DE,"PRIMARY RESPONSIBILITIES Build and maintain large-scale batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.).
Design and support Data Lakes and Marts in Google CloudStorage, BigQuery, and NoSQL (Google Firestore MongoDB).
Implement high-quality test-driven code, participate in code reviews, and own the development of medium-size features.
Collaborate with data scientists and business analysts to enable self-service analytics and reporting.
Support Production systems off hours.
POSITION REQUIREMENTS Experience 2+ years of industry experience in software development or data engineering.
2+ years extracting insights from data.
1+ years hands-on experience manipulating data and building data pipelines.
Hands-on experience with relational and NOSQL databases.
Strong appreciation for test-driven development and developing code standards.
Competencies Strong understanding of Python or Scala.
Understanding of workflow orchestrators (ex.
Airflow, Oozie, or Azkaban).
Understanding of test-driven development techniques.
Ability to proactively communicate and collaborate across a growing distributed team.
Quality Control Help define and manage overall data quality objectives.
Implement high-quality source code that passes tests.
Achieve and maintain team defined quality thresholds that mitigate solution risks.
Please contact recruiter at 248-675-1196Please respond with your updated resume, contact information
","['mongodb', 'google cloud', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'nosql']","['etl', 'risk', 'pipelin']",999,"['mongodb', 'google cloud', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'nosql', 'etl', 'risk', 'pipelin']","['analyt', 'techniqu', 'pipelin', 'relat', 'primari', 'python', 'scientist', 'etl', 'sourc', 'engin', 'particip']","['mongodb', 'google cloud', 'airflow', 'scala', 'bigqueri', 'cloud', 'python', 'nosql', 'etl', 'risk', 'pipelin', 'analyt', 'techniqu', 'pipelin', 'relat', 'primari', 'python', 'scientist', 'etl', 'sourc', 'engin', 'particip']"
DE,"Data Engineer
through some of the worlds leading and most valued brands,
including: News, Sports, the Network, and the Television
Stations.
develop culturally significant content, while building an
organization that thrives on creative ideas, operational
expertise and strategic thinking.
--------------
consumers and customers, and make vital business decisions with
large revenue impacts.
Advertising technology plays a very
important role toward these goals, so as a Data Engineer
supporting the Data Science team in the Ad Ops department, you
will frame, pose and translate business problems to build
AI-powered solutions that directly contribute to data products.
You will analyze the content and advertising data and work on
linking the two together and get the opportunity to work on
complex data pipelines and on the latest cloud-based technologies
like AWS lambda, SageMaker, and Apache Spark.
A SNAPSHOT OF RESPONSIBILITIES
Create and maintain data pipelines and process data to derive
insights form it
Collaborate with product management and engineering departments
Analyze ad-server logs to keep track of thousands of ad
campaigns on multiple devices and multiple types of digital
content
Implement methods to measure KPIs in presence of noise over
long periods of time
Develop and monitor models
Analyze user behavior on Web, linear and digital TV
Build and maintain cloud-based systems on AWS and other cloud
providers
Reconcile data arriving at high speed from multiple diverse
sources
Optimize models for on-device and multi-modal intelligence
WHAT YOU WILL NEED
Experienced Data Engineer with a BS, MS in a quantitative field
(CS, Engineering, Physics, etc.)
Experience with AWS infrastructure
Python ML packages, BOTO, Postgres/Redshift
Experience with Python.
Knowledge of one or two other
programming languages
Strong knowledge of relational and distributed databases
General knowledge and familiarity with digital advertising
ecosystem and stack
Log analysis.
Experience with ad server logs a plus
Experience working with third party data APIs
Knowledge of Spark and distributed data systems
Experience with AWS lambda and SageMaker a plus
Knowledge of Alteryx workflows will be a plus
Statistical Analysis/Inference will be a plus
resumes to annie@ingenium.agency
","['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'postgr', 'cloud']","['pipelin', 'optim', 'statist', 'analyz', 'kpi']",1,"['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'sql', 'cloud', 'pipelin', 'optim', 'statist', 'analyz', 'kpi']","['digit', 'engin', 'spark', 'pipelin', 'ml', 'relat', 'divers', 'power', 'infrastructur', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'quantit', 'sourc', 'linear']","['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'sql', 'cloud', 'pipelin', 'optim', 'statist', 'analyz', 'kpi', 'digit', 'engin', 'spark', 'pipelin', 'ml', 'relat', 'divers', 'power', 'infrastructur', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'quantit', 'sourc', 'linear']"
DE,"Data Engineer
YOUR OPPORTUNITY:
Develop solutions that enable investment professionals to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
YOUR SKILLS & TALENTS:
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker']","['research', 'commun']",1,"['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun']","['program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun', 'program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']"
DE,"What you'll be doing:
Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives
Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way
Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)
Collaborate within a small team with diverse technology backgrounds
Provide mentorship and guidance to junior team members
Team Responsibilities:
Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS
Ingest, validate and process internal & third party data
Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time
Maintain and enhance framework for jobs(primarily aggregate jobs in Hive)
Create different consumers for data in Kafka using Spark Streaming for near time aggregation
Train Developers/Analysts on tools to pull data
Tool evaluation/selection/implementation
Backups/Retention/High Availability/Capacity Planning
24*7 On call rotation for Production support
Airflow - for job scheduling
Docker - Packaged container image with all dependencies
Graphite/Beacon - for monitoring data flows
Hive - SQL data warehouse layer for data in HDFS
Impala- faster SQL layer on top of Hive
Kafka- distributed commit log storage
Kubernetes - Distributed cluster resource manager
Presto - fast parallel data warehouse and data federation layer
Spark Streaming - Near time aggregation
SQL Server - Reliable OLTP RDBMS
Sqoop - Import/Export data to RDBMS
Required Skills:
BA/BS degree in Computer science or related field
5+ years of software engineering experience
Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus
Knowledge and exposure to Cloud migration is a plus
Proficiency in Linux
Fluency in Python, Experience in Scala/Java is a huge plus
Strong understanding of RDBMS, SQL;
Passion for engineering and computer science around data
Willingness to participate in 24x7 on-call rotation
401(k) Match and free access to a financial advisor
Generous paid vacation/company holidays
Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance
$2,000 annual training and development budget
Complimentary annual memberships to One Medical, NY Citi Bike and SF Ford GoBike
Monthly chair massages
Free fitness classes (spin, yoga, boxing)
Gym reimbursement, local gym membership discounts
Onsite flu shots, dental cleanings and vision exams
Paid parental leave and a lot of new parent perks
Emergency childcare credits
Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities
Team lunches, Sip & Social Thursdays, Game Nights, Movie Nights
Healthy snacks and drinks
And there's a lot more!
","['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker']","['optim', 'clean', 'healthcar', 'cluster']",1,"['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker', 'optim', 'clean', 'healthcar', 'cluster']","['stream', 'spark', 'credit', 'divers', 'hadoop', 'provid', 'optim', 'python', 'avail', 'packag', 'parti', 'scientist', 'comput', 'warehous', 'relat', 'engin', 'evalu']","['sql', 'linux', 'spark', 'airflow', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'kubernet', 'kafka', 'docker', 'optim', 'clean', 'healthcar', 'cluster', 'stream', 'spark', 'credit', 'divers', 'hadoop', 'provid', 'optim', 'python', 'avail', 'packag', 'parti', 'scientist', 'comput', 'warehous', 'relat', 'engin', 'evalu']"
DE,"Data Engineer
Duration: Contract
•Must be able to do ETL (SSIS, Azure Data Factory, Informatica)
•Hands-on experience in Azure Data Factory, Informatica and good understanding on data obfuscation or data masking techniques
•Design, construct, install, test and maintain highly scalable data management systems
•Build automated data delivery pipelines and services to integrate data
•Build and deliver cloud-based deployment and monitoring capabilities consistent with DevOps models
•Develop solutions in agile environment for the overall data domain
•Deep experience with developing SQL
•Must have Deep experience deve
","['sql', 'cloud', 'azur']","['etl', 'pipelin']",999,"['sql', 'cloud', 'azur', 'etl', 'pipelin']","['techniqu', 'pipelin', 'azur', 'etl', 'engin']","['sql', 'cloud', 'azur', 'etl', 'pipelin', 'techniqu', 'pipelin', 'azur', 'etl', 'engin']"
DE,"The work will support core business decisions and models that serve as the basis for core product and growth strategy.
Candidates should be able to choose the right tool for the job and learn how to use it if they don't know how to already.
Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.Who You Are:You have at least 2 years in industry.
",[None],[None],999,[],['basi'],['basi']
DE,"Responsibilities Devise and develop data solutions leveraging various cloud-based data, database, and distributed computing technologies.
Please get back to me with your resume and contact details to satya.ksagarsft.com or you can reach me on 858-371-3588 860-924-5866.
Thanks, Satya K
",['cloud'],[None],999,['cloud'],[None],"['cloud', None]"
DE,"Job Details
Level
Experienced
Position Type
Full Time
Education Level
4 Year Degree
Salary Range
Undisclosed
Travel Percentage
Undisclosed
Job Shift
Day
Job Category
Information Technology
Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor's or Master's degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills
","['sql', 'r', 'python', 'vba', 'excel']","['recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']",1,"['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment']","['day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']","['sql', 'r', 'python', 'vba', 'excel', 'recommend', 'pipelin', 'data warehousing', 'information technology', 'etl', 'commun', 'database deployment', 'day', 'analyt', 'pipelin', 'relat', 'provid', 'python', 'scientist', 'comput', 'etl', 'engin']"
DE,"Data Engineer
Global Sports & Entertainment - Secaucus, NJ
Position Summary
The IT department services over 10 internal groups and the Data Engineer will be a seasoned technology leader comfortable with a variety of data technologies.The Data Engineering Group handles a data warehouse that sources data out of over 15 sources and services over 10 internal groups.
The current data technology stack consists primarily of data warehouses using SQL technologies, Neo4J-based Graph DBs along with the use of a variety of NoSQL and AI/ML frameworks and languages like R, Python etc.The ideal candidate will be a technologist who is able to balance the rapid pace of technology change with an authoritative ability to handle client relationships - including working closely with the business and technical teams/vendors, and assisting in the ongoing support of applications.
You'll need to wear many hats, so flexibility and a can-do attitude are critical!
The individual needs to have a consistent track record of successfully delivering value for their organization in a fast-paced environment along with successful management of customer expectations.
A passionate engineer who strives for automation would be ideal for this position.
Experience mentoring and leading other staff, both onsite and offshore will be required for this role to be successful.
As a Data Engineer, you will carry out the data efforts across all products and lines of business.
You must be comfortable switching between multiple projects, and working with both business teams and technology teams to translate business requirements into finished product.You possess strategic vision and tactical mastery and combine it with an entrepreneurial spirit to get it done.
This position reports into the Head of Data Engineering.
MAJOR RESPONSIBILITIES:
Understand business needs and develop solutions that delight consumers and customers
Understand Agile artifacts and develop applications based upon business priority.
Handle relationships with end user communities.
Interact regularly with users to gather feedback, listen to their issues and concerns, recommend solutions.
Help define and implement the Enterprise Data Architecture and help implement it in multi- functional alignment with the data teams that exist across functions, such as: Marketing, Finance, HR etc.
Provide leadership during application design and development for highly complex or critical machine learning projects across numerous lines of business and shared technology.
Evaluate readiness of new big/fast data capabilities to be used to support critical operational processes.
Build, maintain and demonstrate partnership with 3rd parties like Stanford Labs or Google Labs to be at the cutting edge of Data and ML technologies, and foster data-driven decision making.
Ensure cross-collaboration with sister teams in other geographies.
Ensure alignment to enterprise architecture and usage of enterprise platforms when delivering projects.
Provide oversight for budgets and project plans, and handle technology risks and issues.
Partner with your peers within the organization and build multi-functional alignment.
REQUIRED SKILLS/KNOWLEDGE
5+ years of experience developing Big Data and/or machine learning solutions
Experience with a modern cloud platform, such as AWS, Microsoft Azure, GCP, etc.
Experience with SQL, NoSQL, BigData and Graph Technologies along with Programming languages like R, Python, Kafka, Storm etc.
Experience building microservices
Background in Agile SW development and scaled Agile Frameworks
A true believer in measuring success based on working software and in quick prototyping
Someone who is a passionate coder and can spin up a snippet of code quickly
Strategic thinker with the ability to build and execute innovative digital products, combined with tactical ability to execute simultaneously against multiple contending priorities
Someone with an iterative approach, drive to move fast and think big
Demonstrated ability to partner and communicate effectively with non-technical team members, resolving contending or contradictory objectives, and unifying disparate ideas into a homogenized solution
Ability to be versatile and handle multiple projects and re-prioritizations
Possess the ability to influence others, implement change, and standardize processes in a complex business environment
A passion for mentoring and growing the potential of others
Ability to effectively and appropriately interview technical candidates
Passion for automation and hunger for acceleration; keen knowledge of DevOps as well as RPA is a big plus
Experience with architecting applications (e.g.
design Patterns, distributed applications etc.)
with the aim of reuse
Superb communication skills (both written and verbal)
Great teammate - should be ready to go beyond to help immediate team and do not be averse to not shy away from asking for help if needed.
Ability to translate ideas into solutions based on user and business needs
Open Eagerness to learn new technologies and bring new ideas to the table
Education
Bachelors Degree or equivalent.
Masters would be a plus
","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'microsoft', 'kafka']","['big data', 'graph', 'end user', 'risk', 'machine learning', 'financ', 'commun']",1,"['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'microsoft', 'kafka', 'big data', 'graph', 'end user', 'risk', 'machine learning', 'financ', 'commun']","['digit', 'machin', 'program', 'learn', 'ml', 'handl', 'azur', 'line', 'provid', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'sourc', 'engin', 'evalu']","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'r', 'db', 'nosql', 'microsoft', 'kafka', 'big data', 'graph', 'end user', 'risk', 'machine learning', 'financ', 'commun', 'digit', 'machin', 'program', 'learn', 'ml', 'handl', 'azur', 'line', 'provid', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'sourc', 'engin', 'evalu']"
DE,"About UsSimon Data was founded in 2015 by a team of successful serial entrepreneurs.
Simon's platform empowers businesses to use enterprise-scale big data and machine learning to power customer communications in any channel.
Simon's unique approach allows brands to develop incredible personalization capabilities without needing to build and maintain massive bespoke data infrastructure.Our culture is rooted in organizational transparency, empowering individuals, and an attitude of getting things done.
Simon's stack is composed of a wide variety of cutting-edge technologies, and you'll work on a diversity of projects that will accelerate your growth as an engineer.You are smart and passionate about data.
You love designing creative solutions to complex problems and are always thinking about how you can automate processes.
You care deeply about your work and always seek to grow your knowledge and skills.
",[None],"['machine learning', 'commun', 'big data']",999,"['machine learning', 'commun', 'big data']","['machin', 'learn', 'divers', 'power', 'big']","['machine learning', 'commun', 'big data', 'machin', 'learn', 'divers', 'power', 'big']"
DE,"Are you intrigued by large scale data sets?
Are you ready to pivot your strong data engineering background into a consulting position?
The consultancy is looking to add a Data Engineer to their team who will help them to implement and develop technology for data ingestion and data cataloguing for the data system.This is a prime opportunity to engage with a consultancy specializing in data strategy.
Data Engineer Requirements:
Bachelors degree and 4+ years experience
Strong experience with Kafka and Apache Airflow.
Background in Python and Sql.
Understanding on UNIX/Linux.
Ideally worked with cloud infrastructure like AWS.
Worked with technologies like Streamsets, Control -M for Data Pipeline.
#zr 25448
Does the opportunity to work with high-level stakeholders interest you?
Are you looking for an impactful role in consulting?
Do you want to join an organization with a high growth perspective?
","['sql', 'linux', 'airflow', 'unix', 'aw', 'cloud', 'python', 'kafka']",['pipelin'],1,"['sql', 'linux', 'airflow', 'unix', 'aw', 'cloud', 'python', 'kafka', 'pipelin']","['pipelin', 'infrastructur', 'aw', 'python', 'set', 'engin']","['sql', 'linux', 'airflow', 'unix', 'aw', 'cloud', 'python', 'kafka', 'pipelin', 'pipelin', 'infrastructur', 'aw', 'python', 'set', 'engin']"
DE,"Engineers who look at a data set and think about how to make sure it is correct.
",[None],[None],999,[],"['set', 'engin']","['set', 'engin']"
DE,"engineer.
The ideal candidate has experience with Python,
airflow, and BigQuery.
Experience with data modeling and
building data pipelines is required
Some responsibilities include:
Design
creative data architectures that align with business objectives
Collaborate
with team to ensure new sources of data / events are accounted
Deliver high
performance data pipelines that package data in a consumable
format for data
analysts
Requirements
3-4
experience as a Data Engineer / Analytics Engineer
Experience
with MongoDB & Bigquery / GCP
Deep
technical knowledge of and experience with Airflow
Ability to
translate business requirements into robust technical design
Ability to write
production quality ELT code with an eye towards performance and
maintainability
If interested please send resume to kevin@ingenium.agency
","['mongodb', 'gcp', 'airflow', 'bigqueri', 'python']","['data modeling', 'pipelin', 'account']",999,"['mongodb', 'gcp', 'airflow', 'bigqueri', 'python', 'data modeling', 'pipelin', 'account']","['analyt', 'pipelin', 'python', 'sourc', 'engin']","['mongodb', 'gcp', 'airflow', 'bigqueri', 'python', 'data modeling', 'pipelin', 'account', 'analyt', 'pipelin', 'python', 'sourc', 'engin']"
DE,"· Responsibilities
· Collaborate with client and partner data management teams to defines data transfer requirements and logistics
· Build and validate large-scale batch and real-time healthcare data ingestion pipelines with tools on the AWS platform (Redshift, S3, Airflow, Python)
· Manage the overall quality and security of enterprise data assets
· Manage the day-to-day operational data management needs
· Collaborate with data scientists and other engineers and to develop new data-driven product features and deploy machine learning models
Qualifications
· 2+ year experience working in a healthcare payer data operations role strongly preferred
· Strong experience required in Python ( pandas ) and Linux
· Strong experience with SQL preferred
· Strong experience with commercial and/or open-source ETL tool like Airflow
· Experience designing canonical enterprise analytic & reporting schemas
· Experience working on GCP and/or AWS platforms is a plus
· BA in Computer Science, Mathematics, Operations Research, or other related disciplines
· 4+ years of work experience is strongly preferred
","['sql', 'linux', 'gcp', 'airflow', 'panda', 'aw', 'redshift', 's3', 'python']","['research', 'healthcar', 'pipelin', 'machine learning', 'etl', 'logist']",1,"['sql', 'linux', 'gcp', 'airflow', 'panda', 'aw', 'redshift', 's3', 'python', 'research', 'healthcar', 'pipelin', 'machine learning', 'etl', 'logist']","['day', 'analyt', 'asset', 'machin', 'pipelin', 'relat', 'aw', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin']","['sql', 'linux', 'gcp', 'airflow', 'panda', 'aw', 'redshift', 's3', 'python', 'research', 'healthcar', 'pipelin', 'machine learning', 'etl', 'logist', 'day', 'analyt', 'asset', 'machin', 'pipelin', 'relat', 'aw', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin']"
DE,"Responsibilities Include:
• Support existing reporting utilizing Looker or Tableau developer, and Excel
• Continually identify and execute process improvements
• Problem solving / troubleshooting experience /skills to assist Production Support in issue resolution
Basic Qualifications:
Candidates must have 1+ years of experience as a data analyst or BI Developer
Excellent communication skills (must be able to interface with both technical and business leaders in the organization)
Superior written and oral communication skills
Expert knowledge of Amazon Redshift SQL or Google BigQuery SQL, and Excel functions
Bachelors Degree Required
Additional Qualifications:
• Data modeling in LookML and/or dbt
EQUAL EMPLOYMENT OPPORTUNITY
","['sql', 'looker', 'bigqueri', 'bi', 'tableau', 'redshift', 'excel']","['problem solving', 'commun', 'data modeling']",1,"['sql', 'looker', 'bigqueri', 'powerbi', 'tableau', 'redshift', 'excel', 'problem solving', 'commun', 'data modeling']",['bi'],"['sql', 'looker', 'bigqueri', 'powerbi', 'tableau', 'redshift', 'excel', 'problem solving', 'commun', 'data modeling', 'bi']"
DE,"Your Objective:u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Help build and oversee the data warehouse, data lake and own data quality for allocated areas of ownership, including defining and managing SLA for all data sets.
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Partner with the data science team to investigate and implement advanced statistical models and machine learning pipelines.
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Assist client support and sales with client integrations
What you have in your toolkit:
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1-3 years of experience in the data warehouse field
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience with object oriented programming languages
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience with schema design and dimensional data modeling
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience writing SQL statements
u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Bachelors degree or higher in Computer science, electrical engineering or related fields
Job Requirements:
",['sql'],"['machine learning', 'statist', 'pipelin', 'data modeling']",1,"['sql', 'machine learning', 'statist', 'pipelin', 'data modeling']","['machin', 'program', 'learn', 'pipelin', 'set', 'relat', 'comput', 'statist', 'warehous', 'integr', 'engin']","['sql', 'machine learning', 'statist', 'pipelin', 'data modeling', 'machin', 'program', 'learn', 'pipelin', 'set', 'relat', 'comput', 'statist', 'warehous', 'integr', 'engin']"
DE,"**
POSITION OVERVIEW
Summary: The Senior Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams.
The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives.
The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy.
The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies.
This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems.
Responsibilities
Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Create and maintain optimal data pipeline architecture.
Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality.
Partner with business leaders to determine and prioritize delivery initiatives.
Define or influence system, technical and application architectures for major areas of development.
Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance.
Engage with business partners to report (formally and informally) on technology strengths, weaknesses, successes and challenges on a regular basis.
Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture.
Highly organized and analytic, capable of solving business problems using technology.
Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time.
Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms.
Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data.
Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc.
QUALIFICATIONS
Undergraduate degree required, MBA or other advanced degree preferred.
6+ years of experience as a member of an information technology team.
Minimum of 3 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL
Minimum of 3 years experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc.
Ideally a strong Life Insurance background.
Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements.
Experience building solutions for streaming applications.
Should be able to lead critical aspects of the data management and application management.
Experience in UNIX shell scripting, batch scheduling and version control tools.
Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs.
Cultural awareness with excellent interpersonal and relationship building skills.
Passion and drive for continuous improvement and transformational change, with a business owner mentality.
Strong written and verbal communication skills.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status.
","['sql', 'spark', 'scala', 'unix', 'hadoop', 'bi', 'aw', 'python', 'java', 'cloud', 'db', 'excel']","['pipelin', 'dashboard', 'data modeling', 'optim', 'big data', 'information technology', 'etl', 'commun']",1,"['sql', 'spark', 'scala', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'cloud', 'db', 'excel', 'pipelin', 'dashboard', 'data modeling', 'optim', 'big data', 'information technology', 'etl', 'commun']","['basi', 'program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'challeng', 'hadoop', 'optim', 'releas', 'bi', 'aw', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat']","['sql', 'spark', 'scala', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'cloud', 'db', 'excel', 'pipelin', 'dashboard', 'data modeling', 'optim', 'big data', 'information technology', 'etl', 'commun', 'basi', 'program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'challeng', 'hadoop', 'optim', 'releas', 'bi', 'aw', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Bachelor's Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Nice to have:
Experience with version control systems (Git and Bitbucket)
Experience with Atlassian products Jira and Confluence
Experience with Docker containerization
knowledge of Application Programming Interfaces
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'airflow', 'git', 'jira', 'python', 'docker']",['pipelin'],1,"['sql', 'airflow', 'git', 'jira', 'python', 'docker', 'pipelin']","['basi', 'program', 'pipelin', 'python', 'comput', 'relat', 'common', 'engin', 'evalu']","['sql', 'airflow', 'git', 'jira', 'python', 'docker', 'pipelin', 'basi', 'program', 'pipelin', 'python', 'comput', 'relat', 'common', 'engin', 'evalu']"
DE,"In this role you will utilize a variety of data sets to maximize SIMONs earnings potential.
The ideal candidate would be half computer hacker & half number-cruncher.
This role will have the ability to directly impact the business bottom line and make a substantial contribution to a growing fintech startup.
Responsibilities
· Analyzing user behavior to maximize adoption of the platform
· Ingesting external data sets into a computational research environment
· Measuring performance of the platform to reduce operational cost
Minimum Qualifications
· Experienced in at least 1 numeric research framework (python/pandas, R/Splus, Octave/Matlab)
· Some background in probability/statistics
· Familiarity with various database designs (Relational, Columnar, nosql)
Preferred Qualifications
· Familiarity with AWS and infrastructure-as-code (terraform or cloud formation)
· Understanding of machine learning techniques
","['panda', 'aw', 'cloud', 'python', 'r', 'matlab']","['research', 'machine learning', 'probabl', 'statist', 'analyz']",999,"['panda', 'aw', 'cloud', 'python', 'r', 'matlab', 'research', 'machine learning', 'probabl', 'statist', 'analyz']","['machin', 'techniqu', 'learn', 'relat', 'line', 'infrastructur', 'aw', 'python', 'comput', 'statist', 'set']","['panda', 'aw', 'cloud', 'python', 'r', 'matlab', 'research', 'machine learning', 'probabl', 'statist', 'analyz', 'machin', 'techniqu', 'learn', 'relat', 'line', 'infrastructur', 'aw', 'python', 'comput', 'statist', 'set']"
DE,"This is a unique role that will allow this Data Engineer to work in Singapore for the first few months before coming back to work in NYC.
Qualifications:
Proven demonstrable experience designing and implementing ETL pipelines
Experience with any Data warehouses (Teradata / Vertica / Redshift / Snowflake)
Good working knowledge of RDBMS databases (PostgreSQL, Oracle, MySQL)
Extensive Hands on coding experience in Python/Java/Shell / Spark+scala
Good knowledge of testing tools (JMeter / Juint / Python unittest)
Working knowledge of Apache Spark and/or other distributed computing frameworks (MapRed)
Good at tuning and profiling complex Spark programs
Should have some Hadoop experience
Good to have:
Experience with Presto / SparkSQL
Groovy
Cucumber/Gherkin/Selenium
Some relevant AWS experience (EC2/ S3)
Banking and Financial Services
","['spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 's3', 'redshift', 'python', 'postgresql', 'mysql', 'oracl']","['etl', 'pipelin']",999,"['spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 's3', 'redshift', 'python', 'postgresql', 'sql', 'oracl', 'etl', 'pipelin']","['program', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'warehous', 'etl', 'engin']","['spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 's3', 'redshift', 'python', 'postgresql', 'sql', 'oracl', 'etl', 'pipelin', 'program', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'warehous', 'etl', 'engin']"
DE,"In this role you will develop easy to understand reports, dashboards, and tools with the aim of optimizing and streamlining the way data is viewed.
In this highly collaborative role, you will work closely with the architecture and data teams to reach business objectives.
What You'll Do:
Use SQL/Python to create reports, dashboards, and visualizations.
Aggregate/Model data and use that data to build reports in Domo
Analyze data to help improve business performance.
Identify the best data sources for a given analysis.
Develop processes for data mining, data modeling, and data production.
Offer insights and recommendations to improve data reliability and quality.
What You'll Need:
Bachelor’s degree in computer science, statistics or mathematics
3+ year’s experience with Python or Node.js
4+ year’s experience with SQL or mySQL
3+ year’s experience with cloud computing services (AWS)
Working understanding of big data analytics
Experience working with data cleaning and standardizing process
Privacy Disclosure:
All of the information collected in this form and/or by your application by submission of your online profile is necessary and relevant to the performance of the job applied for.
","['sql', 'aw', 'cloud', 'python', 'mysql']","['recommend', 'big data', 'data mining', 'dashboard', 'visual', 'data modeling', 'statist', 'analyz']",1,"['sql', 'aw', 'cloud', 'python', 'recommend', 'big data', 'data mining', 'dashboard', 'visual', 'data modeling', 'statist', 'analyz']","['analyt', 'visual', 'aw', 'python', 'comput', 'statist', 'appli', 'big', 'sourc', 'collect']","['sql', 'aw', 'cloud', 'python', 'recommend', 'big data', 'data mining', 'dashboard', 'visual', 'data modeling', 'statist', 'analyz', 'analyt', 'visual', 'aw', 'python', 'comput', 'statist', 'appli', 'big', 'sourc', 'collect']"
DE,"This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
",[None],[None],999,[],[None],[None]
DE,"And, it currently doubles every 5.3 months!
The Role:
As a Sr. Data Engineer, you will own Vydias multitude of data pipelines.
As a leader on the Data team, your responsibilities will include:
Ensuring the availability and timely delivery of data, company-wide
Modeling new data sets and crafting all new ELT workflows and pipelines
Lead the orchestration of the workflows and contribute strongly to infrastructure decisions
Mentoring and guiding your junior colleagues and leading with vision, and with respect to the companys data strategy
Requirements:
About You:
You are a Python pro and have regularly used AWS or Google Cloud Platform to manage data and move it between applications
Do you love APIs?
When you encounter a new one, do you study it inside and out and learn every corner of it, as though you designed it yourself?
Working with deeply-nested, complex JSON is a fun day at the office for you.
You can articulate the merits and pitfalls of the different approaches in designing a pipeline.
You are passionate about data quality control and know how and where to anticipate potential errors.
Working in the cloud is not a point of distinction for you, it is a given.
You understand what it means to work at a tech startup.
Hopefully, this is what excites you more than anything else about working here.
You intuitively know how to extract value and insights from data.
You love the idea of building the data scene in NJ and being a leader in this community.
You have orchestrated workflows using Airflow and are familiar with the challenges and how to overcome them.
Critically, you're a person who thinks in data; you relate the real world to data, and vice-versa.
","['google cloud', 'airflow', 'aw', 'cloud', 'python']","['commun', 'pipelin']",999,"['google cloud', 'airflow', 'aw', 'cloud', 'python', 'commun', 'pipelin']","['day', 'challeng', 'pipelin', 'infrastructur', 'aw', 'avail', 'python', 'set', 'engin']","['google cloud', 'airflow', 'aw', 'cloud', 'python', 'commun', 'pipelin', 'day', 'challeng', 'pipelin', 'infrastructur', 'aw', 'avail', 'python', 'set', 'engin']"
DE,"This role is based in NYC.DescriptionData and information has become an invaluable asset for technology companies that focus on innovation and are dedicated to providing a great user experience.
This squad and its product area are at the center of this initiative.
You will partner with business and engineering teams to prioritize requests and consistently deliver value against time-sensitive priorities.
If you have a desire to exhibit and grow your technical skills, and are motivated by working directly with stakeholders, product managers and engineers, then this is a great opportunity to join the band.Who you are* Who you areAn BS/MS in CS or any other relevant fields of study* You have 3+ years of experience in the development of high-quality database and data solutions.
* Strong analytical and problem solving ability* Have worked in a team with both Data Engineers and Data Scientists* You are capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions* You are a self-motivated individual contributor and great teammate with the ability to multitask, prioritize and communicate progress in a rapidly changing environment.
* Would like to build skills to further enhance the t-shape within analytics and data engineering* Strong coding skills in preferably Scala, Java and Python* Strong communication and data presentation skills (such as Tableau, PowerPoint, Qlik, etc.
","['qlik', 'scala', 'tableau', 'java', 'python', 'powerpoint']","['problem solving', 'commun']",1,"['qlik', 'scala', 'tableau', 'java', 'python', 'powerpoint', 'problem solving', 'commun']","['day', 'asset', 'analyt', 'python', 'scientist', 'engin']","['qlik', 'scala', 'tableau', 'java', 'python', 'powerpoint', 'problem solving', 'commun', 'day', 'asset', 'analyt', 'python', 'scientist', 'engin']"
DE,"Role Background:
Well expect you to havean in-depth knowledge of distributed systems and data flows.
Combinedwith an understanding of business intelligence and performance requirements,youll breathe life into data and help make it an invaluable part of theplatform and business.
If you are aself-starter, excited about building a culture around data-driven decisions,motivated by making an impact, and pushing the boundaries of your knowledge,you will excel here and do great things!
The Role
Design, implement, and maintain an ever-growing ETL pipeline using state-of-the-art technology
Use best practices and standards for managing large collections of data for analytics
Discover and integrate new heterogeneous data sources
Work closely with data analysts, data scientists, and product managers enabling them to provide insight into key performance metrics of the business
Help to improve data reliability, efficiency, and quality
Your Qualifications:
4+ years of experience designing and developing a data warehouse on a distributed database platforms, such as Snowflake or Redshift
Experience designing, developing, and maintaining high-throughput and low-latency ETL pipelines
Experience with data modeling, data access, and data storage techniques
Experience with big data tools such as Apache Spark
Proficient in SQL and Python
Proficient with at least one RDBMS (MySQL or Postgres preferred)
Successfully implemented data pipelines in the public cloud, especially Amazon Web Services
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Benefits & Perks
Robust health benefits packages including access to a 401k and various medical, dental and vision plans, and $100/month fitness reimbursement
Full support for remote work during COVID-19
Daily lunch delivery credit and other goodies sent to home
Regular company-wide social events (even virtually!)
Generous annual education stipend toward job-related external learning opportunities
An extremely enthusiastic team that appreciates collaboration
","['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'postgr', 'mysql', 'amazon web services']","['etl', 'data modeling', 'pipelin', 'big data']",999,"['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'aw', 'etl', 'data modeling', 'pipelin', 'big data']","['analyt', 'techniqu', 'learn', 'spark', 'pipelin', 'relat', 'credit', 'public', 'python', 'warehous', 'packag', 'scientist', 'big', 'etl', 'sourc', 'collect']","['sql', 'spark', 'cloud', 'snowflak', 'python', 'redshift', 'aw', 'etl', 'data modeling', 'pipelin', 'big data', 'analyt', 'techniqu', 'learn', 'spark', 'pipelin', 'relat', 'credit', 'public', 'python', 'warehous', 'packag', 'scientist', 'big', 'etl', 'sourc', 'collect']"
DE,"Skills AWS, Hadoop, Spark, Python
7 years IT experience and 5 years exp in Big Data area
Knowledge on DWH concepts and SQL query skills
Need AWS experience
Experience developing and supporting Data warehouse ETL applications
Need someone who will help them to convert Pig Latin UDFs to Spark.
Familiar with UDFs
Good understanding of Spark Structured Streaming Vs RDD streaming
Prior exp to perform operations in Hive and Spark
Good experience in Python
Experience with NoSQL and SQL databases
Excellent analytical and problem solving skills
Excellent oral and written communication skills
","['sql', 'spark', 'pig', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'excel']","['etl', 'problem solving', 'commun', 'big data']",999,"['sql', 'spark', 'pig', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'excel', 'etl', 'problem solving', 'commun', 'big data']","['analyt', 'stream', 'spark', 'hadoop', 'aw', 'python', 'warehous', 'big', 'etl']","['sql', 'spark', 'pig', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'excel', 'etl', 'problem solving', 'commun', 'big data', 'analyt', 'stream', 'spark', 'hadoop', 'aw', 'python', 'warehous', 'big', 'etl']"
DE,"Data Engineer
Type - C2C / C2H / W2 / FT
Duration – 12-18 Months
Only GC or USC
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities:
• Gather requirement from Product Owner team; groom the userstories for the requirements and plan the stories for each sprints for project delivery.
• Design the database objects and develop procedures, functions, packages with scalability and high performance
• Extensively work in collections and table type objects with multiple level nested tables
• Work with stakeholders including Managemet team, Product Owners and Architecture teams to assist with data-related technical issues and support thier data infrastructure needs.
• Identify, design and implement internal process improvements: automating manual process and optimizing data delivery.
• Maintain the source codes using version control tools like github and work with DevOps team for seamless deployment of database objects to various environments.
• Prepare Conceptual, Logical and Physical data models for the Databases using tools like Erwin Data Modeler.
Technologies and Tools:
Oracle 11g/12c, Toad for Oracle, Linux, Putty, GitHub, Rally,
Nice to have: Informatica, Python, Aurora, and Hive
Qualifications:
Experience building and optimizing Oracle databse objects like Packages, Procedures, Functions and SQL queries.
Advanced hands-on SQL knowledge and experience working with relational databases for data querying and retrieval.
Experince in performance tuning of DB using Collections and Bulk operations.
Experience in Agile Methodology practice for continuous delivery of the project increments.
Nice to have:
Experience with big data frameworks/tools: Hadoop, AWS, Kafka, Spark, etc.
Experience with relational SQL and NoSQL databases, including Oracle, Hive, HBase.
Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
Experience with data security.
Experience with Java and/or Python
Working with product owners to review the requirement, analyze the business logic, and translate that to the details tasks of the particular user stories in Rally;
Participate daily scrum meetings to collaborate with other scrum team members;
Working closely with team members to implement the requirement captured in the Rally user stories;
Deliver the commitment made to the program increment (PI).
","['sql', 'linux', 'spark', 'hadoop', 'aw', 'db', 'python', 'hive', 'java', 'nosql', 'hbase', 'github', 'kafka', 'oracl']","['tune', 'pipelin', 'big data']",999,"['sql', 'linux', 'spark', 'hadoop', 'aw', 'db', 'python', 'hive', 'java', 'nosql', 'hbase', 'github', 'kafka', 'oracl', 'tune', 'pipelin', 'big data']","['program', 'engin', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'packag', 'big', 'relat', 'sourc', 'collect', 'particip']","['sql', 'linux', 'spark', 'hadoop', 'aw', 'db', 'python', 'hive', 'java', 'nosql', 'hbase', 'github', 'kafka', 'oracl', 'tune', 'pipelin', 'big data', 'program', 'engin', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'packag', 'big', 'relat', 'sourc', 'collect', 'particip']"
DE,"Are you a Data Engineer that likes to build critical data infrastructure?
Do you have an interest in software engineering, data warehousing and business analytics?
If so, this exciting Media & Technology organization based out of New York will motivate you to learn new technologies!
What's The Job?
You will also develop automation processes using the newest technologies.
What Skills Do You Need?
Experience working with Python & SQL Languages.
Experience using SQL Databases.
Experience using tools such as Spark and Airflow.
Experience with GCP or AWS Cloud.
Prior experience with Java, Go, Bash, Docker or Kubernetes is a major PLUS!
Compensation
$120,000 - $150,000 base salary
Generous vacation and parental leave
Full Medical, Dental and Vision
401(K) Matching
","['sql', 'gcp', 'spark', 'airflow', 'aw', 'cloud', 'java', 'python', 'kubernet', 'docker']",['data warehousing'],999,"['sql', 'gcp', 'spark', 'airflow', 'aw', 'cloud', 'java', 'python', 'kubernet', 'docker', 'data warehousing']","['analyt', 'spark', 'infrastructur', 'aw', 'python', 'engin']","['sql', 'gcp', 'spark', 'airflow', 'aw', 'cloud', 'java', 'python', 'kubernet', 'docker', 'data warehousing', 'analyt', 'spark', 'infrastructur', 'aw', 'python', 'engin']"
DE,"Impact
Innovation
If you are seeking an environment where you can drive innovation.
If you want the satisfaction of providing visible benefit to end-users in an iterative fast paced environment, this is your opportunity.
Opportunity
You will be part of a team of creative, top-notch software developers to work hard, have fun, and make history.
You will be working with very large data sets, well beyond the scalability limits of conventional relational databases.
Basic Qualifications
· Bachelors degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
· 5+ years relevant professional experience in Data Engineering and Business Intelligence
· 3+ years in with Advanced SQL (analytical functions), ETL, DataWarehousing.
· Advanced data analysis skills e.g.
pivot table.
· Strong knowledge of data warehousing concepts, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, data modeling and performance tuning.
· Ability to effectively communicate with both business and technical teams.
·
Preferred Qualifications
· Experience on working with Big Data
· Knowledge and experience on working with Hive and the Hadoop ecosystem
· Knowledge of Map Reduce, Spark and Presto
","['sql', 'hive', 'spark', 'hadoop']","['data modeling', 'data warehousing', 'statist', 'big data', 'etl']",1,"['sql', 'hive', 'spark', 'hadoop', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']","['analyt', 'spark', 'set', 'relat', 'hadoop', 'infrastructur', 'comput', 'statist', 'big', 'etl', 'engin']","['sql', 'hive', 'spark', 'hadoop', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'analyt', 'spark', 'set', 'relat', 'hadoop', 'infrastructur', 'comput', 'statist', 'big', 'etl', 'engin']"
DE,"A desire to innovate in the service of financial inclusion.
As SQL Developer, you play a key role.
In this role you will focus on SQL reporting.
If you like challenging problems are analytical, and a great team playerwe want to hear from you!
Responsibilities:
Using SQL Server Integration Services (SSIS), Python, C#
Design, develop, optimize, maintain and support ETL processes using data warehouse design best practices and SQL Server Integration Services (SSIS) to integrate data from multiple source systems into a Data Warehouse, cleansing data, transforming and loading data from multiple formats using SSIS and stored procedures
Create and optimize SQL Statements, Procedures and Functions
Modify databases according to requests and perform tests
Solve database usage issues and malfunctions
Liaise with developers to improve applications and establish best practices
Research and suggest new database products, services and protocols
A successful candidate will have hands-on experience in a multitude of domains; including, but not limited to database design, data warehousing, business intelligence, big data, database tuning, application optimization, security, virtual computing and storage, incident tracking, and general database administration
Documenting procedures for all tools / processes you create.
Qualifications:
Working independently or minimal supervision
Proven work experience as a Database developer
5 years' experience with SSIS
Hands on experience with SQL
Knowledge of software development and user interface web applications
Familiarity working with .Net Framework, JavaScript, HTML and Oracle
Excellent analytical and organization skills
An ability to understand front-end users' requirements and a problem-solving attitude
Excellent verbal and written communication skills
BSc degree in Computer Science or relevant field
Preferred qualifications:
Knowledge of Python, Datomic, Hadoop, Big data
Experience with C# .Net development
All employment is decided on the basis of qualifications, merit, and business need.
","['sql', 'hadoop', 'javascript', 'python', 'c', 'excel', 'oracl']","['research', 'end user', 'optim', 'data warehousing', 'big data', 'supervis', 'etl', 'commun']",1,"['sql', 'hadoop', 'javascript', 'python', 'c', 'excel', 'oracl', 'research', 'end user', 'optim', 'data warehousing', 'big data', 'supervis', 'etl', 'commun']","['basi', 'analyt', 'hadoop', 'optim', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'integr']","['sql', 'hadoop', 'javascript', 'python', 'c', 'excel', 'oracl', 'research', 'end user', 'optim', 'data warehousing', 'big data', 'supervis', 'etl', 'commun', 'basi', 'analyt', 'hadoop', 'optim', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'integr']"
DE,"Data Engineer
Advertising technology plays a very important role toward these goals, so as a Data Engineer supporting the Data Science team in the Ad Ops department, you will frame, pose and translate business problems to build AI-powered solutions that directly contribute to data products.
You will analyze the content and advertising data and work on linking the two together and get the opportunity to work on complex data pipelines and on the latest cloud-based technologies like AWS lambda, SageMaker, and Apache Spark.
A SNAPSHOT OF RESPONSIBILITIES
Create and maintain data pipelines and process data to derive insights form it
Analyze ad-server logs to keep track of thousands of ad campaigns on multiple devices and multiple types of digital content
Implement methods to measure KPIs in presence of noise over long periods of time
Develop and monitor models
Analyze user behavior on Web, linear and digital TV
Build and maintain cloud-based systems on AWS and other cloud providers
Reconcile data arriving at high speed from multiple diverse sources
Optimize models for on-device and multi-modal intelligence
WHAT YOU WILL NEED
Experienced Data Engineer with a BS, MS in a quantitative field (CS, Engineering, Physics, etc.)
Experience with AWS infrastructure
Python ML packages, BOTO, Postgres/Redshift
Experience with Python.
Knowledge of one or two other programming languages
Strong knowledge of relational and distributed databases
General knowledge and familiarity with digital advertising ecosystem and stack
Log analysis.
Experience with ad server logs a plus
Experience working with third party data APIs
Knowledge of Spark and distributed data systems
Experience with AWS lambda and SageMaker a plus
Knowledge of Alteryx workflows will be a plus
Statistical Analysis/Inference will be a plus
resumes to annie@ingenium.agency
","['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'postgr', 'cloud']","['pipelin', 'optim', 'statist', 'analyz', 'kpi']",1,"['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'sql', 'cloud', 'pipelin', 'optim', 'statist', 'analyz', 'kpi']","['digit', 'engin', 'spark', 'pipelin', 'ml', 'relat', 'divers', 'power', 'infrastructur', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'quantit', 'sourc', 'linear']","['spark', 'sagemak', 'aw', 'lambda', 'redshift', 'python', 'sql', 'cloud', 'pipelin', 'optim', 'statist', 'analyz', 'kpi', 'digit', 'engin', 'spark', 'pipelin', 'ml', 'relat', 'divers', 'power', 'infrastructur', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'quantit', 'sourc', 'linear']"
DE,"Data Engineer
JOB SUMMARY:
(EDW).
of a Scrum team to develop new, and support pre-existing Extract, Transform
& Load (ETL) processes.
people who can work effectively within a team as well as take the lead on new
initiatives.
JOB DUTIES AND RESPONSIBILITIES:
• Work with EDW &
Architecture groups to define and implement integration of new systems and
processes.
• Actively participate
in sprint planning, design, coding, unit testing, and sprint reviews.
• Participate in peer
code reviews and adhere to best practice development methodologies.
• Participate in an
on-call rotation to provide warehouse support if issues arise.
• Must be a
self-starter with a great attitude, not afraid to take the initiative, and work
with little supervision
• Preference given for
holders of a University Degree or Diploma in Computer Science or related field.
• Preferred experience
working within an Agile - Scrum environment
• Minimum 3 years ETL
• Minimum 3 years of
experience with SQL server
• Extensive knowledge
and experience using concepts including but not limited to:
o Stored Procedures/TSQL
o Query Performance Tuning
o Data Warehousing & OLAP
o Source control tools & concepts
(CI/CD, Git, Bitbucket, Github)
• Prior experience
working with PowerShell and/or Python would be an asset
• Practical experience
with property & casualty insurance/reinsurance and multi-currency systems
is an asset
• Ability to analyze
users effectively
• Possess a systematic,
disciplined and analytical; approach to problem solving
About
Fairfax
Fairfax is a
casualty insurance and reinsurance and investment management.
About
and specialty insurance and reinsurance solutions.
client service through a global network of offices and branches.
Syndicate 2232 is rated A by Standard & Poor's and AA- by Fitch.
benefits package includes: Health and Dental Insurance, 401k plan, and Group
Term Life Insurance.
Opportunity and Affirmative Action Employer.
All qualified applicants
will be considered for employment without regard to an individual's race,
color, national origin, religion or belief, sex, age, genetic information,
marital or civil partnership status, family status, sexual orientation, gender
identity, or their protected veteran or disability status.
Please visit
","['sql', 'github', 'python', 'git']","['tune', 'data warehousing', 'problem solving', 'supervis', 'etl']",1,"['sql', 'github', 'python', 'git', 'tune', 'data warehousing', 'problem solving', 'supervis', 'etl']","['analyt', 'asset', 'relat', 'python', 'comput', 'packag', 'action', 'etl', 'sourc', 'engin', 'particip', 'integr']","['sql', 'github', 'python', 'git', 'tune', 'data warehousing', 'problem solving', 'supervis', 'etl', 'analyt', 'asset', 'relat', 'python', 'comput', 'packag', 'action', 'etl', 'sourc', 'engin', 'particip', 'integr']"
DE,"# 81883 - New York, New York, United States
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Assist with data-related technical issues and support their data infrastructure needs.
Qualifications for Data Engineer
Bachelor’s degree required, Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is preferred
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience supporting and working with cross-functional teams in a dynamic environment.
They should also have experience using the following software/tools:
Experience with relational SQL and NoSQL databases: MongoDB, Neo4j, etc
Experience with cloud services: GCP, AWS, etc
Experience with object-oriented/object function scripting languages: Python, Java, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with Data Flow, Data Pipeline and workflow management tools: Cloud Composer, Airflow, Luigi, etc
Equal Opportunity Employer
Are you interested in working for Colgate-Palmolive?
Applications received by e-mail are not considered in the selection process.
To learn more about Hill's and the Hill’s Food, Shelter & Love program please visit http://www.hillspet.com.
To learn more about Tom’s of Maine please visit http://www.tomsofmaine.com.
Please contact Application_Accommodation@colpal.com with the subject ""Accommodation Request"" should you require accommodation.
","['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka']","['optim', 'statist', 'pipelin', 'big data']",1,"['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'statist', 'pipelin', 'big data']","['analyt', 'program', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']","['mongodb', 'sql', 'gcp', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'statist', 'pipelin', 'big data', 'analyt', 'program', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'sourc', 'engin']"
DE,"70570
Fundamental Components: Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with data science team to transform data and integrate algorithms and models into automated processes.
Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support Data Science and other internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience: Strong problem solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams.5 or more years of progressively complex related experience.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Experience building data transformation and processing solutions.Has strong knowledge of large scale search applications and building high volume data pipelines.
Masters degree or PhD preferred.Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
from you via e-mail.
Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter.
#LI-DT1
","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",2,"['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']","['cassandra', 'pig', 'unix', 'hadoop', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"Data Strategist - Engineer
NYC Hedge Fund
An exciting opportunity to join the Quantitative Strategy team designing and developing the infrastructure for evaluating, ingesting, parsing and processing structured and big data sets.
The Data Strategist will work closely with data scientists, quantitative researchers, and portfolio managers to design, implement, and scale the organizations overall data architecture.
The Data Strategist will be a significant contributor in enabling their vision and helping the organization to grow and improve their businesses.
The ideal candidate would have a background:
Strong computer science and technical background with experience and an interest in working with data.
(prior experience with big data sets preferred)
Experience with designing and implementing data processing pipelines
Experience using distributed computing frameworks
An interest in financial markets (with prior experience a big plus)
Programming skills required - proficiency in memory managed languages required such as; Python, C#, Java, F#, scala, haskel, ada, smalltalk, prologue, etc.
Python and .Net is a plus.
Ability to work in an open collaborative and highly charged trading floor environment.
You will sit in a trading floor seat in an open environment surrounded by high-intensity colleagues.
Strong desire to contribute and build a business – not just solve novel problems.
Ability to work with a wide range of users – traders, portfolio managers, research analysts, quants and the wider investment management team to develop and implement technology in support of a dynamically growing Hedge fund platform
An individual who loves solving deep and complex technical and business problems and wants to have an outsized impact with the products they build and deliver.
About the Client:
They have a relentlessly focuses on innovation and integration: innovation in new products, markets and businesses as well as new tools, models and technology management and performance structures; integration of fundamental research, quantitative strategies and technical analysis, all supported by an intensive focus on operational excellence and comprehensive risk management.
They employ over 240+ talented professionals in New York, London, and Tokyo locations across portfolio management, trading, credit, research, quantitative strategy, trading technology, investment management analysis and business management administration and strategy.
The management team needs individuals of the highest professional caliber who are leaders, problem solvers, analytic, detail-oriented, and entrepreneurial.
Successful candidates are:
Analytic and relentless in pursuit of the right answer
Strong communicators who excel at rapid synthesis
Able to demonstrate sound business judgment
Able to digest complexity while maintaining an understanding of the “big picture” of business needs
Team players who are energized by a collaborative enterprise
They believe that these standards are the foundation for superior investment performance and are critical to delivering performance to clients
","['scala', 'java', 'python', 'c', 'excel']","['research', 'pipelin', 'risk', 'big data', 'commun']",999,"['scala', 'java', 'python', 'c', 'excel', 'research', 'pipelin', 'risk', 'big data', 'commun']","['analyt', 'pipelin', 'set', 'credit', 'judgment', 'infrastructur', 'python', 'scientist', 'comput', 'big', 'quantit', 'engin', 'integr']","['scala', 'java', 'python', 'c', 'excel', 'research', 'pipelin', 'risk', 'big data', 'commun', 'analyt', 'pipelin', 'set', 'credit', 'judgment', 'infrastructur', 'python', 'scientist', 'comput', 'big', 'quantit', 'engin', 'integr']"
DE,"Job #: 1076197
Title- Data Engineer
Client- Large Financial Institute
Location- Manhattan, NYC
Duration- 12+ Month Contract with Potential to Extend/Convert
Financial regulation for banks has increased dramatically over the past few years.
This role is to work on the Global Banking and Markets (GBAM) Non-Financial Regulatory Reporting (NFRR) Data Delivery program, a new initiative to define and implement consistent and efficient regulatory reporting processes that adhere to enterprise standards, simplify controls and enable re-use.
The DSL components interpret transformation rules written in a configuration style syntax that can be applied to one or more standard data sets to produce a transformed data set.
This framework provides a unified language for describing the data needs of a report.
• Enables users to seamlessly retrieve and combine data from multiple sources
• Enables users to author reports without having to worry about the mechanics of actually retrieving, filtering, projecting, or aggregating the data
• Ensures that the system is scalable enough to run hundreds of reports in parallel, if require
Role
The role requires close partnership with the NFRR Program analysis and development teams.
Responsibilities
The candidate will work directly with the Director of the Data Processing Engine and will participate in all phases of development of the platform and subsequent reports.
• Develop technical specification and component level design for DSL Extensions for input and output
• Design parameterized and configurable modules for DSL Extensions
• Develop DSL for the data outputs needed for NFRR Reports
• Integrate Code Repository and Management
Qualifications
The candidate must be a self-starter, able to work in a fast paced and results driven environment with minimal oversight.
The candidate is required to have excellent communications skills and possess a strong sense of accountability and responsibility.
• 5+ years development experience
• Good general Scala programming skills
• Experience with Hadoop Distributed File System (HDFS), HBase and Hive
• Experience with custom aggregations within Spark/Flink, preferred
• Experience with databases, a plus
• Experience on regulatory or reporting projects, preferred
• Ability to perform detailed and complex data analysis
• Attention to detail and ability to work independently
• Ability to handle tight deadlines, and competing demands in a fast paced environment
• Knowledge of Global Banking and Markets’ products/asset classes and associated data including fixed income, equities, derivatives, and foreign exchange securities, preferred
Scala developer with Apache Hadoop/Spark/Flink experience
5-7 years
If you are interested in applying to this position or learning more please reach out to Yasmin Aminyar directly at yaminyar@apexsystems.com .
EEO Employer
","['spark', 'scala', 'hadoop', 'hive', 'hbase', 'excel']","['commun', 'account']",999,"['spark', 'scala', 'hadoop', 'hive', 'hbase', 'excel', 'commun', 'account']","['asset', 'program', 'spark', 'set', 'input', 'hadoop', 'interpret', 'appli', 'integr', 'sourc', 'engin']","['spark', 'scala', 'hadoop', 'hive', 'hbase', 'excel', 'commun', 'account', 'asset', 'program', 'spark', 'set', 'input', 'hadoop', 'interpret', 'appli', 'integr', 'sourc', 'engin']"
DE,"The data sets, pipelines and tools that you build will be involved in pivotal, company-level decisions for years to come.
RESPONSIBILITIES
Build and maintain data processing services
Write, test, and review primarily microbatch or streaming ETL
Keep yourself up-to-date and informed about new technologies
Encourage the technical growth of your teammates
QUALIFICATIONS
You love working directly with the people whose problems you're solving
You are experienced at data modeling, storage, security, and retrieval
You're motivated and inspired.
You naturally lead a project and ask for help when needed
You enjoy working with dynamic programming languages, relational databases, and distributed systems.
You gain a deep understanding of the products and tools you work with
You check your work and stand behind what you’ve built
5+ years of experience
For more information, visit www.squarespace.com/about.
Benefits & Perks
Health insurance with 100% premium covered for you and your dependent children
Flexible vacation & paid time off
Up to 20 weeks of paid family leave
Retirement benefits with employer match
Fertility and adoption benefits
Free lunch and snacks at all offices
Education reimbursement
Dog-friendly workplace in New York office
",[None],"['etl', 'pipelin', 'data modeling']",999,"['etl', 'pipelin', 'data modeling']","['program', 'pipelin', 'relat', 'etl', 'set']","['etl', 'pipelin', 'data modeling', 'program', 'pipelin', 'relat', 'etl', 'set']"
DE,"Work it!
After all, teamwork makes the dream work.
If you're looking for a deeply fulfilling, financially rewarding, and really fun career, you're in the right place.Sr Data EngineerThe Sr Data Engineer should be an expert with all of the data warehousing technical components (e.g.
ETL, Reporting, Data Model), infrastructure (e.g.
hardware and software) and their integration.
The ideal candidate will be responsible for developing overall architecture and high level design.Key Responsibilities* Lead architecture design and implementation of next generation cloud BI solution.
* Build robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark.
* Build and deliver high quality data architecture to support business analysis, data scientists, and customer reporting needs.
* Interface with other technology teams to extract, transform, and load data from a wide variety of data sources* Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
","['sql', 'spark', 'bi', 'cloud', 'python']","['data warehousing', 'etl', 'hardwar']",999,"['sql', 'spark', 'powerbi', 'cloud', 'python', 'data warehousing', 'etl', 'hardwar']","['spark', 'etl', 'infrastructur', 'bi', 'python', 'scientist', 'integr', 'sourc', 'engin']","['sql', 'spark', 'powerbi', 'cloud', 'python', 'data warehousing', 'etl', 'hardwar', 'spark', 'etl', 'infrastructur', 'bi', 'python', 'scientist', 'integr', 'sourc', 'engin']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers.
You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion.
You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"You should have 5+ years of strong experience with Data Engineering in Data Science.
Must have strong experience in Python Should have experience in Write advance code in SQL for analysis.
Must have experience in Apache Air flow or Google Composer.
Strong Experience in Version Control, Git and Bitbucket Demonstrate excellent communication skills including the ability to effectively communicate with internal and external customers.
Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail.
Strong work ethic with good time management with ability to work with diverse teams and lead meeting
","['sql', 'python', 'excel', 'git']",['commun'],999,"['sql', 'python', 'excel', 'git', 'commun']","['python', 'divers', 'engin']","['sql', 'python', 'excel', 'git', 'commun', 'python', 'divers', 'engin']"
DE,"They are looking for talented individuals who have experience developing and maintaining the data ingestion and data quality processes and platform that feed systems to support risk management and trading in Equities, Fixed Income, Commodities, Credit, and FX business.
This person would be working closely with quants, risk managers, and other technologies in global offices to develop automated rules and validation frameworks to ensure quality of underling inputs and outputs to and from the risk platform.
Requirements:
Strong analytical skills, and experience using Python to build statistical/analytical libraries/systems
Three to Five years of Experience with larger scale python in a data intensive environment
Experience working with Machine Learning/Data related libraries; Scikit-learn, Tensorflow, Keras and/or PyTorch as well as Pandas/Numpy.
Professional experience with a SQL-based database, such as MS SQL Server
Experience with big data tools Spark, Hadoop, and especially streaming platforms like Kafka
Able to work independently in a fast-paced environment.
BA or Master in computer science or any other scientific/data fields
","['sql', 'kera', 'spark', 'numpi', 'panda', 'kafka', 'hadoop', 'python', 'tensorflow', 'scikit', 'pytorch']","['machine learning', 'big data', 'risk', 'statist']",1,"['sql', 'kera', 'spark', 'numpi', 'panda', 'kafka', 'hadoop', 'python', 'tensorflow', 'scikit', 'pytorch', 'machine learning', 'big data', 'risk', 'statist']","['analyt', 'machin', 'stream', 'learn', 'spark', 'input', 'credit', 'hadoop', 'python', 'comput', 'statist', 'big', 'relat']","['sql', 'kera', 'spark', 'numpi', 'panda', 'kafka', 'hadoop', 'python', 'tensorflow', 'scikit', 'pytorch', 'machine learning', 'big data', 'risk', 'statist', 'analyt', 'machin', 'stream', 'learn', 'spark', 'input', 'credit', 'hadoop', 'python', 'comput', 'statist', 'big', 'relat']"
DE,"About the role
Things you're good at
Shipping: Delivering great products that you're proud of on a regular basis.
Architecture: Getting it done is important.
Getting it done in way that will scale is equally important.
Diving in: Taking ownership of the data stack.
Responsibilities
Drive optimization, testing, and tooling to improve data quality
Requirements
4+ years of experience in a Data Engineering role, with a focus on building data pipelines
BI tooling and/or data app development
4 year bachelor degree in Computer Science or other technical or science degree
Proficiency in Python
Experience with some or all of the following: Postgres, Redshift, Celery, Elasticsearch, Kafka, and Airflow.
Benefits
Competitive salary and meaningful equity
401k Match
Health, vision and dental insurance
Free lunch
","['airflow', 'elasticsearch', 'bi', 'python', 'redshift', 'postgr', 'kafka']","['optim', 'pipelin']",1,"['airflow', 'elasticsearch', 'powerbi', 'python', 'redshift', 'sql', 'kafka', 'optim', 'pipelin']","['basi', 'pipelin', 'optim', 'bi', 'python', 'comput', 'engin']","['airflow', 'elasticsearch', 'powerbi', 'python', 'redshift', 'sql', 'kafka', 'optim', 'pipelin', 'basi', 'pipelin', 'optim', 'bi', 'python', 'comput', 'engin']"
DE,"Start Assessment
As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients.
You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more.
The position requires fostering close working relationships with internal marketing teams as well as clients.
What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic.
Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
","['sql', 'bi', 'python', 'excel', 'power bi']","['visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']",1,"['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']","['visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']","['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast', 'visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Do you believe that people with compassion will support one another to create a better world?
GoFundMe is the largest social fundraising community in the world and is just getting started.
With over $9 billion raised from more than 120 million donations, GoFundMe is the largest social fundraising community in the world and is just getting started.Data is at the center of all decisions and strategy at GoFundMe.
)* Integrate data from data warehouse into 3rd party tools to make data actionable* Develop and maintain REST API endpoints for data science products* Provide ongoing maintenance and enhancements to existing data warehouse solutions* Ensure data quality through automated testing* Collaborate with analysts, engineers and business users to design solutions* Research innovative technologies and make continuous improvementsWhat you bring to the role...* 3+ years as a data engineer designing, developing and maintaining enterprise data warehouse solutions consisting of structured and unstructured data* Proficiency with building data pipelines using ETL/data preparation tools* Experience with web APIs and data integrations across internal and external systems* Expertise in writing and optimizing SQL queries* Knowledge of Python, Java, C++ or other scripting languages* Experience with Spark and Scala* Good understanding of database architecture and best practices* Understanding of data science and machine learning technologies a plus* Experience with event tracking is a plus* Bachelor's degree in Engineering* Ping pong skills, a love for boba tea, and a sense of humorWhy you'll love it here...* Your work has real purpose and will be helping to change lives at a global scale.
* Great perks like lunch, snacks, wellness, company/team activities, and full benefits.
Every day friends, family, and members of the community come together to support one another and the causes they care about most.
","['sql', 'spark', 'scala', 'java', 'python']","['research', 'pipelin', 'machine learning', 'etl', 'commun']",1,"['sql', 'spark', 'scala', 'java', 'python', 'research', 'pipelin', 'machine learning', 'etl', 'commun']","['day', 'machin', 'learn', 'spark', 'pipelin', 'provid', 'python', 'parti', '3rd', 'action', 'warehous', 'etl', 'engin', 'integr']","['sql', 'spark', 'scala', 'java', 'python', 'research', 'pipelin', 'machine learning', 'etl', 'commun', 'day', 'machin', 'learn', 'spark', 'pipelin', 'provid', 'python', 'parti', '3rd', 'action', 'warehous', 'etl', 'engin', 'integr']"
DE,"Summary:
Responsibilities:
Design, develop, test, implement and support applications using custom ETL (Extract Transform Load) or open source tools such as Talend
Prepare high-level component architecture; design documents, data flow diagrams, detail design documents, data schema and modeling combined with test plan documents
Design, develop and test highly available and scalable data pipelines and relevant data storage systems to enable business success across a multi-product functionality
Proactively identify operational and systemic issues within the data supply value chain (from collection to processing to reporting) and work with production operations (DevOps) team to implement monitoring solutions
Execute in a fast-paced matrix organization across product and engineering teams to identify best data-driven solutions for the underlying data infrastructure and platform
Required Qualifications:
Experience designing and building large, scalable data systems, preferably across a multi-product portfolio
Strong SQL skills with proven ability to write complex data queries across large data sets.
Exposure to software development, preferably in an Agile/Scrum/Kanban environment across multiple products
Experience analyzing and manipulating data across diverse data sources (Python, Scala)
Experience working across AWS (Amazon Web Services) cloud environment (EC2, S3, RDS, Sagemaker)
Strong exposure to Big Data technology, preferably across a containerized environment (Hadoop, Spark, Hive, Presto)
Experience with sourcing and modeling data from Restful API (Application Programming Interface)
Strong attention to detail with excellent analytical, problem-solving, and communication skills
Good time management and ability to work on concurrent assignments with different priorities
Ability to work in a fast paced, iterative development environment with short turnaround times
Preferred Qualifications:
Experience with A/B Testing and related optimization across desktop and mobile in a digital environment a plus (examples include: Optimizely, Leanplum, deltaDNA)
Experience analyzing and manipulating data across several data formats (JSON, Avro, Parquet, ORC)
Experience building and architecting data warehouse workflows in large cloud-based production environments (Snowflake is an example)
Experience migrating on-prem data solutions to the cloud with a strong data operational hygiene
Prior experience working in educational technology companies or a related competitive landscape is a plus
Medical, Dental, Vision + 401k
Highly competitive PTO policy
Casual Dress Code
Snacks + Drinks (Coca Cola Freestyle Machine)
Gaming room including an Arcade (2,000+ games)
Limitless opportunities for professional growth!
","['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'amazon web services']","['big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun']",999,"['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun']","['program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun', 'program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Who_Are_We
members in 190 countries.
per day, using over 1000 types of devices, generating more than a third of
downstream Internet traffic in North America.
The Content Data Engineering team
is based in the entertainment capital of the world, passionate about providing
finance functions.
the content space is exploding.
Los Angeles, California Product Infrastructure and Tooling Senior Data
internet streaming service with over 150 million members in 190 countries.
members enjoy a wide variety of streamed video per day, using over 1000 types
of devices, generating more than a third of downstream Internet traffic in
North America.
The Content Data Engineering team is based in the entertainment
culture across marketing, streaming, product, and finance functions.
deepens its global reach and develops more and more amazing Originals content,
Lazy in a productive way (find tedious work boring and would rather automate
it).
Charismatic, determined, curious, and industrious, and not just hardworking.
Thrive in a fast paced environment, and see yourself as a partner with the
business with the shared goal of moving the business forward.
Have strong beliefs that are weakly held you can deliberate, and hear, all
sides of a discussion and adapt to new perspectives that emerge from it.
Sharp communicator who can explain complex data problems in clear and concise
language.
Build code that is understandable, simple, and clean, and take pride in its
beauty.
Love freedom and hate being micromanaged.
Given context, you're capable of
self-direction.
Passionate about data quality and delivering effective data to impact the
business.
Motivated to explore new technologies and learn, and can do so without taking
formal education.
Collaborate with
partners to understand needs, model tables using data warehouse standard
methodologies, and develop data pipelines to ensure the timely delivery of high
quality data.
Continually acquire new data sources to develop an increasingly rich dataset
that characterizes content.
Translate data questions into flexible methodologies that scale to answer broad
problems across the organization.
Be a bridge between data engineering and the business, enabling insight that
can empower better decision-making.
Be comfortable outside of your comfort zone - explore new tech, make your own
tool, or find a new way to address an old problem.
Data warehousing, data modeling, and data transformation.
How to write complex SQL in your sleep.
Significant experience with ETL tech (Informatica, SSIS, etc) is very valuable,
but expect to work in a distributed environment.
MPP/Cloud data warehouse solutions (Snowflake, Redshift, BigQuery, Vertica,
Teradata, Greenplum, etc).
Experience with sourcing and modeling data from application APIs.
Python for scripting and automation.
fascinating analytical and technical challenges, and a Freedom & Responsibility
Culture that's truly outstanding.
It s worth learning more http://
Show moreShow less
","['sql', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift']","['pipelin', 'data modeling', 'clean', 'data warehousing', 'financ', 'etl', 'commun']",999,"['sql', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift', 'pipelin', 'data modeling', 'clean', 'data warehousing', 'financ', 'etl', 'commun']","['day', 'analyt', 'stream', 'learn', 'challeng', 'pipelin', 'infrastructur', 'python', 'concis', 'etl', 'sourc', 'engin']","['sql', 'bigqueri', 'cloud', 'snowflak', 'python', 'redshift', 'pipelin', 'data modeling', 'clean', 'data warehousing', 'financ', 'etl', 'commun', 'day', 'analyt', 'stream', 'learn', 'challeng', 'pipelin', 'infrastructur', 'python', 'concis', 'etl', 'sourc', 'engin']"
DE,"What you'll do:
Integrate data flows between operational systems, build tools for data reconciliation
You'll love this job if you:
Love data and have a passion for turning it into insights
Are a dynamite problem solver.
You're motivated to solve customer issues and are always thinking one step ahead.
Create strong partnerships with stakeholders
Love iterating quickly on software
Are a strong advocate of high-quality code
What you'll need:
2+ years software or data engineering experience
Strong SQL skills
Strong analytical and problem-solving skills
Effective communication and interpersonal skills
Bonus Points for:
Experience with Mule ESB, Java, Python, Ruby and/or Apache Airflow
Experience in a retail or ecommerce environment
Benefits & Perks
Thinking about adding little ones to your family?
California Privacy Rights Notice for Californian Job Applicants and Prospective Talent
Effective Date: January 1, 2020
Name
Signature
Social Security Number
Email and mailing address
Telephone number
Education
Employment history
These business purposes include, without limitation:
Legal compliance
Detecting and protecting against security incidents, fraud, and illegal activity;
Debugging;
Internal research for technological improvement; and
Internal operations.
","['sql', 'airflow', 'java', 'python', 'rubi']","['research', 'commun']",999,"['sql', 'airflow', 'java', 'python', 'rubi', 'research', 'commun']","['analyt', 'python', 'engin']","['sql', 'airflow', 'java', 'python', 'rubi', 'research', 'commun', 'analyt', 'python', 'engin']"
DE,"About codeSparkcodeSpark is turning programming into play for kids everywhere.
The ideal candidate is self-sufficient and adept at using large data sets to find opportunities for product and process optimization, and in building and using models to test the effectiveness of different courses of action.
They are experienced in using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations.
They must demonstrate a proven ability to drive business results with their data-based insights.
Finally, they must be comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.Responsibilities:?
Assess the effectiveness and accuracy of new data sources and data gathering techniques.?
Use predictive modeling to increase and optimize customer experiences, onboarding, revenue generation, ad targeting and other business outcomes.?
Leverage A/B testing framework and test model quality.?
Develop processes and tools to monitor and analyze model performance and data accuracy.?
5-7 years of experience manipulating data sets and building statistical models.?
Bachelor's or Master's in Statistics, Mathematics, Computer Science or another quantitative field.?
Coding knowledge of several languages: C#, Python, JavaScript, Go, or similar.?
Querying databases and using statistical computer languages: R, Python, Mongo, JQL, etc.?
Using AWS services.?
Analyzing data from 3rd party providers: Google Analytics, Mixpanel, Facebook, Appsflyer, Email service providers, etc.?
Experience visualizing/presenting data for stakeholders.Highly desired:?
Experience working with product development and marketing teams, especially subscription products and kids games or apps.?
Experience using statistical computer languages to manipulate data and draw insights from large data sets.?
Experience working with and creating data architectures.?
Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.)
and experience with applications.?
Excellent written and verbal communication skills for coordinating across teams.?
A proactive approach to learning and mastering new technologies and techniques.Perks?
The rare opportunity to change the world by helping create the first generation in history that is truly connected and collaborative.?
The fun of working with an effective and multi-award winning product that is beloved by customers.?
Sharp, motivated co-workers in a creative and supportive office environment.Commitment to Diversity and InclusioncodeSpark believes in diversity and inclusion of all people, of all genders, races, ethnicities, sexual orientations, educational backgrounds, religions, abilities, socioeconomic backgrounds, immigration statuses, and more.
","['javascript', 'aw', 'c', 'python', 'r', 'excel']","['data mining', 'regress', 'predict', 'optim', 'statist', 'analyz', 'commun']",1,"['javascript', 'aw', 'c', 'python', 'r', 'excel', 'data mining', 'regress', 'predict', 'optim', 'statist', 'analyz', 'commun']","['analyt', 'techniqu', 'learn', 'set', 'divers', 'predict', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'action', '3rd', 'comput', 'quantit', 'sourc', 'algorithm']","['javascript', 'aw', 'c', 'python', 'r', 'excel', 'data mining', 'regress', 'predict', 'optim', 'statist', 'analyz', 'commun', 'analyt', 'techniqu', 'learn', 'set', 'divers', 'predict', 'provid', 'optim', 'python', 'parti', 'statist', 'aw', 'action', '3rd', 'comput', 'quantit', 'sourc', 'algorithm']"
DE,"Hello, Hope you are doing Well !!!
Below are some key highlights of the position, If you are available in the Job market and interested in this opportunity, Please feel free to connect with me at (408) 520-9163 or reach me out at ravi.satiwarasumerusolutions.com httpsmail.google.commailu0h7ydjsnpg5mq7?ampcswhampvbamptoravi.satiwarasumerusolutions.com .
Skills Required AWS (S3, EMR, Glue), Python, SQL, Shell Scripting, JSON, Postgres.
","['sql', 'aw', 'python', 's3', 'postgr']",[None],999,"['sql', 'aw', 'python', 's3']","['aw', 'avail', 'python']","['sql', 'aw', 'python', 's3', 'aw', 'avail', 'python']"
DE,"Job Summary
You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs.
You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.
The use of computer vision is a high priority in the institution's digital strategy and the success of this work will lead to further exciting projects that put the developed skills, transformation workflows and systems to good use.
You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics.
Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With other Data Engineers, ensure high quality data transformation pipelines can migrate and enhance institutional managed collections
Assess the feasibility of applying data enhancement tools and the quality of their results to determine their suitability for project requirements
Work in an agile way, including supporting testing, continuous integration and deployment
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations
Attention to detail combined with a focus on data usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Familiarity with machine learning techniques and/or tools
Familiarity with cultural heritage data standards, such as Linked Open Data
Familiarity with engineering tools such as git
Familiarity with test driven and agile software development methodologies
","['python', 'excel', 'git']","['machine learning', 'computer vision', 'commun', 'pipelin']",1,"['python', 'excel', 'git', 'machine learning', 'computer vision', 'commun', 'pipelin']","['digit', 'machin', 'program', 'techniqu', 'engin', 'pipelin', 'relat', 'python', 'scientist', 'comput', 'integr', 'collect']","['python', 'excel', 'git', 'machine learning', 'computer vision', 'commun', 'pipelin', 'digit', 'machin', 'program', 'techniqu', 'engin', 'pipelin', 'relat', 'python', 'scientist', 'comput', 'integr', 'collect']"
DE,"Position: Data Engineer
Reason Open: Has a project coming up-very similar to current project and set to launch in September.
This person will lead the Data Side.
Must be a senior person- very innovative product.
Need someone who can think out of the box!
Start: ASAP!
Budget: DOE
Duration: 1+ year contract- ongoing
Work hours: 40 hours per week and must attend 9:30 am PST scrum meeting but can be flexible with hours!
Day to Day:
Lead the data side of a brand new, cutting edge product from the ground up
Product will find new sports insights anonymously (deep insights across multiple dimensions) - will span across multiple data sets (i.e.
NFL, MBL, eventually the news)
Data heavy / data driven project with some AI involved
Work closely with Data Scientist
Data engineer will set the foundation for data scientist to build upon
Startup mentality- must want to wear multiple hats and be comfortable in a fast paced environment with high level direction
Must Haves:
Senior level Data Engineer (5+ years ideally)
Experience with Databases (SQL, redshift, etc- client open to any)
Must have experiencing setting up architecture from scratch
Docker
Will be building on Postgres databases into docker
Must have experience with POSTGRES (AKA PostgreSQL)
Need to have worked in a distributed environment
Sports acumen / go getter / out of the box thinker
Python coding is a HUGE nice to have!
Nice to haves:
Python coding
Redshift
","['sql', 'redshift', 'python', 'postgr', 'postgresql', 'docker']",[None],999,"['sql', 'redshift', 'python', 'postgresql', 'docker']","['day', 'python', 'scientist', 'set', 'engin']","['sql', 'redshift', 'python', 'postgresql', 'docker', 'day', 'python', 'scientist', 'set', 'engin']"
DE,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive.
Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership.
","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r']","['recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']",999,"['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']","['predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl', 'predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']"
DE,"This role is an important part of the rapidly scaling infrastructure and data management demands of being the leader in VideoID technology for enterprise media companies and content owners.
Languages: Python, Scala, Java, and Kotlin
Data stores: PostreSQL, Elasticsearch, Redshift
Data processing: Apache Kafka, Apache Spark
DevOps: Jenkins, Docker, Terraform, Ansible, AWS ECS, AWS EMR
Here's what you'll get to do:
Provide seamless and timely data access for your users
Build reliable and dependable ETL
Build and maintain production machine learning infrastructure
Troubleshoot complex issues in distributed systems
Debate data processing philosophies and methodologies with your team
Bachelor's or Master's degree in Computer Science or related field
Fluency with Python, Java, Kotlin, or Scala
Experience with distributed systems
Strong foundation in data structures, algorithms and software design
Experience with digital media, social media, and video APIs such as YouTube's Data API is a big plus
Thorough testing and code review standards/practices
Strong verbal and written communication skills
Openness to new technologies and creative solutions
","['spark', 'elasticsearch', 'scala', 'aw', 'redshift', 'python', 'java', 'kafka', 'docker']","['machine learning', 'etl', 'commun']",1,"['spark', 'elasticsearch', 'scala', 'aw', 'redshift', 'python', 'java', 'kafka', 'docker', 'machine learning', 'etl', 'commun']","['digit', 'machin', 'spark', 'relat', 'infrastructur', 'provid', 'aw', 'python', 'comput', 'big', 'etl', 'algorithm']","['spark', 'elasticsearch', 'scala', 'aw', 'redshift', 'python', 'java', 'kafka', 'docker', 'machine learning', 'etl', 'commun', 'digit', 'machin', 'spark', 'relat', 'infrastructur', 'provid', 'aw', 'python', 'comput', 'big', 'etl', 'algorithm']"
DE,"The Data Engineer will be a member of the Media Supply Chain (MSC) team within
the WarnerMedia Technology (WMTO) organization, the position will be
responsible for the design and implementation of search and data relationship
functionality for the WB content platform.
The Data Engineer must have
experience with data modeling, modern database technologies, and API driven
search design and development.
The Data Engineer will make technology and
design decisions, and partner across the organization in a collaborative manner
distribution applications.
Media Supply Chain is tasked to architect, engineer, and program manage a wide
operationally manage and distribute WarnerMedia content globally.
applications and technology solutions are responsible for scheduling, image &
asset metadata management, as well as, content processing and delivery as well
as content mastering, localization and preservation.
new ways to process and deliver WarnerMedia content to its global customers
The Media Supply Chain Mission
technology solutions which are highly reliable, automated, measurable,
optimized, modern systems capable of distributing high-quality content and
The Daily
Develop and provide support for core data relationship, data ingest, data
transformation services and search capabilities.
Creates functional and
technical specifications.
Creates and executes against a plan to launch
and maintain applications.
Review project objectives and determine best technology for
implementation.
Implement best practice standards for development, build
and deployment automation.
Evaluate software products and vendors for WarnerMedia (WM) Technology
and other divisions.
Recommend action, develop and lead implementation of
selected products/services.
Work with internal and external developers to ensure (WM) Technology code
standards and best practices are performed for development of
applications.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
3+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL,
and Relational.
Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch,
logstash, kibana).
Demonstrated expertise and experience in modern databases such as Mongo,
Couchbase, Neptune, Neo4j, or equivalent.
Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch
query DSL, cypher, gremlin, or graphql.
xQuery preferred but not
required.
Highly proficient in XML, JSON and YAML data exchange formats.
Experience
in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana
etc.
Experience in working with data streaming technologies such as Amazon
Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch,
RDS, Redshift, Neptune and ec2.
Highly proficient in at least one modern programming language such
python, java, or node.js.
Bash experience preferred.
Demonstrated expertise and experience in deploying containerized
application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira,
or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Systems design and architecture.
Ability to work with outside vendors and clients under sometimes adverse
circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of
personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when
necessary.
Must be able to respond to after-hours pager notifications to provide
support for applications as necessary.
","['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['optim', 'commun', 'graph', 'data modeling']",1,"['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'optim', 'commun', 'graph', 'data modeling']","['analyt', 'asset', 'essenti', 'program', 'optim', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']","['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'optim', 'commun', 'graph', 'data modeling', 'analyt', 'asset', 'essenti', 'program', 'optim', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']"
DE,"Req ID: 95763
The individual will be responsible for the following:
Design, develop, and implement total systems solutions to the enterprise-wide technical requirements of customers.
Validate the ETL design and ensure that technical specifications are complete, consistent, concise, and achievable.
Create ETL processes and reports using Informatica/MSBI/Stored Procedures
Develop ETL code as per the technical specifications and business requirements according to the established designs.
Provide performance tuning of ETL processes, participate in ETL architecture design reviews, and conduct ETL unit testing and code reviews.
Participate in system and integration testing and identifying and remedying solution defects.
Basic Qualifications:
Minimum of 5 years experience with Informatica Powercenter
Minimum of 3 years experience with SQL Server/Oracle
The position requires a Bachelor’s degree in Computer Engineering, Computer Science, Computer Information Systems, Information Technology or related field of study plus 5 years of experience.
Preferred Skills:
Healthcare domain knowledge- 3+ years( Good to have experience on Meditech or Epic EMR systems)
INDHCLSMC
","['sql', 'oracl']","['tune', 'information technology', 'etl', 'healthcar']",1,"['sql', 'oracl', 'tune', 'information technology', 'etl', 'healthcar']","['relat', 'provid', 'comput', 'concis', 'etl', 'engin', 'particip', 'integr']","['sql', 'oracl', 'tune', 'information technology', 'etl', 'healthcar', 'relat', 'provid', 'comput', 'concis', 'etl', 'engin', 'particip', 'integr']"
DE,"As a contributor you will work around 5-10/hrs a week in exchange for stock options.
Key duties & responsibilities
Design and develop tools and algorithms for recommendation and predictive engines
Create and maintain optimal data pipeline architecture
Identify, obtain and prepare data sets for training and testing algorithms
Use Machine Learning to solve modeling and ranking problems across discovery and search
Evaluate potential improvements, prototype, validate, and productionize these strategies
Integration of data from multiple sources including third party sources.
Requirements and qualifications
5+ years of relevant experience as a Data Engineer, Data Scientist or Machine Learning Engineer
Strong software development experience in languages such as Python, Java, Scala, R or C++
Experience working with large data sets using tools like Hadoop, Spark, or ElasticSearch
Knowledge of Machine Learning systems, data mining, search, ranking, recommendations
Experience with relational (SQL) and NoSQL Databases
Sharp analytical and problem-solving skills
Great team player with good communication skills
Good organizational and time-management skills
High level (spoken & written) of English
In addition
You are committed and highly motivated
You are results-oriented, quality-driven, dynamic and hands-on
You are proactive, self-motivated, self-directed and you can work well with remote or limited supervision
You are accurate, versatile, creative, curious, rigorous, adaptable and flexible
You are an independent thinker but also a team worker and a continuous learner
","['sql', 'spark', 'elasticsearch', 'scala', 'hadoop', 'python', 'java', 'r', 'nosql']","['recommend', 'pipelin', 'data mining', 'predict', 'machine learning', 'optim', 'supervis', 'commun']",999,"['sql', 'spark', 'elasticsearch', 'scala', 'hadoop', 'python', 'java', 'r', 'nosql', 'recommend', 'pipelin', 'data mining', 'predict', 'machine learning', 'optim', 'supervis', 'commun']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'set', 'relat', 'predict', 'hadoop', 'optim', 'python', 'algorithm', 'parti', 'scientist', 'integr', 'sourc', 'engin', 'evalu']","['sql', 'spark', 'elasticsearch', 'scala', 'hadoop', 'python', 'java', 'r', 'nosql', 'recommend', 'pipelin', 'data mining', 'predict', 'machine learning', 'optim', 'supervis', 'commun', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'set', 'relat', 'predict', 'hadoop', 'optim', 'python', 'algorithm', 'parti', 'scientist', 'integr', 'sourc', 'engin', 'evalu']"
DE,"About The Role
You thrive in a startup where every individual has a significant impact on the technology is empowered to make decisions and get things done.
What You'll Do
Build large scale fault tolerant data collection and processing pipelines from the ground up
Work with the platform engineering team to collaborate on instrumentation and event stream implementation within NEXTs core platform architecture.
What You'll Have
3-5 Years of experience in Data engineering or related software engineering experience in a fast-paced start-up environment.
Track record of delivering ETLs, machine learning pipelines, and data products within a cloud-based microservices or event streaming architecture.
Experience developing solutions in a continuous delivery eco-system.
Strong verbal and written communication skills and able to communicate effectively to technical and non-technical team members
Motivated by a sense of urgency and ownership
Preferred Qualifications
Graduate degree in computer science.
Active involvement in the open source community.
Experience in transportation, logistics, and supply chain industries.
Experience in venture-backed startups or other hyper-growth environments.
What You'll Receive:
Competitive Base Salary + Equity
Full Medical, Dental and Vision Benefits
Vacation and Holidays
Fun perks: open office, dog-friendly, unlimited snacks, and monthly catered lunches!
",['cloud'],"['pipelin', 'machine learning', 'etl', 'logist', 'commun']",2,"['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun']","['machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']","['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun', 'machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']"
DE,"This role is perfect for an ambitious Data Engineer looking to grow and work with real-time data.
Someone who is engineering-driven, can translate business and data requirements into production-ready stacks, and provide innovative systems with a focus on data resilience and accuracy for the e-commerce space.
What you need to know
Be confident working within a real-time data collection system
Provide data-focused solutions and obtain buy-in from stakeholders
Experience working with stacks such as BigQuery, DataFlow, DataPrep, Data Studio, Pub/Sub, Kafka, Apache Airflow and the like
Experience defining schemas and growing a Data Warehouse
Experience with HBASE derived databases, especially BigTable
What you'll be doing
Creating data pipelines to handle batch and steaming ETLs
Create and implement efficient solutions to enrich existing data workflows
Create software/technology that will be the foundations for future machine learning engines such as personalization and recommendations
Identifying and report data resilience issues to key stakeholders
Bonus if you have
Experience at a B2B and/or SaaS startup
Experience creating data pipelines for SaaS dashboard products like Intercom, Canva, FreshBooks
Worked within highly functional engineering teams
It all starts with having good people, and helping them grow both personally and professionally.
","['airflow', 'bigqueri', 'bigtabl', 'hbase', 'kafka']","['recommend', 'pipelin', 'dashboard', 'machine learning', 'etl']",999,"['airflow', 'bigqueri', 'bigtabl', 'hbase', 'kafka', 'recommend', 'pipelin', 'dashboard', 'machine learning', 'etl']","['machin', 'engin', 'pipelin', 'provid', 'warehous', 'etl', 'collect']","['airflow', 'bigqueri', 'bigtabl', 'hbase', 'kafka', 'recommend', 'pipelin', 'dashboard', 'machine learning', 'etl', 'machin', 'engin', 'pipelin', 'provid', 'warehous', 'etl', 'collect']"
DE,"Search Jobs
Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?
You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.
Responsibilities:
· Understand the business capability/requirements and transform them into robust design solutions
· Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed
· Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.
· Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.
· Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLT
· Protect data integrity and accuracy.
Perform root cause analysis of issues that hinder the data quality.
Work with data source owner to increase quality and accuracy of the source data.
· Help data consumers to correctly understand and use the data.
Qualifications:
· 8+ years of experience in as a Data Engineer handling large volumes of data.
· Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.
· Expertise in writing advanced SQL queries.
· Experience working with Informatica, SAP SDI/SLT
· Expertise in SAP HANA, Hive/Hadoop/Hawq/Spark
· Working knowledge of BI Reporting tools like BOBJ and Tableau
· Experience in Python Scripting
· Strong analytical and troubleshooting skills
· Excellent verbal and written communication skills
· Bachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.
Search Jobs
","['sql', 'spark', 'excel', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'hana']","['etl', 'commun', 'statist']",1,"['sql', 'spark', 'excel', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'hana', 'etl', 'commun', 'statist']","['analyt', 'spark', 'set', 'hadoop', 'infrastructur', 'bi', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'excel', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'hana', 'etl', 'commun', 'statist', 'analyt', 'spark', 'set', 'hadoop', 'infrastructur', 'bi', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin', 'integr']"
DE,"Job Title
Data Engineer
06-Jul-2020
Business Segment
Operations & Technology
Responsibilities
As part of the global Operations & Technology organization, the ACOE is focused on data and analytics strategies for the future.
In the Data Engineer role, you’ll be working with internal stakeholders, data engineers, visualization experts, data scientists, and other technologists across the business.
Here you can create the extraordinary.
Responsibilities:
Collaborate with business leaders, engineers, and product managers to understand data needs.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies
Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Participate in development sprints, demos, and retrospectives, as well as release and deployment
Build and manage relationships with supporting IT teams in order to effectively deliver work products to production
Qualifications:
1+ years of experience in a data engineering role
Direct experience with data modeling, ETL development, and data warehousing
Knowledge of data management fundamentals and data storage principles
Experience with Python/Javascript or similar programming languages
Hands-on experience with SQL and Tableau
Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field.
Desired Characteristics:
Analytical – You have experience in delivering data analytics solutions that promote data discovery
Proficient with ETL processes, application development, and data science principles
Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus
Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and mobile
Experience working with data sources such as Nielsen, Adobe Analytics, comScore, and other industry research sources a plus
Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical
Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust
Strong understanding of Agile principles and best practices
You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment
Job Number
56039BR
Posting Category
Technology & Engineering
Country
United States
Sub-Business
Technology
Here you can create the extraordinary.
State/Province
California
Career Level
Experienced
City
Universal City
Notices
","['sql', 'javascript', 'snowflak', 'python', 'aw', 'tableau', 'cloud', 'excel', 'amazon web services']","['research', 'pipelin', 'visual', 'segment', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'commun']",1,"['sql', 'javascript', 'snowflak', 'python', 'aw', 'tableau', 'cloud', 'excel', 'research', 'pipelin', 'visual', 'segment', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'commun']","['program', 'stream', 'visual', 'python', 'etl', 'integr', 'sourc', 'analyt', 'statist', 'action', 'releas', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'digit', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'javascript', 'snowflak', 'python', 'aw', 'tableau', 'cloud', 'excel', 'research', 'pipelin', 'visual', 'segment', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'commun', 'program', 'stream', 'visual', 'python', 'etl', 'integr', 'sourc', 'analyt', 'statist', 'action', 'releas', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'digit', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"What will you do?
Below is a list of the major initiatives that you will be helping with.
Research, analyze and integrate new 3rd party datasets
Heavy bias towards delivering value
Expect their ideas to be challenged because they believe that the best ideas can come from anywhere
View feedback as a gift that they give and receive
Fun and interesting
Qualifications
3+ years experience
experience with a distributed processing technology (e.g.
Spark, Storm, Presto, Hadoop, Samza, Flink, etc)
solid CS and testing fundamentals
experience working in a startup or an extremely strong desire to do so
","['spark', 'hadoop']","['research', 'analyz']",999,"['spark', 'hadoop', 'research', 'analyz']","['spark', 'challeng', 'hadoop', '3rd', 'parti']","['spark', 'hadoop', 'research', 'analyz', 'spark', 'challeng', 'hadoop', '3rd', 'parti']"
DE,"This team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources.
on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
* Build complex workflows and orchestrate data dependencies.
* Monitor and support data pipelines to honor internal and external SLA's.
* Work within standard engineering practices (i.e.
SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.)
* Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
* Working with a team of engineers where mentorship is valued.
* Ability to learn and adapt to continually evolving technologies in the big data ecosystem.What you need:* 3 years of experience programming in Java.
* 1+ years of experience in the Big Data technologies.
* Experience in any of big data technologies: MapReduce, Spark, HBase,* Proficient in SQL and experience with RDBMS/NoSQL databases.
* Experience working with Cloudera/Hortonworks/EMR distribution in AWS.
* Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work.
* Team-player with strong collaboration and communication skills, who is able to respond positively to feedback.
","['sql', 'mapreduc', 'spark', 'aw', 'java', 'hbase', 'nosql']","['pipelin', 'analyz', 'optim', 'big data', 'commun']",2,"['sql', 'mapreduc', 'spark', 'aw', 'java', 'hbase', 'nosql', 'pipelin', 'analyz', 'optim', 'big data', 'commun']","['program', 'spark', 'pipelin', 'set', 'optim', 'aw', 'parti', '3rd', 'big', 'integr', 'sourc', 'engin']","['sql', 'mapreduc', 'spark', 'aw', 'java', 'hbase', 'nosql', 'pipelin', 'analyz', 'optim', 'big data', 'commun', 'program', 'spark', 'pipelin', 'set', 'optim', 'aw', 'parti', '3rd', 'big', 'integr', 'sourc', 'engin']"
DE,"RESPONSIBILITIES:
The following duties include, but are not limited to:
Work with the Science team to develop this logic in Spark UDFs executed over a Spark cluster.
Modify JavaScript Object Notation (JSON) files to describe the schemas of the datasets, ensuring system functionality through routine maintenance and testing.
Work on a full-stack rapid-cycle analytic application.
Develop highly effective, performant, and scalable components capable of handling large amounts of data for over 10 million patients.
Work with the Science and Product teams to understand and assess client needs, and to ensure optimal system efficiency.
REQUIRED QUALIFICATIONS:
Bachelor's degree or equivalent in Computer Science, Computer Engineering, Information Systems, or a related field.
4 years of experience in the position offered or related position, including 4 years of experience with: designing, developing, maintaining large-scale data ETL pipelines using Java/Scala in AWS, Hadoop, Spark, and DataBricks to manage Apache Spark infrastructure.
Experience building backend modules, low latency REST API in distributed environment using Java, Docker, SQL, MVN, Spring, Jenkins.
Experience writing complex SQL queries, UDFs to process large amounts of data across relational, non-relational databases, JSON and Spark SQLs.
Experience translating requirements from product, DevOps teams to technology solutions using SDLC.
All locations are accessible by various forms of public transportation.
Social and energetic offices – with a modern layout, and a giant kitchen and eating/social area.
Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disabled status or, genetic information.
###
","['sql', 'spark', 'scala', 'hadoop', 'javascript', 'aw', 'java', 'docker']","['optim', 'etl', 'pipelin', 'cluster']",1,"['sql', 'spark', 'scala', 'hadoop', 'javascript', 'aw', 'java', 'docker', 'optim', 'etl', 'pipelin', 'cluster']","['analyt', 'spark', 'pipelin', 'relat', 'public', 'hadoop', 'infrastructur', 'optim', 'amount', 'aw', 'comput', 'etl', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'javascript', 'aw', 'java', 'docker', 'optim', 'etl', 'pipelin', 'cluster', 'analyt', 'spark', 'pipelin', 'relat', 'public', 'hadoop', 'infrastructur', 'optim', 'amount', 'aw', 'comput', 'etl', 'engin']"
DE,"A financial friend to the millions of Americans who use the app.
",[None],[None],999,[],[None],[None]
DE,"Start Assessment
As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients.
You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more.
The position requires fostering close working relationships with internal marketing teams as well as clients.
What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic.
Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
","['sql', 'bi', 'python', 'excel', 'power bi']","['visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']",1,"['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']","['visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']","['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast', 'visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Data Engineer Job Overview
The ideal candidate is self-sufficient and adept at using large data sets to find opportunities for product and process optimization, and in building and using models to test the effectiveness of different courses of action.
They are experienced in using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations.
They must demonstrate a proven ability to drive business results with their data-based insights.
Finally, they must be comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.
Responsibilities:
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Use predictive modeling to increase and optimize customer experiences, onboarding, revenue generation, ad targeting and other business outcomes.
Leverage A/B testing framework and test model quality.
Develop processes and tools to monitor and analyze model performance and data accuracy.
Essential Qualifications:
5-7 years of experience manipulating data sets and building statistical models.
Bachelor’s or Master’s in Statistics, Mathematics, Computer Science or another quantitative field.
Coding knowledge of several languages: C#, Python, JavaScript, Go, or similar.
Querying databases and using statistical computer languages: R, Python, Mongo, JQL, etc.
Using AWS services.
Analyzing data from 3rd party providers: Google Analytics, Mixpanel, Facebook, Appsflyer, Email service providers, etc.
Experience visualizing/presenting data for stakeholders.
Highly desired:
Experience working with product development and marketing teams, especially subscription products and kids games or apps.
Experience using statistical computer languages to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.)
and experience with applications.
Excellent written and verbal communication skills for coordinating across teams.
A proactive approach to learning and mastering new technologies and techniques.
Perks
The rare opportunity to change the world by helping create the first generation in history that is truly connected and collaborative.
The fun of working with an effective and multi-award winning product that is beloved by customers.
Sharp, motivated co-workers in a creative and supportive office environment.
Commitment to Diversity and Inclusion
","['javascript', 'aw', 'c', 'python', 'r', 'excel']","['data mining', 'regress', 'predict', 'optim', 'statist', 'commun']",1,"['javascript', 'aw', 'c', 'python', 'r', 'excel', 'data mining', 'regress', 'predict', 'optim', 'statist', 'commun']","['techniqu', 'predict', 'provid', 'python', 'quantit', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'action', 'learn', 'aw', 'parti', 'set', 'engin', 'divers', '3rd', 'comput']","['javascript', 'aw', 'c', 'python', 'r', 'excel', 'data mining', 'regress', 'predict', 'optim', 'statist', 'commun', 'techniqu', 'predict', 'provid', 'python', 'quantit', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'action', 'learn', 'aw', 'parti', 'set', 'engin', 'divers', '3rd', 'comput']"
DE,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst.
The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions.
The candidate should possess a data engineering background, business acumen to think strategically and love working with people.
Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis.
The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.
Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl']","['dashboard', 'healthcar', 'problem solving']",3,"['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'dashboard', 'healthcar', 'problem solving']","['essenti', 'techniqu', 'engin', 'challeng', 'aw', 'python', 'scientist', 'comput', 'collect']","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'dashboard', 'healthcar', 'problem solving', 'essenti', 'techniqu', 'engin', 'challeng', 'aw', 'python', 'scientist', 'comput', 'collect']"
DE,"Junior Data Engineer
Division: Global Science Organization (GSO), Data Science & AI Lab
The team develops analytic tools, builds data science models for pilot studies and internal stakeholder analytics innovations, and consults on a broad range of data and research best practices.
The developer in this role will help the team build full-stack data offerings and scale them to the cloud.
The position calls for an observant attention to detail, the ability to work well on a small team, and a self-starter approach to problem-solving and debugging.
As a Data Engineer, you will:
Work closely with team members on design of large-scale modeling efforts, contributing to cloud pipelines, including the containerization of current tools
Collaborate on engineering new data science products by translating needs identified with stakeholders into analytic frameworks that can be built into polished user-facing tools
Build, maintain, and enhance existing codebases
Synthesize tech innovations and cloud scalability to elevate business value for clients, both current and prospective
Validate, test, and maintain staging and production analytics environments in the cloud for data science teams globally
Provide consulting for internal teams and clients on data architectures and schemas, data hygiene, and areas for improving process efficiency
Requirements:
High proficiency writing Python and JavaScript (preferably React), including the ability to produce and maintain reusable and modular codebases
Bachelor's degree
Desire to work in a highly collaborative, fun, consensus-oriented environment.
Attentive learner with excellent time-management skills
Experience with deploying data models and processes in the cloud (e.g.
GCP, AWS).
Pluses:
Experience with Linux server and system administration.
Working knowledge of SQL and experience with database design and administration.
Experience in analytics, extracting and surfacing value from quantitative data.
Professional or academic experience with modern techniques and algorithms in machine learning and statistical computing (e.g.
deep learning).
Experience with collaboration tools (e.g.
Atlassian suite) and version control systems (e.g.
Git).
Large dataset manipulation.
Experience in distributed storage and computing.
Experience with ReactJS, TypeScript/JavaScript, visualization libraries (e.g.
d3.js).
Experience creating and deploying web apps (e.g.
Electron).
Advanced degree (M.S., Ph.D.), but not required.
Experience in the field of Market Research.
","['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react']","['research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist']",1,"['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react', 'research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist']","['analyt', 'machin', 'techniqu', 'learn', 'pipelin', 'visual', 'provid', 'aw', 'python', 'statist', 'quantit', 'engin', 'algorithm']","['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react', 'research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist', 'analyt', 'machin', 'techniqu', 'learn', 'pipelin', 'visual', 'provid', 'aw', 'python', 'statist', 'quantit', 'engin', 'algorithm']"
DE,"Start Assessment
As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients.
You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more.
The position requires fostering close working relationships with internal marketing teams as well as clients.
What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic.
Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
","['sql', 'bi', 'python', 'excel', 'power bi']","['visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']",1,"['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast']","['visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']","['sql', 'powerbi', 'python', 'excel', 'visual', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'forecast', 'visual', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'ml', 'input', 'optim', 'statist', 'learn', 'bi', 'set', 'engin', 'digit', 'machin', 'power', 'comput', 'relat']"
DE,"Position Information
Position Information
Job Title: Data Engineer Position Number: 2013141055 Department: Information Technology Job Category: Classified Unit A Time (Percent Time): 100% Term (months/year): 12 months/year Current Work Schedule (days, hours): Monday - Friday: 7:30 a.m. - 4:30 p.m. Salary Range: A-126 Salary: Steps 1 - 6: $6,902- $8,810 per month Shift Differential: Shift differential eligibility based on the current collective bargaining agreement Open Date: 12/18/2019 Initial Screening Date: Open Until Filled: Yes Application Procedure:
Complete application packets will be accepted until the position is filled.
Applicants must submit all of the following materials online unless otherwise noted at Mt.SAC Employment Website to be considered for this position:
A cover letter describing how the applicant meets the required education and experience.
A detailed résumé that summarizes educational preparation and professional experience for the position.
Two letters of recommendation that reflect relevant experience (do not use social media or professional networks as a means to provide letters of recommendation).
Unofficial transcripts are acceptable at the time of application; however, copies of diplomas are not accepted in lieu of transcripts.
Health & Welfare:
Lifetime retirement benefits provided for eligible retirees.
Note Salary and Health & Welfare Benefits are subject to change
Basic Function/Overview:
DEFINITION
Under general direction, leads and coordinates day-to-day operations of the Operational Data Store (ODS), Data Warehouse, and all related technologies; analyzes and transforms data into a format that can be easily used by different departments for reporting; collaborates with programmers, Business Analysts, and functional areas in gathering requirements and clarifying their needs for implementation, generation, optimization, and support of their data; provides complex professional staff assistance to the Director, Enterprise Application Systems in areas of expertise.
SUPERVISION RECEIVED AND EXERCISED
Receives general direction from the assigned managerial personnel.
Provides coordination and lead work direction to staff.
CLASS CHARACTERISTICS
This class is distinguished from Director, Enterprise Application Systems by the latter's management and supervisory authority in planning, organizing, and directing the full scope of enterprise operations within the department.
Essential Duties/Major Responsibilities:
EXAMPLES OF ESSENTIAL FUNCTIONS (Illustrative Only)
Responds and evaluates ad hoc requests for data, statistical analysis, research projects and studies; prepare requests for processing; arrange and maintain project schedules and timelines; design strategies to complete assignments; analyze and compare a variety of data solutions; make team project recommendations to the manager.
Reviews user needs and requests and develops proposed solution for usable data design or format for the users' reporting and analysis needs; monitors and tunes report queries and views.
Assists in implementing ways to improve data reliability, efficiency and quality.
Develops, constructs, tests, and maintains Operational Data Store and Data Warehouse architecture.
Discovers opportunities for data acquisition.
Expands the existing schema design to handle new data formats.
Develops and documents Operational Data Store and Data Warehouse standards, scripts, guidelines, and usage procedures; enforces standards for use, control, updates, and maintenance for the Operational Data Store and Data Warehouse environments.
Interacts and coordinates with other IT areas and key end users.
Ensures data models, design, and architecture that are in place support the requirements of the programmers, Business Analysts, researchers, and different functional areas.
Provides guidance on reporting, query or data extraction design, development, and maintenance, including monitoring performance tuning and optimization in queries; Recommends selection of query or reporting tools, methodologies, and procedures for development of reports and views.
Develops data set processes for data modeling, mining, and production; prepares data for use in predictive and prescriptive modeling.
Leverages large volumes of data from internal and external sources to answer reporting needs.
Participates on committees, task forces, and special assignments, including, but not limited to Screening and Selection Committees and affiliated trainings.
Prepares and delivers oral presentations related to assigned areas if needed.
Performs other related or lower classification duties as assigned.
Other Duties:
Performs other related duties as assigned.
Knowledge Of:
State-of-the-art information systems as applied to large, complex administrative, or educational organizational environments.
Principles and techniques of computer systems and software architectures.
Operating System (OS)-based platforms.
Programming languages, including but not limited to PL/SQL, SQL, Python, and Shell Scripting.
Database front-end programs such as Microsoft (MS) Access, Statistical Package for the Social Sciences (SPSS), and related products.
Data warehousing and techniques.
Principles and concepts of Relational Database Management System (RDBMS), Big Data, and Operational Data Store.
Statistical tools and research methods.
Principles, techniques, and methodologies in project management and leadership.
Business letter writing and record-keeping principles and procedures.
Use, capability, characteristics, and limitations of computer systems and databases
Methods, techniques, and practices of data collection and report writing.
Modern office practices, method, and computer equipment and applications related to the work, including word processing, database, and spreadsheet software.
Skills and Abilities:
Analyze informational requirements and needs, identify problems, provide technical advice and consultation, and ensure efficient computer system utilization.
Analyze data and develop logical solutions to problems.
Experience with Source code management systems.
Code Debugging and Performance troubleshooting.
Master new technologies quickly; stays abreast of current trends and developments in Operational Data Store, Data Warehouse, and Big Data.
Conduct complex research projects on a wide variety of information technology and database administration topics, evaluate alternatives, make sound recommendations, and prepare effective technical staff reports.
Establish and maintain a variety of filing, record-keeping, and tracking systems.
Organize and prioritize a variety of projects and multiple tasks in an effective and timely manner; organize own work, set priorities, and meet critical time deadlines.
Use English effectively to communicate in person, over the telephone, and in writing at both technical and functional levels.
Understand scope of authority in making independent decisions.
Review situations accurately and determine appropriate course of action using judgment according to established policies and procedures.
Establish, maintain, and foster positive and effective working relationships with those contacted in the course of work.
Explores and examines data to find hidden patterns.
Tell stories to key stakeholders based on the analysis.
Learns and applies emerging technologies, as necessary, to perform duties in an efficient, organized, and timely manner.
Minimum Qualifications/ Education & Experience:
Equivalencies: Preferred Qualifications: License(s) & Other Requirements:
The incumbent may periodically be required to travel to a variety of locations.
Examination Requirements: Working Environment:
Incumbents work in an office environment with moderate noise levels, controlled temperature conditions, and no direct exposure to hazardous physical substances.
Incumbents may interact with staff and/or public and private representatives in interpreting and enforcing departmental policies and procedures.
Physical Demands:
This is primarily a sedentary office classification although standing in and walking between work areas is frequently required.
Finger dexterity is needed to access, enter, and retrieve data using a computer keyboard or calculator and to operate standard office equipment.
Incumbents in this classification occasionally bend, stoop, kneel, reach, push, and pull drawers open and closed to retrieve and file information.
Incumbents must possess the ability to lift, carry, push, and pull materials and objects up to 20 pounds.
Hazards: Conditions of Employment:
It is also required that a final offer of employment will only be made after the candidate has successfully been live-scanned and clearance for employment is authorized by Human Resources.
Costs for live-scan services shall be borne by the candidate.
SAC Annual Security Report 2017
Typing Certificate Requirements: Special Notes:
Please note: A confirmation number will be assigned when your application packet indicates the supplemental questions have been answered and a document has been attached to each required link.
Assistance with the online application process is available through the Office of Human Resources at 1100 N. Grand Avenue, Walnut, CA 91789-1399.
Human Resources: (909) 274-4225.
E-mail: employment@mtsac.edu.
DO NOT include photographs or any demographic information (e.g.
D.O.B, place of birth, etc.)
on your application or supporting documents.
TRAVEL POLICY: Costs associated with travel in excess of 150 miles one way from residence for the purpose of an interview will be reimbursed up to $500 maximum.
Travel reimbursement claims (original receipts) must be submitted no later than 30 days following the interview date.
Foreign Transcripts:
Foreign Transcripts: Transcripts issued outside the United States require a course-by-course analysis with an equivalency statement from a certified transcript evaluation service verifying the degree equivalency to that of an accredited institution within the USA.
This report must be attached with the application and submitted by the filing deadline.
Inquiries/Contact:
Human Resources at 1100 N. Grand Avenue, Walnut, CA 91789-1399.
Human Resources: (909) 274-4225.
E-mail: employment@mtsac.edu.
Selection Procedure:
A committee will evaluate applications, taking into account breadth and depth of relevant education, training, experience, skills, knowledge, and abilities.
The screening committee reserves the right to limit the number of interviews granted.
Meeting the minimum qualifications for a position does not assure the applicant of an interview.
Interviews may include a writing sample, committee presentation, and/or performance test.
The start date will be following Board approval and receipt of live scan clearance.
Special Instructions to Applicants:
To be guaranteed consideration, it is the applicant's responsibility to ensure that all required materials are received before the initial screening date and time indicated on the job posting.
Incomplete application packets will not be considered.
SAC Employment Website to complete and submit your application for this position.
Letters of Recommendation
Confidential letters of recommendation are not accepted for this position.
All letters of recommendation must be uploaded to the application.
EEO Policy:
No person shall be denied employment because of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, marital status, sex (gender), age, sexual orientation, or the perception that a person has one or more of these characteristics.
Conflict of Interest
Cancel RTF Policy:
THIS RECRUITMENT MAY BE USED TO FILL FUTURE VACANCIES.
Quick Link http://hrjobs.mtsac.edu/postings/7464
","['sql', 'python', 'spss', 'microsoft']","['recommend', 'research', 'classif', 'big data', 'end user', 'tune', 'predict', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'information technology', 'supervis', 'account']",2,"['sql', 'python', 'spss', 'microsoft', 'recommend', 'research', 'classif', 'big data', 'end user', 'tune', 'predict', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'information technology', 'supervis', 'account']","['day', 'program', 'techniqu', 'predict', 'provid', 'python', 'packag', 'sourc', 'collect', 'evalu', 'essenti', 'judgment', 'human', 'optim', 'avail', 'appli', 'statist', 'action', 'learn', 'public', 'big', 'set', 'warehous', 'engin', 'comput', 'relat']","['sql', 'python', 'spss', 'microsoft', 'recommend', 'research', 'classif', 'big data', 'end user', 'tune', 'predict', 'data modeling', 'optim', 'data warehousing', 'statist', 'analyz', 'information technology', 'supervis', 'account', 'day', 'program', 'techniqu', 'predict', 'provid', 'python', 'packag', 'sourc', 'collect', 'evalu', 'essenti', 'judgment', 'human', 'optim', 'avail', 'appli', 'statist', 'action', 'learn', 'public', 'big', 'set', 'warehous', 'engin', 'comput', 'relat']"
DE,"• Responsibilities include performing data analysis, defining ETL architecture, data modeling and implementing robust data pipelines in the cloud using AWS, IICS and Snowflake.
Requirements:
• 7+ years hands-on experience in BI and Data Warehousing with at least 2 full life cycle implementations.
Includes architecture, design, data modeling (Kimball methodology), ETL and reporting.
• 2+ years experience implementing a Cloud DW/Data Lake.
• Demonstrably deep understanding of SQL, relational and analytical databases (Snowflake, Redshift, BigQuery, etc.)
• Hands-on, expert level experience of Informatica Intelligent Cloud Services
• Experience with AWS services (S3, Lambdas, etc.)
• Knowledge of Agile (Scrum) practices required
• Knowledge of ERPs (Oracle EBS, etc.)
highly desired
","['sql', 'erp', 'bigqueri', 'aw', 'snowflak', 'redshift', 's3', 'lambda', 'bi', 'cloud', 'oracl']","['data warehousing', 'etl', 'pipelin', 'data modeling']",999,"['sql', 'erp', 'bigqueri', 'aw', 'snowflak', 'redshift', 's3', 'lambda', 'powerbi', 'cloud', 'oracl', 'data warehousing', 'etl', 'pipelin', 'data modeling']","['analyt', 'pipelin', 'relat', 'aw', 'bi', 'etl']","['sql', 'erp', 'bigqueri', 'aw', 'snowflak', 'redshift', 's3', 'lambda', 'powerbi', 'cloud', 'oracl', 'data warehousing', 'etl', 'pipelin', 'data modeling', 'analyt', 'pipelin', 'relat', 'aw', 'bi', 'etl']"
DE,"It has been recognized as a corporate priority to drive expansion.
This postion is responsible for using and growing the OtoSense AI framework powering Sensing Interpretation systems for a large variety of customers.
The ideal candidate will be fluent in Python, familiar with the processing of sensing data and familiar with machine learning techniques and implementations methods.
There is no expectation of any deep expertise in Machine Learning or Data Science, but more an eargerness to learn from the system in place and its current architects.
This person must be exceedingly well organized, detail oriented and flexible, embracing the challenges of working with a variety of situations and circumstances while having the ability to interact with collaborators and external stakeholders at all levels in a complex environment.
The AID group is a fast paced and dynamic team where innovation is nurtured.
The variety and scope of projects provides an interesting and challenging opportunity for an impactful individual.Responsibilities* Processing datasets using provided data processing, machine learning and visualization components of the OtoSense AI platform.
* Communicating with all stakeholders (data science, product engineering, marketing, sales etc.)
to present results matching specific evaluation metrics.
* Improving platforms components and processes related to data processing pipelines.
* Respecting software engineering best practices, from abstracted architecture to variable naming.
* Documenting all changes brought to the system following established documentation guidelines.Requirements* Bachelor or Master degree in a data-intensive and software-intensive discipline.
* Proficient in Python.
* Good understanding of featurization and machine learning techniques.
* Excellent analytical and problem-solving skills.
","['python', 'excel']","['machine learning', 'visual', 'pipelin']",1,"['python', 'excel', 'machine learning', 'visual', 'pipelin']","['analyt', 'machin', 'techniqu', 'learn', 'challeng', 'pipelin', 'corpor', 'visual', 'provid', 'python', 'interpret', 'relat', 'engin', 'evalu']","['python', 'excel', 'machine learning', 'visual', 'pipelin', 'analyt', 'machin', 'techniqu', 'learn', 'challeng', 'pipelin', 'corpor', 'visual', 'provid', 'python', 'interpret', 'relat', 'engin', 'evalu']"
DE,"Data Engineer
Bigtime Entertainment Co. building state of the art software and
and makes
vital business decisions with large revenue impacts.
As a Data
Engineer
supporting the Data Science team, you will frame, pose and
translate business
problems to build AI-powered solutions that directly contribute
to data
products.
As a member of the Data Science & Engineering team, you
will be
designing and building scalable models & architectures upon while
ML algorithms
can thrive, as well as refining existing model implementations so
that they
automatically build context in order to perform above and beyond
expectations.
From creating
experiments and prototyping implementations to designing new
resolve challenging and meaningful problems with compelling
business use cases.
the latest
advances in ML research to transform the broader media market
product successes
In this role you will:
· Collaborate with the
data science team to build future-proof frameworks and
abstractions
· Collaborate with
product management and engineering departments to understand
devise AI powered solutions
· Build tools that
will increase the productivity of the Data Analytics team-members
developing AI-based systems
· Implement models
that the data science team develops into working prototypes,
proof of concepts,
self-supporting model ecosystems
· Build data pipelines
that contribute to a self-sustaining data model system
· Build demos and
conduct training in conjunction with data scientist to help the
broader
engineering organization (and/or business partners) effectively
use the product
· Demonstrate and
improved products,
processes, or technologies
· Participate in
cutting-edge research in artificial intelligence and machine
learning
applications
· Optimize models for
on-device and multi-modal intelligence
Qualifications:
• Experienced Data
Engineer with a BS, MS or PhD in a quantitative field (CS,
Engineering,
Physics, etc.)
• 4+ years hands-on
business experience, demonstrated implementations of ML models
and techniques
is a plus
• Advanced in Python
• Experience with AWS
infrastructure (AWS Certifications are a plus)
• Strong knowledge of
relational and distributed databases, extremely strong in SQL
• Multiple
Implementations that feature good memory, disk I/O, and CPU/GPU
management.
• Experience with
Apache infrastructure
• Experience with
streaming data and video manipulation
• Familiarity with
common ML algorithms (i.e.
neural networks, tree-based methods,
unsupervised
learning, feature engineering)
• Familiarity with
ML/AI frameworks, e.g,.
TensorFlow, Spark, and modular/modern
software design
practices.
• History of research
publications and/or implementations of state of the art
techniques hosted on
open source repositories is a plus
• Passionate about
ML/AI and how it can improve both the media industry and the
world
AWS, Python, Python
Data Science packages, Spark, Hadoop, Apache
Resumes sent to: jim@ingenium.agency
Top base salary, excellent benefits and work culture
","['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel']","['research', 'pipelin', 'neural network', 'optim', 'unsupervis']",1,"['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel', 'research', 'pipelin', 'nn', 'optim', 'unsupervis']","['techniqu', 'python', 'packag', 'quantit', 'common', 'sourc', 'algorithm', 'analyt', 'challeng', 'ml', 'hadoop', 'optim', 'learn', 'public', 'infrastructur', 'aw', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'power', 'scientist', 'relat']","['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel', 'research', 'pipelin', 'nn', 'optim', 'unsupervis', 'techniqu', 'python', 'packag', 'quantit', 'common', 'sourc', 'algorithm', 'analyt', 'challeng', 'ml', 'hadoop', 'optim', 'learn', 'public', 'infrastructur', 'aw', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'power', 'scientist', 'relat']"
DE,"Title: Data Engineer
Job ID: TJ3682741021
The ideal candidate thrives in a fast paced environment has the ability to communicate clearly.
This role will be responsible for automating and maintaining batch pipelines that collect and process data; automating and maintaining batch pipelines that collect and process data; designing and building tools that provide confidence in their data quality; and mentoring and coaching other software engineers.
Qualifications:
Bachelor’s degree in Computer Science or comparable field
5+ years experience in Python and SQL
5+ years experience in Java, Scala, or similar OO experience
5+ years experience with Spark, Hadoop, or Databricks
Experience with data analysis, processing, and validation
Professional experience with open source ETL frameworks such as Airflow, Luigi, or similar
Knowledge within a diverse set of public cloud technologies: AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Google Cloud Bigtable, etc.
","['sql', 'google cloud', 'spark', 'airflow', 'scala', 'bigtabl', 'ec2', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'cloud']","['etl', 'pipelin']",1,"['sql', 'google cloud', 'spark', 'airflow', 'scala', 'bigtabl', 'ec2', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'cloud', 'etl', 'pipelin']","['spark', 'pipelin', 'set', 'divers', 'public', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'google cloud', 'spark', 'airflow', 'scala', 'bigtabl', 'ec2', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'cloud', 'etl', 'pipelin', 'spark', 'pipelin', 'set', 'divers', 'public', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"Develops and maintains scalable cloud-based ingress and egress data pipelines
Support continuing increases in data volume and complexity
Collaborates with team’s analytics members to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Writes unit/integration tests, contribute to engineering wiki, and documents work
Performs data integrity tasks required to troubleshoot data-related issues and assist in the resolution of data issues
Design data integrations and data quality framework
Contribute to the continuing technology stacks advancements in a leadership level
Works closely with engineering clients/partners to develop strategies for long term data platform architecture
Qualifications / Skills:
Knowledge of Microsoft Azure-based ETL best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process-oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Experience Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience, specifically in sparkSQL and hiveSQL
4+ years of experience with schema design and dimensional data modeling
2+ years of DataBricks or equivalent spark development using scala or python
2+ years of Apache Data Frames in Microsoft Azure data blob, Databricks Delta Lake experience preferred.
2+ years of experience designing, building, and maintaining ETL systems
Experience of Azure Function Apps, Event Hub and TomCat Servers
Experience of supporting Microsoft SQL and MySQL
Experience of Kafka, Oracle, Splunk, Google Analytics, Apple Store data interface
Experience of developing JDBC and ODBC connection
Experience of developing REST API interface
#LI-AS
All employment decisions are based on merit, experience, and business needs, without regard to race, color, national origin, age, religion, sex, pregnancy (including childbirth or related medical conditions), genetic information, disability (physical or mental), medical condition, marital status, sexual orientation, gender identity or gender expression, military or veteran status, or any other consideration made unlawful by federal, state, or local law.
Learn more about pay transparency.
EEO is the law.
See the EEO poster and supplement.
","['sql', 'spark', 'azur', 'scala', 'excel', 'cloud', 'python', 'java', 'microsoft', 'mysql', 'kafka', 'splunk', 'oracl']","['pipelin', 'data modeling', 'problem solving', 'etl', 'commun']",1,"['sql', 'spark', 'azur', 'scala', 'excel', 'cloud', 'python', 'java', 'microsoft', 'kafka', 'splunk', 'oracl', 'pipelin', 'data modeling', 'problem solving', 'etl', 'commun']","['analyt', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'python', 'avail', 'comput', 'etl', 'engin', 'integr']","['sql', 'spark', 'azur', 'scala', 'excel', 'cloud', 'python', 'java', 'microsoft', 'kafka', 'splunk', 'oracl', 'pipelin', 'data modeling', 'problem solving', 'etl', 'commun', 'analyt', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'python', 'avail', 'comput', 'etl', 'engin', 'integr']"
DE,"Experience with Python Strong experience writing and optimizing SQL statements Some knowledge of schema design (logical and physical) is desired Familiarity with distributed processing (Map Reduce, MPP, etc.)
An interest in learning new languages, identifying and using best-in-class tools, and applying industry best practices
","['sql', 'python']",[None],999,"['sql', 'python']",['python'],"['sql', 'python', 'python']"
DE,"Data Engineer
As a Data Engineer supporting the Data Science team, you will frame, pose and translate business problems to build AI-powered solutions that directly contribute to data products.
As a member of the Data Science & Engineering team, you will be designing and building scalable models & architectures upon while ML algorithms can thrive, as well as refining existing model implementations so that they automatically build context in order to perform above and beyond expectations.
In this role you will:
Collaborate with the data science team to build future-proof frameworks and abstractions
Build tools that will increase the productivity of the Data Analytics team-members developing AI-based systems
Implement models that the data science team develops into working prototypes, proof of concepts, self-supporting model ecosystems
Build data pipelines that contribute to a self-sustaining data model system
Build demos and conduct training in conjunction with data scientist to help the broader engineering organization (and/or business partners) effectively use the product
Participate in cutting-edge research in artificial intelligence and machine learning applications
Optimize models for on-device and multi-modal intelligence
Qualifications:
Experienced Data Engineer with a BS, MS or PhD in a quantitative field (CS, Engineering, Physics, etc.)
4+ years hands-on business experience, demonstrated implementations of ML models and techniques is a plus
Advanced in Python
Experience with AWS infrastructure (AWS Certifications are a plus)
Strong knowledge of relational and distributed databases, extremely strong in SQL
Multiple Implementations that feature good memory, disk I/O, and CPU/GPU management.
Experience with Apache infrastructure
Experience with streaming data and video manipulation
Familiarity with common ML algorithms (i.e.
neural networks, tree-based methods, unsupervised learning, feature engineering)
Familiarity with ML/AI frameworks, e.g,.
TensorFlow, Spark, and modular/modern software design practices.
History of research publications and/or implementations of state of the art techniques hosted on open source repositories is a plus
Passionate about ML/AI and how it can improve both the media industry and the world
AWS, Python, Python Data Science packages, Spark, Hadoop, Apache
Resumes sent to: jim@ingenium.agency
Top base salary, excellent benefits and work culture
","['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel']","['research', 'pipelin', 'neural network', 'machine learning', 'optim', 'unsupervis']",1,"['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel', 'research', 'pipelin', 'nn', 'machine learning', 'optim', 'unsupervis']","['techniqu', 'python', 'packag', 'quantit', 'common', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'learn', 'public', 'infrastructur', 'aw', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'power', 'scientist', 'relat']","['sql', 'spark', 'hadoop', 'aw', 'python', 'tensorflow', 'excel', 'research', 'pipelin', 'nn', 'machine learning', 'optim', 'unsupervis', 'techniqu', 'python', 'packag', 'quantit', 'common', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'learn', 'public', 'infrastructur', 'aw', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'power', 'scientist', 'relat']"
DE,"The ideal candidate would have a passion for and knowledge of data engineering, big data, and distributed/cloud computing, and enjoys supporting and fulfilling the strategic objectives of a highly cross-functional organization.
What You'll Do:
Build, test and refine data pipelines for data analytics and data science
Data modeling, process design, and overall data pipeline architecture
Work closely with the analytics, data science, product, finance, operations, and marketing teams to design, build, and test end-to-end solutions
What You'll Bring:
5+ years of software engineering experience with a focus on data
Solid experience in SQL development
Solid experience with Python, Spark, shell scripting
Extensive ETL development experience with large-scale DBS or big data systems such as Redshift, Snowflake, Databricks, etc.
Experience working with reporting tools such as Tableau or Looker
Extensive experience with design & development of relational databases and data warehouses
Ability to look at solutions unconventionally and explore opportunities and devise innovative solutions
Excellent communication skills (verbal and written) and interpersonal skills and an ability to communicate with both business and technical teams
Experience gathering business requirements and identifying data needs
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Bachelor's degree in computer science or related field
Bonus Points:
Data lake experience
Experience on subscription service products
Knowledge of accounting, FP&A, and marketing functions
What You'll Get:
Amazing benefits including medical, dental, vision, FSA, and 401k
Generous PTO
","['sql', 'spark', 'looker', 'cloud', 'snowflak', 'redshift', 'python', 'tableau', 'db', 'excel']","['pipelin', 'data modeling', 'financ', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'looker', 'cloud', 'snowflak', 'redshift', 'python', 'tableau', 'db', 'excel', 'pipelin', 'data modeling', 'financ', 'big data', 'etl', 'commun']","['analyt', 'spark', 'pipelin', 'relat', 'python', 'warehous', 'comput', 'big', 'etl', 'engin']","['sql', 'spark', 'looker', 'cloud', 'snowflak', 'redshift', 'python', 'tableau', 'db', 'excel', 'pipelin', 'data modeling', 'financ', 'big data', 'etl', 'commun', 'analyt', 'spark', 'pipelin', 'relat', 'python', 'warehous', 'comput', 'big', 'etl', 'engin']"
DE,"Data Engineer
If you are a Data Engineer looking for a new opportunity, read on!
analytics.
What You Need for this Position
- SPARK
- Redshift
- Linux
- UNIX
- Shell
- AWS
- SQL
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'linux', 'spark', 'unix', 'aw', 'redshift']",[None],999,"['sql', 'linux', 'spark', 'unix', 'aw', 'redshift']","['analyt', 'aw', 'spark', 'engin']","['sql', 'linux', 'spark', 'unix', 'aw', 'redshift', 'analyt', 'aw', 'spark', 'engin']"
DE,"The ideal candidate will possess all of the required skills listed below and more!
Duties & Responsibilities:
Build data pipelines orchestration.
Create the design and architecture for data-lake, data-marts, data-models, and data-warehouse.
Ensure efficiency of data science workflows and advanced machine learning algorithms.
Build and optimize performance of Hadoop and Spark batch jobs (Spark, Kafka, Cassandra, etc.).
Construct and improve ElasticSearch performance.
Contribute to open source solutions and communities.
Stay current on emerging tools and technologies.
Collaborate cross-functionally with other software engineers and their teams.
Establish and demonstrate technologies, solutions, and leading practices.
Balance resources, requirements, and complexity.
Qualifications:
5+ years of experience in full software development lifecycle.
5+ years of experience developing bis data apps.
Bachelor’s or Master’s degree in Computer Science, Engineering, Mathematics or Physical Sciences.
Minimum 3 years’ experience with Apache Spark and Spark Streaming.
Minimum 3 years’ experience with Hadoop batch processing framework and map reduce design patterns.
Minimum 3 years’ experience with Java/Scala/Python in support of data applications.
Expertise in Java 1.8+, Scala 2.11, and Linux.
Demonstrated proficiency in programming and analysis (design patterns, hardware, software requirements, systems requirements, deployment protocols, etc.).
Previous experience in an Agile environment using Scrum.
Previous experience with open source technologies, widely used RDBMS’ and SQL, and at least one columnar NoSQL solution.
Preferred Qualifications:
Prior experience developing and maintaining large scale, consumer facing web apps.
Prior experience working for a large data enterprise organization, creating robust and reliable data pipelines, and using multiple NoSQL solutions (HBase, MongoDB, Neo4j).
Previous operational experience with large software systems.
Previous Schema or Cube design experience.
Familiarity with statistics, machine learning, and natural language processing apps.
Knowledge of security and PII and PCI compliance issues.
Previous experience with the following preferred:
Apache Airflow
Amazon AWS
AWS EMR
docker and Kubernetes
Restful APIs
Apache Kafka
Apache HBase
Apache Hive and/or Apache Crunch
Apache Avro
Powered by JazzHR
h23E4mKY4q
","['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'elasticsearch', 'cassandra', 'scala', 'bi', 'aw', 'spark', 'airflow']","['natural language processing', 'pipelin', 'hardwar', 'machine learning', 'optim', 'statist', 'commun']",1,"['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'elasticsearch', 'cassandra', 'scala', 'powerbi', 'aw', 'spark', 'airflow', 'nlp', 'pipelin', 'hardwar', 'machine learning', 'optim', 'statist', 'commun']","['machin', 'program', 'stream', 'learn', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'bi', 'python', 'aw', 'statist', 'comput', 'warehous', 'sourc', 'engin', 'algorithm']","['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'elasticsearch', 'cassandra', 'scala', 'powerbi', 'aw', 'spark', 'airflow', 'nlp', 'pipelin', 'hardwar', 'machine learning', 'optim', 'statist', 'commun', 'machin', 'program', 'stream', 'learn', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'bi', 'python', 'aw', 'statist', 'comput', 'warehous', 'sourc', 'engin', 'algorithm']"
DE,"Pluto TV is the leading free streaming television service in America, delivering 200+ live and original channels and thousands of on-demand movies in partnership with major TV networks, movie studios, publishers, and digital media companies.
Pluto TV is available on all mobile, web and connected TV streaming devices and millions of viewers tune in each month to watch premium news, TV shows, movies, sports, lifestyle, and trending digital series.
Headquartered in Los Angeles, Pluto TV has offices in New York, Silicon Valley, Chicago and Berlin.
As a Data Engineer, you have a solid understanding of both the business and the technical aspects of BI in relation to digital media business.
You will drive the completion of projects within the established scope, while simultaneously planning for and leading unknown future BI requirements in a dynamic environment.
Design, model and develop data sets to support reporting and analytics in a cloud environment.
ETL development: cover all aspects of programming assignments and assist with systems design.
Develop and maintain a technical metadata framework and repository of data events and ETL operations.
Lead and administer Analytics tools and tag management systems.
Help plan and maintain the technical infrastructure, its configuration, performance, and storage requirements, with consideration of tiered data and data archiving.
Generate ad-hoc queries and reports based on business requirements.
Provide ongoing evaluations of technology solutions and capabilities to ensure alignment with business objectives, identify areas of risk, while monitoring the current environment and potential improvement areas.
Work with business partners to gather, analyze, and translate requirements in BI reporting area – either recommending an existing solution, developing a solution, or synthesizing delivery requirements to engineering teams for development.
Actively question and challenge customers to understand their requirements and reach the best solutions, near and long term.
Understand and adhere to development and documentation standards, database design and storage.
Successfully execute process improvements impacting own work and work of others.
On-call application support is required.
3+ years of premier Data engineering experience; at least 2 years on cloud Infrastructure.
Working knowledge of digital media ecosystem, including how digital video streaming, ad servers, DSPs, SSPs work.
Experience with a mix of Cloud and Enterprise data environments with real world implementation of data collection and processing on AWS environment.
Knowledge of web technologies and online advertising systems.
Experience with real-time Big Data analytics.
Experience with Hadoop, MapReduce, Spark, Flink and/or other Big Data processing platforms.
Excellent knowledge of OLAP concepts.
Familiarity with columnar databases like Redshift, Vertica etc.
Programming language such as Java, and scripting languages like python, ruby and Unix shell scripts.
Experienced working in a fast-paced, high-tech environment (preferably software development) and comfortable navigating conflicting priorities and ambiguous problems.
Experience with data visualization tools such as Looker, Tableau.
Great communication and collaboration skills across technical and non-technical partners.
A Bachelor’s degree in Computer Science or equivalent preferred.
","['mapreduc', 'spark', 'looker', 'unix', 'hadoop', 'bi', 'aw', 'python', 'redshift', 'java', 'tableau', 'cloud', 'excel', 'rubi']","['big data', 'visual', 'risk', 'analyz', 'etl', 'commun']",1,"['mapreduc', 'spark', 'looker', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'redshift', 'java', 'tableau', 'cloud', 'excel', 'rubi', 'big data', 'visual', 'risk', 'analyz', 'etl', 'commun']","['visual', 'provid', 'python', 'etl', 'collect', 'evalu', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'big', 'set', 'engin', 'digit', 'spark', 'comput', 'relat']","['mapreduc', 'spark', 'looker', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'redshift', 'java', 'tableau', 'cloud', 'excel', 'rubi', 'big data', 'visual', 'risk', 'analyz', 'etl', 'commun', 'visual', 'provid', 'python', 'etl', 'collect', 'evalu', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'big', 'set', 'engin', 'digit', 'spark', 'comput', 'relat']"
DE,"Qualifications
Bachelor’s Degree in Computer Science, Data Analytics or similar discipline including Mathematics, Statistics, Physics, or Engineering is preferred
Advanced degree in Life or Physical Science, Bioengineering, Biomedical Engineering or closely related discipline is preferred
Minimum of 4 years work related experience with degree or sufficient transferable experience to demonstrate functional equivalence to a degree
Advanced Experience with programming scripts such as Python, Java, Scala, C++ in Linux/Unix, and R
Experience in applying data analysis techniques to a large set of data using big data systems such as Hadoop, Spark, MongoDB, or similar software
Advanced analytics knowledge and application in the field of
Statistics
Mathematical programming
Business acumen and experience with operational or strategic systems
Recruitment, hiring, training, and job assignments are made without regard to race, color, national origin, age, ancestry, religion, sex, sexual orientation, gender identity, gender expression, marital status, disability, or any other protected classification.
","['mongodb', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'python', 'java', 'r']","['classif', 'statist', 'big data']",1,"['mongodb', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'python', 'java', 'r', 'classif', 'statist', 'big data']","['analyt', 'program', 'techniqu', 'spark', 'relat', 'hadoop', 'python', 'comput', 'statist', 'big', 'set', 'engin']","['mongodb', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'python', 'java', 'r', 'classif', 'statist', 'big data', 'analyt', 'program', 'techniqu', 'spark', 'relat', 'hadoop', 'python', 'comput', 'statist', 'big', 'set', 'engin']"
DE,"**WarnerMedia's new streaming entertainment offering, HBO Max?is the culmination of some of the most innovative new technology and greatest creative talent in the industry.
HBO Max will also be home to key third-party library acquisitions, such as _Friends_ , _South Park_ and _Doctor Who_ ,?and?more than?50 exclusive?Originals in its first year, from the likes of groundbreaking creative talent such as J.J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa McCarthy, Robert Zemeckis, Ellen DeGeneres, and other visionaries.
**The Job**As the Data Engineer on the HBO Max Data Insights & Operations team, you will be a part of the team that is responsible for developing, constructing, test existing and new architectures.
You will play an integral role in identifying ways to improve data reliability, efficiency and quality.
This position will work closely with various divisions including Product, Content and Marketing.
**The Daily**+ Strong experience in developing, constructing, testing, and maintaining existing and new architectures.+ Responsible for aligning architecture with business requirements.+ Ability to identify ways to improve data reliability, efficiency, and quality.+ Responsible for preparing data for predictive and prescriptive modeling.+ Ability to deploy sophisticated analytics programs, machine learning and statistical methods.+ Conduct research for industry and business questions that come up.
**The Essentials**+ BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle.+ Minimum of 2-3 years of experience in data science engineering, with big data tools like Hadoop and Spark.+ Robust experience with relational SQL and NoSQL databases like MongoDB, DynamoDB, Redshift/PostGRE, mariaDB, etc.+ Experience with data pipeline and workflow management tools like airflow or alternatives.+ In depth experience and knowledge with AWS cloud services: S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc.+ Experience with stream-processing systems: Spark-Streaming, AWS Kinesis, Kinesis Firehose etc.+ Must be experienced with object-oriented/object function scripting languages: Python, Java, Scala, etc.+ Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must.
Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
","['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'postgr', 'cloud']","['research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data']",1,"['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'cloud', 'research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data']","['program', 'stream', 'predict', 'python', 'integr', 'analyt', 'essenti', 'hadoop', 'statist', 'learn', 'aw', 'parti', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'cloud', 'research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data', 'program', 'stream', 'predict', 'python', 'integr', 'analyt', 'essenti', 'hadoop', 'statist', 'learn', 'aw', 'parti', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Req ID: 177552
The Job
Warner Bros. has been entertaining audiences for more than 90 years through the worlds most-loved characters and franchises.
Warner Bros. employs people all over the world in a wide variety of disciplines.
WB Technology combines Warner Bros. industry-leading technologists and disciplines to ensure global alignment with business strategy and accelerated delivery of innovative technology solutions studio- and industry-wide.
From pre-production through archiving, the WBT organization will provide critical business and technology intelligence and services to all Studio business units.
WBT manages the Studios enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.
As a Data Engineer at WB, you will be responsible for the creating data engineering components using Snowflake in an AWS ecosystem.
The Engineer will also be responsible for hands on development of high-quality scalable enterprise data solutions and data services, managing product launches and maintenance from inception to launch and through continuous delivery.
The ideal candidate must collaboratively partner across the organization to understand Warner Bros. business functions and data requirements.
The engineer will translate those into data products, software features and services to allow Warner Bros. to continue to innovate.
The candidate must be able to implement seamless data services across cross-functional teams and continually improve upon key performance indicators.
You must also be an experienced problem-solver and be comfortable navigating ambiguity and influencing.
You will work closely with product managers, project managers, data architects and data engineers to implement and maintain data pipelines leveraging emerging and established data technologies.
You will need to be able to work in a constantly changing, fast-paced environment and must be able to adapt to new technologies and new ideas.
You must be detail oriented, with a strong affinity for data and analytics to support business decisions.
The Daily
Work closely with the business and technical project manager to understand the business requirement and translate into technical specs.
Provide analysis reports and estimations.
Design, develop, install, test and maintain data integrations from a variety of formats including files, database extracts and external APIs into data stores (including Snowflake, Elastic, S3, etc) using ETL tools, techniques and programming languages like Python, Spark, SQL, etc.
Build high-performance data engineering algorithms and prototypes.
Create flexible data models, tune queries and ETL components.
Manage job orchestration using tools like Airflow.
Research possible customization for tuning, cost optimization, performance enhancements, data reliability and quality.
Ensure that all solutions meet the business/company requirements for solution data reliability, quality and disaster recovery.
Own the application/data end-to-end from requirements to post production working closely with other teams.
Provide engineering leadership by actively advocating best practices and standards for software engineering.
Collaborate with other team members such as data architects, data scientists etc.
Consistently contribute into the project management practices using Agile method.
Present the prototype to the stakeholders and leadership.
The Essentials
Bachelors degree in Computer Science or related field.
Minimum of 5 years of data analytics/data engineering, complex ETL/ELT experience in database systems like Snowflake, Teradata, etc.
using Python.
Minimum 5 years of experience in using SQL/NoSQL, JSON and XML data structures.
Minimum 5 years of big data technologies including Hadoop, Apache Spark, Snowflake and AWS Suite of technologies (S3, EMR, Lambda).
Minimum 3 years of Experience using Restful APIs for ETL purposes.
Expert problem solver with strong analytical skills.
Expert in SQL (Snowflake, Teradata) and Python.
Expert in ETL/ELT tools and techniques.
Experience using big data tools (Hadoop, Map-reduce, Elastic search, Kinesis, Kafka, Solr).
Experience using AWS technologies (EMR, S3, Kinesis, Lambda).
Experience using Restful APIs for ETL purposes.
Experience using Git or SVN and Jira.
Experience using Spark (Scala/Java), Spark SQL and Spark Streaming is a plus.
Experience in Segments, MParticle, Adobe Site Catalyst or any other similar digital analytics product products is a plus.
Experience in Tableau is a plus.
Strong communication skills and proficient in Excel, Word, PowerPoint, MS Project and MS Visio Snowflake.
Ability to work independently or collaboratively.
Detail oriented with strong organization and prioritization skills.
Entertainment and/or Social Media experience a plus
Demonstrated ability to work well under time constraints.
Must be able to work flexible hours, including possible overtime, when necessary.
Must be able to maintain confidentiality.
Management has the right to add or change duties and job requirements at any time.
177552
","['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr']","['research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun']",1,"['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr', 'research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun']","['techniqu', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'essenti', 'hadoop', 'optim', 'aw', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr', 'research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun', 'techniqu', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'essenti', 'hadoop', 'optim', 'aw', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Â
Â
Duration: Long term Contract
Experience: 8+ Years
Rate: DOE$/hr.
Â
Deploy and maintain data pipelines
Assemble large, complex data sets
Build optimal ETL infrastructure using AWS offerings
Build high performance, fault-tolerant, and scalable systems
Collaborate and coordinate with other teams and parts of the business
Communicate technical concepts to non-technical stakeholders
Strong Python programming skills
Strong hands-on Spark programming experience
Strong SQL coding abilityÂ
Redshift experience
Strong experience with AWS Services, well versed with various AWS ETL Services (EMR, Glue, etc.
), RDS, AWS Lambda, etc.
(Preferred)
Work experience with Databricks is highly desirable
Â
Â
Thanks and look forward to working with you,
Â
","['sql', 'spark', 'aw', 'lambda', 'python', 'redshift']","['optim', 'etl', 'commun', 'pipelin']",999,"['sql', 'spark', 'aw', 'lambda', 'python', 'redshift', 'optim', 'etl', 'commun', 'pipelin']","['spark', 'pipelin', 'set', 'infrastructur', 'optim', 'aw', 'python', 'etl', 'â']","['sql', 'spark', 'aw', 'lambda', 'python', 'redshift', 'optim', 'etl', 'commun', 'pipelin', 'spark', 'pipelin', 'set', 'infrastructur', 'optim', 'aw', 'python', 'etl', 'â']"
DE,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst.
The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions.
The candidate should possess a data engineering background, business acumen to think strategically and love working with people.
Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis.
The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.
Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
Working Title: Data Engineer
Department: EIS Data Analytics Team
Business Entity: Corporate Services
City: Los Angeles
Job Category: Information Technology
Job Specialty: Business Intelligence/Reporting
Position Type: Full-time
Shift Length: 8 hour shift
Shift Type: Day
","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl']","['information technology', 'dashboard', 'healthcar', 'problem solving']",3,"['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'information technology', 'dashboard', 'healthcar', 'problem solving']","['day', 'analyt', 'essenti', 'techniqu', 'engin', 'challeng', 'corpor', 'aw', 'python', 'scientist', 'comput', 'collect']","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'information technology', 'dashboard', 'healthcar', 'problem solving', 'day', 'analyt', 'essenti', 'techniqu', 'engin', 'challeng', 'corpor', 'aw', 'python', 'scientist', 'comput', 'collect']"
DE,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst.
The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions.
The candidate should possess a data engineering background, business acumen to think strategically and love working with people.
Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis.
The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.
Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
Working Title: Data Engineer
Department: EIS Data Analytics Team
Business Entity: Corporate Services
City: Los Angeles
Job Category: Information Technology
Job Specialty: Business Intelligence/Reporting
Position Type: Full-time
Shift Length: 8 hour shift
Shift Type: Day
","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl']","['information technology', 'dashboard', 'healthcar', 'problem solving']",3,"['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'information technology', 'dashboard', 'healthcar', 'problem solving']","['day', 'analyt', 'essenti', 'techniqu', 'engin', 'challeng', 'corpor', 'aw', 'python', 'scientist', 'comput', 'collect']","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'information technology', 'dashboard', 'healthcar', 'problem solving', 'day', 'analyt', 'essenti', 'techniqu', 'engin', 'challeng', 'corpor', 'aw', 'python', 'scientist', 'comput', 'collect']"
DE,"Summary:
Responsibilities:
Design, develop, test, implement and support applications using custom ETL (Extract Transform Load) or open source tools such as Talend
Prepare high-level component architecture; design documents, data flow diagrams, detail design documents, data schema and modeling combined with test plan documents
Design, develop and test highly available and scalable data pipelines and relevant data storage systems to enable business success across a multi-product functionality
Proactively identify operational and systemic issues within the data supply value chain (from collection to processing to reporting) and work with production operations (DevOps) team to implement monitoring solutions
Execute in a fast-paced matrix organization across product and engineering teams to identify best data-driven solutions for the underlying data infrastructure and platform
Required Qualifications:
Experience designing and building large, scalable data systems, preferably across a multi-product portfolio
Strong SQL skills with proven ability to write complex data queries across large data sets.
Exposure to software development, preferably in an Agile/Scrum/Kanban environment across multiple products
Experience analyzing and manipulating data across diverse data sources (Python, Scala)
Experience working across AWS (Amazon Web Services) cloud environment (EC2, S3, RDS, Sagemaker)
Strong exposure to Big Data technology, preferably across a containerized environment (Hadoop, Spark, Hive, Presto)
Experience with sourcing and modeling data from Restful API (Application Programming Interface)
Strong attention to detail with excellent analytical, problem-solving, and communication skills
Good time management and ability to work on concurrent assignments with different priorities
Ability to work in a fast paced, iterative development environment with short turnaround times
Preferred Qualifications:
Experience with A/B Testing and related optimization across desktop and mobile in a digital environment a plus (examples include: Optimizely, Leanplum, deltaDNA)
Experience analyzing and manipulating data across several data formats (JSON, Avro, Parquet, ORC)
Experience building and architecting data warehouse workflows in large cloud-based production environments (Snowflake is an example)
Experience migrating on-prem data solutions to the cloud with a strong data operational hygiene
Prior experience working in educational technology companies or a related competitive landscape is a plus
Medical, Dental, Vision + 401k
Highly competitive PTO policy
Casual Dress Code
Snacks + Drinks (Coca Cola Freestyle Machine)
Gaming room including an Arcade (2,000+ games)
Limitless opportunities for professional growth!
","['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'amazon web services']","['big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun']",999,"['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun']","['program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['sql', 'spark', 'scala', 'sagemak', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'hive', 'cloud', 'excel', 'big data', 'pipelin', 'optim', 'analyz', 'etl', 'commun', 'program', 'python', 'etl', 'sourc', 'collect', 'analyt', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"The right candidate would be a self-motivated, highly detail-oriented team-player with a positive drive to strategize and implement BI Solutions that enable the business to derive valuable insights.
Develop detailed ETL specifications based on business requirements* Interface with other technology teams to develop/maintain the ETL footprint and quality from a wide variety of in-house and 3rd party data sources* Implement and monitor machine learning algorithms and solutions in production.
* Constantly Monitor, refine and maintain system performance and provide statistical reporting* Participate in cross-functional meetings to review business requirements/use stories, assist in fit/gap analysis and provide detailed technical design documentation* Partner with business users, senior architects, product managers, engineering teams, and other teams to deliver a robust data services platform* Diagnose ETL and database related issues, perform root cause analysis(RCA), and recommend corrective actions to management* Recommend ways to improve data reliability, efficiency and quality* Profile and understand the large amounts of source data available, including structured and semi-structured/web/mobile activity data* Mentor the team on the industry standards and best practices for effective use of data integration and data quality technologies and exception handling* Provide cross organizational business stakeholders operational support on existing and newly developed data pipelineRequired Skills:* 3+ years of data engineering experience with high performance Big Data platforms including cloud-based Data Warehousing on large scale development efforts leveraging industry standard ETL tools* 3+ years of experience creating and managing data pipelines using Python (experience with Airflow preferred)* 3+ years of Data Warehouse development for 100's of gigabytes of data and billions of records (Snowflake or Redshift is preferred)* Experience developing ELT pipelines using Snowflake, Terradata, Vertica, Redshift or similar data warehousing technologies* Experience implementing streaming pipelines (Kinesis, Kafka, Storm, Spark, Flink)* Experience working in an environment that ingests large amounts of raw data (web logs, Click stream, data feeds)* Experience with source code management using Git or Subversion and release processes.
* Experience with scalable systems in a load balanced environment and experience conducting load tests* Ability to extend your scope into the Analytics domain and partner with that team to optimize the output of the Analytics function* Ability to create and interact with very large data processing pipelines, distributed data stores, and distributed file systems* Experience with Spark or Databricks in a production setting is a plus* Ability to learn quickly and multi-task in a fast-paced, dynamic environment* Understanding of data warehouse architectures (Kimball a plus) & Strong metadata modeling experience* Experience with EDA (Exploratory Data Analysis) and Data Visualization a plus* E-commerce or retail or internet experience a plusTechStyle is an Equal Opportunity Employer: M/F/PV/D (minority, female, protected veteran, disability)
","['spark', 'airflow', 'git', 'bi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka']","['recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl']",999,"['spark', 'airflow', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka', 'recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl']","['stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'avail', 'statist', 'action', 'releas', 'amount', 'bi', 'parti', 'big', 'set', 'warehous', 'engin', 'particip', 'machin', 'spark', 'pipelin', '3rd', 'relat']","['spark', 'airflow', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka', 'recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl', 'stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'avail', 'statist', 'action', 'releas', 'amount', 'bi', 'parti', 'big', 'set', 'warehous', 'engin', 'particip', 'machin', 'spark', 'pipelin', '3rd', 'relat']"
DE,"The Business
GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.
The Role
You are the Scotty Pippin to the Michael Jordans.
You are the Xavi to the Messis.
You'll do things like:
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience
Non negotiables:
SQL
Python
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract
Salary: $100,000.00 /year
Work Remotely:
Yes
","['sql', 'tableau', 'python']","['commun', 'pipelin']",2,"['sql', 'tableau', 'python', 'commun', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']","['sql', 'tableau', 'python', 'commun', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']"
DE,"You will be responsible for designing, training and delivering machine learning solutions to a myriad of datasets.
You will also formulate learning algorithms and implement new ones as well as deliver ML solutions to some of the most challenging IoT problems.
Required Skills & Experience
3+ years in application of ML algorithms and deep learning
2+ years using either CNN, LSTM or RNN
Experience using TensorFlow, Keras, PyTorch
Experience with Python, Java and Matlab
Experience using AWS (EC2, ECS, Lambda, EKS)
Desired Skills & Experience
Familiarty with Graph-based Machine Learning
Familiarity with high impact publications
What You Will Be Doing
75% Data analysis
25% AWS cloud services
Daily Responsibilities
100% Analysis
The Offer
Competitive Salary: Open
Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.
","['kera', 'ec2', 'aw', 'lambda', 'java', 'python', 'cloud', 'tensorflow', 'pytorch', 'matlab']","['lstm', 'graph', 'machine learning', 'deep learning', 'cnn']",999,"['kera', 'ec2', 'aw', 'lambda', 'java', 'python', 'cloud', 'tensorflow', 'pytorch', 'matlab', 'lstm', 'graph', 'machine learning', 'deep learning', 'cnn']","['basi', 'machin', 'learn', 'challeng', 'ml', 'public', 'aw', 'python', 'algorithm']","['kera', 'ec2', 'aw', 'lambda', 'java', 'python', 'cloud', 'tensorflow', 'pytorch', 'matlab', 'lstm', 'graph', 'machine learning', 'deep learning', 'cnn', 'basi', 'machin', 'learn', 'challeng', 'ml', 'public', 'aw', 'python', 'algorithm']"
DE,"Model Integrity and Collaboration
Drive Innovation and Thought Leadership
Support Decision Making at All Levels
Create Value for Clients by Empowering Consumers
Improve Customer Experience Through Simple Design
Celebrate Success Often
Purpose of the Role:
Duties and Responsibilities:
Working with the data science and data analytics team to refine and develop data science and analytics (DSA) product roadmap
Support Redshift cluster management including monitoring, performance tuning, and optimization
Responsible for data loads and data extracts via Airflow DAG and python code.
Engaging in exploratory A/B studies to extract data features and determine the relative value of multiple data types
Building rich and interactive data visualizations to display findings from A/B studies, to guide and inform exploratory data analysis, and to deepen customer engagement
Understanding and applying data mining techniques, including NLP, clustering algorithms and regression analysis to generate deep insight and discover effective solutions to challenging problems
Skills, Abilities, and Experience:
2-5 years of experience in a corporate, start-up, or research environment
2-5 years of experience mentoring data analysts (corporate) or graduate students (academia)
2-5 years of experience in Airflow DAG creation, debugging and maintenance
2-5 years of experience in PostgreSQL and Elasticsearch
Strong background and solid skills in interactive data visualization (Tableau, Django, Shiny, D3.js)
Experience in research methods, exploratory data analysis, and machine learning
Intense intellectual curiosity strong desire to always be learning
Analytical, creative, and innovative approach to solving difficult problems
Minimum Qualifications:
4 year BS/ BA Degree in Computer Science, Computer Engineering or other related field
2 years of direct experience as a data engineer or working directing in data engineering / data science.
1-2 years of experience with Python (Pandas, NumPy, sciKit-learn), SQL and R
*Please note, due to the requirements of this position, responses may automatically disqualify you from moving forward in the application process.
Please review minimum qualifications thoroughly before applying.
Behavioral Competencies:
Customer Focused
Attention to Detail
Independent Self-Starter
Highly Organized
Critical Thinker
Problem Solver
Excellent Communicator
Ability to Prioritize
Team Work & Collaboration
Multi-Tasker with Strong Sense of Urgency
The Perks:
Enjoy Flexible PTO and flexible work hours
401K Program with a 4% match
3 Weeks Paid Maternity/Paternity Leave
Weekly team lunches to celebrate victories
Paid Parking as well as Car Pooling incentives
Laptop fitness stations
Ping pong conference table and Foosball
Free snacks and drinks
Powered by JazzHR
","['sql', 'airflow', 'numpi', 'elasticsearch', 'django', 'panda', 'shini', 'tableau', 'python', 'redshift', 'r', 'postgresql', 'scikit', 'excel']","['research', 'data mining', 'visual', 'tune', 'regress', 'machine learning', 'optim', 'nlp', 'exploratori', 'commun', 'cluster']",1,"['sql', 'airflow', 'numpi', 'elasticsearch', 'django', 'panda', 'shini', 'tableau', 'python', 'redshift', 'r', 'postgresql', 'scikit', 'excel', 'research', 'data mining', 'visual', 'tune', 'regress', 'machine learning', 'optim', 'nlp', 'exploratori', 'commun', 'cluster']","['analyt', 'machin', 'program', 'techniqu', 'learn', 'corpor', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'integr', 'engin', 'algorithm']","['sql', 'airflow', 'numpi', 'elasticsearch', 'django', 'panda', 'shini', 'tableau', 'python', 'redshift', 'r', 'postgresql', 'scikit', 'excel', 'research', 'data mining', 'visual', 'tune', 'regress', 'machine learning', 'optim', 'nlp', 'exploratori', 'commun', 'cluster', 'analyt', 'machin', 'program', 'techniqu', 'learn', 'corpor', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'integr', 'engin', 'algorithm']"
DE,"What you’ll do:
Work with product and technology teams to understand different entities and objects
Define a model to store source data that feeds into a transactional system and analytics platform
Work with content acquisition team in building partnerships with different data providers to enable integration
Create and maintain multiple pipeline architectures
What you’ll need:
10+ years of experience building scalable, modular data lakes/data warehouse or 8+ years of experience building large scale data warehouse systems
Degree in Computer Science with specialization in Analytics, Information System etc.
Strong data modeling skills
Strong analytical skills working with unstructured data
Advanced knowledge in SQL and scripting
Experience with big data tools: EMR, Hadoop, Spark etc.
Experience with Relational and No SQL: Cassandra & Postgres etc.
Experience with workflow management tool: Airflow or something similar
Experience with AWS cloud service: EMR, EC2, ECS etc.
Experience with object-oriented programming: Java, Python, Scala, R
Bonus Points
Experience with BI tools like Tableau/Domo or Looker
Worked in a fast-paced startup environment and has a track record of building scalable infrastructure and analytics platform
What you’ll get
You’ll be given the freedom to do your best work alongside down-to-earth developers, designers, and thinkers who are at the leading edge of their discipline.
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'looker', 'ec2', 'hadoop', 'bi', 'aw', 'python', 'java', 'tableau', 'postgr', 'r', 'cloud']","['data modeling', 'pipelin', 'big data']",1,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'looker', 'ec2', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'cloud', 'data modeling', 'pipelin', 'big data']","['analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'provid', 'bi', 'python', 'aw', 'warehous', 'comput', 'big', 'integr', 'sourc']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'looker', 'ec2', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'cloud', 'data modeling', 'pipelin', 'big data', 'analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'provid', 'bi', 'python', 'aw', 'warehous', 'comput', 'big', 'integr', 'sourc']"
DE,"Sense360 was founded by successful repeat entrepreneurs.We are transforming the way businesses make decisions by combining massive, disparate datasets and turning them into accessible, actionable, and accurate insights that drive the strategic decisions that brands make.
",[None],[None],999,[],['action'],['action']
DE,"Team and Role Overview
The Mission/Outcomes and Objectives
This means that you will be using and building software that runs in real-time to manage data from robots, sensors, cameras, and software systems.
Candidate Profile
You have ideally designed and built large-scale data processing architectures, where your software engineering skills have been used to marry systems together in an efficient manner.
You have also worked in multi-disciplinary environments requiring collaboration with different teams who have different types of data.
You are excited to solve complex problems to which you can offer elegant solutions.
Minimum Required Skills and Competencies
Bachelor's in Computer Science or related technical field and 5+ years of experience in software development
Expert in algorithms and data structures
Expert in one or more object-oriented languages like C++, C#, Python, and Java
Experience designing and building large scale, high performance data processing pipelines and data warehousing
Experience with time-series, SQL, and NoSQL databases, and handling of multimedia data types
Experience using streaming engines and message queuing systems like Apache Kafka, Spark
Experience with database administration/configurations
Experience with Continuous Integration and Agile Development
Preferred Skills and Competencies
Master's in Computer Science or related technical field and 7+ years of experience in software development
Experience with big data systems like , Hive, MapReduce, Cassandra, BigTable or similar
Experience visualizing time series data and building user interfaces
This position must meet Export Control compliance requirements, therefore a United States Person as defined by 22 C.F.R.
§ 120.15 is required.
","['sql', 'mapreduc', 'spark', 'cassandra', 'bigtabl', 'python', 'hive', 'c', 'nosql', 'java', 'kafka']","['pipelin', 'visual', 'data warehousing', 'big data', 'time series']",1,"['sql', 'mapreduc', 'spark', 'cassandra', 'bigtabl', 'python', 'hive', 'c', 'nosql', 'java', 'kafka', 'pipelin', 'visual', 'data warehousing', 'big data', 'time series']","['spark', 'pipelin', 'handl', 'relat', 'visual', 'python', 'comput', 'big', 'integr', 'engin', 'algorithm']","['sql', 'mapreduc', 'spark', 'cassandra', 'bigtabl', 'python', 'hive', 'c', 'nosql', 'java', 'kafka', 'pipelin', 'visual', 'data warehousing', 'big data', 'time series', 'spark', 'pipelin', 'handl', 'relat', 'visual', 'python', 'comput', 'big', 'integr', 'engin', 'algorithm']"
DE,"ISIN code FR0000073298, Reuters ISOS.PA, Bloomberg IPS:FP www.ipsos.com
Junior Data Engineer
Division: Global Science Organization (GSO), Data Science & AI Lab
The team develops analytic tools, builds data science models for pilot studies and internal stakeholder analytics innovations, and consults on a broad range of data and research best practices.
The developer in this role will help the team build full-stack data offerings and scale them to the cloud.
The position calls for an observant attention to detail, the ability to work well on a small team, and a self-starter approach to problem-solving and debugging.
As a Data Engineer, you will:
Work closely with team members on design of large-scale modeling efforts, contributing to cloud pipelines, including the containerization of current tools
Collaborate on engineering new data science products by translating needs identified with stakeholders into analytic frameworks that can be built into polished user-facing tools
Build, maintain, and enhance existing codebases
Synthesize tech innovations and cloud scalability to elevate business value for clients, both current and prospective
Validate, test, and maintain staging and production analytics environments in the cloud for data science teams globally
Provide consulting for internal teams and clients on data architectures and schemas, data hygiene, and areas for improving process efficiency
Requirements:
High proficiency writing Python and JavaScript (preferably React), including the ability to produce and maintain reusable and modular codebases
Bachelor’s degree
Desire to work in a highly collaborative, fun, consensus-oriented environment.
Attentive learner with excellent time-management skills
Experience with deploying data models and processes in the cloud (e.g.
GCP, AWS).
Pluses:
Experience with Linux server and system administration.
Working knowledge of SQL and experience with database design and administration.
Experience in analytics, extracting and surfacing value from quantitative data.
Professional or academic experience with modern techniques and algorithms in machine learning and statistical computing (e.g.
deep learning).
Experience with collaboration tools (e.g.
Atlassian suite) and version control systems (e.g.
Git).
Large dataset manipulation.
Experience in distributed storage and computing.
Experience with ReactJS, TypeScript/JavaScript, visualization libraries (e.g.
d3.js).
Experience creating and deploying web apps (e.g.
Electron).
Advanced degree (M.S., Ph.D.), but not required.
Experience in the field of Market Research
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or any other protected class and will not be discriminated against on the basis of disability.
For all future offer negotiations – please target April 6th or after for their start date.
Required Skills
Required Experience
Culver City, US-CA
","['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react']","['research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist']",1,"['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react', 'research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist']","['basi', 'analyt', 'machin', 'techniqu', 'learn', 'pipelin', 'visual', 'provid', 'aw', 'python', 'statist', 'quantit', 'engin', 'algorithm']","['sql', 'linux', 'gcp', 'excel', 'git', 'javascript', 'aw', 'python', 'cloud', 'react', 'research', 'pipelin', 'visual', 'machine learning', 'deep learning', 'statist', 'basi', 'analyt', 'machin', 'techniqu', 'learn', 'pipelin', 'visual', 'provid', 'aw', 'python', 'statist', 'quantit', 'engin', 'algorithm']"
DE,"Senior Data Engineer
Location/City: Playa Vista, CA or Any Where in the Unites States
Responsibilities
As a Senior Data Engineer, you will:
Work across all phases of the software development lifecycle in a cross-functional, agile development team setting
Collaborate with data scientists and analysts to prepare complex data sets that can be used to solve difficult problems
Deliver high-quality, well-tested technical solutions that make sense for the problem at hand
Fearlessly work across components, services, and concerns to deliver business value
Help define, implement, and reinforce data engineering best practices and processes
Skills & Requirements
Significant data engineering and/or software development experience (5-7 years minimum)
Experience with ingesting, processing, and transforming data at scale
Demonstrated proficiency with SQL, relational database, and data warehousing concepts
Demonstrated aptitude with ETL concepts and tools such as Airflow or AWS Glue
Experience of the AWS data ecosystem (Glue, Kinesis, S3, Lambda, EMR, Redshift, etc.)
Understanding of event-driven and/or streaming workflows with tools like Kafka and Spark
Experience administering cloud-based analytics databases (like Snowflake or Redshift)
Knowledge of Python, R or other common languages used in data science
Ability to thrive in a fast-paced and dynamic environment
Ability to work well in teams of all sizes with representatives from a diverse set of technical backgrounds.
Preferred: Experience with infrastructure automation through tools like Terraform or CloudFormation
Preferred: Experience with indexing and search technologies like elasticsearch, SOLR, etc.
Preferred: Experience building or maintaining a data science modeling environment such as Sagemaker or Databricks, including deployment and monitoring using tools like MLFlow
Bachelor's or Master's degree in Computer Science or equivalent experience
Benefits
","['sql', 'spark', 'airflow', 'elasticsearch', 'sagemak', 'solr', 'aw', 'snowflak', 'redshift', 's3', 'python', 'lambda', 'r', 'cloud', 'kafka', 'mlflow']","['data warehousing', 'etl', 'reinforc']",1,"['sql', 'spark', 'airflow', 'elasticsearch', 'sagemak', 'solr', 'aw', 'snowflak', 'redshift', 's3', 'python', 'lambda', 'r', 'cloud', 'kafka', 'mlflow', 'data warehousing', 'etl', 'reinforc']","['analyt', 'spark', 'set', 'relat', 'divers', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'etl', 'common', 'engin']","['sql', 'spark', 'airflow', 'elasticsearch', 'sagemak', 'solr', 'aw', 'snowflak', 'redshift', 's3', 'python', 'lambda', 'r', 'cloud', 'kafka', 'mlflow', 'data warehousing', 'etl', 'reinforc', 'analyt', 'spark', 'set', 'relat', 'divers', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'etl', 'common', 'engin']"
DE,"A growing, well backed startup in El Segundo is currently looking to bring on a Data Engineer with a background in software development to their small team of 6.
Working closely with product manager, engineers, and business stakeholders, you will work to build and optimize ETL processes using Python, AWS, and SQL.Required Skills & Experience* At least 3 years of experience with ETL development* Python or Java backend development experience* SQL, RDBMS, and data modeling experience* Preferably experience working in an AWS environmentThe Offer* Competitive Salary: Up to $160,000/year, DOEYou will receive the following benefits:* Comprehensive health, dental, and vision with lots of options* Lots of PTO* 401k and equity packages* Flexible schedule and work from home* Summer hours* Growth opportunity!Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) across 10 major North American markets.
","['sql', 'java', 'python', 'aw']","['etl', 'data modeling']",999,"['sql', 'java', 'python', 'aw', 'etl', 'data modeling']","['basi', 'aw', 'python', 'packag', 'etl', 'engin']","['sql', 'java', 'python', 'aw', 'etl', 'data modeling', 'basi', 'aw', 'python', 'packag', 'etl', 'engin']"
DE,"The Data Engineer will be responsible for creating solutions, comprehensive analytics and will be integral in implementing company-wide data strategy.
Responsibilities:
Collaborate with other departments to understand data needs
Develop and optimize ETL processes
Optimize data warehouse by defining technical requirements
Auditing and automating data quality
Work closely with development teams to create long term plans for problem resolution
Requirements:
Bachelors Degree or greater (technical or science degree preferred).
2 years of experience in the following:
scalable architectures and large data processing
AWS, Linux and Shell Scripting
ETL and data mining
open source programming language
data warehousing concepts
SQL, and System-level DBA functions ex: configuring replication, automating backups, performance tuning and RDBMS
","['sql', 'aw', 'linux']","['data mining', 'tune', 'optim', 'data warehousing', 'etl']",1,"['sql', 'aw', 'linux', 'data mining', 'tune', 'optim', 'data warehousing', 'etl']","['analyt', 'optim', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['sql', 'aw', 'linux', 'data mining', 'tune', 'optim', 'data warehousing', 'etl', 'analyt', 'optim', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"Responsibilities:* Responsible for building and maintaining the machine learning data and development platform.
* Create and maintain scalable data pipeline in the cloud (AWS and GCP).
* Assemble large, complex data sets that meet functional / non-functional business requirements.
* Identify, design, and implement processes automation and data delivery.
* Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
* Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
* Build analytics tools to provide actionable insights into business and product performance.
* Keep data separated, isolated and secured.
* Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Maintain specifications and metadata; follow the best practices.
* Recommend and implement process improvements.
* Maintain specifications and metadata; follow and develop best practices.
* Coach and technically train data analysts, if needed.Qualifications:* 5+ years as a data engineer.
* Experience with SQL, Python, R languages.
* ETL experience using Python.
* Experience with Hadoop, Spark, Hive.
Presto is a plus.
* Practical experience with GIT version control.
* Strong familiarity with GCP, AWS, SQL Server.
* Comfortable working with open source tools in Unix/Linux environments.
* Data warehousing experience, data modeling and database design.
* Experience with machine learning packages and various ML algorithms.
* Experience with predictive and prescriptive analytics, modeling, and segmentation.
* Experience with data analytics, big data, and analytics architectures.
* Comfortable handling large amounts of data.
* Experience ensuring data and modeling accuracy, cleanliness, reliability.
* Works independently without the need for supervision.
* Experience translating business requirements into functional, and non-functional requirements.
","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r']","['recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']",999,"['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']","['predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl', 'predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']"
DE,"If you're a team-playing innovator, you'll fit right in.
",[None],[None],999,[],[None],[None]
DE,"The Job
Product, Platform & Strategy: Content Mgmt & Distribution department.
The Sr.
Data Engineer will be a member of the Media Supply Chain (MSC) team within the
WarnerMedia Technology (WMTO) organization, the position will be responsible
for the design and implementation of search and data relationship functionality
within the Media Supply Chain.
The Sr. Data Engineer must have experience with
data modeling, modern database technologies, and API driven search design and
development.
The Sr. Data Engineer will mentor and lead engineers, make
technology and design decisions, and partner across the organization in a
data management and distribution applications.
Media Supply Chain is tasked to architect, engineer, and program manage a wide
operationally manage and distribute WarnerMedia content globally.
applications and technology solutions are responsible for scheduling, image &
asset metadata management, as well as, content processing and delivery as well
as content mastering, localization and preservation.
new ways to process and deliver WarnerMedia content to its global customers
The Media Supply Chain Mission
technology solutions which are highly reliable, automated, measurable,
optimized, modern systems capable of distributing high-quality content and
The Daily
Develop and provide support for core data relationships, data ingest,
data transformation services and search capabilities.
Creates functional
and technical specifications.
Creates and executes against a plan to
launch and maintain applications.
Review project objectives and determine best technology for
implementation.
Implement best practice standards for development, build
and deployment automation while mentoring and leading a team of
engineers.
Review emerging technologies and evaluate potential uses for WarnerMedia
(WM) Technology and other divisions.
Develop prototype projects using new
technologies.
Evaluate software products and vendors for WM Technology and other
divisions.
Recommend action, develop and lead implementation of selected
products/services.
Work with internal and external developers to ensure WB Technology code
standards and best practices are performed for development of
applications.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
6+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL,
and Relational.
Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch,
logstash, kibana).
Experience in sizing, modelling and deploying multi-node elasticsearch
clusters.
Expertise / experience in sizing, modelling and deploying multi-node
noSQL DB mongo clusters replica sets or sharded clusters.
Demonstrated expertise and experience in modern databases such as Mongo,
Couchbase, Neptune, Neo4j, or equivalent.
Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch
query DSL, cypher, gremlin, or graphql.
xQuery preferred but not
required.
Highly proficient in XML, JSON and YAML data exchange formats.
Experience
in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana
etc.
Experience in working with data streaming technologies such as Amazon
Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch,
RDS, lambda, Neptune and ec2.
Highly proficient in at least one modern programming language such
python, java, or node.js.
Bash experience preferred.
Demonstrated expertise and experience in deploying containerized
application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira,
or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Advanced Systems design and architecture.
Experience mentoring developers preferred.
Ability to work with outside vendors and clients under sometimes adverse
circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of
personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, when necessary.
Must be able to respond to after-hours pager notifications to provide
support for applications as necessary.
Show moreShow less
","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker']","['graph', 'data modeling', 'optim', 'commun', 'cluster']",1,"['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'graph', 'data modeling', 'optim', 'commun', 'cluster']","['analyt', 'asset', 'essenti', 'program', 'relat', 'optim', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin', 'evalu']","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'graph', 'data modeling', 'optim', 'commun', 'cluster', 'analyt', 'asset', 'essenti', 'program', 'relat', 'optim', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin', 'evalu']"
DE,"Job Details
Job ID: 12030
Responsibilities
Do you have a passion for complex ontologies, data modeling and software development?
Daily responsibilities:
Work with the PDS IMG and EN operations team and data providers to track and move datasets through the different stages of data publication.
Design, develop, and implement the PDS4 metadata for new data deliveries and for migration of PDS3 archives.
Extend and guide the PDS4 data model as needed by EN and IMG and its user community.
Develop scripts and approaches for efficient and high-quality QA/QC of datasets ingested into the archive to build valuable data.
Improve user experience and usability of the datasets, including documentation.
Prepare scripts, tutorials, and workshops to enhance the use of PDS services and datasets at conferences (LPSC, AGU, PSIDA, etc.)
Provide science and technical support for remote sensing data distribution tools and services.
Qualifications
Typically requires a Bachelor’s degree in Information Science, Computer Science or related discipline with a minimum of 6 years of related experience; or Master’s degree with 4+ years of related experience; or PhD with 2+ years related experience.
Solid experience in information management, data management and archiving, particularly in the area of information modeling and architect
Demonstrated experience with ontology/taxonomy design and background with information architecture and modeling
Significant experience in building and coordinating the development of documentation for datasets and software tool/services.
Proficiency with version control (i.e.
GitHub) and Agile development practices.
Familiarity and experience metadata standards (ISO11975) and conventions (e.g.
JSON-LD).
This also includes dataset access and discovery of data services through DOIs.
Ability to analyze requirements and operational needs in line with the project/customer needs and constraints.
Strong interpersonal and customer service skills and ability to work with the Earth science community and/or project management.
Excellent data documentation skills and working effectively as a team member.
Proficiency working in a UNIX/Linux command-line environment and shell scripting.
Consistent record dealing with multiple issues, tasks, and priorities concurrently independently and in a dynamic team environment
Excellent verbal, written, and presentation skills.
Ability to solve critical problems on-the-fly with targeted execution in decision-making.
Willingness to travel to science conferences and technical workshops.
Preferred Skills:
Knowledge of all phases of the design, development, system integration, test and operation of data systems (e.g.
archives).
Ability to work on project assignments without appreciable direction with accountability for technical performance.
Software engineering experience.
Developing use cases, extracting/interpreting/managing software design requirements, modularized programming, data i/o, database i/o, information architecture, data management, configuration management, and containerized software deployment.
Information Science experience.
Understanding and applying ground breaking technologies for both data and metadata management in a complex workflow of observational or simulated data including: data acquisition, QA/QC, format transformation, data/metadata curation, interoperability, archival, dissemination, and user-tailored documentation.
Demonstrated authorship/co-authorship of scientific or technical publications and presentations related to planetary processes and/or remote sensing at domestic/international conferences, workshops, and/or science team meetings.
Experience in working with cloud vendors (e.g.
Amazon AWS) and with cloud-based architectures.
Work Authorization
U.S. Citizen or Permanent Resident
JPL is an Equal Opportunity Employer.
All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, marital status, physical or mental disability, medical condition, genetic information, pregnancy or perceived pregnancy, gender, gender identity, gender expression, sexual orientation, protected military or veteran status or any other characteristic or condition protected by Federal, state or local law.
In addition, JPL is a VEVRAA Federal Contractor.
EEO is the Law
EEO is the Law Supplement
Pay Transparency Nondiscrimination Provision
1324b(a)(3).
The Designated Countries List is available here.
Print
Share
Facebook
Twitter
LinkedIn
","['linux', 'unix', 'aw', 'cloud', 'github', 'excel']","['data modeling', 'commun', 'account']",1,"['linux', 'unix', 'aw', 'cloud', 'github', 'excel', 'data modeling', 'commun', 'account']","['relat', 'line', 'public', 'provid', 'aw', 'avail', 'comput', 'integr', 'engin']","['linux', 'unix', 'aw', 'cloud', 'github', 'excel', 'data modeling', 'commun', 'account', 'relat', 'line', 'public', 'provid', 'aw', 'avail', 'comput', 'integr', 'engin']"
DE,"Minimum 5 years of data engineering, ETL or software engineering experience with a focus on data extraction, transformation and publishing.
Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.
Experience in relevant technical languages and tools/technologies such as SQL, Python, REST APIs, Airflow, Spark, AWS
Work closely with data architect to design and implement data solutions.
Develop automated pipelines for ingesting, cleansing and publishing data.
Develop extracts from REST API sources in a recoverable manner.
Support existing solutions for Digital Application analytics.
","['sql', 'spark', 'airflow', 'aw', 'python']","['cleans', 'etl', 'pipelin']",999,"['sql', 'spark', 'airflow', 'aw', 'python', 'cleans', 'etl', 'pipelin']","['analyt', 'digit', 'spark', 'pipelin', 'etl', 'aw', 'python', 'integr', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'aw', 'python', 'cleans', 'etl', 'pipelin', 'analyt', 'digit', 'spark', 'pipelin', 'etl', 'aw', 'python', 'integr', 'sourc', 'engin']"
DE,"Job_Description
How Do You Fit In?
As the Data Engineer, you will take ownership of Development oversight,
guidance, and direction setting for the data pipeline as well as the Data
Management platforms.
The right candidate would be a self-motivated, highly
detail-oriented team-player with a positive drive to strategize and implement
BI Solutions that enable the business to derive valuable insights.
You will
to achieve aggressive goals and meet timelines to drive the business forward.
This role is critical in laying the foundation for the key Decision support
This position will report to the Director of Data Analytics
Responsibilities
Design, Build and Maintain hundreds of data pipelines using Airflow
(Python)+ SQL(Snowflake) to extract, transform, clean, audit and move
data from internal or external systems into a Cloud Based Data Warehouse
(Snowflake)
Work with Data / Data Warehouse Architect to develop data warehouse
models, design specifications, metadata process and documentation.
Develop detailed ETL specifications based on business requirements
Interface with other technology teams to develop/maintain the ETL
footprint and quality from a wide variety of in-house and 3rd party data
sources
Implement and monitor machine learning algorithms and solutions in
production.
Constantly Monitor, refine and maintain system performance and provide
statistical reporting
Participate in cross-functional meetings to review business requirements/
use stories, assist in fit/gap analysis and provide detailed technical
design documentation
Partner with business users, senior architects, product managers,
engineering teams, and other teams to deliver a robust data services
platform
Diagnose ETL and database related issues, perform root cause analysis
(RCA), and recommend corrective actions to management
Recommend ways to improve data reliability, efficiency and quality
Profile and understand the large amounts of source data available,
including structured and semi-structured/web/mobile activity data
Mentor the team on the industry standards and best practices for
effective use of data integration and data quality technologies and
exception handling
Provide cross organizational business stakeholders operational support on
existing and newly developed data pipeline
Required_Skills
3+ years of data engineering experience with high performance Big Data
platforms including cloud-based Data Warehousing on large scale
development efforts leveraging industry standard ETL tools
3+ years of experience creating and managing data pipelines using Python
(experience with Airflow preferred)
3+ years of Data Warehouse development for 100 s of gigabytes of data
and billions of records (Snowflake or Redshift is preferred)
Experience developing ELT pipelines using Snowflake, Terradata, Vertica,
Redshift or similar data warehousing technologies
Experience implementing streaming pipelines (Kinesis, Kafka, Storm,
Spark, Flink)
Experience working in an environment that ingests large amounts of raw
data (web logs, Click stream, data feeds)
Experience with source code management using Git or Subversion and
release processes.
Experience with scalable systems in a load balanced environment and
experience conducting load tests
Ability to extend your scope into the Analytics domain and partner with
that team to optimize the output of the Analytics function
Ability to create and interact with very large data processing pipelines,
distributed data stores, and distributed file systems
Experience with Spark or Databricks in a production setting is a plus
Ability to learn quickly and multi-task in a fast-paced, dynamic
environment
Understanding of data warehouse architectures (Kimball a plus) & Strong
metadata modeling experience
Experience with EDA (Exploratory Data Analysis) and Data Visualization a
plus
E-commerce or retail or internet experience a plus
protected veteran, disability)
Show moreShow less
","['sql', 'spark', 'airflow', 'git', 'bi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka']","['recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'clean', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl']",999,"['sql', 'spark', 'airflow', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka', 'recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'clean', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl']","['stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'avail', 'statist', 'action', 'releas', 'amount', 'bi', 'parti', 'big', 'set', 'warehous', 'engin', 'particip', 'machin', 'spark', 'pipelin', '3rd', 'relat']","['sql', 'spark', 'airflow', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'cloud', 'kafka', 'recommend', 'pipelin', 'visual', 'data modeling', 'machine learning', 'clean', 'data warehousing', 'statist', 'big data', 'exploratori', 'etl', 'stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'avail', 'statist', 'action', 'releas', 'amount', 'bi', 'parti', 'big', 'set', 'warehous', 'engin', 'particip', 'machin', 'spark', 'pipelin', '3rd', 'relat']"
DE,"**WarnerMedia's new streaming entertainment offering, HBO Max?is the culmination of some of the most innovative new technology and greatest creative talent in the industry.
HBO Max will also be home to key third-party library acquisitions, such as _Friends_ , _South Park_ and _Doctor Who_ ,?and?more than?50 exclusive?Originals in its first year, from the likes of groundbreaking creative talent such as J.J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa McCarthy, Robert Zemeckis, Ellen DeGeneres, and other visionaries.
**The Job**As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures.
You will play an integral role in identifying ways to improve data reliability, efficiency and quality.
This position will work closely with various divisions including Product, Content and Marketing.
**The Daily**+ Strong experience in developing, constructing, testing, and maintaining existing and new architectures.+ Responsible for aligning architecture with business requirements.+ Ability to identify ways to improve data reliability, efficiency, and quality.+ Responsible for preparing data for predictive and prescriptive modeling.+ Ability to deploy sophisticated analytics programs, machine learning and statistical methods.+ Conduct research for industry and business questions that come up.
**The Essentials**+ BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle.+ Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark.+ Robust experience with relational SQL and NoSQL databases like MongoDB, DynamoDB, Redshift/PostGRE, mariaDB, etc.+ Experience with data pipeline and workflow management tools like airflow or alternatives.+ In depth experience and knowledge with AWS cloud services: S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc.+ Experience with stream-processing systems: Spark-Streaming, AWS Kinesis, Kinesis Firehose etc.+ Must be experienced with object-oriented/object function scripting languages: Python, Java, Scala, etc.+ Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must.
Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
","['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'postgr', 'cloud']","['research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data']",1,"['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'cloud', 'research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data']","['program', 'stream', 'predict', 'python', 'integr', 'analyt', 'essenti', 'hadoop', 'statist', 'learn', 'aw', 'parti', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['mongodb', 'sql', 'linux', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'java', 'cloud', 'research', 'pipelin', 'predict', 'machine learning', 'data modeling', 'statist', 'big data', 'program', 'stream', 'predict', 'python', 'integr', 'analyt', 'essenti', 'hadoop', 'statist', 'learn', 'aw', 'parti', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Duties & Responsibilities:
Construct data pipelines orchestration.
Build and optimize performance of Hadoop/Spark batch jobs, Spark, Kafka, Cassandra, ELK, etc.
Build and optimize performance of ElasticSearch cluster and relevance.
Work with cross-functional teams and other software engineers.
Design and architect high quality data-lake, data-warehouse, and data-marts data models.
Facilitate Data Science workflows and advanced machine learning algorithms.
Contribute to Open Source solutions and communities.
Keep updated on current and emerging technologies and tools.
Provide innovative ideas to a larger professional community.
Demonstrate technologies, solutions, and leading practices.
Balance resources, requirements, and complexity.
Qualifications:
BS in Computer Science or related field.
5+ years experience as a Data Engineer.
Passionate about coding.
(You will be asked to code before or during the interview.)
Understanding of distributed systems and computation.
Working knowledge of Scala, Java, Python, and Go-Lang.
Working knowledge of data Apache Hadoop/Spark ecosystem (Spark, Hive, Presto, Oozie, Pig, Hue, Zeppelin).
Demonstrated working knowledge of data modeling.
Excellent communication and interpersonal skills.
Knowledge of the following, required:
Unit, Integration, and Load testing.
Developing REST APIs.
Git.
Ant, Maven, SBT, and/or Gradle.
Unix/Linux.
Docker containers building and deployment.
Knowledge of the following, preferred:
GraphQL knowledge
Kubernetes knowledge
Apache Spark MLlib
Apache Spark GraphX
Amazon AWS or other cloud Services
Jenkins
Powered by JazzHR
","['linux', 'python', 'java', 'kafka', 'pig', 'mllib', 'unix', 'git', 'hadoop', 'hive', 'kubernet', 'docker', 'elasticsearch', 'cassandra', 'scala', 'aw', 'excel', 'spark', 'cloud']","['pipelin', 'data modeling', 'machine learning', 'optim', 'commun', 'cluster']",1,"['linux', 'python', 'java', 'kafka', 'pig', 'mllib', 'unix', 'git', 'hadoop', 'hive', 'kubernet', 'docker', 'elasticsearch', 'cassandra', 'scala', 'aw', 'excel', 'spark', 'cloud', 'pipelin', 'data modeling', 'machine learning', 'optim', 'commun', 'cluster']","['machin', 'spark', 'pipelin', 'relat', 'power', 'hadoop', 'provid', 'optim', 'python', 'aw', 'comput', 'warehous', 'integr', 'sourc', 'engin', 'algorithm']","['linux', 'python', 'java', 'kafka', 'pig', 'mllib', 'unix', 'git', 'hadoop', 'hive', 'kubernet', 'docker', 'elasticsearch', 'cassandra', 'scala', 'aw', 'excel', 'spark', 'cloud', 'pipelin', 'data modeling', 'machine learning', 'optim', 'commun', 'cluster', 'machin', 'spark', 'pipelin', 'relat', 'power', 'hadoop', 'provid', 'optim', 'python', 'aw', 'comput', 'warehous', 'integr', 'sourc', 'engin', 'algorithm']"
DE,"This is a 100% Remote position.
Candidate may reside anywhere within the United States
You will architect and design “big data” systems which require queries returning within sub-second response times.
Ready for a challenge?
Responsibilities
Design systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data
Build systems that handle scale
Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies
Collect, parse, analyze, and visualize large sets of data
Turn data into insights
Qualifications
Experience with large-scale data and query optimization techniques
Experience with ETL to data warehouse systems
Experience with AWS cloud services: EC2, RDS, Redshift, Aurora
Expert in SQL, NoSQL, and RDBMS
Knowledge in multiple scripting languages (e.g.
Python)
Knowledge of cloud, distributed systems, and stream-processing systems
Passionate about learning new technologies and solving hard problems in a fast-paced environment
The ideal fit…
Has a Computer Science degree
Is a ""student of the game"" and thrives on new challenges
Enjoys learning from teammates, and isn't afraid to teach others at the same time
Sees the glass half-full.
This is a new industry space...your vision could make all the difference!
Wants to make a lasting impact and lifelong connections, this is not just another paycheck
Must Haves:
Expert level SQL
Expert level Python
Deep experience with AWS
Experience building SaaS products through data insights
Because you care about people, the work you do, and the connections you make.
Work is such a large part of life, it only makes sense to make it awesome.
This is a rapidly changing space so if you thrive on ambiguity, are hungry for a challenge, and have the guts to speak your mind it could be a perfect fit.
Come for the challenges, come for engaging people in a casual and friendly environment.
Come for the unlimited PTO, the health benefits, the 401k plan, the annual retreats (including family), the twice-a-year hackathons, the 10% exploratory time, the ability to contribute to open source, and the potential to work from anywhere.
Whatever the reason, your new co-workers along with a leadership team who truly believes in your growth both professionally and personally will keep you here.
You'll get fast results and superior candidates.
Get scalable solutions for your business
","['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql']","['analyz', 'optim', 'big data', 'exploratori', 'etl']",1,"['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'analyz', 'optim', 'big data', 'exploratori', 'etl']","['techniqu', 'stream', 'challeng', 'set', 'infrastructur', 'optim', 'amount', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'collect']","['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'analyz', 'optim', 'big data', 'exploratori', 'etl', 'techniqu', 'stream', 'challeng', 'set', 'infrastructur', 'optim', 'amount', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'collect']"
DE,"Job Summary
You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs.
You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.
The use of computer vision is a high priority in the institution's digital strategy and the success of this work will lead to further exciting projects that put the developed skills, transformation workflows and systems to good use.
You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics.
Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With other Data Engineers, ensure high quality data transformation pipelines can migrate and enhance institutional managed collections
Assess the feasibility of applying data enhancement tools and the quality of their results to determine their suitability for project requirements
Work in an agile way, including supporting testing, continuous integration and deployment
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations
Attention to detail combined with a focus on data usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Familiarity with machine learning techniques and/or tools
Familiarity with cultural heritage data standards, such as Linked Open Data
Familiarity with engineering tools such as git
Familiarity with test driven and agile software development methodologies
","['python', 'excel', 'git']","['machine learning', 'computer vision', 'commun', 'pipelin']",1,"['python', 'excel', 'git', 'machine learning', 'computer vision', 'commun', 'pipelin']","['digit', 'machin', 'program', 'techniqu', 'engin', 'pipelin', 'relat', 'python', 'scientist', 'comput', 'integr', 'collect']","['python', 'excel', 'git', 'machine learning', 'computer vision', 'commun', 'pipelin', 'digit', 'machin', 'program', 'techniqu', 'engin', 'pipelin', 'relat', 'python', 'scientist', 'comput', 'integr', 'collect']"
DE,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive.
Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership.
","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r']","['recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']",999,"['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']","['predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl', 'predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']"
DE,"Job Title
Senior Data Engineer
01/15/2020
POSITION PURPOSE:
Design, develop and implement data integration components and backend solutions.
Support project and enterprise level data strategies by taking lead responsibility for processes in all areas of the data warehouse.
ESSENTIAL FUNCTIONS:
Work closely with Data Analysts and Data Modelers to develop enterprise data integration solutions that promote re-usability and standardization.
Prepare and present data collection and analyses for business.
Modify and create backend processes using shell scripts/java/perl/python.
Execute all phases of programming activities including program design, coding, debugging, testing, documentation,validation and implementation.
Develop automated data quality checks to audit collected data for completeness, accuracy, and errors.
Re-Develop and tune existing Data Warehouse applications to ensure optimum performance.
Monitor existing Data Warehouse production processes and resolve issues as needed.
Encourage and support less experienced coworkers to promote a team environment.
EDUCATION: Bachelor's Degree
YEARS OF EXPERIENCE: 4-8 Years
Req #
22452BR
LA – World Headquarters
Los Angeles, CA
","['python', 'perl']",['tune'],1,"['python', 'perl', 'tune']","['essenti', 'program', 'engin', 'python', 'warehous', 'integr', 'collect']","['python', 'perl', 'tune', 'essenti', 'program', 'engin', 'python', 'warehous', 'integr', 'collect']"
DE,"This is a high visibility, hands-on data engineering lead role.
The Team
You will be joining a new team of world-class high-performing and low ego engineers and scientists that have an entrepreneurial and hacker spirit.
The team will operate in a very self-driven, agile and fast-paced environment.
The team operates by self-organizing, around shared values, vision and objectives.
Keys to Hiring
You are the best candidate for this position if you stay up to date with the latest and greatest data engineering innovations and off-the-shelf solutions.
You are both an architect and a hands-on data engineer.
You enjoy working with and supporting data scientists.
You generate new ideas and initiatives to solve problems and identify trends and opportunities.
You are comfortable working independently and figuring things out, and also enjoy working collaboratively in a small, fast-paced entrepreneurial environment.
You enjoy being challenged and respectfully challenging others.
You write code almost every day and are comfortable working with your team to produce tools, pipelines, packages, modules, features, dashboards, or whatever is needed to move the team forward.
You have a hacker’s spirit.
The Role
Primary
Build a scalable architecture to support real-time analytics and offline ML model training pipelines.
Build an auto-scalable (Kubernetes) containerized ML microservices for anomaly detection, prediction, forecasting, correlation analysis, etc.
Ensure a modular platform architecture for deployment in public and private clouds.
Build a scalable AI platform to build, train and deploy custom ML models as microservices for real-time predictive intelligence, deep actionable insights for faster RCA.
Use off-the-shelf tools as needed to prevent reinventing the wheel.
Attract and foster a world-class data team
Qualifications
15 years of hands-on experience in data engineering
4+ years of experience leading data engineering teams for AI/ML platforms that serve real-time products
4+ years of experience leading data engineering recruitment, mentoring and conducted hands-on training sessions to data scientists, analysts and BI engineers.
2+ years of experience collaborating to design standards, championing data engineering culture and developing world-class technical talent.
2+ years of experience as a Data Architect for AI/ML platforms and driving engineering roadmap.
Track record of being a hands-on contributor for various data science & engineering projects
Track record of bringing 2 enterprise products to market from conception to deployment
Experience building and delivering a cloud-based analytics solution
Proven track record of meeting aggressive release schedules; demonstrated ability to balance multiple priorities in a complex environment and manage teams to successful project completion.
Sponsorship tag select one!
*******************************************************************************
Languages:
","['bi', 'kubernet', 'cloud']","['pipelin', 'anomali', 'dashboard', 'correl', 'predict']",999,"['powerbi', 'kubernet', 'cloud', 'pipelin', 'anomali', 'dashboard', 'correl', 'predict']","['day', 'analyt', 'challeng', 'ml', 'pipelin', 'public', 'predict', 'primari', 'bi', 'scientist', 'packag', 'action', 'releas', 'engin']","['powerbi', 'kubernet', 'cloud', 'pipelin', 'anomali', 'dashboard', 'correl', 'predict', 'day', 'analyt', 'challeng', 'ml', 'pipelin', 'public', 'predict', 'primari', 'bi', 'scientist', 'packag', 'action', 'releas', 'engin']"
DE,"The Data Team at ReSci is in a new phase of its evolution.
Your expertise at algorithms will be paired closely with the software expertise of Data Engineers, and you will jointly own these ML systems.
You should want to make a huge impact in a fast-paced and cutting-edge start-up environment.
You should have great experience with scala/spark or python/serverless and working closely with ML algorithms and data scientist.
You’ll be building robust real-time data pipelines that process billions of events per day and sitting on the team that builds end-to-end ML algorithms.
The code and ideas that you contribute will have a tangible impact on the business as a whole.
Your code will touch millions of end-users.
You will have full responsibility for your projects and have a real sense of product ownership.
About You:
5+ years experience working on production data products in Python, Scala, or similar
Proficient in at least one statically typed language
Experience designing and implementing large, scalable services
Passionate about enforcing software engineering principles, production code quality, and regular use of design patterns
Experience interfacing with APIs - SOAP, REST, etc.
Comfortable using Git, Bitbucket/Github
Strong belief that tests and code go hand-in-hand
Deep understanding of SQL, query optimizations, joins etc.
Excellent CS foundation: data structures, time complexities, algorithms, etc.
Startup work experience a major plus!
ReSci's mission is to make artificial intelligence accessible and usable for brands.
Inspire with passion.
Persevere with determination.
Collaborate with unity.
Grow without bounds.
Create with impact.
Lead with character.
Cortex makes 3.5+ billion predictions per day and processes 5k+ events per second.
","['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel']","['optim', 'pipelin', 'predict']",999,"['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel', 'optim', 'pipelin', 'predict']","['day', 'spark', 'pipelin', 'ml', 'predict', 'optim', 'python', 'scientist', 'engin', 'algorithm']","['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel', 'optim', 'pipelin', 'predict', 'day', 'spark', 'pipelin', 'ml', 'predict', 'optim', 'python', 'scientist', 'engin', 'algorithm']"
DE,"Senior Data Engineer
If you are a Senior Data Engineer with experience, please read on!
What You Will Be Doing
You'll be able to automate existing processes, optimizing data delivery, and even re-designing infrastructure to better overcome the challenges of scalability.
Your day to day tech-stack will include SQL, AWS S3, Redshift and Big Data tools like Spark.
You'll have the opportunity to collaborate and continually learn, while improving reporting and analysis processes for the growing business need, and evolving technical challenges.
What You Need for this Position
1) 5+ years of Data Engineering experience
2) 3+ years using Python
3) Data Warehousing experience
4) Understanding of relational databases; including mySQL, Postress, and DynamoDB
5) Knowledge of ETL architecture
Nice to have, but Not required:
1) Spark streaming experience
2) Prior exposure to Docker containerization
3) Leadership or Mentoring experience
4) A BS on MS in Mathematics, computer science, or related field
What's In It for You
- Competitive base salary - up to $150K (DOE)
- 10% annual bonus
- Excellent benefits package
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'mysql', 'excel', 'docker']","['data warehousing', 'etl', 'big data']",1,"['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'excel', 'docker', 'data warehousing', 'etl', 'big data']","['day', 'spark', 'challeng', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'big', 'etl', 'engin']","['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'excel', 'docker', 'data warehousing', 'etl', 'big data', 'day', 'spark', 'challeng', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'big', 'etl', 'engin']"
DE,"This is a 100% Remote position.
Candidate may reside anywhere within the United States
You will architect and design big data systems which require queries returning within sub-second response times.
Ready for a challenge?
Responsibilities
Design systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data
Build systems that handle scale
Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS big data technologies
Collect, parse, analyze, and visualize large sets of data
Turn data into insights
Qualifications
Experience with large-scale data and query optimization techniques
Experience with ETL to data warehouse systems
Experience with AWS cloud services: EC2, RDS, Redshift, Aurora
Expert in SQL, NoSQL, and RDBMS
Knowledge in multiple scripting languages (e.g.
Python)
Knowledge of cloud, distributed systems, and stream-processing systems
Passionate about learning new technologies and solving hard problems in a fast-paced environment
The ideal fit
Has a Computer Science degree
Is a ""student of the game"" and thrives on new challenges
Enjoys learning from teammates, and isn't afraid to teach others at the same time
Sees the glass half-full.
This is a new industry space...your vision could make all the difference!
Wants to make a lasting impact and lifelong connections, this is not just another paycheck
Must Haves:
Expert level SQL
Expert level Python
Deep experience with AWS
Experience building SaaS products through data insights
Because you care about people, the work you do, and the connections you make.
Work is such a large part of life, it only makes sense to make it awesome.
This is a rapidly changing space so if you thrive on ambiguity, are hungry for a challenge, and have the guts to speak your mind it could be a perfect fit.
Come for the challenges, come for engaging people in a casual and friendly environment.
Come for the unlimited PTO, the health benefits, the 401k plan, the annual retreats (including family), the twice-a-year hackathons, the 10% exploratory time, the ability to contribute to open source, and the potential to work from anywhere.
Whatever the reason, your new co-workers along with a leadership team who truly believes in your growth both professionally and personally will keep you here.
","['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql']","['analyz', 'optim', 'big data', 'exploratori', 'etl']",1,"['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'analyz', 'optim', 'big data', 'exploratori', 'etl']","['techniqu', 'stream', 'challeng', 'set', 'infrastructur', 'optim', 'amount', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'collect']","['sql', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'analyz', 'optim', 'big data', 'exploratori', 'etl', 'techniqu', 'stream', 'challeng', 'set', 'infrastructur', 'optim', 'amount', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'collect']"
DE,"Thank you.
If you're a team-playing innovator, you'll fit right in.
A sense of humor helps, too.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identification, national origin, disability, or protected veteran status.
Reasonable Accommodation: Dollar Shave Club provides reasonable accommodation so that qualified applicants with a disability may participate in the selection process.
Only reasonable accommodation requests related to applying for a specific position within Dollar Shave Club will be reviewed at the e-mail address supplied.
Dollar Shave Club will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Thank you for considering a career with Dollar Shave Club
",[None],[None],999,[],['relat'],['relat']
DE,"Req ID: 177936
The Job
Warner Bros. Entertainment Inc. seeks a Sr. Data Engineer for the Digital Product, Platform & Strategy: Content Mgmt & Distribution department.
The Sr. Data Engineer will be a member of the Media Supply Chain (MSC) team within the WarnerMedia Technology (WMTO) organization, the position will be responsible for the design and implementation of search and data relationship functionality within the Media Supply Chain.
The Sr. Data Engineer must have experience with data modeling, modern database technologies, and API driven search design and development.
The Media Supply Chain Mission
The Daily
Develop and provide support for core data relationships, data ingest, data transformation services and search capabilities.
Creates functional and technical specifications.
Creates and executes against a plan to launch and maintain applications.
Review project objectives and determine best technology for implementation.
Implement best practice standards for development, build and deployment automation while mentoring and leading a team of engineers.
Review emerging technologies and evaluate potential uses for WarnerMedia (WM) Technology and other divisions.
Develop prototype projects using new technologies.
Evaluate software products and vendors for WM Technology and other divisions.
Recommend action, develop and lead implementation of selected products/services.
Work with internal and external developers to ensure WB Technology code standards and best practices are performed for development of applications.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
6+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL, and Relational.
Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch, logstash, kibana).
Experience in sizing, modelling and deploying multi-node elasticsearch clusters.
Expertise / experience in sizing, modelling and deploying multi-node noSQL DB mongo clusters replica sets or sharded clusters.
Demonstrated expertise and experience in modern databases such as Mongo, Couchbase, Neptune, Neo4j, or equivalent.
Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch query DSL, cypher, gremlin, or graphql.
xQuery preferred but not required.
Highly proficient in XML, JSON and YAML data exchange formats.
Experience in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana etc.
Experience in working with data streaming technologies such as Amazon Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch, RDS, lambda, Neptune and ec2.
Highly proficient in at least one modern programming language such python, java, or node.js.
Bash experience preferred.
Demonstrated expertise and experience in deploying containerized application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira, or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Advanced Systems design and architecture.
Experience mentoring developers preferred.
Ability to work with outside vendors and clients under sometimes adverse circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, when necessary.
Must be able to respond to after-hours pager notifications to provide support for applications as necessary.
177936
","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker']","['commun', 'graph', 'cluster', 'data modeling']",1,"['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'commun', 'graph', 'cluster', 'data modeling']","['analyt', 'digit', 'essenti', 'relat', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin', 'evalu']","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'commun', 'graph', 'cluster', 'data modeling', 'analyt', 'digit', 'essenti', 'relat', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin', 'evalu']"
DE,"As a Sr Data Engineer, you will specialize in data pipelines, large-scale data processing and distributed systems.
You will also help establish best engineering practices.
Local candidates only.
RESPONSIBILITIES:
Build scalable production data pipelines while improving on existing architecture
Design and implement data processing jobs, transformations while improving on existing architecture
Manage and maintain data infrastructure
Collaborate with Infrastructure Engineers to ensure that standards are being met for resource provisioning, network security and continuous deployment
SUCCESS FACTORS:
Guaranteed data cleanliness and integrity
Flexibility to work with different platforms, languages and pipelines.
Commitment to lifelong learning.
SKILLS & QUALIFICATIONS:
Bachelor’s degree in Computer Science or related field, or equivalent work experience
3+ years holding similar responsibilities
Skills/Technologies
Languages: Python, SQL, Scala/Java
Databases: Redshift, Redshift Spectrum, Aurora, BigQuery, Snowflake, MySQL/PostgreSQL
Technologies: Kubernetes, Docker, Spark, Pandas, Airflow
Cloud Tools: AWS Glue, EMR, AWS Batch, Dataflow, CloudComposer, PubSub
Preferred: Familiarity with AWS and GCP’s data tools
Powered by JazzHR
OUZr0CwE8G
","['sql', 'gcp', 'spark', 'airflow', 'scala', 'panda', 'bigqueri', 'aw', 'snowflak', 'python', 'redshift', 'java', 'cloud', 'postgresql', 'mysql', 'kubernet', 'docker']",['pipelin'],1,"['sql', 'gcp', 'spark', 'airflow', 'scala', 'panda', 'bigqueri', 'aw', 'snowflak', 'python', 'redshift', 'java', 'cloud', 'postgresql', 'kubernet', 'docker', 'pipelin']","['spark', 'pipelin', 'relat', 'power', 'infrastructur', 'aw', 'python', 'comput', 'integr', 'engin']","['sql', 'gcp', 'spark', 'airflow', 'scala', 'panda', 'bigqueri', 'aw', 'snowflak', 'python', 'redshift', 'java', 'cloud', 'postgresql', 'kubernet', 'docker', 'pipelin', 'spark', 'pipelin', 'relat', 'power', 'infrastructur', 'aw', 'python', 'comput', 'integr', 'engin']"
DE,"The Job
The Data Engineer must have experience with data modeling, modern database technologies, and API driven search design and development.
The Media Supply Chain – Mission
The Daily
Develop and provide support for core data relationship, data ingest, data transformation services and search capabilities.
Creates functional and technical specifications.
Creates and executes against a plan to launch and maintain applications.
Implement best practice standards for development, build and deployment automation.
Recommend action, develop and lead implementation of selected products/services.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
3+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL, and Relational.
Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch, logstash, kibana).
Demonstrated expertise and experience in modern databases such as Mongo, Couchbase, Neptune, Neo4j, or equivalent.
Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch query DSL, cypher, gremlin, or graphql.
xQuery preferred but not required.
Highly proficient in XML, JSON and YAML data exchange formats.
Experience in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana etc.
Experience in working with data streaming technologies such as Amazon Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch, RDS, Redshift, Neptune and ec2.
Highly proficient in at least one modern programming language such python, java, or node.js.
Bash experience preferred.
Demonstrated expertise and experience in deploying containerized application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira, or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Systems design and architecture.
Ability to work with outside vendors and clients under sometimes adverse circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when necessary.
Must be able to respond to after-hours pager notifications to provide support for applications as necessary.
Requisition #
177937BR
Area of Interest
Industry
Film Production and Distribution
United States - California - Burbank
Position Type
Full Time
Business Unit
Business Unit Overview
WBT manages the Studio’s enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.
Warner Media, LLC and its subsidiaries are equal opportunity employers.
Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
","['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['recommend', 'commun', 'graph', 'data modeling']",1,"['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'recommend', 'commun', 'graph', 'data modeling']","['analyt', 'essenti', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin']","['linux', 'elasticsearch', 'jira', 'git', 'aw', 'tableau', 'python', 'redshift', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'recommend', 'commun', 'graph', 'data modeling', 'analyt', 'essenti', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin']"
DE,"Digital Product, Platform & Strategy: Content Mgmt & Distribution department.
The Software Engineer will be a member of the Media Supply Chain (MSC) team
within the WarnerMedia Technology (WMTO) organization, the position will be
responsible for hands on development as well as managing application launches
and software maintenance from inception to launch.
The Software Engineer must
understand business functions and requirements and translate those to working
applications that allow Media Supply Chain technology to continue innovating in
support of business need.
Media Supply Chain is tasked to architect, engineer, and program manage a wide
operationally manage and distribute WarnerMedia content globally.
applications and technology solutions are responsible for scheduling, image &
asset metadata management, as well as, content processing and delivery as well
as content mastering, localization and preservation.
new ways to process and deliver WarnerMedia content to its global customers
The Media Supply Chain Mission
technology solutions which are highly reliable, automated, measurable,
optimized, modern systems capable of distributing high-quality content and
The Daily
Develop and provide support for core functionality and components for
applications and service in support of various content platforms by
starting with business needs and objectives, creating functional and
technical specifications and executing against a plan to launch and
maintain applications.
Review project objectives and determine best technology for
implementation.
Implement best practice standards for development, build
and deployment automation while mentoring and leading a team of
engineers.
Review emerging technologies and evaluate potential uses for WarnerMedia
(WM) Technology and other divisions.
Develop prototype projects using new
technologies.
Evaluate software products and vendors for WM Technology and other
divisions.
Recommend action, develop and lead implementation of selected
products/services.
Work with internal and external developers to ensure WB Technology code
standards and best practices are performed for development of
applications.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
6+ years software development experience.
4+ years experience with Java, including significant demonstrable
experience with Java 8.
2+ years experience in Node.js, Python or equivalent scripting lanague.
Bash experience preferred.
Demonstrated expertise and experience in ELK stack (elasticsearch,
logstash, kibana)
Demonstrated experience implementing database technologies such as NoSQL,
and Relational.
Experience in graph databases a plus.
Experience with a Javascript front end framework such as React.js or
Polymer.
Polymer experience preferred.
Experience with a Java front-end
framework such as Hibernate, Faces or Grails is a plus.
3+ years experience in working with noSQL databases such as Mongo,
Couchbase, Dynamo, Cassandra or equivalent.
Experience in graph databases
such as Neptune or Neo4j preferred but not required.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Experience with source code and knowledge repositories such as git, jira,
or equivalent systems.
Experience developing in AWS at scale leveraging core services such as
Lambda, RDS, and ec2.
Experience with Build and Deployment tools such as Jenkins, Chef, Puppet,
or equivalent.
Demonstrated expertise and experience in deploying containerized
applications using Docker, Kubernetes or equivalent.
Demonstrated expertise and experience in automating development pipelines
using Cloudformation or Teraform.
Proficient in a Linux environment.
Solid understanding of core DevOps and SDLC in an agile environment.
Overall knowledge of WarnerMedia.
businesses, business processes,
technologies and applications that support it is preferred.
Ability to work with outside vendors and clients under sometimes adverse
circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of
personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when
necessary.
Must be able to respond to after-hours pager notifications to provide
support for applications as necessary.
","['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker']","['optim', 'commun', 'graph', 'pipelin']",1,"['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker', 'optim', 'commun', 'graph', 'pipelin']","['asset', 'essenti', 'program', 'digit', 'pipelin', 'optim', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']","['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker', 'optim', 'commun', 'graph', 'pipelin', 'asset', 'essenti', 'program', 'digit', 'pipelin', 'optim', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']"
DE,"Successful Marketing Automation SaaS platform startup in West LA is looking for a Senior Data Engineer who will be architecting highly scalable data integration and transformation platform processing high volume of data under defined SLA.
You will be creating and building the platform that includes ingestion and transformation of data, data governance, machine learning, analytics and consumer insights.
You will be solving complex problems with a business that celebrates innovation and values your contributions.
Qualifications:
3+ years working in Big Data and related technologies
5+ years Java and Spring Boot experience
Experience building high-performance, and scalable distributed systems
AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)
Experience in a variety of relevant technologies including Cassandra, AWS
DynamoDB, Kafka, AWS Kinesis, Elasticsearch, Machine Learning, Spark, Hadoop, Hive, Presto
Experience in ETL and ELT workflow management
Familiarity with AWS Data and Analytics technologies such as Glue, Athena,
Redshift, Spectrum, Data Pipeline
MS preferred, or BA/BS degree in computer science, related field, or equivalent
practical experience
A fantastic opportunity to be part of a growing start-up.
A chance to work with a passionate, driven and fun team.
An incredible work environment - fun, casual and fast-paced
Monthly team activities and outings
Loft-style office with plenty of break-out space
heart desires
Great benefits - Health, Dental, Vision, and Vacation
","['spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'java', 'kafka']","['machine learning', 'etl', 'pipelin', 'big data']",1,"['spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'java', 'kafka', 'machine learning', 'etl', 'pipelin', 'big data']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'comput', 'big', 'etl', 'engin', 'integr']","['spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'java', 'kafka', 'machine learning', 'etl', 'pipelin', 'big data', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"* Proactively identify opportunities to improve data integrations and data security.
* Understand business and technology goals; present options and considerations in solution development and problem solving, being mindful of the cost/benefit prospect of developing quickly versus developing for long term sustainability.
* Support and train non-technical personnel in editorial, events, marketing, underwriting and membership.
* Other duties as assigned.Required Education and Experience:* Bachelor's Degree or equivalent work experience* 3+ years of work experience in a full-stack and/or CRM development capacity* Proven experience building integrations between disparate data platforms and CRMs* Experience with architecting, development and performance tuning of relational databases* Experience in working with large-scale, consumer-oriented websites* Experience working with content management systemsRequired Skills, Knowledge and Abilities:* Experience with version control, code base maintenance and peer review* Expertise writing queries in SQL and NoSQL* Experience with relational, non-relational and columular databases* Experience building and consuming REST and SOAP APIs* Experience with cloud-based infrastructure (AWS and GCP prefered)* Excellent communication skills, especially around technical requirements* Ability to interpret user needs, draft requirements and provide accurate work estimates* Proven ability to work as an individual contributor* Experience writing object-oriented code, preferably in Ruby and/or PHP* Experience writing in scripting language, such as Python or JavaScript* You care about the details* You excel in an interdisciplinary team.
You seek perpetual growth* You have a desire to understand and try out new technologies* You find working creatively with constraints around resources, time, and ambiguity excitingReporting to this Position: NonePhysical Demands and Working Conditions:* Must be able to perform the essential duties of the position with or without reasonable accommodation.
* Physical Demands:* May be required to work from home* Required to walk, sit, and stand; reach with hands and arms; balance, stoop, kneel, or crouch* Frequent use of hands for data entry/keystrokes and simple grasping.
","['sql', 'gcp', 'javascript', 'aw', 'python', 'php', 'cloud', 'nosql', 'excel', 'rubi']","['problem solving', 'commun', 'tune']",1,"['sql', 'gcp', 'javascript', 'aw', 'python', 'php', 'cloud', 'nosql', 'excel', 'rubi', 'problem solving', 'commun', 'tune']","['essenti', 'relat', 'infrastructur', 'aw', 'python', 'integr']","['sql', 'gcp', 'javascript', 'aw', 'python', 'php', 'cloud', 'nosql', 'excel', 'rubi', 'problem solving', 'commun', 'tune', 'essenti', 'relat', 'infrastructur', 'aw', 'python', 'integr']"
DE,"The Data Engineer will be responsible for creating solutions,
comprehensive analytics and will be integral in implementing
company-wide data strategy.
Responsibilities:
Collaborate with other departments to understand data needs
Develop and optimize ETL processes
Optimize data warehouse by defining technical requirements
Auditing and automating data quality
Work closely with development teams to create long term plans
for problem resolution
Requirements:
Bachelors Degree or greater (technical or science degree
preferred).
2 years of experience in the following:
scalable architectures and large data processing
AWS, Linux and Shell Scripting
ETL and data mining
open source programming language
data warehousing concepts
SQL, and System-level DBA functions ex: configuring replication,
automating backups, performance tuning and RDBMS
","['sql', 'aw', 'linux']","['data mining', 'tune', 'optim', 'data warehousing', 'etl']",1,"['sql', 'aw', 'linux', 'data mining', 'tune', 'optim', 'data warehousing', 'etl']","['analyt', 'optim', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['sql', 'aw', 'linux', 'data mining', 'tune', 'optim', 'data warehousing', 'etl', 'analyt', 'optim', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"About The Role
You thrive in a startup where every individual has a significant impact on the technology is empowered to make decisions and get things done.
What You'll Do
Build large scale fault tolerant data collection and processing pipelines from the ground up
Work with the platform engineering team to collaborate on instrumentation and event stream implementation within NEXTs core platform architecture.
What You'll Have
3-5 Years of experience in Data engineering or related software engineering experience in a fast-paced start-up environment.
Track record of delivering ETLs, machine learning pipelines, and data products within a cloud-based microservices or event streaming architecture.
Experience developing solutions in a continuous delivery eco-system.
Strong verbal and written communication skills and able to communicate effectively to technical and non-technical team members
Motivated by a sense of urgency and ownership
Preferred Qualifications
Graduate degree in computer science.
Active involvement in the open source community.
Experience in transportation, logistics, and supply chain industries.
Experience in venture-backed startups or other hyper-growth environments.
What You'll Receive:
Competitive Base Salary + Equity
Full Medical, Dental and Vision Benefits
Vacation and Holidays
Fun perks: open office, dog-friendly, unlimited snacks, and monthly catered lunches!
",['cloud'],"['pipelin', 'machine learning', 'etl', 'logist', 'commun']",2,"['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun']","['machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']","['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun', 'machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']"
DE,"Handling the potential these data offer is a tremendous and complex task.
You can translate security requirements into sustainable data architecture and workflows.
You will work with Security and Software Engineers to solve current problems and create new opportunities.
You will work with engineers across Information Security group to establish and promote standards for collecting, processing, storing, and analyzing data.
Responsibilities:
Create and promote a technical design and architectural vision for security operations data systems and tooling
Work with engineers on the team to manage infrastructure for moving and processing large-scale data
Improve log flows efficiency through data processing to reduce the cost of analysis and storage
Convert log flows from Logstash to Elastic Common Schema
Work with Analysts to determine and implement the best practices for data retention and storage
Design and promote standards for security operations data telemetry and processing
Help engineers across the team select data technologies for their development needs
Required Qualifications:
2+ years experience working with data systems
2+ years experience with data warehousing, processing, pipelines, infrastructure, and query patterns
2+ years experience with Spark or Hadoop
2+ years experience with open source ETL frameworks such as Airflow, Luigi, or similar
4+ years experience with Python and SQL
Bachelor's degree in Computer Science or related field
Desired Qualifications:
Experience with systems for data processing (Spark, Flink, Hadoop, Airflow) and storage (S3, Kafka, ElasticSearch, Dynamo, MySQL, or Postgres)
Experience with ELK
Experience reading and optimizing data schema queries for content and performance
Experience working in agile project management methodologies (Scrum, Kanban)
For this role, you'll find success through craft expertise, a collaborative spirit, and decision-making that prioritizes the delight of players.
If you embody player empathy and care about the experiences of players, this could be the role for you!
Life insurance, parental leave, plus short-term and long-term disability coverage are also available.
===
Don't forget to include a resume and cover letter.
","['sql', 'spark', 'airflow', 'elasticsearch', 'hadoop', 'python', 's3', 'postgr', 'mysql', 'kafka']","['data warehousing', 'etl', 'pipelin']",1,"['sql', 'spark', 'airflow', 'elasticsearch', 'hadoop', 'python', 's3', 'kafka', 'data warehousing', 'etl', 'pipelin']","['spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'comput', 'etl', 'common', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'elasticsearch', 'hadoop', 'python', 's3', 'kafka', 'data warehousing', 'etl', 'pipelin', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'comput', 'etl', 'common', 'sourc', 'engin']"
DE,"Req ID: 177572
The Job
Warner Bros. has been entertaining audiences for more than 90 years through the worlds most-loved characters and franchises.
Warner Bros. employs people all over the world in a wide variety of disciplines.
WB Technology combines Warner Bros. industry-leading technologists and disciplines to ensure global alignment with business strategy and accelerated delivery of innovative technology solutions studio- and industry-wide.
From pre-production through archiving, the WBT organization will provide critical business and technology intelligence and services to all Studio business units.
WBT manages the Studios enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.
As a Data Engineer at WB, you will be responsible for the creating data engineering components using Snowflake in an AWS ecosystem.
The Engineer will also be responsible for hands on development of high-quality scalable enterprise data solutions and data services, managing product launches and maintenance from inception to launch and through continuous delivery.
The ideal candidate must collaboratively partner across the organization to understand Warner Bros. business functions and data requirements.
The engineer will translate those into data products, software features and services to allow Warner Bros. to continue to innovate.
The candidate must be able to implement seamless data services across cross-functional teams and continually improve upon key performance indicators.
You must also be an experienced problem-solver and be comfortable navigating ambiguity and influencing.
You will work closely with product managers, project managers, data architects and data engineers to implement and maintain data pipelines leveraging emerging and established data technologies.
You will need to be able to work in a constantly changing, fast-paced environment and must be able to adapt to new technologies and new ideas.
You must be detail oriented, with a strong affinity for data and analytics to support business decisions.
The Daily
Work closely with the business and technical project manager to understand the business requirement and translate into technical specs.
Provide analysis reports and estimations.
Design, develop, install, test and maintain data integrations from a variety of formats including files, database extracts and external APIs into data stores (including Snowflake, Elastic, S3, etc) using ETL tools, techniques and programming languages like Python, Spark, SQL, etc.
Build high-performance data engineering algorithms and prototypes.
Create flexible data models, tune queries and ETL components.
Manage job orchestration using tools like Airflow.
Research possible customization for tuning, cost optimization, performance enhancements, data reliability and quality.
Ensure that all solutions meet the business/company requirements for solution data reliability, quality and disaster recovery.
Own the application/data end-to-end from requirements to post production working closely with other teams.
Provide engineering leadership by actively advocating best practices and standards for software engineering.
Collaborate with other team members such as data architects, data scientists etc.
Consistently contribute into the project management practices using Agile method.
Present the prototype to the stakeholders and leadership.
THE ESSENTIALS
Bachelors degree in Computer Science or related field.
Minimum of 5 years of data analytics/data engineering, complex ETL/ELT experience in database systems like Snowflake, Teradata, etc.
using Python.
Minimum 5 years of experience in using SQL/NoSQL, JSON and XML data structures.
Minimum 5 years of big data technologies including Hadoop, Apache Spark, Snowflake and AWS Suite of technologies (S3, EMR, Lambda).
Minimum 3 years of Experience using Restful APIs for ETL purposes.
Expert problem solver with strong analytical skills.
Expert in SQL (Snowflake, Teradata) and Python.
Expert in ETL/ELT tools and techniques.
Experience using big data tools (Hadoop, Map-reduce, Elastic search, Kinesis, Kafka, Solr).
Experience using AWS technologies (EMR, S3, Kinesis, Lambda).
Experience using Restful APIs for ETL purposes.
Experience using Git or SVN and Jira.
Experience using Spark (Scala/Java), Spark SQL and Spark Streaming is a plus.
Experience in Segments, MParticle, Adobe Site Catalyst or any other similar digital analytics product products is a plus.
Experience in Tableau is a plus.
Strong communication skills and proficient in Excel, Word, PowerPoint, MS Project and MS Visio Snowflake.
Ability to work independently or collaboratively.
Detail oriented with strong organization and prioritization skills.
Entertainment and/or Social Media experience a plus.
Demonstrated ability to work well under time constraints.
Must be able to work flexible hours, including possible overtime, when necessary.
Must be able to maintain confidentiality.
Management has the right to add or change duties and job requirements at any time.
177572
","['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr']","['research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun']",1,"['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr', 'research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun']","['techniqu', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'essenti', 'hadoop', 'optim', 'aw', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'jira', 'python', 's3', 'java', 'nosql', 'kafka', 'git', 'hadoop', 'snowflak', 'lambda', 'powerpoint', 'scala', 'aw', 'tableau', 'excel', 'spark', 'airflow', 'solr', 'research', 'pipelin', 'segment', 'optim', 'big data', 'etl', 'commun', 'techniqu', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'essenti', 'hadoop', 'optim', 'aw', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Job Title: Sr. Data Engineer
For more information, visit www.inmarket.com.
About You
You build real products in the real world.
You understand that off the shelf solutions vs building them yourself have tradeoffs and when to make the right decision.
You have the ability to mentor junior engineers, and understand tradeoffs for scale, elegance of solution and time.
You may have a preferred programming language but you're not afraid to tackle problems in other languages you've never used before.
You love the fact that code is continuously tested and released for fast feedback.
You are great at communicating and making sure decisions are made that satisfy multiple stakeholders.
You will design well-constructed datasets and work cross-functionally with analysts, product managers, and other engineers to effectively deliver actionable results.
The ideal candidate has a proven track record of working with large datasets.
In this role you will have the opportunity to work with Redshift, BigQuery, Hadoop, Spark, EMR, Ruby, NodeJS/React, and other technologies.
Requirements:
Solid CS fundamentals
5+ full time years experience in software development (in at least one of Python, Scala, Javascript, Java, or Ruby)
Experience working with large databases, distributed systems, data structures, concurrency
Experience working with Cloud IaaS (AWS, GCP)
Experience with large scale data processing (Hive, Hadoop, Redshift, BigQuery)
Benefits Summary
Competitive salary, stock options, flexible vacation
Medical, dental and Flexible Spending Account (FSA)
Unlimited PTO (Within reason)
Talented co-workers and management
Agile Development Program (For continued learning/professional development)
Paid Paternity & Maternity Leave
Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status.
","['nodej', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'javascript', 'aw', 'python', 'redshift', 'hive', 'java', 'cloud', 'react', 'rubi']",['account'],999,"['nodej', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'javascript', 'aw', 'python', 'redshift', 'hive', 'java', 'cloud', 'react', 'rubi', 'account']","['program', 'spark', 'hadoop', 'aw', 'python', 'action', 'releas', 'engin']","['nodej', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'javascript', 'aw', 'python', 'redshift', 'hive', 'java', 'cloud', 'react', 'rubi', 'account', 'program', 'spark', 'hadoop', 'aw', 'python', 'action', 'releas', 'engin']"
DE,"Builds and maintains distributed systems, services and data pipelines.
This role is required to write highly scalable and fault tolerant code
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark on Databricks, Apache Airflow and AWS ‘ big data’ technologies like AWS Glue, AWS Athena.
Solid experience in SQL development
Solid experience with Python, Spark, shell scripting
Extensive ETL development experience with large-scale DBS or big data systems such as Redshift, Snowflake, Databricks, etc.
Experience working with reporting tools such as Tableau or Looker
Extensive experience with design & development of relational databases and data warehouses
Ability to look at solutions unconventionally and explore opportunities and devise innovative solutions
Technologies:
Python
Scala
Java
AWS Glue
AWS Redshift
AWS EMR
AWS Data Pipeline
AWS Kinesis
GCP Dataflow
GCP BigQuery
Looker
Snowflake
Redshift Spectrum
","['sql', 'gcp', 'spark', 'airflow', 'looker', 'scala', 'bigqueri', 'aw', 'snowflak', 'java', 'python', 'redshift', 'tableau', 'db']","['optim', 'etl', 'pipelin', 'big data']",999,"['sql', 'gcp', 'spark', 'airflow', 'looker', 'scala', 'bigqueri', 'aw', 'snowflak', 'java', 'python', 'redshift', 'tableau', 'db', 'optim', 'etl', 'pipelin', 'big data']","['spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'big', 'etl', 'sourc']","['sql', 'gcp', 'spark', 'airflow', 'looker', 'scala', 'bigqueri', 'aw', 'snowflak', 'java', 'python', 'redshift', 'tableau', 'db', 'optim', 'etl', 'pipelin', 'big data', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'big', 'etl', 'sourc']"
DE,"Â
Â
Â
Duration: 6+ Months
Rate: $DOE/hr.
Â
Â
Key: Python, Spark, SQL, Redshift, AWS (with certification)
Â
Deploy and maintain data pipelines.
Assemble large, complex data sets.
Build optimal ETL infrastructure using AWS offerings.
Build high performance, fault tolerant, and scalable systems.
Collaborate and coordinate with other teams and parts of the business.
Communicate technical concepts to non-technical stakeholders.
Â
Â
Strong Python programming skills.
Spark programming knowledge.
Redshift experience.
Strong experience with AWS Services, well versed with various AWS ETL Services (EMR, Glue, etc.
), RDS, AWS Lambda, etc.
(Preferred).
Should have hands on experience with effort estimations, setting up RDS, IAM roles / policies management, etc.
Work experience with Databricks is highly desirable.
Strong written and verbal communication skills in English.
Â
Thanks & Regards,
Â
Vishnuwar S
Direct: +1 (908) 533 â 7359
Mobile: +1 (732) 588 â 7828
Email: vishnus@trovetechs.com
Â
LinkedIn: https://www.linkedin.com/in/vishnu-mba-us-it-recruiter-trovetechs-a2810a131
","['sql', 'spark', 'aw', 'lambda', 'redshift', 'python']","['optim', 'etl', 'commun', 'pipelin']",999,"['sql', 'spark', 'aw', 'lambda', 'redshift', 'python', 'optim', 'etl', 'commun', 'pipelin']","['spark', 'pipelin', 'set', 'infrastructur', 'optim', 'aw', 'python', 'etl', 'â']","['sql', 'spark', 'aw', 'lambda', 'redshift', 'python', 'optim', 'etl', 'commun', 'pipelin', 'spark', 'pipelin', 'set', 'infrastructur', 'optim', 'aw', 'python', 'etl', 'â']"
DE,"As a data engineer, you’ll work closely with data scientists, engineers, psychometricians, as well as UX designers and researchers to actualize the potential derived from combining some of the richest behavioral data sets available with cognitive data.
You'll not only get to push your own boundaries, but the boundaries of artificial intelligence.
WHAT YOU WILL OWN AND DRIVE
Job orchestration and deployment.
Understand existing database architecture and study the needs of the business team to propose new solutions and design
Proactively seek new value-add opportunities for customers and convert those new opportunities to realized value.
Advising and implementing best practices in multiple technical domains
Python codebase with pandas DataFrame.
WHAT YOU NEED TO BRING TO THE TABLE
Experienced in IT with at least 2-3 years in a direct Data Engineer position.
Experience with real time data streaming tools (e.g.
Apache Kafka, AWS Kinesis)
Hands-on experience with leading commercial Cloud platforms, preferably GCP
A good understanding of Software Development Life Cycle, Disaster Recovery and Data Resiliency.
Experience working in Agile/Scrum environments
A portfolio of activities / a side project that showcases your intellectual curiosity
Software engineering skills: Python
WHAT MAKES YOU STAND OUT
Familiarity with Lambda Architecture, GCS, Bigquery.
Detail-oriented, with excellent analytical, technical and problem-solving skills
Good ability to derive and design technical specifications from general product requirements
Experience building personalization or recommendation engines
Experience with Machine Learning/Data Science
Familiarity with and interest in psychometric data
","['gcp', 'panda', 'bigqueri', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'excel']","['recommend', 'machine learning', 'research']",999,"['gcp', 'panda', 'bigqueri', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'excel', 'recommend', 'machine learning', 'research']","['analyt', 'machin', 'stream', 'learn', 'aw', 'avail', 'python', 'scientist', 'set', 'engin']","['gcp', 'panda', 'bigqueri', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'excel', 'recommend', 'machine learning', 'research', 'analyt', 'machin', 'stream', 'learn', 'aw', 'avail', 'python', 'scientist', 'set', 'engin']"
DE,"Job Title:
Sr. Data Engineer
Requisition ID:
R001902
Job Title: Sr. Data Engineer
Reporting To: Director, Data Governance
Department: Central Tech
Your Platform
Central Tech is distributed globally with offices across the U.S., and in Canada, England, Ireland and Japan.
Your Mission
Do you have a passion for solving complex privacy engineering and data governance problems?
Are you an outspoken advocate for the implementation of best in class data privacy and governance standards and practices?
The ideal candidate will have extensive hands-on experience in designing and developing scalable and robust engineering solutions.
Previous gaming industry experience is a plus.
You should be willing and able to wear multiple hats.
Responsibilities:
Develop scalable, efficient, forward-looking privacy solutions that support requirements of GDPR, CA Privacy Act, and other global privacy regulations.
Work closely with other engineering teams and business stakeholders to design automated solutions for privacy and compliance.
Implement best in class data protection standards and solutions for data pseudonymization, data anonymization, access control, logging, and auditing.
Implement data governance tools such as data catalog and processes for effectively managing metadata, data classification, and lineage.
Develop solutions for data profiling, data quality monitoring, reporting, and testing.
Support creation and implementation of data governance policies, standards, and processes.
Player Profile
Proven verbal and written communication skills, including the ability to communicate complex technical ideas to the business stakeholders.
Experience working with big data technologies (Hadoop, Hive, Spark, Presto, or similar.)
4+ years of production engineering experience.
Experience in working with cloud infrastructures.
Familiarity with GDPR, CCPA, and other privacy regulations.
Production experience in Python and/or Java
Experience writing / debugging complex SQL statements
Experience in implementation of Data Catalog tools such as Atlas, Collibra, or Alation
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
","['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python']","['classif', 'commun', 'big data']",999,"['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python', 'classif', 'commun', 'big data']","['basi', 'spark', 'hadoop', 'infrastructur', 'python', 'big', 'engin']","['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python', 'classif', 'commun', 'big data', 'basi', 'spark', 'hadoop', 'infrastructur', 'python', 'big', 'engin']"
DE,"Job Details:
Skills & Experience:
5+ years of applicable engineering experience.
Strong proficiency in Python with an emphasis in building data pipelines.
Ability to write complex SQL to perform common types of analysis and aggregations.
Experience with Apache Airflow or Google Composer.
Detail-oriented and document all the work.
Can develop industrial strength applications in the data space using Python (preferred)/Spark or Scala or both (ideal), substantial programming experience.
Strong SQL, can think in sets, can write optimized ETL solutions.
Strong understanding of SQL WINDOW functions and their usages.
Experience with Python and programming background is required.
Expertise using both understand Spark or MPP architecture.
Experience with Snowflake or a modern MPP DB (Redshift, Vertica) is a plus.
Strong CS fundamentals (has to pass easy to medium level coding test in the language preferred + SQL Test).
Strong Communication skills and leadership skills.
Ready to work as an individual contributor and can take ownership of task assigned.
Past experience with AWS stack is a plus.
Analytical skills should be strong.
How will you grow?
Role-based Training programs
Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional
What's in it for you?
Excellent benefits plan: medical, dental, vision, life, FSA, & PTO
Roll over vacation days
Commuter benefits
Excellent growth and advancement opportunities
Certification reimbursement
Rewards and recognition programs
LTI values diversity and inclusion and is committed to the principles of Equal Employment Opportunity EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity.
","['sql', 'spark', 'airflow', 'scala', 'aw', 'snowflak', 'redshift', 'python', 'db', 'excel']","['optim', 'etl', 'commun', 'pipelin']",999,"['sql', 'spark', 'airflow', 'scala', 'aw', 'snowflak', 'redshift', 'python', 'db', 'excel', 'optim', 'etl', 'commun', 'pipelin']","['day', 'analyt', 'program', 'spark', 'pipelin', 'set', 'divers', 'optim', 'aw', 'python', 'etl', 'common', 'engin']","['sql', 'spark', 'airflow', 'scala', 'aw', 'snowflak', 'redshift', 'python', 'db', 'excel', 'optim', 'etl', 'commun', 'pipelin', 'day', 'analyt', 'program', 'spark', 'pipelin', 'set', 'divers', 'optim', 'aw', 'python', 'etl', 'common', 'engin']"
DE,"Req ID: 34961
Experience Level: Professional
But its more than that.
Youll be part of a team that genuinely cares about helping you succeed.
In return for your contributions, youll receive premier compensation and benefits, and a company-funded retirement plan that ranks among the most generous.
If you are looking to join a team that offers autonomy, plenty of space to run with new ideas, working in a nurturing environment, and challenging problems to solve at a significant scale, this might be the job for you!
What youll be doing:
Build large-scale distributed data processing systems, data lakes, and optimize for both computational and storage efficiency on cloud platforms like AWS.
Design, implement and automate data pipelines sourcing data from internal and external systems, transforming the data for the optimal needs of various systems.
Design data schema and operate cloud-based data warehouses and SQL/NoSQL/temporal database systems.
Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs.
Own the design, development and maintenance of ongoing metrics, reports, analyses, dashboards, etc.
to drive key business decisions.
Monitor and troubleshoot operational or data issues in the data pipelines.
Drive architectural plans and implementation for future data storage, ETL, reporting, and analytic solutions.
Influence your teams technical and business strategy by making insightful contributions to team priorities and approach.
Provide insightful code reviews, receive code reviews constructively and take ownership of outcomes (you ship it, you own it), working very efficiently and routinely deliver the right things in the front-end UI area.
Building relationships with your customers, partner teams and the engineers on your team.
Influence your teams technical decisions by making insightful contributions to team priorities and approach.
Your background and who you are:
You have a background in data and software engineering and a passion to learn.
You've made mistakes in the past and have learned a lot from them.
You believe there are generally multiple ways to solve a technical problem, each with different trade-offs.
You approach projects, tasks, and unknowns with curiosity, and enjoy sharing what you know and what you learn with the people around you.
You believe that a team is strongest when it is diverse and includes multiple perspectives.
You are able to put yourself into your customer's shoes.
You frequently immerse yourself in the customer experience to understand how you can better serve them.
Qualifications:
BS in Computer Science or related field, or an equivalent in relevant work experience.
3+ years experience implementing big data processing technology: Hadoop, Apache Spark, etc.
3+ years coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.).
Experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets.
Experience in cloud-first design, preferably AWS (VPC, Serverless databases and functions, dynamic autoscaling, container orchestration, etc.).
Experience in data architecture, databases (e.g., MySQL, Oracle, PostgreSQL), SQL and DDD/ER/ORM design.
Interest and curiosity in emerging technologies on the web like GraphQL, web assembly, Lambda functions, MLaaS etc
Knowledge of software engineering practices & best practices for the software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.
#GD-Tech
","['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'mysql', 'oracl', 'rubi']","['pipelin', 'dashboard', 'optim', 'big data', 'etl']",1,"['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'oracl', 'rubi', 'pipelin', 'dashboard', 'optim', 'big data', 'etl']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'provid', 'optim', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'oracl', 'rubi', 'pipelin', 'dashboard', 'optim', 'big data', 'etl', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'provid', 'optim', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"The Data Team at ReSci is in a new phase of its evolution.
Your expertise at algorithms will be paired closely with the software expertise of Data Engineers, and you will jointly own these ML systems.
You should want to make a huge impact in a fast-paced and cutting-edge start-up environment.
You should have great experience with scala/spark or python/serverless and working closely with ML algorithms and data scientists.
You’ll be building robust real-time data pipelines that process billions of events per day and sitting on the team that builds end-to-end ML algorithms.
The code and ideas that you contribute will have a tangible impact on the business as a whole.
Your code will touch millions of end-users.
You will have full responsibility for your projects and have a real sense of product ownership.
About You:
5+ years experience working on production data products in Python, Scala, or similar
Proficient in at least one statically typed language
Experience designing and implementing large, scalable services
Passionate about enforcing software engineering principles, production code quality, and regular use of design patterns
Experience interfacing with APIs - SOAP, REST, etc.
Comfortable using Git, Bitbucket/Github
Strong belief that tests and code go hand-in-hand
Deep understanding of SQL, query optimizations, joins etc.
Excellent CS foundation: data structures, time complexities, algorithms, etc.
Startup work experience a major plus!
ReSci's mission is to make artificial intelligence accessible and usable for brands.
Inspire with passion.
Persevere with determination.
Collaborate with unity.
Grow without bounds.
Create with impact.
Lead with character.
Cortex makes 3.5+ billion predictions per day and processes 5k+ events per second.
Job Type: Full-time
Benefits:
401(k)
401(k) Matching
Dental Insurance
Disability Insurance
Employee Assistance Program
Flexible Schedule
Health Insurance
Life Insurance
Paid Time Off
Parental Leave
Professional Development Assistance
Vision Insurance
Schedule:
Monday to Friday
Experience:
production data engineering with python or scala: 5 years (Required)
Innovative -- innovative and risk-taking
Outcome-oriented -- results-focused with strong performance culture
Team-oriented -- cooperative and collaborative
retentionscience.com
Work Remotely:
Temporarily due to COVID-19
","['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel']","['optim', 'risk', 'pipelin', 'predict']",999,"['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel', 'optim', 'risk', 'pipelin', 'predict']","['day', 'program', 'spark', 'pipelin', 'ml', 'predict', 'optim', 'python', 'scientist', 'employe', 'engin', 'algorithm']","['sql', 'spark', 'scala', 'git', 'python', 'github', 'excel', 'optim', 'risk', 'pipelin', 'predict', 'day', 'program', 'spark', 'pipelin', 'ml', 'predict', 'optim', 'python', 'scientist', 'employe', 'engin', 'algorithm']"
DE,"OverviewUnder the direction of Director - Data Engineer, the Senior Data Engineer works closely with business leaders, managers, staff and vendor to accurately gather and interpret requirements, specifications.
Develop technical specifications and recommend, design, develop, test, implement, and support innovative and optimal data solutions.
Serves as a coach and mentor to Data engineering teamResponsibilitiesLead on all aspects of a modern and flexible SDLC approach, applied to data engineering, analytics and data science efforts Provide business guidance and steer efforts toward desired outcomes; partner with internal stakeholders and external vendors to meet strategic data warehouse goals Research, evaluate and formally recommend third party software and technology package Responsible for new & existing integrated system & data flow enterprise architecture Set standards for data engineering functions; design templates for the data management which are scalable, repeatable, and simple Defines and develops appropriate controls to monitor quality of the enterprise data warehouse process Preparation of technical specification Perform all other related duties as assignedQualificationsMinimum 7 years' experience in a complex data warehouse role for a mid to large size organization; Health Care industry experience highly desired Minimum 4 years hands on experience with ETL/Data Integration, BI, data mining and modeling, SSIS strongly preferred Proficiency in all facets of DW Architecture, data flow strategy, data modeling, metadata and master data management Mastery expertise in SQL and RDBMS systems such as Microsoft SQL Server Knowledge of and experience working with reporting tools like SSRS and Tableau Stay abreast of established and industry emerging data technologies Demonstrated ability to produce high quality technical documentation.
Experience in implementing a data architecture with separation between storage and compute preferred.
Deep understanding of data architecture and ability to coordinate with the implementation team is highly desired.#LI-JP1
","['sql', 'ssr', 'bi', 'tableau', 'microsoft']","['research', 'data mining', 'data modeling', 'optim', 'etl']",2,"['sql', 'ssr', 'powerbi', 'tableau', 'microsoft', 'research', 'data mining', 'data modeling', 'optim', 'etl']","['analyt', 'set', 'relat', 'evalu', 'provid', 'optim', 'bi', 'parti', 'packag', 'appli', 'comput', 'warehous', 'etl', 'engin', 'integr']","['sql', 'ssr', 'powerbi', 'tableau', 'microsoft', 'research', 'data mining', 'data modeling', 'optim', 'etl', 'analyt', 'set', 'relat', 'evalu', 'provid', 'optim', 'bi', 'parti', 'packag', 'appli', 'comput', 'warehous', 'etl', 'engin', 'integr']"
DE,"Interview Process Coding Test and Final onsite interview.
Required 6+ years of professional software experience Expert in Python or a JVM language (Scala, Java) Experience with Apache Spark (or related big data technology) Deep knowledge of SQL and love for data wrangling Experience with Docker.
","['sql', 'spark', 'scala', 'java', 'python', 'docker']",['big data'],999,"['sql', 'spark', 'scala', 'java', 'python', 'docker', 'big data']","['big', 'relat', 'python', 'spark']","['sql', 'spark', 'scala', 'java', 'python', 'docker', 'big data', 'big', 'relat', 'python', 'spark']"
DE,"Summary:
You will be taking on exciting new development projects and not just maintenance work, working with rapidly growing scale that presents exciting challenges...
What you'll be doing:
Work with various teams to build an infrastructure/pipeline to collect and analyze all necessary data
Translate high level requirements to actionable tasks/deliverables and creative problem solving a must
Bachelor's Degree in Computer Science/related field or equivalent work experience
5+ years of experience in development of large scale data processing systems and ETL pipelines
Must have good command of Scala/Python/Java, Hadoop/Spark and Kinesis/Kafka
Experience with tuning Hadoop clusters, Amazon EMR, AirFlow, and other big data ETL technologies
Experience with Docker, Jenkins, or similar build tools
Experience with monitoring tools
Experience with Hive, Presto, Storm, Flink, Druid, Elastic Search, DynamoDB/Cassandra/HBase/Riak is a plus
Experience with AWS for data applications is a plus
Benefits & Perks:
A fun environment where work-life balance is valued
Very competitive salary
Generous bonus plan
Employer-matched 401(k) plan
Competitive benefits package
Healthy snacks
Local gym discount
Attractive paid time off policy - Open/Flexible vacation policy
All applicants must be authorized to work in the U.S.
This organization uses E-Verify.
Category: Computer/Software
","['spark', 'airflow', 'cassandra', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'hbase', 'kafka', 'docker']","['pipelin', 'problem solving', 'big data', 'etl', 'cluster']",1,"['spark', 'airflow', 'cassandra', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'hbase', 'kafka', 'docker', 'pipelin', 'problem solving', 'big data', 'etl', 'cluster']","['spark', 'challeng', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'action', 'big', 'etl']","['spark', 'airflow', 'cassandra', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'hbase', 'kafka', 'docker', 'pipelin', 'problem solving', 'big data', 'etl', 'cluster', 'spark', 'challeng', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'action', 'big', 'etl']"
DE,"Data Engineer - City Solution's
Who are you?
You are an experienced data engineer looking to tackle some of the most challenging technical problems impacting the world of micro-mobility.
You enjoy solving complex data problems and working closely with cross-functional teams, including Product, Data, and Ops.
Responsibilities
Design, develop, and maintain large scale data processing systems
Develop and maintain core data pipeline
Create, develop, and support technical infrastructure
Build APIs and design documents
Work closely with various business partners (Product, Firmware team, UI/UX)
Lead architecture design
Requirements
Bachelor's degree in Computer Science or related technical discipline
3+ years of engineering experience
2+ years of experience with real-time streaming and data-processing frameworks using Flink, Spark, Hadoop, MapReduce, or similar technologies
2+ years of experience building data pipelines using Kafka or similar technologies
Preferred Qualifications
Experience in Java, Kotlin, JVM based languages (Scala, Clojure, Groovy, JRuby)
SQL experience (Postgresql is a plus)
Distributed computing experience
Excellent understanding of computer science fundamentals, data structures, and algorithms
Expertise in object-oriented design methodology and large-scale application development in an object-oriented language
Understanding of distributed systems and service-oriented architecture
Flink
Hadoop
Kafka
Redis
People first, people.
Perks up
Sound like a place you'd like to work?
Sweet.
Let's chat.
","['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'clojur', 'java', 'postgresql', 'kafka', 'excel']",['pipelin'],1,"['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'clojur', 'java', 'postgresql', 'kafka', 'excel', 'pipelin']","['spark', 'challeng', 'pipelin', 'hadoop', 'infrastructur', 'comput', 'relat', 'engin', 'algorithm']","['sql', 'mapreduc', 'spark', 'scala', 'hadoop', 'clojur', 'java', 'postgresql', 'kafka', 'excel', 'pipelin', 'spark', 'challeng', 'pipelin', 'hadoop', 'infrastructur', 'comput', 'relat', 'engin', 'algorithm']"
DE,"Minimum 3 years of experience in cloud technology (Azure) Strong hands on knowledge on Spark (with Python as language).
End to end implementation experience in data analytics solutions (data ingestion, processing, provisioning and Orchestration.
Strong experience in Azure ecosystem such as HDInsight, Azure Data Factory, Azure Data Lake and SQL DW and ADLS2 Strong SQL and Shell script knowledge.
Hands on experience developing enterprise solutions using designing and building frameworks, enterprise patterns, database design and development.
End to end Cloud solution on Azure (Azure SQL DW, Azure Data factory, HDInsight, SQL on Azure) Batch solution and distributed computing using ETLELT (Spark SQLSpark Data frameADF).
Implementation of data encryption at rest and in transit.
DWBI (MSBI, Oracle, SQL Server), Data modelling, performance tuning, memory optimization, DB partitioning.
Frameworks, reusable components, accelerators, CICD automation Mentor and lead a data engineering teams to design, develop, test and deploy high performance data analytics solutions.
If you refer a candidate with the desired qualifications and your candidate accepts the role, you can earn a generous referral fee.
","['sql', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl']",['optim'],999,"['sql', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim']","['analyt', 'spark', 'azur', 'optim', 'python', 'engin']","['sql', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim', 'analyt', 'spark', 'azur', 'optim', 'python', 'engin']"
DE,"Translate business requirements into visualizations that provide actionable insights to stakeholders and empower decision making.
Develop KPIs and perform quantitative analysis based on an understanding of both business objectives and complex data models.
Implement agreed upon solutions to documented business needs and provide ongoing support for those solutions.
Drive ideas and projects by communicating the strategic value for the organization to leadership.
Qualifications 3+ years of industry experience utilizing BI tools such as Domo, Tableau, or similar.
Experience converting ambiguous business needs to highly scalable data models and datasets.
Ability to perform analysis over large and complex datasets and communicate findings to stakeholders.
Experience working with SQL is a must.
Previous experience with AWS or similar cloud environment.
Desired Experience working with HTMLCSSJavaScript.
Experience with Superset, D3.js or similar open source tools.
Prior experience working in an Agile environment.
Ability to travel as needed Gamer or experience in the gaming industry a plus.
","['sql', 'bi', 'tableau', 'aw', 'cloud']","['commun', 'visual', 'kpi']",999,"['sql', 'powerbi', 'tableau', 'aw', 'cloud', 'commun', 'visual', 'kpi']","['visual', 'bi', 'aw', 'action', 'quantit', 'sourc']","['sql', 'powerbi', 'tableau', 'aw', 'cloud', 'commun', 'visual', 'kpi', 'visual', 'bi', 'aw', 'action', 'quantit', 'sourc']"
DE,"They are actively looking for a Senior Data Engineer to join their team.
They have an awesome collaborative culture and learning environment.
",[None],[None],999,[],"['learn', 'engin']","['learn', 'engin']"
DE,"Job Title Big Data Engineer (with Azure Data factory Exp)
Duration : Long term/ Fulltime
Skills Pyspark, Azure (Azure SQL DW, Azure Data factory, HDInsight), Big data
Min 8 to 10 years of IT experience, including experience in developing and implementing Big Data & Azure cloud solution
Minimum 3 years of experience in cloud technology (Azure) Strong hands on knowledge on Spark (with Python as language)
End to end implementation experience in data analytics solutions (data ingestion, processing, provisioning and Orchestration
Strong experience in Azure ecosystem such as HDInsight, Azure Data Factory, Azure Data Lake and SQL DW and ADLS2 Strong SQL and Shell script knowledge
Hands on experience developing enterprise solutions using designing and building frameworks, enterprise patterns, database design and development
End to end Cloud solution on Azure (Azure SQL DW, Azure Data factory, HDInsight, SQL on Azure)
Batch solution and distributed computing using ETLELT (Spark SQLSpark Data frameADF)
Implementation of data encryption at rest and in transit
DWBI (MSBI, Oracle, Sql Server), Data modelling, performance tuning, memory optimization, DB partitioning
Frameworks, reusable components, accelerators, CICD automation Mentor and lead a data engineering teams to design, develop, test and deploy high performance data analytics solutions
","['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl']","['optim', 'big data']",999,"['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim', 'big data']","['analyt', 'spark', 'azur', 'optim', 'python', 'big', 'engin']","['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim', 'big data', 'analyt', 'spark', 'azur', 'optim', 'python', 'big', 'engin']"
DE,"Job Title – Big Data Engineer (with Azure Data Exp)
Skills – Pyspark, Azure (Azure SQL DW, Azure Data factory, HDInsight), Big data
· Min 8 to 10 years of IT experience, including experience in developing and implementing Big Data & Azure cloud solution
· Minimum 3 years of experience in cloud technology (Azure) Strong hands on knowledge on Spark (with Python as language)
· End to end implementation experience in data analytics solutions (data ingestion, processing, provisioning and Orchestration
· Strong experience in Azure ecosystem such as HDInsight, Azure Data Factory, Azure Data Lake and SQL DW and ADLS2 Strong SQL and Shell script knowledge
· Hands on experience developing enterprise solutions using designing and building frameworks, enterprise patterns, database design and development
· End to end Cloud solution on Azure (Azure SQL DW, Azure Data factory, HDInsight, SQL on Azure)
· Batch solution and distributed computing using ETLELT (Spark SQLSpark Data frameADF)
· Implementation of data encryption at rest and in transit
· DWBI (MSBI, Oracle, Sql Server), Data modelling, performance tuning, memory optimization, DB partitioning
· Frameworks, reusable components, accelerators, CICD automation Mentor and lead a data engineering teams to design, develop, test and deploy high performance data analytics solutions
","['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl']","['optim', 'big data']",999,"['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim', 'big data']","['analyt', 'spark', 'azur', 'optim', 'python', 'big', 'engin']","['sql', 'pyspark', 'spark', 'azur', 'cloud', 'db', 'python', 'oracl', 'optim', 'big data', 'analyt', 'spark', 'azur', 'optim', 'python', 'big', 'engin']"
DE,"Data Engineer
Do you have Streaming Platform experience and want to build the next Netflix?
If yes and you are a Data Engineer with experience, please read on!
As the Data Engineer on the team, you will be a part of the team that is responsible for developing, constructing, test existing and new architectures.
You will play an integral role in identifying ways to improve data reliability, efficiency, and quality.
This position will work closely with the Content Intelligence team along with all of the Data Engineering and cross-functional teams.
What You Will Be Doing
"" Provide thought leadership and lead best practice solutions for the entire Data Engineering team.
""
Take lead in driving a culture of high quality, innovation, and incremental experimentation.
""
Design, implement and fully own critical portions of analytical data models and pipelines used to populate them.
""
Lead the collaboration with stakeholders to understand requirements, lead and design data structures using best practices, and develop batch or real-time data pipelines to ensure the timely delivery of high-quality data.
""
Own, maintain and support existing data pipelines and platforms storing petabytes of data quickly and reliably.
""
Creatively explore and demonstrate the value of data for the team.
Be part of a driving force that enables and empowers better decision making through data.
""
Has a knack of using flexible and scalable methodologies that can be applied to a broad set of problems across the Data Engineering organization.
""
Always be thirsty for optimizations and always be eager to find new, improved ways to address an old problem.
What You Need for this Position
"" 2-3 years in a lead capacity.
""
SQL Rockstar: can code complex SQL logic, lead best practices and drive the development of other data engineers.
""
Proven Computer Science fundamentals in Algorithms and Data Structures.
""
7-8 years of experience in Python or other programming languages such as Scala or Java.
""
Significant experience with Big Data technologies such as Apache Spark.
""
2-3 years of experience with MPP/Cloud Data Warehouses such as Snowflake.
""
Five years of experience with core AWS services related to data engineering.
""
Education: Bachelor's or Master's degree in computer science or related field.
The Nice to Haves
"" Experience with using Apache Kafka or AWS Kinesis as a message bus for storing and processing high-velocity real-time data.
""
Experience with building data notebooks using Jupyter, AWS Sagemaker, or Databricks.
""
Experience with building and productionizing ML Pipelines and other solutions.
""
Experience with Graph-based workflow orchestration engine such as Apache Airflow.
""
Familiarity with marketing and media platforms/data sets.
What's In It for You
Great Benefits Package!
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'scala', 'jupyt', 'sagemak', 'aw', 'cloud', 'python', 'kafka']","['optim', 'graph', 'pipelin', 'big data']",1,"['sql', 'scala', 'jupyt', 'sagemak', 'aw', 'cloud', 'python', 'kafka', 'optim', 'graph', 'pipelin', 'big data']","['analyt', 'pipelin', 'ml', 'set', 'relat', 'provid', 'optim', 'python', 'aw', 'packag', 'appli', 'warehous', 'comput', 'big', 'integr', 'engin', 'algorithm']","['sql', 'scala', 'jupyt', 'sagemak', 'aw', 'cloud', 'python', 'kafka', 'optim', 'graph', 'pipelin', 'big data', 'analyt', 'pipelin', 'ml', 'set', 'relat', 'provid', 'optim', 'python', 'aw', 'packag', 'appli', 'warehous', 'comput', 'big', 'integr', 'engin', 'algorithm']"
DE,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst.
The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions.
The candidate should possess a data engineering background, business acumen to think strategically and love working with people.
Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis.
The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.
Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl']","['dashboard', 'healthcar', 'problem solving']",3,"['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'dashboard', 'healthcar', 'problem solving']","['essenti', 'techniqu', 'engin', 'challeng', 'aw', 'python', 'scientist', 'comput', 'collect']","['qlik', 'linux', 'aw', 'tableau', 'python', 'redshift', 's3', 'cloud', 'oracl', 'dashboard', 'healthcar', 'problem solving', 'essenti', 'techniqu', 'engin', 'challeng', 'aw', 'python', 'scientist', 'comput', 'collect']"
DE,"Clients.
Big
Data Engineer
The purpose of the opportunity is to
identify, propose, develop, validate, and deploy innovative data insight
Responsibilities
Leads and supports efforts to provide
Identify, propose, develop, validate,
and deploy innovative data insight systems to business partners.
Provide business insights by integrating
data from disparate systems and services in maintainable ways and developing
validated solutions to provide those insights.
Provide support to other engineers in
the form of solution identification, mentoring, automation and data or effort
validation.
Identify opportunities for data sharing
and develop federated systems to supply that data.
Basic
Qualifications.
8+ years of software engineering 5+
years of large scale systems
A strong knowledge of the Java
programming language
Working experience with Hadoop batch
processing system and the map-reduce horizontally scalable paradigm
Experience with the Linux operating
system
Strong grounding in object oriented
programming, aspect oriented programming, design patterns, concurrency,
algorithms & data structures
Development experience using service
oriented architecture, JAX-RS and JAXB .
Strong ability to research solutions,
processes, industry trends and best practices.
Experience with alternative data
processing platforms including Storm, Spark, Shark, Apache Mesos, Hive, PIG,
and Apache Crunch
Experience with an analytical approach
to tuning models
Experience with machine learning and
machine learning libraries like Apache Mahout
Experience with deployment environments
and systems including Amazon Web Services & Chef
Experience with diverse storage systems,
platforms and methods including hBase, MongoDB, Apache Avro, PostgreSQL,
Greenplum, Teradata
Experience with data integration
including Apache Camel, Spring Batch and Talend
Experience with pilot application
frameworks including Spring Roo, GWT 2.0, Portlets and Spring MVC
Experience with disciplined development
practices including using tools like Maven, TestNG, Findbugs, CheckStyle,
Checker, Sonar, wikis, coding standards and Git
Experience working in an evolved
technical organization including contributing to and following coding
guidelines, best practices, documenting, and presenting at brown bags,
professional groups and conferences.
Experienced in participating in scrum or
other agile development environments.
consideration, please send your updated resume and contact info via email
rohit.sheelvant@prolim.com or Contact (248) 8760896 Ext 225.
Technologies.
Automotive, Aerospace, High Tech and Industrial Machinery companies throughout
the world.
Product Lifecycle Management (PLM) practice is powered by a team of consultants
who provide comprehensive end-to-end service offerings.
solutions to the most-pressing PLM challenges.
The CoE is supported by the
right skills, resources, technologies and methodologies and helps develop
customized solutions to better manage the entire product lifecycle.
","['mongodb', 'linux', 'spark', 'pig', 'git', 'hadoop', 'java', 'hive', 'mahout', 'hbase', 'postgresql', 'amazon web services']","['research', 'machine learning']",999,"['mongodb', 'linux', 'spark', 'pig', 'git', 'hadoop', 'java', 'hive', 'mahout', 'hbase', 'postgresql', 'aw', 'research', 'machine learning']","['analyt', 'machin', 'program', 'learn', 'spark', 'divers', 'power', 'hadoop', 'provid', 'big', 'integr', 'engin']","['mongodb', 'linux', 'spark', 'pig', 'git', 'hadoop', 'java', 'hive', 'mahout', 'hbase', 'postgresql', 'aw', 'research', 'machine learning', 'analyt', 'machin', 'program', 'learn', 'spark', 'divers', 'power', 'hadoop', 'provid', 'big', 'integr', 'engin']"
DE,"Req ID: 177549
The Job
Warner Bros. has been entertaining audiences for more than 90 years through the worlds most-loved characters and franchises.
Warner Bros. employs people all over the world in a wide variety of disciplines.
WB Technology combines Warner Bros. industry-leading technologists and disciplines to ensure global alignment with business strategy and accelerated delivery of innovative technology solutions studio- and industry-wide.
From pre-production through archiving, the WBT organization will provide critical business and technology intelligence and services to all Studio business units.
WBT manages the Studios enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.
As a Data Engineering at WB, you will be responsible for building high-quality scalable enterprise data solutions and data services that exceed data, reporting and analytical needs of the organization.
The ideal candidate will have technical skills to deal with multiple terabytes of data using Apache Spark framework, Elastic Search and Snowflake in an AWS ecosystem.
You will work closely with Product Managers, Data Architects, Data Engineers to implement & maintain Data pipelines leveraging Big Data Technologies.
The role will work closely with engineering leadership to plan and write high quality performant code.
You will also participate in peer code reviews, mentor junior engineers and champion a high standard of code excellence.
Your will manage applications and own the technology stack end-to-end to enable data to be exchanged across the studio.
Job Responsibilities
Data Integration and Pipelines Design and develop data integrations from a variety of formats including files, database extracts and external APIs.
Operations and Tuning - Investigate problems and resolve as required, including working with various internal teams and vendors.
Proactively monitor the data flows with a focus on continued performance improvements.
Develop POCs and best practices for application development.
Application Onwership Own the application/data end-to-end from requirements to post production, working closely with other teams.
Provide engineering leadership by actively advocating best practices and standards for software engineering.
Share knowledge and guide junior engineers to level-up the whole team.
THE ESSENTIALS
Bachelors degree in Computer Science or related field.
Minimum or 7 years of software engineering/development experience.
Minimum of 5 years of Data Analytics/data engineering, Complex ETL/ELT experience.
Minimum of 3 years working with Big data technologies including Hadoop, Apache Spark, Snowflake and AWS Suite of technologies (S3, EMR, Lambda).
Minimum of 3 years architecting datastores NoSQL/SQL.
Experience with scripting (Shell, Python) in a linux environment.
Experience with consuming and creating restful APIs.
Expert problem solver with strong analytical skills.
Expert in Spark (Scala/Java), Spark SQL and Spark Streaming.
Expert in SQL (Snowflake, MySQL).
Experience using big data tools (Hadoop, Map-reduce, Elastic search, Kinesis, Kafka, Solr).
Experience using AWS technologies (EMR, S3, Kinesis, Lambda).
Strong object oriented and functional programming skills.
Experience writing BaSH Scripts.
Experience using Git or SVN and Jira.
Experience using Python is a plus.
Experience using Restful APIs is a plus.
Experience with Ad platforms or CRM solutions is a plus.
Strong communication skills.
Ability to work independently or collaboratively.
Detail oriented with strong organization and prioritization skills.
Entertainment and/or Social Media experience a plus.
Demonstrated ability to work well under time constraints.
Must be able to work flexible hours, including possible overtime, when necessary.
Must be able to maintain confidentiality.
Management has the right to add or change duties and job requirements at any time.
177549
","['sql', 'linux', 'spark', 'scala', 'solr', 'jira', 'git', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'lambda', 'mysql', 'hadoop', 'kafka', 'excel']","['pipelin', 'tune', 'big data', 'etl', 'commun']",1,"['sql', 'linux', 'spark', 'scala', 'solr', 'jira', 'git', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'lambda', 'hadoop', 'kafka', 'excel', 'pipelin', 'tune', 'big data', 'etl', 'commun']","['analyt', 'essenti', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'provid', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'linux', 'spark', 'scala', 'solr', 'jira', 'git', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'lambda', 'hadoop', 'kafka', 'excel', 'pipelin', 'tune', 'big data', 'etl', 'commun', 'analyt', 'essenti', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'provid', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"The Role
Job Summary & Responsibilities:
The Junior Business Intelligence Data Engineer (full-time, salaried, regular) reports into the business intelligence division.
The BI data engineering team is responsible for collecting, analyzing and distributing data using public cloud and open source technologies and offers transparency into customer behavior and business performance
Responsibilities:
• Collaborate with product teams and data analysts to design and build data-forward solutions
• Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably
• Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices
• Build and maintain dimensional data warehouses in support of business intelligence tools
• Develop data catalogs and data validations to ensure clarity and correctness of key business metrics
• Drive and maintain a culture of quality, innovation and experimentation
• Deliver strong Python and SQL development and maintenance techniques surrounding data movement to include technologies
• Investigate and understand different data sources and ability to connect to a wide variety of 3rd party APIs
• Design, enhance and implement ETL/data ingestion platform on the cloud
• Development of ETL source and target mapping design/specifications based on the business requirements.
Create ETLs/ELTs to take data from various operational systems and create a unified/enterprise data model for analytics and reporting
• Develop load and transformation processes in support of the requirements, validate that they meet business and technical specifications, manage ongoing maintenance of the system and data, and make recommendations for process improvements to optimize data movement from source to target
• Provide production and operational support to existing ETL jobs.
Monitor and manage production ETL jobs to verify execution and measure performance to assure ongoing data quality and optimization of the system to manage scalability and performance and identify improvement opportunities for key ETL processes.
• Strong troubleshooting and problem-solving skills in large data environment
• Capable of investigating, familiarizing and mastering new data sets quickly
Education/Experience:
• Bachelor's degree Computer Science or equivalent
• Strong background in scripting language using Python, Bash, Perl, PHP or any other language to solving data problems
• Experience with relational SQL and NoSQL databases, including Postgres, ,Neo4j and MongoDb
• Experience with Big Data tools; Hadoop, Spark, Kafka, Hive etc
• Proficiency with the AWS cloud services : EC2, EMR, RDS, S3, Redshift (spectrum)
• Proficiency with data exchange types and protocols (json, xml, soap, rest)
• Experience with Stream Processing systems: Storm ,Spark-Streaming etc
• Experience with BI tools like Tableau or any other open source BI tools etc.
Knowledge, Skills, & Abilities:
• Knowledge of the Python data ecosystem using pandas and numpy
• Data integration tools
• Proficiency in SQL, data modeling, and data warehousing
• Excellent problem solving skills
• Exposure to cloud platforms (preferably AWS)
Physical Requirements:
• The ability to sit for prolonged period of time and view computer screen.
Equipment/Software Used:
• Microsoft Office Suite (Outlook, Word, Excel, PowerPoint)
• SQL, MySQL or other relational databases
• Linux, Python, AWS Stack (EC2,EMR S3, Redshift)
• Tableau or any other data visualization tool
• SiteCatalyst (Omniture)/Google Analytics or any other web analytics tools experience (Nice to have)
Work Environment:
• Work is performed in an office environment that is well lit and ventilated.
","['sql', 'linux', 'python', 's3', 'redshift', 'nosql', 'php', 'kafka', 'perl', 'hadoop', 'hive', 'postgr', 'powerpoint', 'mongodb', 'panda', 'ec2', 'bi', 'aw', 'tableau', 'microsoft', 'mysql', 'excel', 'spark', 'numpi', 'cloud']","['recommend', 'pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'etl']",1,"['sql', 'linux', 'python', 's3', 'redshift', 'nosql', 'php', 'kafka', 'perl', 'hadoop', 'hive', 'powerpoint', 'mongodb', 'panda', 'ec2', 'powerbi', 'aw', 'tableau', 'microsoft', 'excel', 'spark', 'numpi', 'cloud', 'recommend', 'pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'etl']","['techniqu', 'stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'analyt', 'hadoop', 'optim', 'public', 'bi', 'aw', 'parti', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', '3rd', 'comput', 'relat']","['sql', 'linux', 'python', 's3', 'redshift', 'nosql', 'php', 'kafka', 'perl', 'hadoop', 'hive', 'powerpoint', 'mongodb', 'panda', 'ec2', 'powerbi', 'aw', 'tableau', 'microsoft', 'excel', 'spark', 'numpi', 'cloud', 'recommend', 'pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'etl', 'techniqu', 'stream', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'analyt', 'hadoop', 'optim', 'public', 'bi', 'aw', 'parti', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', '3rd', 'comput', 'relat']"
DE,"Build highly scalable resilient data pipelines and models which produce high quality datasets.
Deliver visualizations that distill clear, actionable insights from large, complex datasets.
Qualifications 4+ years of industry experience building highly scalable data pipelines (batch andor streaming) utilizing Spark, Hive, Presto or other open source frameworks Architecture.
Python and shell scripting experience for automation and data manipulation.
Prior experience utilizing dashboarding tools such as Domo, Tableau, Superset or similar.
Prior experience utilizing dashboarding tools such as Tableau, Superset or similar.
Experience translating ambiguous business needs to highly scalable data models and datasets.
Background in software engineering - able to write elegant, scalable and maintainable code.
Strong SQL skills Desired Experience with Airflow, Superset or similar open source tools.
Previous experience with AWS or similar cloud environment.
Experience preparing large, complex datasets for Machine Learning pipelines a plus.
Prior experience working in an Agile environment.
Gamer or experience in the gaming industry a plus.
","['sql', 'spark', 'airflow', 'aw', 'tableau', 'python', 'hive', 'cloud']","['machine learning', 'visual', 'pipelin']",999,"['sql', 'spark', 'airflow', 'aw', 'tableau', 'python', 'hive', 'cloud', 'machine learning', 'visual', 'pipelin']","['machin', 'learn', 'spark', 'pipelin', 'visual', 'aw', 'python', 'action', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'aw', 'tableau', 'python', 'hive', 'cloud', 'machine learning', 'visual', 'pipelin', 'machin', 'learn', 'spark', 'pipelin', 'visual', 'aw', 'python', 'action', 'sourc', 'engin']"
DE,"Position Overview
In this role, you will work closely with cross-departmental resources to meet requirements imposed by both Data Scientists and other Software Engineers collecting, parsing, analyzing and visualizing large sets of data.
You also will set a lot of best practices for ETL processes, and data quality.
Responsibilities and Job Duties
Identify the best technology stacks to enable production grade live data processing.
Work alongside data scientists to ensure data is processed correctly and efficiently.
Attend daily stand-ups led by TPM.
Write clean testable code.
Minimum Qualifications
6+ years of hands-on experience in “big-data” technologies in a Hadoop environment.
BS or MS in Computer Science or a related degree.
Hands-on experience and strong proficiency with either Python, Scala, or Java.
Familiarity with software engineering practices for the full software development life cycle, including coding standards, code reviews, source control management, agile development, build processes, testing, and operations.
A deep understanding of the fundamentals of distributed data processing, data modeling, and ETL.
Ability to extract data from multiple data sources and load them into a centralized data warehouse to facilitate unified reporting.
Desired Experience:
Hands-on experience with working with Spark or Storm.
Hands-on experience with Hive and HBase.
Experience creating and ingesting data streams with Apache Kafka or similar pub/sub message systems.
Experience in determining testing strategy and execution of test cases.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
","['spark', 'scala', 'hadoop', 'java', 'hive', 'python', 'hbase', 'kafka']","['clean', 'data modeling', 'etl']",1,"['spark', 'scala', 'hadoop', 'java', 'hive', 'python', 'hbase', 'kafka', 'clean', 'data modeling', 'etl']","['stream', 'spark', 'set', 'relat', 'hadoop', 'python', 'warehous', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin']","['spark', 'scala', 'hadoop', 'java', 'hive', 'python', 'hbase', 'kafka', 'clean', 'data modeling', 'etl', 'stream', 'spark', 'set', 'relat', 'hadoop', 'python', 'warehous', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"They need someone with experience using Java, Scala, Python, Spring, Angular, and Big Data tools such as Spark or Hive.
","['spark', 'scala', 'angular', 'python', 'java', 'hive']",['big data'],999,"['spark', 'scala', 'angular', 'python', 'java', 'hive', 'big data']","['big', 'python', 'spark']","['spark', 'scala', 'angular', 'python', 'java', 'hive', 'big data', 'big', 'python', 'spark']"
DE,"About The Role
You thrive in a startup where every individual has a significant impact on the technology is empowered to make decisions and get things done.
What You'll Do
Build large scale fault tolerant data collection and processing pipelines from the ground up
Work with the platform engineering team to collaborate on instrumentation and event stream implementation within NEXTs core platform architecture.
What You'll Have
7 - 12 Years of experience in Data engineering or related software engineering experience in a fast-paced start-up environment.
Track record of delivering ETLs, machine learning pipelines, and data products within a cloud-based microservices or event streaming architecture.
Experience developing solutions in a continuous delivery eco-system.
Strong verbal and written communication skills and able to communicate effectively to technical and non-technical team members
Motivated by a sense of urgency and ownership
Preferred Qualifications
Graduate degree in computer science.
Active involvement in the open source community.
Experience in transportation, logistics, and supply chain industries.
Experience in venture-backed startups or other hyper-growth environments.
What You'll Receive:
Competitive Base Salary + Equity
Full Medical, Dental and Vision Benefits
Vacation and Holidays
Fun perks: open office, dog-friendly, unlimited snacks, and monthly catered lunches!
",['cloud'],"['pipelin', 'machine learning', 'etl', 'logist', 'commun']",2,"['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun']","['machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']","['cloud', 'pipelin', 'machine learning', 'etl', 'logist', 'commun', 'machin', 'stream', 'engin', 'pipelin', 'relat', 'comput', 'etl', 'sourc', 'collect']"
DE,"Summary:
This role will also grow and scale the data team providing both technical and team leadership.
Responsibilities:
Lead enterprise-wide data platforms, operations, and architecture management activities
Proactively identify operational and systemic issues within the data supply value chain (from collection to processing to reporting) and work with production operations (DevOps) team to implement monitoring solutions.
Recruit, retain and actively coach data engineers, including data quality and warehousing
Required Qualifications:
Strong SQL skills with proven ability to write complex data queries across large data sets
Strong exposure to big data technology preferably across a containerized environment (Hadoop, Spark, Hive, Presto)
Strong attention to detail with excellent analytical, problem-solving, and communication skills
Bachelor’s degree in Computer Science, Computer Engineering or Information Technology or related fields is required
Good time management and ability to work on concurrent assignments with different priorities
Ability to work in a fast paced, iterative development environment with short turn-around times
Preferred Qualifications:
Experience with A/B Testing and related optimization across desktop and mobile in a digital environment a plus (examples include: Optimizely, Leanplum, deltaDNA)
Experience analyzing and manipulating data across several data formats (JSON, Avro, Parquet, ORC)
Experience building and architecting data warehouse workflows in large cloud-based production environments (Snowflake as an example)
Prior experience working in Educational Technology and/or Gaming Studios an added plus
Experience with sourcing and modeling data from Restful API (Application Programming Interface)
Medical, Dental, Vision + 401k
Highly competitive PTO policy
Casual Dress Code
Snacks + Drinks (Coca Cola Freestyle Machine)
Gaming room including an Arcade (2,000+ games)
Limitless opportunities for professional growth!
","['sql', 'spark', 'hadoop', 'cloud', 'snowflak', 'hive', 'excel']","['analyz', 'optim', 'big data', 'information technology', 'commun']",1,"['sql', 'spark', 'hadoop', 'cloud', 'snowflak', 'hive', 'excel', 'analyz', 'optim', 'big data', 'information technology', 'commun']","['analyt', 'digit', 'machin', 'program', 'engin', 'spark', 'relat', 'hadoop', 'optim', 'comput', 'warehous', 'big', 'set', 'sourc', 'collect']","['sql', 'spark', 'hadoop', 'cloud', 'snowflak', 'hive', 'excel', 'analyz', 'optim', 'big data', 'information technology', 'commun', 'analyt', 'digit', 'machin', 'program', 'engin', 'spark', 'relat', 'hadoop', 'optim', 'comput', 'warehous', 'big', 'set', 'sourc', 'collect']"
DE,"Interested in joining an organization that is disrupting the food delivery space?
They are growing their suite of products and have ample opportunities for growth into leadership roles.
In this role you will be building data pipelines, intelligent monitoring, ETL, & working with databases such as PostgreSQL, DynamoDB, & Snowflake.You'd also be interacting with API's, Docker / Kubernetes, & CI / CD pipelines.
Think millions of queries per second, petabytes of data, with solutions that have a a massive scale impact (used by millions of people).Ready to be a database pioneer?Required Skills & Experience* Python, Java development* 5+ years experience with relational SQL databases & distributed NoSQL data stores (ETL, MongoDB, Hadoop, databases/stores, APIs, MySQL, Postgres, DynamoDB, etc)* Docker ( Kubernetes is a plus)* Skilled at data validation, quality, ETL, and data streams.
* ElasticSearch, ZooKeeper, HBase, Memcache, Kafka, Openshift* CI / CD pipelines with Jenkins is a plus* Adept knowledge in AWS + AWS tools (ElasticSearch, RDS, EC2, S3, ELB, etc.
)The Offer* Competitive Salary: Up to $170K/yearYou will receive the following benefits:* Award winning culture* 100% employer paid benefits (medical, dental, vision)* Unlimited Snacks & drinks* Kombucha on tap* 401(k) with match and immediate vesting* Equity* Unlimited and encouraged Paid Time Off* Wellness Programs* Team EventsApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.
This position does not offer sponsorship.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['mongodb', 'sql', 'elasticsearch', 'ec2', 'kafka', 'hadoop', 'aw', 'python', 's3', 'java', 'nosql', 'postgresql', 'mysql', 'kubernet', 'hbase', 'postgr', 'docker']","['etl', 'pipelin']",999,"['mongodb', 'sql', 'elasticsearch', 'ec2', 'kafka', 'hadoop', 'aw', 'python', 's3', 'java', 'nosql', 'postgresql', 'kubernet', 'hbase', 'docker', 'etl', 'pipelin']","['basi', 'program', 'stream', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'etl']","['mongodb', 'sql', 'elasticsearch', 'ec2', 'kafka', 'hadoop', 'aw', 'python', 's3', 'java', 'nosql', 'postgresql', 'kubernet', 'hbase', 'docker', 'etl', 'pipelin', 'basi', 'program', 'stream', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'etl']"
DE,"Senior Big Data Engineer - Java, Big Data, Remote
Are you a fit?
Salary: $150,000 - $200,000
(Remote Opportunity)
If you are an Principle Software Architect with great technical and client facing skills, then please read on.
150-200k Base!
Extremely Competitive Stock Package!
Flexible Work Schedules!
Accelerated Career Growth!
Job Details
Is your background a fit?
MS preferred, or Bachelor's Degree in Computer Science Plus:
5+ years Java and Spring Boot for Microservices
3+ years working in Big Data (High Scale Data-Pipeline)
Kafka or (RabbitMQ, Active MQ)
Cassandra (NoSQL such as DynamoDB, MongoDB, and Reddis)
AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)
Experience in ETL and workflow management (1 billion records a day)
Experience with Streaming Technology (Flink)
Familiar with AWS Kinesis, Elasticsearch, Machine Learning, Spark, Hadoop, Hive, Presto, Athena, Glue, Spectrum, Data Pipeline
Interested in hearing more?
","['mongodb', 'spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'nosql', 'java', 'kafka']","['machine learning', 'etl', 'pipelin', 'big data']",1,"['mongodb', 'spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'nosql', 'java', 'kafka', 'machine learning', 'etl', 'pipelin', 'big data']","['day', 'machin', 'stream', 'learn', 'spark', 'pipelin', 'hadoop', 'aw', 'comput', 'packag', 'big', 'etl', 'engin']","['mongodb', 'spark', 'elasticsearch', 'cassandra', 'ec2', 'hadoop', 'aw', 'lambda', 'redshift', 's3', 'hive', 'nosql', 'java', 'kafka', 'machine learning', 'etl', 'pipelin', 'big data', 'day', 'machin', 'stream', 'learn', 'spark', 'pipelin', 'hadoop', 'aw', 'comput', 'packag', 'big', 'etl', 'engin']"
DE,"Sr Data Engineer 3 Positions
Type : C2C / W2 / FT / C2H
Duration : Long Term
JD
Mandate skill AWS Redshift, Python SQL, Pyspark
Experience on AWS and its web service offering S3, Redshift, EC2, EMR, Lambda, CloudWatch, RDS, Step functions, Spark streaming etc.
Good knowledge of Configuring and working on Multi node clusters and distributed data processing framework Spark.
Hands on 3 years of experience with EMR Apache Spark Hadoop technologies
Experience with must have Linux, Python and PySpark, Spark SQL.
Experience in working with large volumes of data Tera-bytes, analyze the data structures
Experience in designing scalable data pipelines, complex event processing, analytics components using big data technology Spark Python Scala PySpark,
Expert in SQLPLSQL, redshift, NoSQL database
Experience in process orchestration tools Apache Airflow, Apache NiFi etc.
Hands on knowledge of design, development and enhancement of Data Lakes, constantly evolve with emerging tools and technologies
","['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql']","['pipelin', 'cluster', 'big data']",999,"['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql', 'pipelin', 'cluster', 'big data']","['analyt', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']","['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql', 'pipelin', 'cluster', 'big data', 'analyt', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']"
DE,"The Data Engineer will lead an exciting team in developing innovative technology and delivering uniquely creative content globally.
The salary for the Data Engineer starts at $90 per hour.
Duties & Responsibilities:
Build and optimize performance of Hadoop and Spark batch jobs (Spark, Kafka, Cassandra, etc.).
Construct and improve ElasticSearch performance.
Build data pipelines orchestration.
Create the design and architecture for data-lake, data-marts, data-models, and data-warehouse.
Ensure efficiency of data science workflows and advanced machine learning algorithms.
Contribute to open source solutions and communities.
Stay current on emerging tools and technologies.
Collaborate cross-functionally with other software engineers and their teams.
Establish and demonstrate technologies, solutions, and leading practices.
Balance resources, requirements, and complexity.
Qualifications:
At least, 5 years Data Engineer experience.
Bachelor’s in Computer Science or similar field.
Possess a passion for coding.
Knowledge of distributed systems and computation.
Experience with Scala, Java, Python, and Go-Lang.
Experience with Apache Hadoop/Spark ecosystem.
Demonstrated working knowledge of data modeling.
Stellar interpersonal and communication skills.
Required Experience:
Git, Unix/Linux, Unit, Integration, Load Testing, developing REST APIs, Ant, Maven, SBT, Gradle, and Docker containers building and deployment.
Experience, preferred:
Jenkins, GraphQL, Amazon AWS (or other cloud services), Kubernetes, Apache Spark (MLib and Graph X).
Salary for the Data Scientist:
Starts at $90/hour.
Powered by JazzHR
XgGlT0mf3l
","['linux', 'spark', 'elasticsearch', 'cassandra', 'scala', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'java', 'kubernet', 'kafka', 'docker']","['graph', 'pipelin', 'data modeling', 'machine learning', 'optim', 'commun']",1,"['linux', 'spark', 'elasticsearch', 'cassandra', 'scala', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'java', 'kubernet', 'kafka', 'docker', 'graph', 'pipelin', 'data modeling', 'machine learning', 'optim', 'commun']","['machin', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'aw', 'python', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin', 'algorithm']","['linux', 'spark', 'elasticsearch', 'cassandra', 'scala', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'java', 'kubernet', 'kafka', 'docker', 'graph', 'pipelin', 'data modeling', 'machine learning', 'optim', 'commun', 'machin', 'spark', 'pipelin', 'power', 'hadoop', 'optim', 'aw', 'python', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin', 'algorithm']"
DE,"• Solid knowledge of Data Warehousing fundamentals and Data Modeling
• Must have Scripting/coding experience with SQL, Python or Java
• Must have experience in securing PII data such as data masking
• Must have experience working with Snowpipe and Snowflake API
• Experience writing, troubleshooting, and debugging complex SQL queries on distributed systems/MPP (massively parallel processing) architecture
• Strong knowledge of RDBMS, SQL data types, functions, stored procedures, security & encryption
• Database migration and ETL/ELT experience
• Experience working with data formats like CSV, XML, AVRO, JSON, PARQUET etc
• Good understanding of Amazon AWS ecosystem
• Experience working on a Scrum team or other Agile delivery methodology
• Working knowledge of GitHub or GitLab
Nice to Have:
• ETL Pipeline development experience using any one of these tools such as Informatica, Matillion, Glue or similar tools
• Familiarity with Spark, PySpark etc
• Experience with workload automation tools such as Airflow, Autosys.
• Knowledge of Big Data (HDFS, Hive), AWS (S3, IAM), and various data formats
• Knowledge of DevOps tools such as Jenkins
","['sql', 'pyspark', 'spark', 'airflow', 'aw', 'snowflak', 'python', 's3', 'hive', 'java', 'github']","['pipelin', 'data modeling', 'data warehousing', 'big data', 'etl']",2,"['sql', 'pyspark', 'spark', 'airflow', 'aw', 'snowflak', 'python', 's3', 'hive', 'java', 'github', 'pipelin', 'data modeling', 'data warehousing', 'big data', 'etl']","['spark', 'pipelin', 'aw', 'python', 'big', 'etl']","['sql', 'pyspark', 'spark', 'airflow', 'aw', 'snowflak', 'python', 's3', 'hive', 'java', 'github', 'pipelin', 'data modeling', 'data warehousing', 'big data', 'etl', 'spark', 'pipelin', 'aw', 'python', 'big', 'etl']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"AWS Data Engineer for my client, Petco, in San Diego CA.
-Ability to integrate on premise infrastructure with public cloud (AWS) infrastructure Requirements -3-5 years’ experience with data engineering using AWS platform and Python -3-5 years’ experience AWS cloud and AWS services such as S3 Buckets, Lambda, API Gateway, SQS queues -3-5 years’ experience using Python for data processing using CSV, JSON, Delimited, XML, AVRO, Parquet and other file formats -3-5 years’ experience with major data warehousing platform such as Oracle, Teradata, Netezza, AWS Redshift, Snowflake etc.
","['aw', 'lambda', 'redshift', 'python', 's3', 'snowflak', 'cloud', 'oracl']",['data warehousing'],999,"['aw', 'lambda', 'redshift', 'python', 's3', 'snowflak', 'cloud', 'oracl', 'data warehousing']","['public', 'infrastructur', 'aw', 'python', 'engin']","['aw', 'lambda', 'redshift', 'python', 's3', 'snowflak', 'cloud', 'oracl', 'data warehousing', 'public', 'infrastructur', 'aw', 'python', 'engin']"
DE,"188171/ 212628
Sr Data Engineer 2 Positions (AWS RedShift, Python,SQL,Spark)
Playa Vista CA
Pay rate 85/hr or 140 K
Experience on AWS and its web service offering S3, Redshift, EC2, EMR, Lambda, CloudWatch, RDS, Step functions, Spark streaming etc.
Good knowledge of Configuring and working on Multi node clusters and distributed data processing framework Spark.
Hands on 3 years of experience with EMR Apache Spark Hadoop technologies
Experience with must have Linux, Python and PySpark, Spark SQL.
Experience in working with large volumes of data Tera-bytes, analyze the data structures
Experience in designing scalable data pipelines, complex event processing, analytics components using big data technology Spark Python Scala PySpark,
Expert in SQLPLSQL, redshift, NoSQL database
Experience in process orchestration tools Apache Airflow, Apache NiFi etc.
Hands on knowledge of design, development and enhancement of Data Lakes, constantly evolve with emerging tools and technologies
","['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql']","['pipelin', 'cluster', 'big data']",999,"['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql', 'pipelin', 'cluster', 'big data']","['analyt', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']","['sql', 'linux', 'pyspark', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'node', 'aw', 'lambda', 'redshift', 's3', 'python', 'nosql', 'pipelin', 'cluster', 'big data', 'analyt', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']"
DE,"This will be working on the main platform to do the analysis of healthcare data to look at patient's overall health and price doctors according to how healthy they are keeping their patients.Required Skills & Experience* 5+ years of professional development experience* Bachelors degree in Computer science or related field* Strong written and verbal commutation skills* Background in Python or Scala* Experience with or exposure to Big data tools like Pyspark, Hadoop, CassandraDesired Skills & Experience* ETL experience* AWS/GCP ExperienceWhat You Will Be DoingDaily Responsibilities* 80% Hands On* 20% Team CollaborationThe Offer* Up to $160k/year, DOEYou will receive the following benefits:* Medical Insurance & Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Pre-tax Commuter Benefit* Remote Flexibility* Competitive PTOApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['pyspark', 'gcp', 'scala', 'hadoop', 'aw', 'python']","['etl', 'big data', 'healthcar', 'account']",1,"['pyspark', 'gcp', 'scala', 'hadoop', 'aw', 'python', 'etl', 'big data', 'healthcar', 'account']","['basi', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl']","['pyspark', 'gcp', 'scala', 'hadoop', 'aw', 'python', 'etl', 'big data', 'healthcar', 'account', 'basi', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl']"
DE,"Req ID: 177939
The Job
Warner Bros. Entertainment Inc. seeks a Sr Full Stack Data Engineer for the Digital Product, Platform & Strategy: Content Mgmt & Distribution department.
The Software Engineer will be a member of the Media Supply Chain (MSC) team within the WarnerMedia Technology (WMTO) organization, the position will be responsible for hands on development as well as managing application launches and software maintenance from inception to launch.
The Software Engineer must understand business functions and requirements and translate those to working applications that allow Media Supply Chain technology to continue innovating in support of business need.
The Media Supply Chain Mission
The Daily
Develop and provide support for core functionality and components for applications and service in support of various content platforms by starting with business needs and objectives, creating functional and technical specifications and executing against a plan to launch and maintain applications.
Review project objectives and determine best technology for implementation.
Implement best practice standards for development, build and deployment automation while mentoring and leading a team of engineers.
Review emerging technologies and evaluate potential uses for WarnerMedia (WM) Technology and other divisions.
Develop prototype projects using new technologies.
Evaluate software products and vendors for WM Technology and other divisions.
Recommend action, develop and lead implementation of selected products/services.
Work with internal and external developers to ensure WB Technology code standards and best practices are performed for development of applications.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
6+ years software development experience.
4+ years experience with Java, including significant demonstrable experience with Java 8.
2+ years experience in Node.js, Python or equivalent scripting lanague.
Bash experience preferred.
Demonstrated expertise and experience in ELK stack (elasticsearch, logstash, kibana)
Demonstrated experience implementing database technologies such as NoSQL, and Relational.
Experience in graph databases a plus.
Experience with a Javascript front end framework such as React.js or Polymer.
Polymer experience preferred.
Experience with a Java front-end framework such as Hibernate, Faces or Grails is a plus.
3+ years experience in working with noSQL databases such as Mongo, Couchbase, Dynamo, Cassandra or equivalent.
Experience in graph databases such as Neptune or Neo4j preferred but not required.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Experience with source code and knowledge repositories such as git, jira, or equivalent systems.
Experience developing in AWS at scale leveraging core services such as Lambda, RDS, and ec2.
Experience with Build and Deployment tools such as Jenkins, Chef, Puppet, or equivalent.
Demonstrated expertise and experience in deploying containerized applications using Docker, Kubernetes or equivalent.
Demonstrated expertise and experience in automating development pipelines using Cloudformation or Teraform.
Proficient in a Linux environment.
Solid understanding of core DevOps and SDLC in an agile environment.
Overall knowledge of the entertainment industry business preferred.
Overall knowledge of WarnerMedia.
businesses, business processes, technologies and applications that support it is preferred.
Ability to work with outside vendors and clients under sometimes adverse circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when necessary.
Must be able to respond to after-hours pager notifications to provide support for applications as necessary.
177939
","['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker']","['commun', 'graph', 'pipelin']",1,"['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker', 'commun', 'graph', 'pipelin']","['digit', 'essenti', 'pipelin', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']","['linux', 'cassandra', 'elasticsearch', 'ec2', 'jira', 'git', 'javascript', 'lambda', 'python', 'java', 'aw', 'nosql', 'kubernet', 'docker', 'commun', 'graph', 'pipelin', 'digit', 'essenti', 'pipelin', 'aw', 'python', 'comput', 'action', 'relat', 'sourc', 'engin', 'evalu']"
DE,"E-Discovery Data Engineer (ADS)
Engineer and implement solutions and workflows for clients and internal teams
Perform both standardized and ad-hoc data analysis, reporting, and modifications via SQL
Create, implement, and support custom Event Handlers
Creatively utilize and combine existing scripts, recipes, and applications (both proprietary and third-party) to automate tasks and workflows
Contribute to solution and script ownership, including providing training and guidance on use and supporting enhancements and troubleshooting
",['sql'],[None],999,['sql'],"['parti', 'engin']","['sql', 'parti', 'engin']"
DE,"Looking to work with cutting-edge technologies?
You'll be working alongside the DevOps team to transform their customer experience platform.On the data team, you'll be creating reporting and data visualization capabilities on top of their data warehouse.
The ideal candidate will be proficient with Python, Spark, AWS, and Docker.
It's a polyglot team, so you'll be exposed to different technologies however, you'll be focusing on the back-end.Required Skills & Experience* 5+ years with Python* 3+ years with Spark* Cloud experience with AWS (EC2, S3, Lambda)What You Will Be DoingTech Breakdown* 50% Python* 40% Spark* 10% AWSDaily Responsibilities* 60% Hands On* 40% Team CollaborationThe Offer* Competitive Salary: Up to $160K/year, DOEYou will receive the following benefits:* Medical Insurance & Health Savings Account (HSA)* Unlimited PTO* 401(k) with matching options* Paid maternity/paternity leave* Vacation days* Bring your dog to work* Free office snacks* Summer FridaysApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['spark', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'docker']","['visual', 'account']",999,"['spark', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'docker', 'visual', 'account']","['day', 'basi', 'spark', 'visual', 'aw', 'python']","['spark', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'docker', 'visual', 'account', 'day', 'basi', 'spark', 'visual', 'aw', 'python']"
DE,"Job Information
Industry IT Services Work Experience 5+ years Salary $155,000- 180,000 City Anaheim State/Province California Zip/Postal Code 90005
The Data Engineer will be responsible for the following:
Assess, transform, organize, and optimize data for use by machine learning algorithms
Generating representative data sets for systems development and data science initiatives
Build data pipelines that enables data scientists and engineers and other stakeholders
Requirements
8+ years of experience designing data models and data warehouses supporting analytics, using both relational and non-relational distributed data storage systems
Demonstrated experience building and maintaining ETL pipelines.
Bonus points for experience with Apache Beam
Demonstrated experience with large-scale distributed processing
Experience building data pipelines in ML frameworks.
Kubeflow experience is desired
Experience working in a fast-paced agile environment
Experience in a high-level programming language, such as Java, Python... etc.
Demonstrated proficiency with Git version control systems
Experience working in Linux environments
Benefits
- 6% 401k match
Health insurance
Dental insurance.
Pet insurance.
Life insurance.
","['linux', 'git', 'java', 'python', 'kubeflow']","['machine learning', 'etl', 'pipelin']",999,"['linux', 'git', 'java', 'python', 'kubeflow', 'machine learning', 'etl', 'pipelin']","['analyt', 'machin', 'pipelin', 'ml', 'set', 'relat', 'python', 'scientist', 'warehous', 'etl', 'engin', 'algorithm']","['linux', 'git', 'java', 'python', 'kubeflow', 'machine learning', 'etl', 'pipelin', 'analyt', 'machin', 'pipelin', 'ml', 'set', 'relat', 'python', 'scientist', 'warehous', 'etl', 'engin', 'algorithm']"
DE,"Job Title:
Sr. Data Engineer
Requisition ID:
R001902
Job Title: Sr. Data Engineer
Reporting To: Director, Data Governance
Department: Central Tech
Your Platform
Central Tech is distributed globally with offices across the U.S., and in Canada, England, Ireland and Japan.
Your Mission
Do you have a passion for solving complex privacy engineering and data governance problems?
Are you an outspoken advocate for the implementation of best in class data privacy and governance standards and practices?
The ideal candidate will have extensive hands-on experience in designing and developing scalable and robust engineering solutions.
Previous gaming industry experience is a plus.
You should be willing and able to wear multiple hats.
Responsibilities:
Develop scalable, efficient, forward-looking privacy solutions that support requirements of GDPR, CA Privacy Act, and other global privacy regulations.
Work closely with other engineering teams and business stakeholders to design automated solutions for privacy and compliance.
Implement best in class data protection standards and solutions for data pseudonymization, data anonymization, access control, logging, and auditing.
Implement data governance tools such as data catalog and processes for effectively managing metadata, data classification, and lineage.
Develop solutions for data profiling, data quality monitoring, reporting, and testing.
Support creation and implementation of data governance policies, standards, and processes.
Player Profile
Proven verbal and written communication skills, including the ability to communicate complex technical ideas to the business stakeholders.
Experience working with big data technologies (Hadoop, Hive, Spark, Presto, or similar.)
4+ years of production engineering experience.
Experience in working with cloud infrastructures.
Familiarity with GDPR, CCPA, and other privacy regulations.
Production experience in Python and/or Java
Experience writing / debugging complex SQL statements
Experience in implementation of Data Catalog tools such as Atlas, Collibra, or Alation
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
","['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python']","['classif', 'commun', 'big data']",999,"['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python', 'classif', 'commun', 'big data']","['basi', 'spark', 'hadoop', 'infrastructur', 'python', 'big', 'engin']","['sql', 'spark', 'hadoop', 'cloud', 'java', 'hive', 'python', 'classif', 'commun', 'big data', 'basi', 'spark', 'hadoop', 'infrastructur', 'python', 'big', 'engin']"
DE,"CIC Montreal is an IBM Client Innovation Centre operated by LGS, a wholly owned subsidiary of IBM.
The Centre provides services in application development and support to public and private Canadian organizations.
It is distinguished by a capacity to simultaneously offer large-scale projects, highly stimulating professional challenges, an environment favorable to continuous learning, professional mentoring and, lastly, opportunities for career advancement.
What you will do
The Data Engineer role is part of the Cognitive Process Transformation (CPT) team which focuses on those activities that support the integration, design and modeling, storage, and organization of data.
This individual must have a solid understanding of SAP data services and process automation techniques.
This individual must have strong experience in building large complex data sets, be able to articulate best practices related to data quality and cleansing and be able to incorporate those practices into delivered solutions.
What you need to have
Ability to work with the client directly to understand and meet their requirements;
Excellent verbal and written communication abilities: must effectively communicate with technical and non-technical teams;
Participate in teams working in an Agile/Scrum or Waterfall process and ensure the stories/tasks are well defined and have all the information and tools to be successful
Ability to work independently on tasks and deliver with a high-level of quality;
Ability to work in teams and be open to comments and feedback;
Ability to learn quickly and to adapt to a fast-paced environment;
Data processing, data design and modeling, deploying the model;
Technical competencies
3+ years experience in SAP data services
3+ years Python experience (preferable using PySpark);
3+ year of in-depth database knowledge of SQL and NoSQL including the ability to write, tune, and interpret SQL queries;
Experience with Cloud services (Azure, AWS, IBM Cloud);
3+ year experience in data warehousing (Hadoop, MapReduce, HIVE, PIG, Apache Spark, Kafka)
Experience building large, complex big data sets and delivery mechanisms to support advanced analytics and insights analysis
Knowledge of tools to perform data quality, data cleansing, data wrangling and data standards;
Experience with other languages such as C #, Javascript, Matlab an asset
Experience working with container-orchestration system( Kubernetes) or any other containerized applications systems also an asset
Solid knowledge on different operating sytems such as Unix, Microsoft windows etc (an asset)
Basic Machine Learning Familiarity (an asset)
Previous experience industrializing machine learning projects will be highly considered.
1+years experience translating business needs to data requirements and designing data-driven solutions supporting analytics and insights.
Education
Must have a technical academic degree in the Comp engineering, Comp Science, Systems Engineering or Mathematics & Statistics field.
Job Type: Full-time
Work Remotely:
No
","['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab']","['cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun']",1,"['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab', 'cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun']","['techniqu', 'python', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'statist', 'asset', 'learn', 'public', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'relat']","['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab', 'cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun', 'techniqu', 'python', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'statist', 'asset', 'learn', 'public', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'relat']"
DE,"The Job
Warner Bros. Entertainment Inc. seeks a Sr. Data Engineer for the Digital Product, Platform & Strategy: Content Mgmt & Distribution department.
The Sr. Data Engineer must have experience with data modeling, modern database technologies, and API driven search design and development.
The Media Supply Chain – Mission
The Daily
Develop and provide support for core data relationships, data ingest, data transformation services and search capabilities.
Creates functional and technical specifications.
Creates and executes against a plan to launch and maintain applications.
Implement best practice standards for development, build and deployment automation while mentoring and leading a team of engineers.
Develop prototype projects using new technologies.
Recommend action, develop and lead implementation of selected products/services.
The Essentials
B.S.
in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
6+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL, and Relational.
Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch, logstash, kibana).
Experience in sizing, modelling and deploying multi-node elasticsearch clusters.
Expertise / experience in sizing, modelling and deploying multi-node noSQL DB mongo clusters – replica sets or sharded clusters.
Demonstrated expertise and experience in modern databases such as Mongo, Couchbase, Neptune, Neo4j, or equivalent.
Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch query DSL, cypher, gremlin, or graphql.
xQuery preferred but not required.
Highly proficient in XML, JSON and YAML data exchange formats.
Experience in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana etc.
Experience in working with data streaming technologies such as Amazon Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch, RDS, lambda, Neptune and ec2.
Highly proficient in at least one modern programming language such python, java, or node.js.
Bash experience preferred.
Demonstrated expertise and experience in deploying containerized application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira, or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Advanced Systems design and architecture.
Experience mentoring developers preferred.
Ability to work with outside vendors and clients under sometimes adverse circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, when necessary.
Must be able to respond to after-hours pager notifications to provide support for applications as necessary.
Requisition #
177936BR
Area of Interest
Industry
Film Production and Distribution
United States - California - Burbank
Position Type
Full Time
Business Unit
Business Unit Overview
WBT manages the Studio’s enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.
Warner Media, LLC and its subsidiaries are equal opportunity employers.
Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker']","['commun', 'graph', 'cluster', 'data modeling']",1,"['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'commun', 'graph', 'cluster', 'data modeling']","['analyt', 'digit', 'essenti', 'relat', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin']","['linux', 'elasticsearch', 'node', 'jira', 'git', 'lambda', 'python', 'java', 'aw', 'nosql', 'tableau', 'db', 'kubernet', 'kafka', 'docker', 'commun', 'graph', 'cluster', 'data modeling', 'analyt', 'digit', 'essenti', 'relat', 'aw', 'python', 'comput', 'action', 'set', 'sourc', 'engin']"
DE,"Senior Data Engineer
If you are a Senior Data Engineer with experience, please read on!
What You Will Be Doing
You'll be able to automate existing processes, optimizing data delivery, and even re-designing infrastructure to better overcome the challenges of scalability.
Your day to day tech-stack will include SQL, AWS S3, Redshift and Big Data tools like Spark.
You'll have the opportunity to collaborate and continually learn, while improving reporting and analysis processes for the growing business need, and evolving technical challenges.
What You Need for this Position
1) 5+ years of Data Engineering experience
2) 3+ years using Python
3) Data Warehousing experience
4) Understanding of relational databases; including mySQL, Postress, and DynamoDB
5) Knowledge of ETL architecture
Nice to have, but Not required:
1) Spark streaming experience
2) Prior exposure to Docker containerization
3) Leadership or Mentoring experience
4) A BS on MS in Mathematics, computer science, or related field
What's In It for You
- Competitive base salary - up to $150K (DOE)
- 10% annual bonus
- Excellent benefits package
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'mysql', 'excel', 'docker']","['data warehousing', 'etl', 'big data']",1,"['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'excel', 'docker', 'data warehousing', 'etl', 'big data']","['day', 'spark', 'challeng', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'big', 'etl', 'engin']","['sql', 'spark', 'aw', 'python', 's3', 'redshift', 'excel', 'docker', 'data warehousing', 'etl', 'big data', 'day', 'spark', 'challeng', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'packag', 'big', 'etl', 'engin']"
DE,"Req ID: 34961
Experience Level: Professional
But its more than that.
Youll be part of a team that genuinely cares about helping you succeed.
In return for your contributions, youll receive premier compensation and benefits, and a company-funded retirement plan that ranks among the most generous.
If you are looking to join a team that offers autonomy, plenty of space to run with new ideas, working in a nurturing environment, and challenging problems to solve at a significant scale, this might be the job for you!
What youll be doing:
Build large-scale distributed data processing systems, data lakes, and optimize for both computational and storage efficiency on cloud platforms like AWS.
Design, implement and automate data pipelines sourcing data from internal and external systems, transforming the data for the optimal needs of various systems.
Design data schema and operate cloud-based data warehouses and SQL/NoSQL/temporal database systems.
Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs.
Own the design, development and maintenance of ongoing metrics, reports, analyses, dashboards, etc.
to drive key business decisions.
Monitor and troubleshoot operational or data issues in the data pipelines.
Drive architectural plans and implementation for future data storage, ETL, reporting, and analytic solutions.
Influence your teams technical and business strategy by making insightful contributions to team priorities and approach.
Provide insightful code reviews, receive code reviews constructively and take ownership of outcomes (you ship it, you own it), working very efficiently and routinely deliver the right things in the front-end UI area.
Building relationships with your customers, partner teams and the engineers on your team.
Influence your teams technical decisions by making insightful contributions to team priorities and approach.
Your background and who you are:
You have a background in data and software engineering and a passion to learn.
You've made mistakes in the past and have learned a lot from them.
You believe there are generally multiple ways to solve a technical problem, each with different trade-offs.
You approach projects, tasks, and unknowns with curiosity, and enjoy sharing what you know and what you learn with the people around you.
You believe that a team is strongest when it is diverse and includes multiple perspectives.
You are able to put yourself into your customer's shoes.
You frequently immerse yourself in the customer experience to understand how you can better serve them.
Qualifications:
BS in Computer Science or related field, or an equivalent in relevant work experience.
3+ years experience implementing big data processing technology: Hadoop, Apache Spark, etc.
3+ years coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.).
Experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets.
Experience in cloud-first design, preferably AWS (VPC, Serverless databases and functions, dynamic autoscaling, container orchestration, etc.).
Experience in data architecture, databases (e.g., MySQL, Oracle, PostgreSQL), SQL and DDD/ER/ORM design.
Interest and curiosity in emerging technologies on the web like GraphQL, web assembly, Lambda functions, MLaaS etc
Knowledge of software engineering practices & best practices for the software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.
#GD-Tech
","['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'mysql', 'oracl', 'rubi']","['pipelin', 'dashboard', 'optim', 'big data', 'etl']",1,"['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'oracl', 'rubi', 'pipelin', 'dashboard', 'optim', 'big data', 'etl']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'provid', 'optim', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'hadoop', 'aw', 'lambda', 'python', 'java', 'cloud', 'nosql', 'postgresql', 'oracl', 'rubi', 'pipelin', 'dashboard', 'optim', 'big data', 'etl', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'provid', 'optim', 'python', 'aw', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"Job Title: Senior Data Engineer & Analyst
Duration: Long Term Contract
Â
Skills:
8+ years of overall IT experience
Experienced in SQL, SSRS and T-SQL
Have good experience on creating and maintaining a Data Warehouse
Good experience in ETL Design, monitoring and development
Good with PowerBI; demonstratable advance skills in creating Complex reports in PowerBI
Should be able to troubleshoot and fix issues with any report or ETL job in production environments
","['sql', 'powerbi', 'ssr']",['etl'],999,"['sql', 'powerbi', 'ssr', 'etl']","['warehous', 'etl', 'engin', 'â']","['sql', 'powerbi', 'ssr', 'etl', 'warehous', 'etl', 'engin', 'â']"
DE,"Role: Sr. Talend Data Engineer
Skills: Data Warehousing Concepts, MySQL, ETL, AWS, Talend, Redshift, Big Data, Quality Assurance, AWS CLI, Data warehousing, QA
Responsibilities:
Requirement Gathering and design of data warehouse and ETL process.
Architectural design for Migration of legacy data from Mysql to Amazon Redshift
Developing Talend ETL Batch jobs to extract data from Parquet file and then load data into Redshift data base.
Designed job orchestration, audit and logging framework of ETL using Talend
Work on Talend Administrative Center for Job scheduling and Talend User Management
Code deployment process and code version control using GitHub
Perform AWS DevOps activities related to deployment of ETL jobs on Dev, QA and Prod environment
Coordinate with offshore teams for development tasks
Required skills:
Analysis, Design, Develop, and Support of ETL applications which includes strong experience on Data Warehouse Good understanding and best practice of Data Warehousing Concepts, involved in Full Development life cycle of Data Warehousing Analyze, design and develop ETL strategies and processes, writing ETL specifications
Experience in Talend Open Studio (6.x7.x) for Data Integration, Data Quality and Big Data.
Expertise on Talend Data Integration suite and Bigdata Integration Suite for Design and development of ETL Bigdata code and Mappings for Enterprise DWH ETL Talend Projects.
Widespread experience in using Talend features such as context variables, triggers, connectors for Database and flat files.
Hands on Involvement to design Jobs used Context Variables to Parameterize Talend Jobs Tracking Daily Data load, Monthly Data extracts and send to client for their verification.
For any queries, contact Praveen at 760-307-4276 or praveen@dantatechnologies.net
","['mysql', 'github', 'redshift', 'aw']","['end user', 'analyz', 'data warehousing', 'big data', 'etl']",999,"['sql', 'github', 'redshift', 'aw', 'end user', 'analyz', 'data warehousing', 'big data', 'etl']","['relat', 'aw', 'warehous', 'big', 'etl', 'engin', 'integr']","['sql', 'github', 'redshift', 'aw', 'end user', 'analyz', 'data warehousing', 'big data', 'etl', 'relat', 'aw', 'warehous', 'big', 'etl', 'engin', 'integr']"
DE,"Cloud Data Engineer
Contract to Hire or Full time open
Long Beach, CA
JOB PURPOSE:
Perform a key role in the areas of advanced data techniques, including data modeling, data access, data integration, data visualization, data discovery, statistical methods and database design.
Assist IT leads to develop IT strategies and to oversee data architectural designs for implementation of cloud.
Focus on enabling SCAN end to end journey from On-Premise to Cloud in hybrid architecture.
ESSENTIAL JOB RESULTS:
Cloud data architecture, design, research, testing and evaluation of new technologies that improve and enhance enterprise systems reliability, availability and serviceability.
Collaborate with internal and external partners to ensure that cloud data architecture designs and blueprints align with the overall enterprise architecture framework.
Maintain knowledge of current and emerging technologies/products/trends related to cloud data architecture.
Ensure alignment of data architecture designs with business and IT goals/objectives.
Facilitate and participate in identification of cloud data solutions, options and features.
Communicate solution options to business owners/stakeholders (e.g.
data architecture reviews).
Identify gaps between established standards and proposed solutions.
Maintain awareness of current state of enterprise data architecture and define future state of data architectures.
Share best practices knowledge related to cloud architecture, database design and data modeling.
Mentor team members to assist with technical skills related to data architecture, data modeling, database design, data security and data integration.
Create and maintain data architecture design to ensure high levels of speed, effectiveness and availability of data across platforms.
Deep understanding of Operational Data Storage, Enterprise Data Warehouse, Data Marts and Data Lake.
Contribute to team efforts by accomplishing related results as needed.
Excellent interpersonal and organizational skills, ability to handle diverse situations, multiple initiatives and rapidly changing priorities.
","['cloud', 'excel']","['research', 'visual', 'data modeling', 'statist', 'commun']",999,"['cloud', 'excel', 'research', 'visual', 'data modeling', 'statist', 'commun']","['essenti', 'techniqu', 'relat', 'visual', 'divers', 'avail', 'statist', 'warehous', 'integr', 'engin', 'evalu']","['cloud', 'excel', 'research', 'visual', 'data modeling', 'statist', 'commun', 'essenti', 'techniqu', 'relat', 'visual', 'divers', 'avail', 'statist', 'warehous', 'integr', 'engin', 'evalu']"
DE,"(#LI-MA1)
You are the right fit for this role if.....
A Strong Vision for How to Build Efficient, Enduring, and Scalable Reporting Solutions
You are known as a technical thought leader in data solution design, advising on best practice from metrics collection to successful reporting.
You rely on a deep understanding of diverse data structures as well as business operations to deliver thoughtfully designed reporting solutions that are transparent, easy to use, and drive data-based decision making.
You should have a healthy respect for the details and a sincere passion for fixing problems at their root.
Data Driven Culture Ambassador
You make data-based decisions and challenge others to do so.
You seize every interaction as an opportunity to inject data education and best practices.
Relentlessly Curious
You are unafraid to ask as many questions as it takes to have a complete understanding of the challenge at hand.
Is this data accurate?
Are these requirements complete?
Are the right stakeholders connected?
Question everything!
Role Responsibilities
Role-models excellent customer support when assessing requests and connecting internal customers to relevant data
Independently designs and builds complex Tableau dashboards spanning a variety of departmental data sets
Performs data quality-control and debugs Tableau workbooks
Builds and modifies straightforward Tableau data-sources
Key contributor to Business Analytics Tableau end-user, desktop, and data-best practice education efforts
Preferred Qualifications
Displays extreme passion for data and problem solving - is driven to ask the right questions and surface up business insights
Intermediate knowledge of a CRM tool, preferably Salesforce.com
Experience with Snowflake
Ability to analyze and document complex business processes
Excellent communicator, both written and verbal
Excels at cross-team collaboration and knowledge sharing
Experience in a customer (internal or external) facing role
Minimum Qualifications
Bachelor's Degree in Business, math/statistics/data science, or similar discipline and at least 3 years equivalent work experience
Works efficiently under pressure and meets deadlines while balancing multiple projects and multiple stakeholders
Hands on experience with SQL
Intermediate experience using a data visualization tool such as Tableau, Looker, PowerBI, other
So, please come as you are.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.
","['sql', 'looker', 'snowflak', 'tableau', 'excel', 'powerbi']","['dashboard', 'visual', 'problem solving', 'statist', 'commun', 'math']",1,"['sql', 'looker', 'snowflak', 'tableau', 'excel', 'powerbi', 'dashboard', 'visual', 'problem solving', 'statist', 'commun', 'math']","['basi', 'analyt', 'challeng', 'visual', 'divers', 'statist', 'set', 'sourc', 'collect']","['sql', 'looker', 'snowflak', 'tableau', 'excel', 'powerbi', 'dashboard', 'visual', 'problem solving', 'statist', 'commun', 'math', 'basi', 'analyt', 'challeng', 'visual', 'divers', 'statist', 'set', 'sourc', 'collect']"
DE,"**
Job Overview
Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, reference data or master data management, data architecture, data modeling, data governance, data analysis, business intelligence.
Skills & Requirements:
MUST BE HANDS ON.
Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge:
Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)
Must have significant hands-on experience with SQL, data profiling, and data discovery
Experience building business intelligence, analytics, or reporting solutions - either front-end consumption mechanisms (e.g., Microsoft, Tableau & Qlik) or supply of data for these purposes
Familiarity with data architecture principles/approaches, data environment infrastructure considerations, and data modeling principles/approaches
Ability to drive out technical requirements with business and IT stakeholders for implementations of data solutions
Hands-on experience with Agile delivery methodology
Prior professional experience in an IT management, management consulting, or client facing role is preferred
Knowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred
Demonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions
Tools and Technology:
Proficient at leveraging tools and technology to drive value for clients.
Examples include the following;
Database Management Tools:
Relational – e.g.
Oracle, MySQL, Microsoft SQL Server, PostgreSQL, DB, or similar
NoSQL – e.g.
MongoDB, Couchbase, DataStax, Redix, MarkLogic, or similar
Cloud – e.g.
AWS, Azure, xxx, xxx, xxx
ETL Tools - e.g.
Informatica, Talend, Microsoft SSIS, or similar
Data Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar
Industry Leading BI tools - e.g., Business Objects, Microsoft, Cognos, Tableau, OBIEE, Qlickview, or similar
General:
3+ years of professional experience working in a related role
Must be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude
Strong desire to work on interesting projects with smart and creative people
Willingness to travel up to 80% of the week (M-Th)
Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.
","['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'bi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'mysql', 'oracl']","['healthcar', 'data modeling', 'data warehousing', 'etl', 'commun']",2,"['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'powerbi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'etl', 'commun']","['analyt', 'azur', 'relat', 'infrastructur', 'bi', 'aw', 'etl', 'engin']","['mongodb', 'sql', 'qlik', 'azur', 'cogno', 'postgresql', 'powerbi', 'tableau', 'aw', 'db', 'cloud', 'nosql', 'microsoft', 'oracl', 'healthcar', 'data modeling', 'data warehousing', 'etl', 'commun', 'analyt', 'azur', 'relat', 'infrastructur', 'bi', 'aw', 'etl', 'engin']"
DE,"The Job Details are as follows:
OVERVIEW
In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player
","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker']","['data warehousing', 'commun']",1,"['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun']","['day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun', 'day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']"
DE,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois
Data Engineer
You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.
You will:
- Work with product owners to understand desired application capabilities and testing scenarios
- Continuously improve software engineering practices
- Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies
- Lead the craftsmanship, availability, resilience, and scalability of your solutions
- Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
- Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity
Basic Qualifications:
- Bachelors Degree
- At least 3 years of SDLC experience using Java technologies
- At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper
Preferred Qualifications:
- Master's Degree
- 2+ year experience with Spark
- 3+ years experience developing software solutions to solve complex business problems
","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi']","['commun', 'big data']",1,"['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi', 'commun', 'big data']","['spark', 'divers', 'hadoop', 'infrastructur', 'aw', 'avail', 'python', 'big', 'relat', 'engin']","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi', 'commun', 'big data', 'spark', 'divers', 'hadoop', 'infrastructur', 'aw', 'avail', 'python', 'big', 'relat', 'engin']"
DE,"What You Will Do:
Partner with other architects and technical leads to collaborate, design and validate appropriate solutions and institute appropriate data governance mechanisms.
Partner with Digital Farming Platform leads to establish and steward a multi-year Data Pipelines technical capabilities roadmap.
Develop cloud-based pipeline architectures that enable massively scalable data processing, while maintaining less than linear operating cost profiles.
Evangelize and mentor others on topics such as data governance, data quality, and data management best practices and techniques.
Basic Qualifications:
BS, MS or equivalent in Computer Science or related technical field
10+ years of software engineering with OOP languages
7+ years of architectural experience developing scalable data pipelines and data processing frameworks in a cloud-native or distributed computing environment.
3+ years serving as a lead data or pipeline architect in a large-scale, cloud-based, high-volume data processing organization or business division.
Demonstrated experience writing architectural requirements and systems design documents.
Experience designing and governing scalable cloud-native backend compute capabilities (REST APIs, microservices, distributed computing frameworks, geospatial processing and indexing, messaging frameworks and paradigms, data quality management).
Expertise designing and deploying solutions on at least one cloud-based provider such as Amazon Web Services, Google Cloud Platform or Microsoft Azure.
Preferred Qualifications:
Expertise in processing and aggregating high-volume, geospatially-oriented IoT data.
Expertise in imagery processing and feature extraction from satellite and aerial sources.
Experience with layered geospatial data structures and data representations.
Expertise designing and implementing highly scalable data-intensive distributed computing solutions using modern cloud-native processing frameworks.
Experience with Amazon Web Services (EC2, S3, RDS, SQS, etc.)
is strongly preferred
Experience with a compiled JVM language, including Scala, is a plus
Working knowledge of open-source and commercial data pipeline tooling (Apache NiFi, Cloudera, Informatica, etc) strongly preferred.
Superb medical, dental, vision, life, disability benefits, and a 401k matching program
Inspire one another
Be direct and transparent
https://youtu.be/c5TgbpE9UBI or visit https://climate.com/careers
#LI-BW1
","['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'amazon web services']","['pipelin', 'geospati']",1,"['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'aw', 'pipelin', 'geospati']","['digit', 'program', 'techniqu', 'engin', 'pipelin', 'azur', 'provid', 'comput', 'relat', 'sourc', 'linear']","['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'aw', 'pipelin', 'geospati', 'digit', 'program', 'techniqu', 'engin', 'pipelin', 'azur', 'provid', 'comput', 'relat', 'sourc', 'linear']"
DE,"Are you ready to transform an industry?
ABOUT THE TEAM
• Ruby, Java, Python, and React.js
• Kubernetes, Docker, Kafka
• PostgreSQL, NoSQL
• AWS
ABOUT THE ROLE
YOU WILL
Participate in architecture discussions and bring your experience in scalable data pipelines using Kafka Streams and/or other Big Data tools.
Take ownership of design and implementation of scalable and fault tolerant projects.
Maintain and incrementally improve existing solutions.
Get to build brand new pipelines with the technology stack including Spark, Spark Structured Streaming, Kafka, Hadoop, MySql, Python.
Solid understanding of distributed system fundamentals.
Experience in developing, troubleshooting, diagnosing, and performance tuning of distributed data pipelines at scale.
Demonstrated professional experience working with various components of Big Data ecosystem: Spark/Spark Streaming, Hive, Kafka/KSQL, Hadoop (or similar NoSQL ecosystem), et.
al, in a production system.
Strong software engineering skills with Python.
Knowledge of some flavor of SQL (MySQL, Oracle, Hive, Impala), including the fundamentals of data modeling and performance.
EVEN BETTER IF YOU HAVE
Skills in real-time streaming applications.
Knowledge of Scala.
A development workflow using Docker containers.
Compulsion for automating your day-to-day processes.
Ability to see your direct impact on a high visibility project.
An opportunity to both create new projects and help improve the existing big data pipelines.
401K plan with employer matching.
Commuter pre-tax contributions.
Flexible working hours and work-from-home days.
Health plan.
In-office snacks.
Organized team events.
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'mysql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi']","['tune', 'data modeling', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi', 'tune', 'data modeling', 'pipelin', 'big data']","['day', 'stream', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi', 'tune', 'data modeling', 'pipelin', 'big data', 'day', 'stream', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']"
DE,"These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.
What will make you successful here?
Strong analytical and technical skills
A real passion for problem solving and learning new technology
Vision to balance speed and maintainability in solution design
The ability to handle multiple, concurrent projects
Crafting and implementing requirements, keeping projects on track, and engaging partners
Communicating complex technical details in meaningful business context
A low ego and humility; an ability to gain trust by doing what you say you will do
What you might do in your first year:
Work with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health
5+ years of full-time experience including extensive experience with healthcare data
Ability to understand and design relational data structures required
Very strong capabilities manipulating data using SQL
Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g.
Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)
Experience or willingness to learn building information pipelines utilizing Python or Java a plus
BS/MS in computer science, math, engineering, or other related fields is required.
Track record of successfully executing projects with multiple partners
Competitive salary, bonus, and health benefits
Paid gym membership
Fun, fast-paced, startup environment (with snacks)
Pre-tax savings on commute expenses
Remote flexibility
A highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.
","['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java']","['healthcar', 'problem solving', 'pipelin', 'math']",1,"['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java', 'healthcar', 'problem solving', 'pipelin', 'math']","['day', 'analyt', 'pipelin', 'python', 'comput', 'clinic', 'relat', 'engin']","['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java', 'healthcar', 'problem solving', 'pipelin', 'math', 'day', 'analyt', 'pipelin', 'python', 'comput', 'clinic', 'relat', 'engin']"
DE,"What will your day look like?
You will assist the organization through the continued build-up and operationalization of an enterprise class Modern Data environment, including various components within the Hadoop stack.
Resources to do the job require substantial hands-on experience working with the technologies encompassed within the Hadoop technology stack while also having knowledge and capabilities as a systems developer.
The Senior Data Engineer coordinates, designs, builds, and integrates complex application technology solutions, aligned to architectural standards and definitions, and will help ensure IT services are delivered effectively and efficiently.
Responsibilities
Do you see yourself doing this?
Responsible for day to day operation and support of Hadoop and Modern Data environments
Collaborate with data center and systems engineering teams on all cluster infrastructure setup, software installation, testing, upgrading/patching, monitoring, tuning/optimizing, troubleshooting, maintenance
Collaborate with development and strategy teams on component and 3rd party tool identification, recommendation, installation and management of Spark jobs
Collaborate with the data architecture and Infrastructure teams in technical investigations, development, and prototypes
Collaborate with Corporate IT function around integrating Hadoop ecosystem(s) with critical enterprise systems
Provide hardware architectural guidance
Develop and manage all cluster related testing activities
Create roadmaps for ongoing cluster deployment and growth
Perform capacity monitoring and capacity planning on infrastructure and resources
Manage cluster hardening activities through the implementation and maintenance of security and governance components across various clusters
Participate in the design and implementation of a Disaster Recovery strategy for all Modern Data components
Participate in design, implementation and management of alignment activities with all pertinent audit and compliance activities
Function as expert consulting resources around Hadoop integration points with any ETL, BI, and EDW teams
Provide input/develop new processes/standards in support of the organization's business/functional short-term strategies, with limited impact on the business/function overall results
Influence adoption of Modern Data new concepts, practices, and approaches
Design, build, deploy and maintain data pipelines using NiFi/Kafka/Spark Streaming or related data integration technologies
Qualifications
What makes you a great fit?
You’ll be a great fit if in addition to the completion of a Bachelor’s degree in Computer Science or a related field, required, and you have:
5+ years’ Cloudera or Hortonworks experience in IT Data Development or Data Related Support teams
Proven development and operational experience within Hadoop ecosystem (Spark/Python, HDFS, YARN, Hive, HBase, Sqoop, etc.
), preferable with Hortonworks or Cloudera distribution
High proficiency in Java, SQL, and Linux shell (Scala/Cascading experience a plus)
Expert knowledge of key data structures and algorithms in Hadoop, Cloudera or Hortonworks systems
Experience with the entire Software Development Lifecycle (SDLC) process such as change management, defect and issue tracking, to resolve data issues or to implement development enhancements
Hands on experience with monitoring tools (preferably Ambari, Nagios, etc.)
Familiar with OS, network configuration, protocols, and enterprise security solutions such as LDAP and/or Kerberos
Knowledge of metadata management and governance capabilities using Atlas
Familiarity with Data Science notebooks such as Apache Zeppelin, Jupyter
Automation experience with Chef, Puppet, or Ansible
Prior experience working in Financial Services industry preferred
Project Management experience with agile and project management methodologies (Scrum and/or Kanban)
Excellent written and verbal communication skills
Has an analytical and problem-solving mindset
Is highly organized and efficient
Ability to leverage strategic and tactical thinking
Works calmly under pressure and with tight deadlines
Demonstrates effective decision-making skills
Competitive medical, dental, and free vision benefits
Competitive compensation plan
Contributions towards gym memberships
Generous PTO and banking holidays off
Still not convinced?
","['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'bi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel']","['recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster']",1,"['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'powerbi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel', 'recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster']","['day', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'input', 'hadoop', 'corpor', 'infrastructur', 'bi', 'parti', 'engin', 'particip', 'spark', 'pipelin', '3rd', 'comput', 'relat']","['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'powerbi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel', 'recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster', 'day', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'input', 'hadoop', 'corpor', 'infrastructur', 'bi', 'parti', 'engin', 'particip', 'spark', 'pipelin', '3rd', 'comput', 'relat']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).
Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.
Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation.
","['java', 'javascript', 'python', 'salesforc']","['recommend', 'healthcar', 'machine learning', 'risk']",999,"['java', 'javascript', 'python', 'salesforc', 'recommend', 'healthcar', 'machine learning', 'risk']","['analyt', 'machin', 'python', 'set', 'sourc', 'integr']","['java', 'javascript', 'python', 'salesforc', 'recommend', 'healthcar', 'machine learning', 'risk', 'analyt', 'machin', 'python', 'set', 'sourc', 'integr']"
DE,"Each employee is hand-picked not only for their skills, but for their personality and broad expertise.
Build and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).
Uncover and recommend remediations for data quality anomalies.
Investigate, recommend and implement data ingestion and ETL performance improvements.
Document data ingestion and ETL program designs, present findings, conduct peer code reviews.
Develop and execute test plans to validate code.
Your Expertise:
4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.
3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.
Experience configuring big data solutions in a cloud environment (AWS, Azure or GCP).
Experience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.
Experience developing complex technical and ETL programs within a Hadoop ecosystem.
Must have a bachelors degree in Computer Science, Technology, Computer Information Systems, Computer Applications, Engineering, or a related field.
Your X-Factor:
Aptitude - You have an innate capacity to transition from project to project without skipping a beat.
Communication - You have excellent written and verbal communication skills for coordination across projects and teams.
Impact - You are a critical thinker with an emphasis on creativity and innovation.
Passion - You have the drive to succeed paired with a continuous hunger to learn.
Leadership - You are trusted, empathetic, accountable, and empower others around you.
Why Were Proud To Be Mavens!
Google Cloud North America Services Partner of the Year 2019, 2018
#21 Best Workplaces in Chicago, FORTUNE, 2018
Great Place To Work Certification, Great Place to Work, 2017 & 2018
Fast Fifty, Crain's Chicago Business
101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)
Top Google Cloud Partner, Clutch
Fastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine
Top IT Services Companies, Clutch
Google Global Rising Star Partner of the Year
Ready to Learn More?
Check out the Data Team
See what Glassdoor has to say
Real Customer Stories
","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel']","['recommend', 'anomali', 'big data', 'etl', 'commun', 'account']",1,"['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account']","['program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account', 'program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']"
DE,"Data Engineer
YOUR OPPORTUNITY:
Develop solutions that enable investment professionals to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
YOUR SKILLS & TALENTS:
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker']","['research', 'commun']",1,"['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun']","['program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun', 'program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']"
DE,"Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists.
Seeks to understand the data being worked with as its often unstructured data sets.
Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.
Job Summary Wording
You’ll also be responsible for integrating these applications with the architecture used across the organization.
Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset.
Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems.
Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases.
Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g.
RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions.
Able to employ design patterns and generalize code to address common use cases.
Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g.
Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Applied knowledge of data modeling principles (e.g.
dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g.
Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e.
Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience.
This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists.
Seeks to understand the data being worked with as its often unstructured data sets.
Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.
Job Summary Wording
You’ll also be responsible for integrating these applications with the architecture used across the organization.
Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset.
Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems.
Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases.
Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g.
RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions.
Able to employ design patterns and generalize code to address common use cases.
Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g.
Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Applied knowledge of data modeling principles (e.g.
dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g.
Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e.
Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience.
This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
","['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker']","['pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account']",1,"['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker', 'pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account']","['visual', 'python', 'etl', 'common', 'sourc', 'analyt', 'challeng', 'azur', 'hadoop', 'optim', 'appli', 'primari', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker', 'pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account', 'visual', 'python', 'etl', 'common', 'sourc', 'analyt', 'challeng', 'azur', 'hadoop', 'optim', 'appli', 'primari', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Want to be a part of a team of diverse collaborators in an authentically fun culture?
More About the Role
Data Engineer will be responsible for building efficient data pipelines that transform raw data into a format usable by downstream applications that serve both analytical and operational use cases.
Working with high volumes of data to efficiently process and expose for analysis
Collaborating with other engineering teams on strategies for data
Work with cutting edge data processing technologies
Analyze data to measure impacts of data schemas and use it to iterate on improvements
Translate from technical to business, and vice versa.
You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them.
Then turn around and do it the other way
Excellent knowledge on SQL, data modelling and patterns.
5-7 years experience with Python or another general purpose programming language
Background in writing ETL jobs within a Business Intelligence context
A bachelor's degree, preferably in a computer-related discipline.
Enthusiasm for the job.
Are you excited about data?
Do you love your users?
Got These?
Even Better:
Experience big data processing with Spark and other big data tools a plus
Excellent communication skills, including the ability to crystallize and broadly socialize insights
Problem analysis and problem-solving skills
Rigorous attention to detail and accuracy
Exposure to Amazon AWS or another cloud provider
Adaptability and collaborative skills
Flexible PTO.
Health and Wellness.
Learning and Career Growth.
MealPerks.
Who’s ready for some lunch?
Fun.
Social Impact.
The EEO is the Law poster is available here: DOL Poster.
Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address.
","['sql', 'spark', 'aw', 'cloud', 'python', 'excel']","['pipelin', 'analyz', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'aw', 'cloud', 'python', 'excel', 'pipelin', 'analyz', 'big data', 'etl', 'commun']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'provid', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']","['sql', 'spark', 'aw', 'cloud', 'python', 'excel', 'pipelin', 'analyz', 'big data', 'etl', 'commun', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'provid', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']"
DE,"Senior Data Engineer
Chicago
Job Code
3299
Senior Data Engineer
A successful candidate will have the following characteristics:
· 7-10 years of experience in data transformation and extraction: some combination of ETL/ELT including auditing and quality control, table and database design, query design, performance analysis and optimization
· Senior-level design and architecture experience in data transformation.
Personal responsibility for data engineering design
· Significant experience with Informatica Integration strongly preferred; extensive experience with related tech (SSIS/ADF, Talend, Pentaho, etc.)
· Significant experience with one of more data-preparation tools (Alteryx, Paxata, Catalytic) and/or capabilities (DataBricks, Python, Azure Functions) strongly preferred
· Experience managing and mentoring data engineering teams preferred
· Experience working with offshore engineering and QA teams preferred
","['python', 'pentaho', 'azur']","['optim', 'etl']",999,"['python', 'pentaho', 'azur', 'optim', 'etl']","['azur', 'relat', 'optim', 'python', 'etl', 'engin', 'integr']","['python', 'pentaho', 'azur', 'optim', 'etl', 'azur', 'relat', 'optim', 'python', 'etl', 'engin', 'integr']"
DE,"Are you someone who likes to have fun at work?
Are you passionate about picking problems apart, driving results, and making an impact?
Do you enjoy working in a fast-paced, dynamic environment where no two days are the same?
It has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day and evaluate thousands of data-points in real-time all while responding in just a few milliseconds.
What you'll do:
Work on Big Data technologies, lead the design, coding and maintenance of highly scalable backend data processing platform for large throughput
Work on the data modelling for the MPP columnar databases to handle high volume of queries with sub-second response times
Lead the entire software lifecycle including hands-on development, code reviews, testing, continuous integration, continuous deployment and documentation using modern programming languages (such as Java, Scala, Python)
Perform tuning of systems for optimal performance
Mentor junior team members
5+ years of recent hands-on experience in one or more of the modern programming languages (Java, Scala, Python)
Good understanding of collections, multi-threading, JVM memory model, algorithms, scalability and various tradeoffs in a Big Data setting.
Experience developing and maintaining ETL applications and data pipelines using big data technologies
Strong SQL knowledge (OLAP) and experience working with mpp columnar databases (Vertica, SnowFlake,etc)
Excellent interpersonal and communication skills
Understanding of full software development life cycle, agile development and continuous integration
What puts you over the top:
Data warehouse experience in SnowFlake and experience writing ETL pipelines in SnowFlake
Experience working with AWS technologies such EMR, step functions, data pipeline, cloudformation, etc.
Experience working with hadoop mapreduce, spark, pig, hive, etc.
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'spark', 'scala', 'pig', 'hadoop', 'aw', 'snowflak', 'python', 'hive', 'java', 'excel']","['pipelin', 'tune', 'optim', 'big data', 'etl', 'commun']",999,"['sql', 'mapreduc', 'spark', 'scala', 'pig', 'hadoop', 'aw', 'snowflak', 'python', 'hive', 'java', 'excel', 'pipelin', 'tune', 'optim', 'big data', 'etl', 'commun']","['day', 'program', 'spark', 'pipelin', 'set', 'power', 'hadoop', 'optim', 'aw', 'python', 'algorithm', 'warehous', 'big', 'etl', 'collect', 'integr']","['sql', 'mapreduc', 'spark', 'scala', 'pig', 'hadoop', 'aw', 'snowflak', 'python', 'hive', 'java', 'excel', 'pipelin', 'tune', 'optim', 'big data', 'etl', 'commun', 'day', 'program', 'spark', 'pipelin', 'set', 'power', 'hadoop', 'optim', 'aw', 'python', 'algorithm', 'warehous', 'big', 'etl', 'collect', 'integr']"
DE,"The expectation is that the data engineer has sound programming skills, strong business acumen, and a strong interest in finance.
Responsibilities:
Develop solutions that enable internal analysts to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, bespoke processes), transformations (Python, Perl) and interface (API, schema design, events, etc.)
Build tooling and automation around data pipelines that improve the efficiency, quality and resiliency of the data platform
Partner with internal analysts, quants and data scientists to design, develop, test and deploy solutions that answer fundamental questions about financial markets.
Take on an entrepreneurial mentality by building and selling your own ideas.
Requirements:
A passion for working with data and developing software to address data processing challenges
Proficiency with Python, C++, Java or equivalent
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Prior experience building data pipelines from structured or unstructured data preferably including web crawlers
Prior work developing BI tooling and/or application development for data analytics
Advanced technical communication skills
Working knowledge of statistics, predictive analytics or machine learning techniques
Strong business acumen with prior experience in investment research or direct exposure to product or data science teams AND passionate about using data for investment decisions
If you would like to be considered for the position of Enterprise Data Engineer or wish to discuss the role further then please leave your details below.
Thank you
","['spark', 'dask', 'perl', 'hadoop', 'bi', 'python', 's3', 'java', 'nosql']","['research', 'pipelin', 'predict', 'machine learning', 'statist', 'financ', 'commun']",999,"['spark', 'dask', 'perl', 'hadoop', 'powerbi', 'python', 's3', 'java', 'nosql', 'research', 'pipelin', 'predict', 'machine learning', 'statist', 'financ', 'commun']","['analyt', 'machin', 'techniqu', 'learn', 'spark', 'challeng', 'pipelin', 'predict', 'hadoop', 'bi', 'python', 'scientist', 'statist', 'comput', 'engin']","['spark', 'dask', 'perl', 'hadoop', 'powerbi', 'python', 's3', 'java', 'nosql', 'research', 'pipelin', 'predict', 'machine learning', 'statist', 'financ', 'commun', 'analyt', 'machin', 'techniqu', 'learn', 'spark', 'challeng', 'pipelin', 'predict', 'hadoop', 'bi', 'python', 'scientist', 'statist', 'comput', 'engin']"
DE,"The Job Details are as follows:
OVERVIEW
In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player
","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker']","['data warehousing', 'commun']",1,"['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun']","['day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun', 'day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']"
DE,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois
Data Engineer
You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.
You will:
- Work with product owners to understand desired application capabilities and testing scenarios
- Continuously improve software engineering practices
- Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies
- Lead the craftsmanship, availability, resilience, and scalability of your solutions
- Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
- Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity
Basic Qualifications:
- Bachelors Degree
- At least 3 years of SDLC experience using Java technologies
- At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper
Preferred Qualifications:
- Master's Degree
- 2+ year experience with Spark
- 3+ years experience developing software solutions to solve complex business problems
","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi']","['commun', 'big data']",1,"['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi', 'commun', 'big data']","['spark', 'divers', 'hadoop', 'infrastructur', 'aw', 'avail', 'python', 'big', 'relat', 'engin']","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'react', 'rubi', 'commun', 'big data', 'spark', 'divers', 'hadoop', 'infrastructur', 'aw', 'avail', 'python', 'big', 'relat', 'engin']"
DE,"To learn more: https://www.kalderos.com/company/about
What Data Engineers Do
They can work within the Ledger and Master Data team, the Data Science team, or others as needs arise.
Work with product teams to understand and develop data models that can meet requirements and operationalize well
Build out automated ETL jobs that reliably process large amounts of data, and ensure these jobs runs consistently and well
Build tools that enable other data engineers to work more efficiently
Try out new data storage and processing technologies in proof of concepts and make recommendations to the broader team
Tune existing implementations to run more efficiently as they become bottlenecks, or migrate existing implementations to new paradigms as needed
4+ years work experience as a Data Engineer in a full-time role
Excellent project managements skills, and familiarity working in an agile environment
Some ways you may demonstrate this are:
Describing a time in which you had a large project you needed to manage
Relevant work experience on a Agile team
Proven data engineering experience that involved creating, and maintaining a database and implementing data processing pipelines
Some ways you may demonstrate this are:
Professional experience described on your resume
Open source implementations of data intensive applications
A personal side project that involved maintaining a database like a website
Online course completion in relevant areas
Vendor certification in a relevant field
Programming experience with a modern computing language that supports data engineering work (Python, C#, JVM-languages like Scala)
Some ways you may demonstrate this are:
Professional project descriptions on your resume
GitHub repositories that have working code that you created
Sending a code snippet that exemplifies your work
Advanced SQL skills and understanding performance trade-offs of various SQL implementations
Some ways you may demonstrate this are:
Professional project descriptions on your resume
SQL Certifications you have earned
Answering SQL questions in a phone interview
Advanced SQL queries you wrote in your Github repository
What May Set You Apart
Experience with SQL databases like MS SQL Server
Experience with Azure cloud computing such as hosted databases
Experience with streaming technologies such as Kafka or Event Hubs
Experience with orchestration frameworks like Azure Data Factory or Airflow
Experience with NoSQL data stores such as MongoDB and CosmosDB
Experience in the healthcare or pharmaceutical industries
Experience with large scale migrations of databases
Experience creating RESTful APIs to service data needs for web applications
Experience with implementing Software Development Lifecycle approaches to database work, such as Testing, CI/CD and other automation
If you think you meet some of the list of the above, but not everything, that’s perfectly fine!
401K plan with matching
Healthcare benefits
Flexible schedule and a fair PTO system that allows for a healthy work-life balance
Opportunity to work on new technologies and learn new skills
Celebration, and education stipends
To learn more: https://www.kalderos.com/company/culture
","['mongodb', 'sql', 'airflow', 'azur', 'scala', 'python', 'c', 'nosql', 'github', 'kafka', 'excel']","['recommend', 'etl', 'healthcar', 'tune']",2,"['mongodb', 'sql', 'airflow', 'azur', 'scala', 'python', 'c', 'nosql', 'github', 'kafka', 'excel', 'recommend', 'etl', 'healthcar', 'tune']","['azur', 'set', 'amount', 'python', 'comput', 'etl', 'sourc', 'engin']","['mongodb', 'sql', 'airflow', 'azur', 'scala', 'python', 'c', 'nosql', 'github', 'kafka', 'excel', 'recommend', 'etl', 'healthcar', 'tune', 'azur', 'set', 'amount', 'python', 'comput', 'etl', 'sourc', 'engin']"
DE,"Key Responsibilities:
· Develop solutions that enable internal analysts to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, bespoke processes), transformations (Python, Perl) and interface (API, schema design, events, etc.)
· Partner with internal analysts, quants and data scientists to design, develop, test and deploy solutions that answer fundamental questions about financial markets.
· Take on an entrepreneurial mentality by building and selling your own ideas.
Required Skills
· A deep passion for working with data and developing software to address data processing challenges
· Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
·Proficiency within one or more programming languages like Java, Python, Perl or JavaScript.
· Proficiency with RDBMS, or NoSQL
· Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning or Software Architecture
· Experience with any of the following systems: Apache Airflow, AWS/GCP/Azure, Jupyter, Kafka, Docker, Nomad/Kubernetes
· Strong written and verbal communications skills
· Ability to manage multiple tasks and thrive in a fast-paced team environment
","['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['machine learning', 'natural language processing', 'commun']",1,"['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'machine learning', 'nlp', 'commun']","['machin', 'program', 'learn', 'challeng', 'azur', 'aw', 'python', 'scientist', 'comput']","['gcp', 'airflow', 'perl', 'azur', 'jupyt', 'javascript', 'aw', 'python', 's3', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'machine learning', 'nlp', 'commun', 'machin', 'program', 'learn', 'challeng', 'azur', 'aw', 'python', 'scientist', 'comput']"
DE,"Position: Data Engineer
Type: Permanent/Direct-hire*
Start: ASAP
*Sorry, this client is unable to sponsor or work with 3rd party vendors*
Requirements:
Experience on Apache Airflow tool.
Must have hands on experience implementing AWS Big data lake using EMR and Spark.
Working experience on AWS platform.
Experience in using CI/CD pipeline (Gitlab)
Experience in Code Quality implementation (Used Pep8/Pylint) tools or any other code quality tool.
Experience of Python Plugins /operators like FTP Sensor, Oracle Operator etc.
Implement Industry Standards /Best Practices.
Excellent analytical and problem-solving skills
Business acumen to work directly with clients.
Excellent verbal and written communication skills
","['spark', 'airflow', 'aw', 'python', 'excel', 'oracl']","['commun', 'pipelin', 'big data']",999,"['spark', 'airflow', 'aw', 'python', 'excel', 'oracl', 'commun', 'pipelin', 'big data']","['analyt', 'spark', 'pipelin', 'aw', 'python', 'parti', '3rd', 'big', 'engin']","['spark', 'airflow', 'aw', 'python', 'excel', 'oracl', 'commun', 'pipelin', 'big data', 'analyt', 'spark', 'pipelin', 'aw', 'python', 'parti', '3rd', 'big', 'engin']"
DE,"Travel: Up to 30%
ACE in a nutshell
The ACE team sits at the intersection of technology, innovation, creativity, and blended learning.
Expect rapid career growth—we preach recognition and reward those with the aptitude and attitude to get it right.
You’ll learn to develop complex data architectures, solve for real-time data needs of large businesses, and navigate the changing landscape of the Azure stack.
You will partner with clients across several key industries including Healthcare, Manufacturing and Industrial Products, Financial Services, Professional Services, Engineering and High Technology, Retail and more.
Beyond the tech, you'll benefit from dollar for dollar 401(k) matching (up to 6% of your salary), ongoing structured learning opportunities, subsidized healthcare, vision, and dental plans (amongst others), a fully stocked snack, coffee, and drink bar, and a laid-back team looking to disrupt the status quo.
Relevant experience
You should have experience working with others collaboratively, thinking critically about solving problems and technically executing.
You should be hungry to learn new technology, humble with your willingness to help others, and socially capable of working collaboratively.
You must be able to work with customer stakeholders to understand business and technical requirements.
You should have skill in SQL at an advanced level, including complex queries, data definition, constraint specification, coding with SQL or similar, and database modeling.
You are inherently iterative, with knowledge of Agile methodologies, estimation techniques, and (optionally) workshop and prototype techniques.
You have a strong understanding of one or more of the following:
Power BI or similar data visualization tool
Azure or similar cloud platform
Python or Spark or similar Data Engineering language
PowerShell or similar scripting language
Azure Data Warehouse (ADW) or similar MPP database platform
Azure Data Lake (ADL)
Nice to haves:
Experience with Azure Data Platform (Azure Data Factory v.2, Azure Data Warehouse, Databricks, etc.)
Knowledge of the overall internal structure of a database system including indexing, query processing, transaction management, and fault tolerance.
Ability to design a database, implement a design in a real database system, and construct user interfaces to the database via an API or 3rd party tool.
2+ internships in related roles.
","['sql', 'spark', 'azur', 'bi', 'cloud', 'python', 'power bi']","['healthcar', 'visual']",999,"['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'healthcar', 'visual']","['techniqu', 'spark', 'azur', 'visual', 'power', 'bi', 'python', 'parti', '3rd', 'warehous', 'relat', 'engin']","['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'healthcar', 'visual', 'techniqu', 'spark', 'azur', 'visual', 'power', 'bi', 'python', 'parti', '3rd', 'warehous', 'relat', 'engin']"
DE,"Job Overview
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets to meet functional and non-functional business requirements.
Author the pipeline code required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with Data Scientists and Systems Engineers to design data delivery architecture.
Work with stakeholders including the Executive and Product teams to assist with data-related technical issues and support their data insight needs.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
PostgreSQL administration familiarity a plus.
Strong Python scripting skills.
Ruby a plus.
Familiarity with API endpoint interactions and techniques for handling query complications.
Understanding of Containerization, Micro-Service, and Server-less a strong plus.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
The ideal candidate would have 3+ years of experience in a Data Engineer role, or 5+ years in any Software Engineering role with demonstrable familiarity of the Data Engineering/Science space.
Work experience, academic instruction, and/or code portfolio will all considered.
Powered by JazzHR
sEaphUQBLW
","['sql', 'python', 'postgresql']","['optim', 'pipelin']",999,"['sql', 'python', 'postgresql', 'optim', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'relat', 'power', 'optim', 'python', 'scientist', 'set', 'sourc', 'engin']","['sql', 'python', 'postgresql', 'optim', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'relat', 'power', 'optim', 'python', 'scientist', 'set', 'sourc', 'engin']"
DE,"Chicago, Illinois
Skills : AWS (s3, redshift, EMR, EC2, lambda, SNS), unix shell scripting,python spark,scala
AWS (s3, redshift, EMR, EC2, lambda, SNS),
unix shell scripting,python
spark,scala
PREFER TO HAVE
snowflake,presto,
arrow,Airflow,Hadoop,Hive
AWS (s3, redshift, EMR, EC2, lambda, SNS), unix shell scripting,python spark,scala
","['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 's3', 'hive']",[None],999,"['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 's3', 'hive']","['aw', 'python', 'spark', 'hadoop']","['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 's3', 'hive', 'aw', 'python', 'spark', 'hadoop']"
DE,"Core Responsibilities & Qualifications
In this role, you will focus on the design, implementation, operation, and refactoring of data management systems to meet the Brads Deal's business needs.
You will take business requirements, transform them into data models, and develop ETL processes to populate those models.
This could include automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
You should have hands-on experience with a variety of the data warehousing concepts and practices, covering both technical development as well as 'not-necessarily technical practices' such as data governance.
This list of experiences includes data manipulation, database partitioning, data structures, data management, and best engineering practices.
You should have advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of database platforms.
You have a successful history of manipulating, processing and extracting value from large disconnected datasets.
In addition to developing and implementing ETL processes, you have experience dealing with performance and scaling issues.
You have at least 3+ years of end-to-end experience with data warehousing and BI systems.
This includes data modeling, ETL architectures, OLAP, and Big Data tools (such as Hive).
You have hands-on experience with AWS and the different data tools that are offered on that platform (Redshift, EMR, Kinesis, S3, etc).
), exposure to over 50M consumers a year and a consistent national media presence (USA Today, Today Show, WSJ, MSNBC).
","['sql', 'bi', 'aw', 'redshift', 'hive', 's3']","['data warehousing', 'etl', 'data modeling', 'big data']",999,"['sql', 'powerbi', 'aw', 'redshift', 'hive', 's3', 'data warehousing', 'etl', 'data modeling', 'big data']","['relat', 'infrastructur', 'bi', 'aw', 'big', 'etl', 'engin']","['sql', 'powerbi', 'aw', 'redshift', 'hive', 's3', 'data warehousing', 'etl', 'data modeling', 'big data', 'relat', 'infrastructur', 'bi', 'aw', 'big', 'etl', 'engin']"
DE,"Are you up to the challenge?
Data Engineer
About the role
The Data Engineer is responsible for deploying and managing ETL/ELT pipelines, jobs, orchestration frameworks, and ensuring data quality.
The data engineer can expect to work closely with business analysts, data scientists, analytics experts and developers to build the ETL/RLT pipelines, and data models.
The data engineer will also have the opportunity to inform the design, implementation, and best practices or this system including deployment of modern tools.
Responsibilities
Build cloud-based data warehousing environments, data processing pipelines, and data models that support a variety of business needs
Work with analysts and developers in the product development process to ensure that newly designed data models meet analytics requirements and follow best practices
About you
5+ years of experience as a data engineer using data warehousing technologies like, Amazon Redshift, RDS, S3, Athena, EMR, and Hadoop/Hive/Spark
Proficient in SQL including one or more relational databases like MySQL, Oracle, Postgres, or similar
5+ years experience with ETL and job scheduling or orchestration using tools like Airflow, Luigi, Oozie, or similar
5+ years experience programming in python and familiarity with AWS and git
Excellent communication skills and ability to work on a growing team
Bonus points if you have
Experience with web-scale data or working with healthcare data in a HIPAA-compliant environment
Experience with Healthcare Payer data as well as Optum Impact Intelligence tool.
Experience with data modeling, data visualization, and/or BI tools like Looker, Metabase, or Tableau
Experience with AB Testing
Benefits
Stock Options
PTO
Unlimited sick and sanity days
Commuter benefits
Medical, Dental, Vision
401K with matching
Unlimited snacks in office
Powered by JazzHR
17RCjfuDup
","['sql', 'spark', 'airflow', 'looker', 'git', 'hadoop', 'bi', 'aw', 'redshift', 's3', 'python', 'hive', 'tableau', 'mysql', 'postgr', 'cloud', 'excel', 'oracl']","['healthcar', 'pipelin', 'visual', 'data modeling', 'data warehousing', 'etl', 'commun', 'ab testing']",999,"['sql', 'spark', 'airflow', 'looker', 'git', 'hadoop', 'powerbi', 'aw', 'redshift', 's3', 'python', 'hive', 'tableau', 'cloud', 'excel', 'oracl', 'healthcar', 'pipelin', 'visual', 'data modeling', 'data warehousing', 'etl', 'commun', 'ab testing']","['day', 'analyt', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'visual', 'power', 'hadoop', 'bi', 'aw', 'python', 'scientist', 'etl', 'engin']","['sql', 'spark', 'airflow', 'looker', 'git', 'hadoop', 'powerbi', 'aw', 'redshift', 's3', 'python', 'hive', 'tableau', 'cloud', 'excel', 'oracl', 'healthcar', 'pipelin', 'visual', 'data modeling', 'data warehousing', 'etl', 'commun', 'ab testing', 'day', 'analyt', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'visual', 'power', 'hadoop', 'bi', 'aw', 'python', 'scientist', 'etl', 'engin']"
DE,"Experience with big data tools Hadoop, Apache Spark Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
(Nice to have) Experience with relational SQL, Snowflake and NoSQL databases Detailed overview of functional and technical role expectations Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have working experience using the following softwaretools 3+ years of experience (Mid-level) Strong Programming experience with object-orientedobject function scripting languages Python 3+ years of experience (Mid-level) Experience with big data tools Hadoop, Apache Spark, Kafka, etc 1+ years of experience with AWS cloud services S3, EC2, EMR, RDS, Redshift.
Experience with stream-processing systems Storm, Spark-Streaming, etc.
(Nice to have).
1+ Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'Big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Required Skills Python, Hadoop, Apache Spark, AWS, SnowflakeSQL knowledge.
","['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'postgr', 'cloud', 'kafka']","['optim', 'statist', 'pipelin', 'big data']",2,"['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'cloud', 'kafka', 'optim', 'statist', 'pipelin', 'big data']","['program', 'stream', 'python', 'quantit', 'sourc', 'analyt', 'hadoop', 'optim', 'statist', 'action', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'cloud', 'kafka', 'optim', 'statist', 'pipelin', 'big data', 'program', 'stream', 'python', 'quantit', 'sourc', 'analyt', 'hadoop', 'optim', 'statist', 'action', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Data Engineers not only build and optimize data pipelines that transform and store data in a way that allows the rest of the organization (analysts, data scientists, other stakeholders) analyze and consume data.
They are also in charge of building the systems and establishing the processes to enable the rest of the data team to develop, test, and deploy analyses and code in an efficient and scalable way.
If you're seeking a role that is high impact and full of ownership....please read on.
Being in Chicago would be great, but not necessary.
What you'll tackle
Design new enterprise data models and ETL processes to populate them
Extract and transform data from production databases and 3rd party services to provide consumable data and support functions across the organization
Detect quality issues, track them to their root source, and implement fixes and preventative audits
Manage and optimize Redshift clusters/data lake to ensure current health and performance and future scaling needs
What you bring to the table
Experience with AWS; expertise in Redshift, Postgres or other RDBSs (preferably column-oriented)
Expertise in SQL and ability to write and optimize complex queries
Experience with Docker, Elastic Container Service, Lambda a plus
Ability to write customized software in Python, Bash, Go or other common open source languages.
Experience with Airflow or similar scheduling service a plus
Experience with CI/CD tools like Jenkins or Drone
Creativity in approaching data organization challenges with an understanding of the end goal
A collaborative nature and entrepreneurial spirit.
Prior startup experience a huge plus
","['sql', 'airflow', 'aw', 'lambda', 'redshift', 'python', 'postgr', 'docker']","['etl', 'pipelin', 'cluster']",999,"['sql', 'airflow', 'aw', 'lambda', 'redshift', 'python', 'docker', 'etl', 'pipelin', 'cluster']","['pipelin', 'aw', 'python', 'parti', '3rd', 'scientist', 'etl', 'common', 'sourc', 'engin']","['sql', 'airflow', 'aw', 'lambda', 'redshift', 'python', 'docker', 'etl', 'pipelin', 'cluster', 'pipelin', 'aw', 'python', 'parti', '3rd', 'scientist', 'etl', 'common', 'sourc', 'engin']"
DE,"Ready to write the best chapter of your career?
The ideal candidate is an experienced data pipeline builder and wrangler who enjoys optimizing data systems.
Mapping heterogeneous data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology/schema in which it is stored.
Managing a task-queue/message broker architected system for data pipelines and interfacing with the infrastructure team
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, data indexing, re-designing infrastructure for greater scalability, etc.
Document data relationships and translate business rules to data rules.
Work with stakeholders including the Executive, Product, Data Science, and Client Success teams to assist with data-related technical issues and support their data infrastructure needs.
Be able to perform manual data quality testing as requested in business requirements.
Deploy these data flows in modern cloud environments: AWS and Azure.
Background & Qualifications
In order to be successful, you will need the following:
2+ years of experience in a Data Engineer/Data Architect role and knowledge of building data pipelines from multiple data sources including third party APIs, flat files or DB queries.
Advanced working knowledge of SQL and Relational Databases.
Python with scientific python libraries (numpy, pandas, scikit-learn, etc)
Be a consummate collaborator, able to establish good relationships with technical, product, and business owners also to work with vendor technical teams
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large distributed datasets.
Preferred:
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in developing and maintaining ETL/ELT data pipelines.
Having experience in the following software/tools/languages:
PostGres SQL, Amazon RDS, EC2 and EMR
Python, Git
Flask, Redis, Kubernetes
Sisense Self Hosted instances or other Reporting platforms.
Powered by JazzHR
","['sql', 'scikit', 'azur', 'panda', 'ec2', 'git', 'aw', 'cloud', 'python', 'db', 'postgr', 'flask', 'kubernet']","['etl', 'pipelin']",999,"['sql', 'scikit', 'azur', 'panda', 'ec2', 'git', 'aw', 'cloud', 'python', 'db', 'flask', 'kubernet', 'etl', 'pipelin']","['analyt', 'learn', 'pipelin', 'azur', 'relat', 'power', 'infrastructur', 'aw', 'python', 'parti', 'etl', 'sourc', 'engin']","['sql', 'scikit', 'azur', 'panda', 'ec2', 'git', 'aw', 'cloud', 'python', 'db', 'flask', 'kubernet', 'etl', 'pipelin', 'analyt', 'learn', 'pipelin', 'azur', 'relat', 'power', 'infrastructur', 'aw', 'python', 'parti', 'etl', 'sourc', 'engin']"
DE,"5+ years of experience in a Data Engineer role Experience with big data tools Hadoop, Spark, Kafka and relational SQL and NoSQL databases,
","['sql', 'spark', 'hadoop', 'nosql', 'kafka']",['big data'],999,"['sql', 'spark', 'hadoop', 'nosql', 'kafka', 'big data']","['spark', 'hadoop', 'big', 'relat', 'engin']","['sql', 'spark', 'hadoop', 'nosql', 'kafka', 'big data', 'spark', 'hadoop', 'big', 'relat', 'engin']"
DE,"This group also includes Critical Mass, Targetbase, Proximity, Credera, and sparks&honey and other well-known agencies.
ABOUT YOU
As a data engineer, you define solutions for the use, extraction and manipulation of data - driven by the balanced combination of business needs and consumer preferences.
You work in close collaboration with multidisciplinary teams to provide the data needed, in the optimal format for making critical, real-time decision.
You can communicate through a project with ease and understanding and you know your tools like the back of your hand.
You are experienced with writing complex SQL statements and mapping relational database structure.
You strive in designing & continuously improving data workflows using enterprise campaign management solutions such as Adobe Campaign, Unica, Eloqua, Marketo and/or Salesforce Marketing Cloud.
You may have experimented in the past with BI tools such as Tableau, Domo or QlikView, and have R/Python in your radar
Most importantly not only can you do the job, you can explain it to a relative and they actually understand.
You are self-directed, highly motivated and have fun while working hard in a fast-paced environment.
","['sql', 'salesforc', 'bi', 'tableau', 'python', 'r', 'cloud']",['optim'],999,"['sql', 'salesforc', 'powerbi', 'tableau', 'python', 'r', 'cloud', 'optim']","['bi', 'optim', 'python', 'relat', 'engin']","['sql', 'salesforc', 'powerbi', 'tableau', 'python', 'r', 'cloud', 'optim', 'bi', 'optim', 'python', 'relat', 'engin']"
DE,"In this role, you will work with other members of Engineering, Product and Project Management, and various business groups to ensure timely availability of usable data to all parts of the business that need it.
This is inclusive of interviewing, onboarding and each role day-to-day.
Responsibilities:
Design, develop and deploy batch and streaming data pipelines.
Monitor and ensure operational stability of data pipelines.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Skills and Experience:
Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Experience extracting data from relational and document databases.
Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.
Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).
Proficiency in Java or Python programming languages.
Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.
Benefits and Perks:
Open vacation policy
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
#LI-JC1
","['sql', 'git', 'snowflak', 'python', 'redshift', 'java']","['pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account']",1,"['sql', 'git', 'snowflak', 'python', 'redshift', 'java', 'pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account']","['day', 'analyt', 'program', 'pipelin', 'relat', 'avail', 'python', 'comput', 'employe', 'engin']","['sql', 'git', 'snowflak', 'python', 'redshift', 'java', 'pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account', 'day', 'analyt', 'program', 'pipelin', 'relat', 'avail', 'python', 'comput', 'employe', 'engin']"
DE,"Demonstrated ability to engage and deliver in technical projects related to agility, business value, data warehousing, and cloud-oriented data solutions is also a must.
Data & AI Engineers are key enablers for other consultants and partner staff by sharing knowledge in large enterprise implementations, best practices, and frameworks or patterns.
Responsibilities
Assist and author functional requirements and technical design documentation
Participate in milestone meetings and planning discussions for aligned client projects
Work with BA and PM teams to plan project sprints, scope and resource allocation
Manage at project milestones to ensure successful solution delivery and client satisfaction
Continuous learning and research on modern data solutions, and relevant technologies
Promote service offerings through blogs posts, industry groups, and speaking events
Qualifications
Bachelor's Degree in Computer Science or related fields, or the equivalent in work experience
Three or more years of experience in enterprise systems and warehouse architecture design, development, testing, deployment, and operational support
Experience with advanced integration scenarios such as hybrid cloud architectures
Experience with enterprise architecture, server topologies, and distributed systems
Knowledge of Agile methodologies
Demonstrated verbal and written communication skills
Demonstrated technical documentation skills
Prior consulting experience is a plus
Skill Requirements
Azure ETL tools such as Synapse, Databricks or Data Factory
Azure Data Platform experience required
Data Modeling (Tabular)
Data security monitoring and data governance
Power BI
SQL Server and T-SQL Development
Up to 15% travel required
Machine Learning languages such as R or Python a plus
IoT experience a plus
Spark experience a plus
In addition, all colleagues are eligible for a number of rewards and recognition programs, excellent training program and bonus opportunities.
","['sql', 'spark', 'azur', 'bi', 'cloud', 'python', 'r', 'excel', 'power bi']","['research', 'data modeling', 'machine learning', 'data warehousing', 'etl', 'commun']",1,"['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'r', 'excel', 'research', 'data modeling', 'machine learning', 'data warehousing', 'etl', 'commun']","['machin', 'program', 'learn', 'spark', 'azur', 'relat', 'power', 'bi', 'python', 'comput', 'warehous', 'etl', 'engin', 'particip', 'integr']","['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'r', 'excel', 'research', 'data modeling', 'machine learning', 'data warehousing', 'etl', 'commun', 'machin', 'program', 'learn', 'spark', 'azur', 'relat', 'power', 'bi', 'python', 'comput', 'warehous', 'etl', 'engin', 'particip', 'integr']"
DE,"Are you interested in building the future of healthcare and transforming the patient experience?
Are you hopeful about what data and medical research can do to improve medicine?
As a core member of the Analytics department, you will be in a very dynamic environment that works cross-functionally with all other departments, such Engineering, Sales, and Product.
You will work on a broad array of problems that rely heavily on solid data engineering principles being in place: business intelligence, data science, and software engineering.
Your work will be impactful across the entire organization.
Role Responsibilities
Develop and maintain ETL infrastructure to support the ingestion of external data sources
Perform data quality assurance checks
Requirements
Ideal Qualifications
BS/MS in Computer Science, Engineering, Mathematics, or related field
Deep knowledge of SQL and at least one database technology
Proficient in Python
Experience with version control systems (e.g.
git) and writing reusable and extensible code
Highly self-motivated with strong analytical problem-solving skills and attention to detail
Nice to Haves
Experience with workflow management systems such as luigi or airflow
Experience designing, building, and maintaining ETL infrastructure in a production setting
Experience in machine learning and/or business intelligence
Experience with ETL tools like Apache Kafka, Logstash, Segment, Informatica
Experience with cloud technologies such as AWS, Google Cloud Platform, or Azure
Experience working in Healthcare, Finance or another regulated industry
Benefits
Great Benefits - top-notch health, dental and vision insurance.
Additional perks available including 401K.
True Idea Meritocracy - great ideas win out.
No need to track it or save up.
","['sql', 'google cloud', 'airflow', 'azur', 'git', 'aw', 'cloud', 'python', 'kafka']","['research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl']",1,"['sql', 'google cloud', 'airflow', 'azur', 'git', 'aw', 'cloud', 'python', 'kafka', 'research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl']","['analyt', 'machin', 'learn', 'azur', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin']","['sql', 'google cloud', 'airflow', 'azur', 'git', 'aw', 'cloud', 'python', 'kafka', 'research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl', 'analyt', 'machin', 'learn', 'azur', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin']"
DE,"This position will be the Senior technical resource driving architecture for the integration of large 3rd party partner integrations with companies like Facebook, Google and Twitter to name a few.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
This position will require the ability to own and lead data initiatives on a cross-functional team.The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
You should be able to not take yourself too seriously as well.
And most of all, you will enjoy working with great people who are changing the entire industry.
What you'll do:
Migrate existing data pipelines from on-prem regional data centers to AWS and GCP.
Architect a new modern event driven architecture with both batching and streaming
Adjust existing pipelines to fit the AWS processing model such as integration with S3, migrate to open source version of hadoop, adjustments to security model, etc...
Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Partner and work closely with the QA Engineers to develop automated tests
Participate in training and mentoring of junior team members
8+ years of experience designing and building data-intensive applications
5+ years architecting systems in a big data ecosystem using MapReduce, Spark, MPP Data Warehouses, and sql/nosql databases.
5+ years recent hands-on experience with object oriented languages (Java, Scala, Python)
5+ years Hands on experience building production level systems in a cloud environment (AWS or GCP)
Excellent interpersonal and communication skills in English
Proven experience leading the design and execution of event driven architectures for distributed systems
Experience designing systems for performance, scalability, and reliability
In-depth understanding of object oriented programming concepts
Low level working knowledge of collections, multi-threading, JVM memory model, etc.
Solid understanding of database fundamentals and SQL
Understanding the full software development life cycle, agile development and continuous integration
Ability to clearly communicate with team-members in a cross-matrix environment
What puts you over the top:
Built systems in a containerized environment with familiarity in Docker, ECS, Kubernetes
Exposure to Data Warehousing solutions like Snowflake and BigQuery
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker']","['pipelin', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun']","['engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun', 'engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']"
DE,"About the Role
You will provide interesting and actionable insights into the business at a macro - and client level, and build robust predictive analysis tools to help guide internal and client decision making.
You will expose your data products via API endpoints that can be used to deliver customer-facing features.
Key Responsibilities:
Architect and design a data warehouse from the ground up.
Create and manage ETL operations.
Determine the most effective way to store, report, and leverage data for data science.
Ability to deliver ad-hoc reporting and identify trends in ad-hoc requests to determine and then build out reporting features that benefit all clients.
Basic Qualifications:
5+ years of SQL experience
5+ years of experience setting up database environments, database schema, and managing deploys/ updates to the schema.
3+ years of experience with ETL tooling.
3+ years of architecting data warehouses.
BONUS POINTS FOR CANDIDATES WITH…
A bachelor’s degree in Actuarial Science, Math, Statistics, Engineering, Data Science, Computer Science or other related majors, or equivalent experience.
Solid experience with Python or Ruby on Rails.
Data visualization knowledge including the usage of Tableau or Power BI.
Experience configuring infrastructure components via code, ideally using Terraform, is a plus.
Experience with AWS, Azure, or Google Cloud Platform (GCP)
Familiarity with scientific tools such as R or SPSS is a plus.
Experience in data visualization.
Published apps, websites, or contributed to open source libraries (feel free to include with application).
Strong intellectual curiosity and passion to learn.
Excellent organizational, communication, and time management skills.
Ability to take complex/ technical ideas and distill them into simple and intuitive code.
Comfortable performing in a fast-paced and ever-evolving business.
Benefits
Competitive salary and meaningful equity
Health insurance
Vacation days, sick days, and corporate holidays
Complimentary gym membership
Fast growth environment with potential for quick upward mobility
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.
","['sql', 'google cloud', 'gcp', 'azur', 'spss', 'bi', 'aw', 'python', 'tableau', 'r', 'cloud', 'excel', 'power bi', 'rubi']","['visual', 'predict', 'statist', 'etl', 'commun', 'math']",1,"['sql', 'google cloud', 'gcp', 'azur', 'spss', 'powerbi', 'aw', 'python', 'tableau', 'r', 'cloud', 'excel', 'rubi', 'visual', 'predict', 'statist', 'etl', 'commun', 'math']","['day', 'azur', 'corpor', 'relat', 'visual', 'power', 'predict', 'infrastructur', 'bi', 'aw', 'python', 'comput', 'statist', 'action', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'google cloud', 'gcp', 'azur', 'spss', 'powerbi', 'aw', 'python', 'tableau', 'r', 'cloud', 'excel', 'rubi', 'visual', 'predict', 'statist', 'etl', 'commun', 'math', 'day', 'azur', 'corpor', 'relat', 'visual', 'power', 'predict', 'infrastructur', 'bi', 'aw', 'python', 'comput', 'statist', 'action', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Requisition Number: 75536
Microsoft Worldwide AI Partner of the Year, 2018
Microsoft Worldwide Modern Desktop Partner of the Year, 2018
You’ll utilize the most cutting edge technology such as machine learning, artificial intelligence, big data, IoT, and azure.
Here you will have practical, hands-on knowledge of modern data architectures and tools such as data warehousing, ETL/ELT, analytics, and the Azure cloud platform.
You should be driven to provide quality solutions to challenging problems.
You will work closely with client stakeholders and an award-winning team of engineers, architects and thought leaders to design, build and implement next-generation solutions in advanced analytics, Big Data, BI and the cloud.
You will build enterprise-grade data solutions for a variety of external clients.
Design and code solutions to tough data challenges and provide feedback on others’ work.
Work directly with client stakeholders to develop technical solutions for business cases.
4+ years of experience working with data and data analytics development within the Microsoft data platform and an excellent grasp of some of following technologies:
SQL Server, Azure SQL Database, and Azure SQL Data Warehouse
Power BI
Tableau
Analysis Services and DAX
Reporting Services
Integration Services
Azure Data Factory
PowerShell scripting
Azure Automation
2 year of experience in some of the following:
Big Data technologies such as Hadoop or HDInsight, Hive, Pig, Python, Spark, Oozie, or any of the other tools with the Hadoop ecosystem
Azure Data Lake and Azure Data Lake Analytics
Predictive analytics: R, Azure Machine Learning
Strong analytical and reasoning skills that result in clear technical execution.
Skill at translating requirements into clean, efficient, quality code
Proven ability to prioritize, self-direct and execute at velocity
Passion to deliver craftsman-quality work both individually and as part of a team
Solid communication skills with both technical and non-technical stakeholders
Desire to learn new skills and grow competencies
Bachelor’s degree in Computer Science or related discipline
Requires travel to Chicago Area client sites (local only).
In the News
•https://searchitchannel.techtarget.com/feature/Data-center-infrastructure-spending-gets-AI-boost
•https://www.insight.com/en_US/about/newsroom/press-releases/2019/07012019-insight-recognized-at-no14-on-crns-2019-solution-provider-500-list.html
•https://investor.insight.com/press-release/insight-enterprises-acquire-pcm-inc
•https://insight.hqprod.businesswire.com/press-release/insight-hosts-global-ai-competition-healthcare-innovation-cincinnati
•https://investor.insight.com/press-release/houston-schools-deploying-iot-enabled-building-safety-platform-improve-emergency
https://investor.insight.com/press-release/insight-recognized-magic-quadrant-managed-workplace-services-north-america
Today's talent leads tomorrow's success.
For a comprehensive list of physical demands and work environment for this position, click here.
Today, every business is a technology business.
Discover more at insight.com.
Founded in 1988 in Tempe, Arizona
7,400+ teammates in 19 countries providing Intelligent Technology Solutions for organizations across the globe
$7.1 billion in revenue in 2018
Ranked #417 on the 2018 Fortune 500, #12 on the 2018 CRN Solution Provider 500
2018 Dell EMC Server Partner of the Year, 2018 Intel Retail Solution Partner of the Year, 2018 Microsoft Worldwide Artificial Intelligence Partner of the Year
Ranked #23 on the 2019 Fortune 50 Best Workplaces in Technology and #5 on the Phoenix Business Journal 2018 list of Best Places to Work (Extra Large Business)
Signatory of the United Nations (UN) Global Compact and Affiliate Member of the Responsible Business Alliance
Today's talent leads tomorrow's success.
","['sql', 'spark', 'azur', 'pig', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'power bi']","['healthcar', 'boost', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'azur', 'pig', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'healthcar', 'boost', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun']","['predict', 'provid', 'python', 'etl', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'power', 'comput', 'relat', 'school']","['sql', 'spark', 'azur', 'pig', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'healthcar', 'boost', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun', 'predict', 'provid', 'python', 'etl', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'power', 'comput', 'relat', 'school']"
DE,"What You Will Do:
Partner with other architects and technical leads to collaborate, design and validate appropriate solutions and institute appropriate data governance mechanisms.
Partner with Digital Farming Platform leads to establish and steward a multi-year Data Pipelines technical capabilities roadmap.
Develop cloud-based pipeline architectures that enable massively scalable data processing, while maintaining less than linear operating cost profiles.
Evangelize and mentor others on topics such as data governance, data quality, and data management best practices and techniques.
Basic Qualifications:
BS, MS or equivalent in Computer Science or related technical field
10+ years of software engineering with OOP languages
7+ years of architectural experience developing scalable data pipelines and data processing frameworks in a cloud-native or distributed computing environment.
3+ years serving as a lead data or pipeline architect in a large-scale, cloud-based, high-volume data processing organization or business division.
Demonstrated experience writing architectural requirements and systems design documents.
Experience designing and governing scalable cloud-native backend compute capabilities (REST APIs, microservices, distributed computing frameworks, geospatial processing and indexing, messaging frameworks and paradigms, data quality management).
Expertise designing and deploying solutions on at least one cloud-based provider such as Amazon Web Services, Google Cloud Platform or Microsoft Azure.
Preferred Qualifications:
Expertise in processing and aggregating high-volume, geospatially-oriented IoT data.
Expertise in imagery processing and feature extraction from satellite and aerial sources.
Experience with layered geospatial data structures and data representations.
Expertise designing and implementing highly scalable data-intensive distributed computing solutions using modern cloud-native processing frameworks.
Experience with Amazon Web Services (EC2, S3, RDS, SQS, etc.)
is strongly preferred
Experience with a compiled JVM language, including Scala, is a plus
Working knowledge of open-source and commercial data pipeline tooling (Apache NiFi, Cloudera, Informatica, etc) strongly preferred.
Superb medical, dental, vision, life, disability benefits, and a 401k matching program
Inspire one another
Be direct and transparent
https://youtu.be/c5TgbpE9UBI or visit https://climate.com/careers
#LI-BW1
","['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'amazon web services']","['pipelin', 'geospati']",1,"['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'aw', 'pipelin', 'geospati']","['digit', 'program', 'techniqu', 'engin', 'pipelin', 'azur', 'provid', 'comput', 'relat', 'sourc', 'linear']","['google cloud', 'azur', 'scala', 'ec2', 'cloud', 's3', 'microsoft', 'aw', 'pipelin', 'geospati', 'digit', 'program', 'techniqu', 'engin', 'pipelin', 'azur', 'provid', 'comput', 'relat', 'sourc', 'linear']"
DE,"These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.
What will make you successful here?
Strong analytical and technical skills
A real passion for problem solving and learning new technology
Vision to balance speed and maintainability in solution design
The ability to handle multiple, concurrent projects
Crafting and implementing requirements, keeping projects on track, and engaging partners
Communicating complex technical details in meaningful business context
A low ego and humility; an ability to gain trust by doing what you say you will do
What you might do in your first year:
Work with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health
5+ years of full-time experience including extensive experience with healthcare data
Ability to understand and design relational data structures required
Very strong capabilities manipulating data using SQL
Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g.
Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)
Experience or willingness to learn building information pipelines utilizing Python or Java a plus
BS/MS in computer science, math, engineering, or other related fields is required.
Track record of successfully executing projects with multiple partners
Competitive salary, bonus, and health benefits
Paid gym membership
Fun, fast-paced, startup environment (with snacks)
Pre-tax savings on commute expenses
Remote flexibility
A highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.
","['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java']","['healthcar', 'problem solving', 'pipelin', 'math']",1,"['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java', 'healthcar', 'problem solving', 'pipelin', 'math']","['day', 'analyt', 'pipelin', 'python', 'comput', 'clinic', 'relat', 'engin']","['mongodb', 'sql', 'cassandra', 'redshift', 's3', 'python', 'java', 'healthcar', 'problem solving', 'pipelin', 'math', 'day', 'analyt', 'pipelin', 'python', 'comput', 'clinic', 'relat', 'engin']"
DE,"Vahidin Topcagic at vtopcagic@relevante.biz, 314-853-6670
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities of the Data Engineer:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Requirements of the Data Engineer:
Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Masters Preferred).
5+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with health care datasets, clinical data, payer/claims data, SDOH data, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
(Preferred)
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Skilled in problem-solving with strong attention to detail.
Excellent customer service skills and the ability to react diplomatically and patiently to internal and external customers.
Excellent follow-up skills paired with the ability to multi-task and determine root causes.
Strong written and verbal communication skills coupled with the ability to read, analyze and interpret technical procedures.
MCSE or equivalent is strongly desired but not required
Benefits of the Data Engineer:
Health Insurance
Dental Insurance
Life Insurance
Long Term Disability
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'postgr', 'kafka', 'excel']","['big data', 'pipelin', 'optim', 'statist', 'analyz', 'commun']",1,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'excel', 'big data', 'pipelin', 'optim', 'statist', 'analyz', 'commun']","['stream', 'python', 'quantit', 'sourc', 'analyt', 'hadoop', 'optim', 'statist', 'action', 'clinic', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'excel', 'big data', 'pipelin', 'optim', 'statist', 'analyz', 'commun', 'stream', 'python', 'quantit', 'sourc', 'analyt', 'hadoop', 'optim', 'statist', 'action', 'clinic', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"For this position the candidate needs to have experience with Spark, Scala, and AWSCloud Infrastructure.
The candidate will need to demonstrate strong attention to details, proactive thinking, the ability to adapt to change quickly, and teamwork.
","['scala', 'spark']",[None],999,"['scala', 'spark']","['infrastructur', 'spark']","['scala', 'spark', 'infrastructur', 'spark']"
DE,"The lead must be able to collaborate with offshore and nearshore resources using agile methodology.In addition, the role requires interfacing with project teams to ensure data solution meets each target operating model.
Other responsibilities include supporting Enterprise BI & Analytics organization self-service analytics and data modelling strategy.Responsibilities* Provide facing with project or technical teams on solution design and data delivery to meet the required SLAs.
* Analyze docket or business data requirements, estimate work effort and coordinate with PM to meet target deliveries.
* Mentor development team.
Decide whether to develop new or reuse services from DV library.
Responsible for reviewing and cataloging published data services.
* Assist EMS via separation of duty process to ensure timely deployment and testing.
* Ability to work with architects to deliver POC to integrate Composite with variety of new technology in the bank such as Kafka, Alteryx, MongoDB, GreenPlum, etc..* Able to efficiently manage time and multi-tasking as role requires working on multiple projects and attending to firecalls.
* Provide L3 application support for Data Virtualization platform.
Assist L1/L2 team to ensure platform is operational 24x5.
* Serve as an expert resource or go to person for CDV application team.
Assist with reviewing and testing of host server patching and software upgrade.
* Stay on top of the new innovations in data management and data integration technologies.
Help support their adoption at NT.Knowledge/Skills* 10+ years of overall IT experience* 5+ years of hands-on knowledge and administration of Tibco Data Virtualization (aka CDV or Composite in the bank).
","['mongodb', 'bi', 'kafka']",['analyz'],999,"['mongodb', 'powerbi', 'kafka', 'analyz']","['analyt', 'provid', 'integr', 'bi']","['mongodb', 'powerbi', 'kafka', 'analyz', 'analyt', 'provid', 'integr', 'bi']"
DE,"The Business
GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.
The Role
You are the Scotty Pippin to the Michael Jordans.
You are the Xavi to the Messis.
You'll do things like:
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience
Non negotiables:
SQL
Python
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract
Salary: $100,000.00 /year
Work Remotely:
Yes
","['sql', 'tableau', 'python']","['commun', 'pipelin']",2,"['sql', 'tableau', 'python', 'commun', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']","['sql', 'tableau', 'python', 'commun', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']"
DE,"The overarching responsibilities for this role are (i) on-boarding successfully and efficiently new clients, (ii) maintaining the data and ecosystem on an ongoing basis and introducing & implementing improvements as needed.
The successful applicant will be joining an open and diverse team with a ""Can Do"" attitude, and a strong desire to make an impact in a start-up organization.
RESPONSIBILITIES & TASKS
Optimize and manage existing clientsâ data pipelines
Monitor, maintain, and, if needed rectify, various clientsâ data integrity
Develop and/or enhance automated processes to proactively identify any data related issues and/or simplify process
Gather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution base
Collaborate with team members to automate queries as needed
REQUIRED QUALIFICATIONS
Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field.
Working and practical knowledge of programming principles, techniques, standards and analytical ability
Strong proficiency in Java
Proven experience with complex SQL query design and optimization
Experience with Bash scripting
Experience in data cleansing, curation, parsing, integration, semantic mapping, or editing
Experience with analytics systems (data warehouses, dimensional models, etc.)
Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environment
Strong team player with a passion for data and problem solving
Excellent oral and written communication skills
PREFERRED QUALIFICATIONS
Degree in Computer Science, Computer Engineering, Management Information Systems or related field
1-3 years of applied data engineering-related experience
Strong competence in Python
Experience with Google Cloud
Experience with Linux/Unix
PDIâs employee-oriented culture provides a supportive and dynamic work environment for high achievers.
Powered by JazzHR
","['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel']","['pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math']",1,"['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel', 'pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math']","['basi', 'analyt', 'techniqu', 'pipelin', 'relat', 'divers', 'power', 'optim', 'employe', 'python', 'comput', 'appli', 'statist', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel', 'pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math', 'basi', 'analyt', 'techniqu', 'pipelin', 'relat', 'divers', 'power', 'optim', 'employe', 'python', 'comput', 'appli', 'statist', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Exact Job Location/Work Address Richmond, VA/Chicago, IL
Project Duration (relevant for CWR) 7+ Months
Required Technologies
Strong Programming experience with object-oriented/object function scripting languages: Python/Scala, Spark.
Experience with big data tools: Hadoop, Apache Spark etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
(Nice to have)
Experience with relational SQL, Snowflake and NoSQL databases
They should also have working experience using the following software/tools:
3+ years of experience (Mid-level) Strong Programming experience with object-oriented/object function scripting languages: Python/Scala, Spark.
3+ years of experience (Mid-level) Experience with big data tools: Hadoop, Apache Spark, Kafka, etc
1+ years of experience Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
(Nice to have)
1+ Years of experience Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Responsibilities for Data Engineer:
Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'Big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Preferred Skills Python/Scala, Hadoop, Apache Spark, AWS, Snowflake/SQL knowledge
Years of Experience Required 5+
Headquartered in the U.S., a member of the NASDAQ-100 is ranked 205 on the Fortune 500 and is consistently listed among the most admired companies in the world.
","['sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'postgr', 'cloud', 'kafka']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Architect, build and refine data warehouse that is scalable, efficient, and flexible, as new games and features are introduced.
Develop and maintain ETL pipelines from multiple data sources -- telemetry, sales, product cost, marketing, etc -- to fuel data applications and business intelligence.
Implement best-practices with metadata files and data models.
Documenting pipeline details, changes to telemetry, and other important information.
Load new data sources into a centralized data warehouse.
Discovering and fixing data quality, structure and integrity issues.
Defining telemetry details to add or change existing event data.
Identifying data and structure needed in order to analyze mobile games with different design patterns.
Leading large projects on a data & analytics roadmap.
Deploying and supporting predictive models to optimize marketing or operations.
Writing and monitoring JIRA/development tickets.
3+ years of experience working as a data architect or engineer.
3+ years of experience in building data pipelines.
3+ years of writing SQL.
B.S.
or M.S.
in Computer Science, Mathematics, Statistics, Economics, Analytics, Engineering or equivalent combination of education and experience.
Strong working knowledge of one or more of the following tools -- Python, Scala, R, Spark, AWS.
Ability to collaborate effectively and work as part of a team at a growing start-up.
Bonus Points Awarded For
M.S.
or Ph.D. in Computer Science, Mathematics, Statistics, Economics, Analytics, Engineering or equivalent combination of education and experience.
Experience working on mobile game analytics and/or on medical analytics.
Experience in AWS and Snowflake.
Experience in architecting data structures for business intelligence.
Multiple health insurance plans with 100% company-paid premiums
401(k) with company-paid match
Dental and vision insurance
Pre-tax flex spending and commuter accounts
Paid vacation, sick days and holidays
Unlimited cold brew and gourmet coffee, kombucha, Bevi sparkling water, and craft beer
Full kitchen and food options including breakfast, and both healthy and comfort snacks
Team after-hours events, like Board Game Night
Interested?
No Agencies or Recruiters, please.
Legal authorization to work in the U.S. is required.
","['sql', 'spark', 'scala', 'jira', 'aw', 'snowflak', 'python', 'r']","['pipelin', 'predict', 'statist', 'etl', 'econom', 'account']",1,"['sql', 'spark', 'scala', 'jira', 'aw', 'snowflak', 'python', 'r', 'pipelin', 'predict', 'statist', 'etl', 'econom', 'account']","['day', 'analyt', 'spark', 'pipelin', 'predict', 'aw', 'python', 'comput', 'statist', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'scala', 'jira', 'aw', 'snowflak', 'python', 'r', 'pipelin', 'predict', 'statist', 'etl', 'econom', 'account', 'day', 'analyt', 'spark', 'pipelin', 'predict', 'aw', 'python', 'comput', 'statist', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"Plenty of perks and benefits included upon hire.
Experiences:
5+ years of experience working with enterprise data platforms, building and managing data lakes and using big data technologies
2+ years of experience with Spark using Python/Scala.
Experience with Spark streaming, building real time data pipelines is preferred
2+ years of experience working with AWS platform.
Experience with solutioning on AWS infrastructure using services like AWS S3, Lambda, EMR, Redshift (or Snowflake)
Experience with automating and orchestrating jobs on a big data platform using Oozie, Airflow, Jenkins or something similar
Good understanding and experience working with various products in the Big data ecosystem like Hive, HDFS, Presto, NoSQL databases like Cassandra, DynamoDB
Experience with setting up and using Kafka for real time streaming is a big plus
Has to be a team player and open to working with newer technologies as well as supporting legacy systems
Prior experience with working in a SQL server based environment and using SSIS, SSRS, TSQL is a plus.
Prior experience with traditional ETL tools like Talend Open Studio, Pentaho or something similar is a plus
Job is also known as: Data Engineer
GDMY
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ssr', 'aw', 'lambda', 'redshift', 'python', 's3', 'nosql', 'snowflak', 'hive', 'kafka', 'pentaho']","['etl', 'pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ssr', 'aw', 'lambda', 'redshift', 'python', 's3', 'nosql', 'snowflak', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'python', 'big', 'etl', 'engin']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'ssr', 'aw', 'lambda', 'redshift', 'python', 's3', 'nosql', 'snowflak', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'python', 'big', 'etl', 'engin']"
DE,"Role responsibilities
Work with clients to model their data landscape, obtain data extracts, and define secure data exchange approaches
Plan and deliver secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for bleeding edge Advanced Analytics and Data Science
What you’ll learn
Influence, develop and master best-practice around modern tools that support rapid development of analytics solutions like Python, Spark, Airflow and others.
Learn to deeply understand the data science and data engineering process and develop impactful and reusable patterns and abstractions
Build products alongside the core engineering team and evolve the engineering process to handle more users, resolutions for complex problems and advanced client situations.
Work with an extensive range of technologies around big data tools.
Work with clear objectives for your Data Engineering expertise while collaborating with the Data Science, Machine Learning Engineer and Design teams
You will guide global companies through data science solutions to transform their businesses and enhance performance across industries including healthcare, automotive, energy and elite sport.
Innovative Work Culture – Creativity, insight and passion come from being balanced.
At QuantumBlack you have the best of both worlds; all the benefits of being part of one of the leading management consultancies globally and the autonomy to thrive in a fast growth tech culture:
","['cloud', 'python', 'spark', 'airflow']","['machine learning', 'healthcar', 'big data']",2,"['cloud', 'python', 'spark', 'airflow', 'machine learning', 'healthcar', 'big data']","['analyt', 'machin', 'learn', 'spark', 'handl', 'python', 'big', 'integr', 'sourc', 'engin']","['cloud', 'python', 'spark', 'airflow', 'machine learning', 'healthcar', 'big data', 'analyt', 'machin', 'learn', 'spark', 'handl', 'python', 'big', 'integr', 'sourc', 'engin']"
DE,"Data Engineer
Requisition Number: 75536
Microsoft Worldwide AI Partner of the Year, 2018
Microsoft Worldwide Modern Desktop Partner of the Year, 2018
You'll utilize the most cutting edge technology such as machine learning, artificial intelligence, big data, IoT, and azure.
Here you will have practical, hands-on knowledge of modern data architectures and tools such as data warehousing, ETL/ELT, analytics, and the Azure cloud platform.
You should be driven to provide quality solutions to challenging problems.
You will work closely with client stakeholders and an award-winning team of engineers, architects and thought leaders to design, build and implement next-generation solutions in advanced analytics, Big Data, BI and the cloud.
You will build enterprise-grade data solutions for a variety of external clients.
Design and code solutions to tough data challenges and provide feedback on others' work.
Work directly with client stakeholders to develop technical solutions for business cases.
4+ years of experience working with data and data analytics development within the Microsoft data platform and an excellent grasp of some of following technologies:
SQL Server, Azure SQL Database, and Azure SQL Data Warehouse
Power BI
Tableau
Analysis Services and DAX
Reporting Services
Integration Services
Azure Data Factory
PowerShell scripting
Azure Automation
2 year of experience in some of the following:
Big Data technologies such as Hadoop or HDInsight, Hive, Pig, Python, Spark, Oozie, or any of the other tools with the Hadoop ecosystem
Azure Data Lake and Azure Data Lake Analytics
Predictive analytics: R, Azure Machine Learning
Strong analytical and reasoning skills that result in clear technical execution.
Skill at translating requirements into clean, efficient, quality code
Proven ability to prioritize, self-direct and execute at velocity
Passion to deliver craftsman-quality work both individually and as part of a team
Solid communication skills with both technical and non-technical stakeholders
Desire to learn new skills and grow competencies
Bachelor's degree in Computer Science or related discipline
Requires travel to Chicago Area client sites (local only).
In the News
-
-
*
Today's talent leads tomorrow's success.
For a comprehensive list of physical demands and work environment for this position, click here.
Today, every business is a technology business.
Discover more at .
Founded in 1988 in Tempe, Arizona
7,400+ teammates in 19 countries providing Intelligent Technology Solutions for organizations across the globe
$7.1 billion in revenue in 2018
Ranked #417 on the 2018 Fortune 500, #12 on the 2018 CRN Solution Provider 500
2018 Dell EMC Server Partner of the Year, 2018 Intel Retail Solution Partner of the Year, 2018 Microsoft Worldwide Artificial Intelligence Partner of the Year
Ranked #23 on the 2019 Fortune 50 Best Workplaces in Technology and #5 on the Phoenix Business Journal 2018 list of Best Places to Work (Extra Large Business)
Signatory of the United Nations (UN) Global Compact and Affiliate Member of the Responsible Business Alliance
Today's talent leads tomorrow's success.
Nearest Major Market: Chicago
Job Segment: Database, Developer, Engineer, Supply, Computer Science, Technology, Engineering, Operations
","['sql', 'spark', 'azur', 'pig', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'power bi']","['segment', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'azur', 'pig', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'segment', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun']","['predict', 'provid', 'python', 'etl', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'learn', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'power', 'comput', 'relat']","['sql', 'spark', 'azur', 'pig', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'r', 'cloud', 'microsoft', 'excel', 'segment', 'predict', 'machine learning', 'clean', 'data warehousing', 'big data', 'etl', 'commun', 'predict', 'provid', 'python', 'etl', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'learn', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'power', 'comput', 'relat']"
DE,"Responsibilities
Here are just a few different responsibilities you can expect off the bat:
Some Characteristics That Define You
Self-Starter.
Building a data warehouse is no simple task.
Analytical.
Developer.
In the ever changing world of artificial intelligence, it’s not enough just to build models.
You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient.
As a data engineer, you know that you work with extremely large data sets on a daily basis.
Creative.
Student.
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Benefits
Comprehensive medical & dental insurance
Generous PTO, including sick days & holidays
A state-of-the-art office environment
Year-round gym memberships
Paid continuing education
Casual dress code
Flexible scheduling
Free-Lunch-Friday
But don’t just take my word for it.
","['azur', 'sa', 'c', 'python', 'microsoft']","['graph', 'statist']",1,"['azur', 'sa', 'c', 'python', 'microsoft', 'graph', 'statist']","['day', 'basi', 'analyt', 'azur', 'relat', 'sa', 'python', 'comput', 'statist', 'warehous', 'set', 'engin']","['azur', 'sa', 'c', 'python', 'microsoft', 'graph', 'statist', 'day', 'basi', 'analyt', 'azur', 'relat', 'sa', 'python', 'comput', 'statist', 'warehous', 'set', 'engin']"
DE,"In this role, you will work with other members of Engineering, Product and Project Management, and various business groups to ensure timely availability of usable data to all parts of the business that need it.
This is inclusive of interviewing, onboarding and each role day-to-day.
Responsibilities:
Design, develop and deploy batch and streaming data pipelines.
Monitor and ensure operational stability of data pipelines.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Skills and Experience:
Bachelor's Degree in computer science or equivalent experience required.
4+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Experience extracting data from relational and document databases.
Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.
Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).
Proficiency in Java or Python programming languages.
Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.
Benefits and Perks:
Open vacation policy
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
*LI-JC1
","['sql', 'git', 'snowflak', 'python', 'redshift', 'java']","['pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account']",1,"['sql', 'git', 'snowflak', 'python', 'redshift', 'java', 'pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account']","['day', 'analyt', 'program', 'pipelin', 'relat', 'avail', 'python', 'comput', 'employe', 'engin']","['sql', 'git', 'snowflak', 'python', 'redshift', 'java', 'pipelin', 'data modeling', 'data warehousing', 'problem solving', 'analyz', 'account', 'day', 'analyt', 'program', 'pipelin', 'relat', 'avail', 'python', 'comput', 'employe', 'engin']"
DE,"Data Engineer
US-IL-Chicago
Chicago, IL
Full Time Perm
Do you love working in a collaborative environment with free breakfast, coffee/tea/soda, social events and much more?
Who You Are As a Data Engineer, you will be responsible for data management tasks including design, development, and technical administration in an AWS environment.
You will also provide technical leadership to the team and be responsible for maintaining technical specifications.
As a Data Engineer, you will have a heavy focus on designing the solutions to deliver data products.
To be successful in this role you will need a solid understanding of data management concepts and how cloud technology can solve data issues.
Responsibilities and Duties As the Cloud Data Architect, your responsibilities will include, but are not limited to:
• Oversight of the design and standards of AWS (S3, Redshift/Snowflake/SQL Server, Glue,) data applications • Train and coach data team developers • Oversee and implement security features, access and standards around data management • Assist in capacity and budgeting for data systems • Provide estimates and oversight within a Scrum environment • Strong SQL skills in data warehouse environment • Spark, Python and/or Scala experience • Lead code reviews Qualifications and Skills • At least 3 years IT experience in AWS data services • Hands on experience working with complex Data Warehouses and or customer linking systems • Data Lake experience using Spark, Scala, EMR and/or Glue • Data Modeling experience • Proficient with SQL • Solid S3 understanding • Experience with AWS Aurora, Oracle and/or SQL Server developer experience • Hands on experience using AWS RDS Nice to Have • Experience with Redshift or Snowflake using Matillion or other ETL tools • Identity & Access Management (Security Provisioning) understanding and knowledge
","['sql', 'spark', 'scala', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'oracl']","['etl', 'data modeling']",999,"['sql', 'spark', 'scala', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'oracl', 'etl', 'data modeling']","['spark', 'aw', 'provid', 'python', 'warehous', 'etl', 'engin']","['sql', 'spark', 'scala', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'oracl', 'etl', 'data modeling', 'spark', 'aw', 'provid', 'python', 'warehous', 'etl', 'engin']"
DE,"Responsibilities Include:
Product Mgt) and other Data Engineering personnel to understand department-level data requirements.
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the platform.
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the platform.
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure consistently high-quality data.
Support Operations: triage alerts channeled to you and remediate as necessary.
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development.
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
",[None],"['etl', 'pipelin']",999,"['etl', 'pipelin']","['program', 'pipelin', 'provid', 'etl', 'sourc', 'engin']","['etl', 'pipelin', 'program', 'pipelin', 'provid', 'etl', 'sourc', 'engin']"
DE,"Kafka, MongoDB, Neo4j is a plus Experience with developing software products for cloud platform, e.g.
AWS.
Experience in creating and customizing Reports and Dashboards Experience working on Large enterprise level data sets and ETL Experience in Data analysis, data consolidation and normalization Comfortable on Linux for both development and operations Experience with advanced analytics and modern machine learning techniques is a plus Experience with cloud native services such as AWS EMR
","['mongodb', 'linux', 'aw', 'cloud', 'kafka']","['machine learning', 'dashboard', 'etl', 'normal']",999,"['mongodb', 'linux', 'aw', 'cloud', 'kafka', 'machine learning', 'dashboard', 'etl', 'normal']","['analyt', 'machin', 'techniqu', 'set', 'aw', 'etl']","['mongodb', 'linux', 'aw', 'cloud', 'kafka', 'machine learning', 'dashboard', 'etl', 'normal', 'analyt', 'machin', 'techniqu', 'set', 'aw', 'etl']"
DE,"scripting languages) to marry systems together+ Recommend ways to improve data reliability, efficiency and quality+ Collaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmaps.+ Achieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies.+ Mentor team members and acts as a technical lead for less senior team members.location: Chicago, Illinoisjob type: Permanentsalary: $90,000 - 120,000 per yearwork hours: 8am to 5pmeducation: Bachelorsresponsibilities:**ESSENTIAL DUTIES AND RESPONSIBILITIES**+ Work closely with business and technical teams to deliver enterprise grade datasets that are reliable, flexible, scalable, and provide low cost of ownership.+ Analyzes and estimates feasibility, costs, time, and resources needed to develop, and implement enterprise datasets as needed.+ Create and advocate the use of standards around data documentation.+ Advocate for the use of enterprise datasets on advanced analytical applications at ABC.
scripting languages) to marry systems together+ Recommend ways to improve data reliability, efficiency and quality+ Collaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmaps.+ Achieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies.+ Mentor team members and acts as a technical lead for less senior team members.qualifications:+ Experience level: Experienced+ Minimum 6 years of experience+ Education: Bachelorsskills:+ Data Warehouse+ data engineering (5 years of experience is required)+ Tableau+ Azure+ PythonEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.
",[None],"['recommend', 'analyz']",999,"['recommend', 'analyz']","['analyt', 'essenti', 'learn', 'engin']","['recommend', 'analyz', 'analyt', 'essenti', 'learn', 'engin']"
DE,"You would be part of their Marketing Analytics team, specifically on a customer - related project.
The role would need you to extract marketing datasets to be transferred to EDH, set up infrastructure, streaming data to AWS, and so on.
Above all, the team needs a data engineer with strong DevOps and AWS experience.Required Skills & Experience* 3+ years of professional experience* Python* Programming and DevOps experience* Kubernetes and Docker* AWS (S3, Lambda, etc.
)* Hadoop, Hive, Apache Spark, and other Big Data technologies* B.S.
in Computer Science or related fieldDesired Skills & Experience* Data warehousing experience* Certification in cloud computing or big data* Visualization tools like Tableau* M.S.
in Computer Science or related fieldWhat You Will Be DoingTech Breakdown* 80% Data Engineering* 20% DevOpsDaily Responsibilities* 85% Hands On* 15% Team CollaborationThe Offer* Competitive Pay: Up to $50/hour, DOE* Contract Duration: 3 MonthsYou will receive the following benefits:* Medical & Dental Insurance* Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Pre-tax Commuter Benefit* Add additional perks specific to the work environmentApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['spark', 'hadoop', 'aw', 'lambda', 'python', 'hive', 's3', 'tableau', 'cloud', 'kubernet', 'docker']","['data warehousing', 'visual', 'big data', 'account']",1,"['spark', 'hadoop', 'aw', 'lambda', 'python', 'hive', 's3', 'tableau', 'cloud', 'kubernet', 'docker', 'data warehousing', 'visual', 'big data', 'account']","['basi', 'analyt', 'program', 'spark', 'visual', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'relat', 'engin']","['spark', 'hadoop', 'aw', 'lambda', 'python', 'hive', 's3', 'tableau', 'cloud', 'kubernet', 'docker', 'data warehousing', 'visual', 'big data', 'account', 'basi', 'analyt', 'program', 'spark', 'visual', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'relat', 'engin']"
DE,"Are you ready to transform an industry?
ABOUT THE TEAM
• Ruby, Java, Python, and React.js
• Kubernetes, Docker, Kafka
• PostgreSQL, NoSQL
• AWS
ABOUT THE ROLE
YOU WILL
Participate in architecture discussions and bring your experience in scalable data pipelines using Kafka Streams and/or other Big Data tools.
Take ownership of design and implementation of scalable and fault tolerant projects.
Maintain and incrementally improve existing solutions.
Get to build brand new pipelines with the technology stack including Spark, Spark Structured Streaming, Kafka, Hadoop, MySql, Python.
Solid understanding of distributed system fundamentals.
Experience in developing, troubleshooting, diagnosing, and performance tuning of distributed data pipelines at scale.
Demonstrated professional experience working with various components of Big Data ecosystem: Spark/Spark Streaming, Hive, Kafka/KSQL, Hadoop (or similar NoSQL ecosystem), et.
al, in a production system.
Strong software engineering skills with Python.
Knowledge of some flavor of SQL (MySQL, Oracle, Hive, Impala), including the fundamentals of data modeling and performance.
EVEN BETTER IF YOU HAVE
Skills in real-time streaming applications.
Knowledge of Scala.
A development workflow using Docker containers.
Compulsion for automating your day-to-day processes.
Ability to see your direct impact on a high visibility project.
An opportunity to both create new projects and help improve the existing big data pipelines.
401K plan with employer matching.
Commuter pre-tax contributions.
Flexible working hours and work-from-home days.
Health plan.
In-office snacks.
Organized team events.
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'mysql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi']","['tune', 'data modeling', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi', 'tune', 'data modeling', 'pipelin', 'big data']","['day', 'stream', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'rubi', 'tune', 'data modeling', 'pipelin', 'big data', 'day', 'stream', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'big', 'engin']"
DE,"What will your day look like?
You will assist the organization through the continued build-up and operationalization of an enterprise class Modern Data environment, including various components within the Hadoop stack.
Resources to do the job require substantial hands-on experience working with the technologies encompassed within the Hadoop technology stack while also having knowledge and capabilities as a systems developer.
The Senior Data Engineer coordinates, designs, builds, and integrates complex application technology solutions, aligned to architectural standards and definitions, and will help ensure IT services are delivered effectively and efficiently.
Responsibilities
Do you see yourself doing this?
Responsible for day to day operation and support of Hadoop and Modern Data environments
Collaborate with data center and systems engineering teams on all cluster infrastructure setup, software installation, testing, upgrading/patching, monitoring, tuning/optimizing, troubleshooting, maintenance
Collaborate with development and strategy teams on component and 3rd party tool identification, recommendation, installation and management of Spark jobs
Collaborate with the data architecture and Infrastructure teams in technical investigations, development, and prototypes
Collaborate with Corporate IT function around integrating Hadoop ecosystem(s) with critical enterprise systems
Provide hardware architectural guidance
Develop and manage all cluster related testing activities
Create roadmaps for ongoing cluster deployment and growth
Perform capacity monitoring and capacity planning on infrastructure and resources
Manage cluster hardening activities through the implementation and maintenance of security and governance components across various clusters
Participate in the design and implementation of a Disaster Recovery strategy for all Modern Data components
Participate in design, implementation and management of alignment activities with all pertinent audit and compliance activities
Function as expert consulting resources around Hadoop integration points with any ETL, BI, and EDW teams
Provide input/develop new processes/standards in support of the organization's business/functional short-term strategies, with limited impact on the business/function overall results
Influence adoption of Modern Data new concepts, practices, and approaches
Design, build, deploy and maintain data pipelines using NiFi/Kafka/Spark Streaming or related data integration technologies
Qualifications
What makes you a great fit?
You’ll be a great fit if in addition to the completion of a Bachelor’s degree in Computer Science or a related field, required, and you have:
5+ years’ Cloudera or Hortonworks experience in IT Data Development or Data Related Support teams
Proven development and operational experience within Hadoop ecosystem (Spark/Python, HDFS, YARN, Hive, HBase, Sqoop, etc.
), preferable with Hortonworks or Cloudera distribution
High proficiency in Java, SQL, and Linux shell (Scala/Cascading experience a plus)
Expert knowledge of key data structures and algorithms in Hadoop, Cloudera or Hortonworks systems
Experience with the entire Software Development Lifecycle (SDLC) process such as change management, defect and issue tracking, to resolve data issues or to implement development enhancements
Hands on experience with monitoring tools (preferably Ambari, Nagios, etc.)
Familiar with OS, network configuration, protocols, and enterprise security solutions such as LDAP and/or Kerberos
Knowledge of metadata management and governance capabilities using Atlas
Familiarity with Data Science notebooks such as Apache Zeppelin, Jupyter
Automation experience with Chef, Puppet, or Ansible
Prior experience working in Financial Services industry preferred
Project Management experience with agile and project management methodologies (Scrum and/or Kanban)
Excellent written and verbal communication skills
Has an analytical and problem-solving mindset
Is highly organized and efficient
Ability to leverage strategic and tactical thinking
Works calmly under pressure and with tight deadlines
Demonstrates effective decision-making skills
Competitive medical, dental, and free vision benefits
Competitive compensation plan
Contributions towards gym memberships
Generous PTO and banking holidays off
Still not convinced?
","['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'bi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel']","['recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster']",1,"['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'powerbi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel', 'recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster']","['day', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'input', 'hadoop', 'corpor', 'infrastructur', 'bi', 'parti', 'engin', 'particip', 'spark', 'pipelin', '3rd', 'comput', 'relat']","['sql', 'linux', 'spark', 'scala', 'jupyt', 'hadoop', 'powerbi', 'python', 'hive', 'java', 'hbase', 'kafka', 'excel', 'recommend', 'pipelin', 'hardwar', 'etl', 'commun', 'cluster', 'day', 'stream', 'provid', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'input', 'hadoop', 'corpor', 'infrastructur', 'bi', 'parti', 'engin', 'particip', 'spark', 'pipelin', '3rd', 'comput', 'relat']"
DE,"RESPONSIBILITIES
Develop implementation patterns leveraging AWS technologies to support.
Understand business and technical requirements.
Develop Conceptual and Logical Data Solution for data acquisition, data models and pipelines.
Review Solution Data Designs, Models, Pipelines Prototype New Solutions Technical guidance to data designers and developers Solution Implementation Reviews.
TYPICAL DAY
Understand business and technical requirements.
Develop Conceptual and Logical Data Solution for data acquisition, data models and pipelines.
Review Solutions with Platform, Business, and Application teams.
Position Requirements:
S. in Computer Science, Information Systems, or related major or equivalent IS / business experience.
10+ years of experience designing, implementing data persistence and processing solutions.
AWS Certifications
TECHNICAL SKILLS
Required Skills
AWS Cloud Based Technologies for Data Processing and Persistance (DynamoDB, S3, Aurora, Kinesis, SQS, SNS, Lambda, Fargate, Glue).
Ability to design and communicate solutions to meet business and technical requirements.
Demonstrated Data Architecture and Design for Big Data, Analytics, and applications.
SOFT SKILLS
Strong written and verbal communication.
Able to produce architecture and design artifacts in Visio and Office 365.
Additional Information:
An Equal Opportunity Employer
","['aw', 'lambda', 's3', 'cloud']","['commun', 'pipelin', 'big data']",999,"['aw', 'lambda', 's3', 'cloud', 'commun', 'pipelin', 'big data']","['day', 'analyt', 'pipelin', 'aw', 'comput', 'big', 'relat']","['aw', 'lambda', 's3', 'cloud', 'commun', 'pipelin', 'big data', 'day', 'analyt', 'pipelin', 'aw', 'comput', 'big', 'relat']"
DE,"As a Data Engineer, you will implement the data models and data structures needed for each use case, in the most convenient format to be used by the Data Science and Business Intelligence teams.
Through regular interactions with stakeholders and functional business unit leaders, you will build high-performance algorithms, predictive models, and prototypes that influence data storage, piping, and usage.
Additionally, you will participate in data requirements, modeling and testing activities.
Each day will be unique, requiring an ability to think strategically and on your feet, be creative, take initiative, and employ a diverse set of skills.
WHO YOU ARE
Knowledgeable, Analytical, and Solution-Oriented.
Without a doubt, you(TM)ve got strong quantitative skills and are comfortable analyzing large data set, spotting trends and patterns, and synthesizing relevant observations.
You use a hypothesis-driven approach to engage in analysis that will deliver on your client questions.
You like thinking outside the box to come up with innovative points of view on new challenges, relying on your previous analytic work and experience to help guide you along the way.
Results-Oriented.
You demonstrate an inherent sense of urgency to drive great results, while being precise in executing your work.
You are facile with creating and communicating a clear project plan, tracking progress, and keeping your business partners in the loop along the way.
Intellectually Curious.
You're inherently interested in the ""why"" so that you can identify opportunities that represent unconventional solutions to the problems you are trying to solve.
Strong Communicator.
Your writing and speaking skills are concise, articulate, and effective, providing an ability to interact with all levels/various teams across the organization, be understood, and develop trust and rapport within the organization.
Technologically Savvy.
Microsoft Excel is a basic tool to you that you know like the back of your hand.
You also have a strong skill set in R, Python, ArcGIS, machine learning, neural networks and/or other advanced analytics tools and techniques.
A Trusted Team Player.
You enjoy partnering with others and build constructive working relationships that foster the collaboration necessary to deliver great results.
You are accountable to your teammates and follow through on commitments.
Organized and Confident.
You are flexible, composed, and able to prioritize multiple tasks and deadlines simultaneously, while confidently interacting with a variety of individuals, across all levels of the organization.
You handle pressure well and do so with confidence.
WHAT YOU(TM)LL DO
Create data models and data processes, providing the right format and structure for use case solutions.
Participate in early data modeling and testing for use case development, providing input on how to improve proposed solutions and implement necessary changes.
Help to build, document, and maintain best practices, including but not limited to codebase management, work and issue tracking, testing and quality control/assurance measures, data dictionaries, and a documentation hub for both production level code and ad hoc analyses.
Interact with stakeholders and functional subject matter experts to understand all data requirements in order to develop effective business insights and translate them into actionable data structures and data models.
Assemble large, complex data sets that meet both functional and non-functional business requirements.
Extract relevant data to solve analytical challenges the organization and/or functional business units may face.
Work closely with IT teams on internal data acquisition (e.g., CRM, ERP, etc.).
Partner with stakeholders to provide technical support related to data structures, data models, data management and data infrastructure needs.
Recommend different ways to constantly improve data reliability and quality.
Research new uses for existing data.
Produce various reports for stakeholders, as requested, to highlight areas of opportunity; works with teams to develop and implement changes, as needed.
Develop and maintain formal documentation that describes data and data structures, including data modeling.
PREVIOUS EXPERIENCE & REQUIREMENTS
Bachelor's Degree required, preferably in computer science, software/computer engineering, applied mathematics, or physics statistics.
Minimum 2 years data modeling experience and working with data management systems; deep expertise in data modeling and structuring required.
2+ years experience in high volume data environments and core data engineering activities (i.e.
familiarity with cloud database set up, automation scheduling using directed acyclic graph (such as Airflow) and database optimization, including but not limited to partitioning, group and sort keys, and indexes).
Familiarity with a broad base of analytical methods e.g.
data modeling (variable transformation and summarization) and processing (i.e.
Spark, SQL Server, Hadoop/Hive, neo4j, etc).
Strong attention to detail and ability to think critically/conceptually.
Team oriented and flexible with proven track record in collaborating with multiple stakeholders.
Effective written and verbal communication skills required.
Demonstrated ability to quickly learn new technologies a must.
Ability to think creatively when problem solving for new solutions and to work on numerous projects concurrently while effectively prioritizing workload.
Tolerance for ambiguity required.
Tools/software:
Familiarity with data loading and management tools (i.e.
Azure Storage""BlockBlob and relational and NoSQL databases and tools such as SQL Server, MongoDB, Data Stax, etc) required.
Must have programming and/or scripting experience (Python, Java) as well as experience with version control systems (Git/GitHub), continuous integration (circleCI) and other programming frameworks/approaches.
Proficiency in MS and Google application suites.
Must be available for overnight travel (approximately 10%)
","['mongodb', 'sql', 'erp', 'spark', 'airflow', 'azur', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'microsoft', 'r', 'github', 'excel']","['recommend', 'research', 'graph', 'hypothesi', 'neural network', 'predict', 'machine learning', 'optim', 'data modeling', 'problem solving', 'statist', 'commun', 'account']",1,"['mongodb', 'sql', 'erp', 'spark', 'airflow', 'azur', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'microsoft', 'r', 'github', 'excel', 'recommend', 'research', 'graph', 'hypothesi', 'nn', 'predict', 'machine learning', 'optim', 'data modeling', 'problem solving', 'statist', 'commun', 'account']","['day', 'program', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'input', 'hadoop', 'optim', 'avail', 'appli', 'statist', 'action', 'infrastructur', 'concis', 'set', 'engin', 'particip', 'machin', 'spark', 'divers', 'comput', 'relat']","['mongodb', 'sql', 'erp', 'spark', 'airflow', 'azur', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'microsoft', 'r', 'github', 'excel', 'recommend', 'research', 'graph', 'hypothesi', 'nn', 'predict', 'machine learning', 'optim', 'data modeling', 'problem solving', 'statist', 'commun', 'account', 'day', 'program', 'predict', 'python', 'quantit', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'input', 'hadoop', 'optim', 'avail', 'appli', 'statist', 'action', 'infrastructur', 'concis', 'set', 'engin', 'particip', 'machin', 'spark', 'divers', 'comput', 'relat']"
DE,"*What if there was a different way of looking at money?
This is a great opportunity to join a Fortune 500 and not-for-profit organization that gives you a great sense of purpose!As a full-time Data Engineer you will work closely with the Data Architect as they provide guidance and vision so you can develop, construct and maintain the data architecture for the enterprise data professionals.
You will work with large scale data processing systems that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
Communication is key in this role as you will work with users of all levels across the organization.Job DescriptionJob Duties and Responsibilities:* Oversee and direct efforts to identify information and technology solutions that enable business needs and strategies.
* Lead efforts to analyze IT industry and market trends and determine potential impacts.
* Develop concepts and constructs necessary to create technology-enabled business systems.
* Influence technology direction and provide thought leadership and execution to large complex efforts.
* Utilize breadth of technical understanding and dive deep when necessary.
* Consult on and manage initiatives to ensure alignment across multiple business and IT areas.
* Proactively mitigate risks across multiple assets, information domains, technologies and platforms.
* Provide leadership, mentoring and technical guidance to others to drive initiatives.
* Facilitate communications that involve obtaining cooperation and agreement on issues that may be complex or controversial.
* Utilize negotiation and persuasion to come to agreement and to effectively form partnerships.
* Act as a change agent to continuously improve and move the organization forward.
* Accountable to successfully deliver the right results on initiatives in a timely and effective manner.
* Direct the work of others to lead initiatives that cross multiple assets, technologies, platforms, departments and vendors.
* Ability to work within a diverse team of skillsets and experience levels to deliver results.Required Job Qualifications:* Bachelor's degree or equivalent experience in MIS, Computer Science, Mathematics, Business, or related field.
* 10+ years of experience in Technology related field including 3+ years prior lead experience.
* Expert knowledge of predictive analytics, statistical modeling, advanced mathematics, data integration concepts, business intelligence and data warehousing and implementing large systems* Implement and configure data platforms including but not limited to Hadoop, Spark, Kafka and batch integration is preferred.
","['kafka', 'spark', 'hadoop']","['risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']",1,"['kafka', 'spark', 'hadoop', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']","['analyt', 'asset', 'techniqu', 'spark', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'comput', 'statist', 'integr', 'sourc', 'engin']","['kafka', 'spark', 'hadoop', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account', 'analyt', 'asset', 'techniqu', 'spark', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'comput', 'statist', 'integr', 'sourc', 'engin']"
DE,"Kafka, MongoDB, Neo4j is a plus Experience with developing software products for cloud platform, e.g.
AWS.
Experience in creating and customizing Reports and Dashboards Experience working on Large enterprise level data sets and ETL Experience in Data analysis, data consolidation and normalization Comfortable on Linux for both development and operations Experience with advanced analytics and modern machine learning techniques is a plus Experience with cloud native services such as AWS EMR
","['mongodb', 'linux', 'aw', 'cloud', 'kafka']","['machine learning', 'dashboard', 'etl', 'normal']",999,"['mongodb', 'linux', 'aw', 'cloud', 'kafka', 'machine learning', 'dashboard', 'etl', 'normal']","['analyt', 'machin', 'techniqu', 'set', 'aw', 'etl']","['mongodb', 'linux', 'aw', 'cloud', 'kafka', 'machine learning', 'dashboard', 'etl', 'normal', 'analyt', 'machin', 'techniqu', 'set', 'aw', 'etl']"
DE,"Locations Richmond VA, McLean, VA, Wilmington, DE, Chicago, IL Required Technologies Strong Programming experience with object-orientedobject function scripting languages Python, PySpark, Scala, etc.
Experience with big data tools Hadoop, Apache Spark, Kafka, etc.
Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Should also have working experience using the following softwaretools Strong Programming experience with object-orientedobject function scripting languages Python, PySpark, Scala, etc.
Experience with big data tools Hadoop, Apache Spark, Kafka, etc.
Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'postgr', 'cloud', 'kafka']","['optim', 'pipelin', 'big data']",999,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Data Engineer
Contribute to all aspects of the project lifecycle, including requirements gathering, design, development, testing, and deployment.
Become familiar with Elkay’s business model and the various business functions that utilize Elkay’s analytics infrastructure, including sales, marketing, supply chain, operations, pricing, finance, and customer care.
Keep abreast of trends, architectures, and tools in the analytics landscape and evaluate them for incorporation into Elkay’s analytics landscape.
Learn quickly and juggle multiple tasks for various customers.
Communicate complex technical problems effectively to management and to the business.
Collaborate with other technical team members on solutions.
Effectively prioritize support tasks with project work.
Specific duties include:
Design, develop and maintain data models, database architectures, and associated database objects in Snowflake, Oracle, and other database solutions such as Azure.
Design, develop, and maintain data integrations using Informatica Power Center, Informatica Integrated Cloud Services, and data prep tools.
Participate in or drive project activities such as requirements gathering, design, develop, test, and deploy.
Assist in the set-up of, and administer, on premise and cloud tools used in the Elkay analytics infrastructure.
Create and maintain necessary technical documentation, including requirements, design, and test documents.
Identify emerging trends, processes, and techniques impacting Elkay’s analytics infrastructure and make suggestions for incorporation of these into the analytics infrastructure.
Must have's:
Hands on experience in writing and understanding complex SQL (e.g.
CTE’s others).
Nice to have's:
EOE/M/F/D/V/SO
","['sql', 'azur', 'cloud', 'snowflak', 'oracl']","['commun', 'financ']",999,"['sql', 'azur', 'cloud', 'snowflak', 'oracl', 'commun', 'financ']","['analyt', 'techniqu', 'learn', 'azur', 'set', 'power', 'infrastructur', 'integr', 'engin', 'particip']","['sql', 'azur', 'cloud', 'snowflak', 'oracl', 'commun', 'financ', 'analyt', 'techniqu', 'learn', 'azur', 'set', 'power', 'infrastructur', 'integr', 'engin', 'particip']"
DE,"It has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day and evaluate thousands of data-points in real-time all while responding in just a few milliseconds.
What you'll do:
Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Mentor junior team members
5+ years of recent hands-on Java experience
Strong knowledge of collections, multi-threading, JVM memory model, etc.
Great understanding of designing for performance, scalability, and reliability
Superb understanding of algorithms, scalability and various tradeoffs in a Big Data setting
In-depth understanding of object oriented programming concepts
Excellent interpersonal and communication skills
Understanding of full software development life cycle, agile development and continuous integration
Good knowledge of Linux command line tools
Experience with Hadoop MapReduce, Spark, Pig
Solid understanding of database fundamentals, good knowledge of SQL
What puts you over the top:
Exposure to messaging frameworks like Kafka or RabbitMQ
Some exposure to functional programming languages like Scala
Experience with Spark
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'linux', 'spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel']","['etl', 'commun', 'big data']",999,"['sql', 'mapreduc', 'linux', 'spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'etl', 'commun', 'big data']","['day', 'program', 'spark', 'set', 'line', 'power', 'hadoop', 'algorithm', 'big', 'etl', 'collect', 'integr']","['sql', 'mapreduc', 'linux', 'spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'etl', 'commun', 'big data', 'day', 'program', 'spark', 'set', 'line', 'power', 'hadoop', 'algorithm', 'big', 'etl', 'collect', 'integr']"
DE,"Chicago, Illinois
Skills : Hadoop,big data,AWS,spark,Python,snowflake,presto,hive
MUST HAVE
AWS (s3, redshift, EMR, EC2, lambda, SNS), Unix shell scripting, Python, Spark, Scala
PREFER TO HAVE
Snowflake, Presto, Arrow, Airflow, Hadoop, Hive
Additonal Notes:
Between 3-5 years exp (can exceed 5 years), looking for mid-senior level experience
2 types of work they are doing – one is they have batch jobs which process the files from their partners and other files from their data processer.
These files currently processed in ab initio.
Trying to go from ab initio to AWS (EMR)
They have other jobs which are currently running in production, some are ab initio
Hadoop,big data,AWS,spark,Python,snowflake,presto,hive
","['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 'hive']",['big data'],999,"['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 'hive', 'big data']","['spark', 'hadoop', 'aw', 'python', 'big']","['spark', 'airflow', 'scala', 'unix', 'hadoop', 'ec2', 'aw', 'snowflak', 'python', 'redshift', 'hive', 'big data', 'spark', 'hadoop', 'aw', 'python', 'big']"
DE,"Role Data Engineer Exact Job LocationWork Address Chicago, IL Project Duration (relevant for CWR) 7+ Months Required Technologies Strong Programming experience with object-orientedobject function scripting languages Python.
Experience with big data tools Hadoop, Apache Spark etc.
Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
They should also have working experience using the following softwaretools 3+ years of experience (Mid-level) Strong Programming experience with object-orientedobject function scripting languages Python 3+ years of experience (Mid-level) Experience with big data tools Hadoop, Apache Spark, Kafka, etc 1+ years of experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
(Nice to have) 1+ Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS lsquoBig datarsquo technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
","['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'postgr', 'cloud', 'kafka']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'cassandra', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers.
You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion.
You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Mandatory Skills:
Knowledge and experience in API development, AI/ML, SageMaker, Data Lake, Data Analytics, Cloud Monitoring, and Analytics.
Strong cloud programming skills with experience in API and AWS Lambda functions or any other scripting languages like Python / Bash using Python and Node.js.
Good understanding in using AWS CLI, Cloud formation, Terraform, Ansible with troubleshooting experiences.
Strong knowledge of Cloud Security practices and IAM Policy preparation for AWS.
Good to have:
Knowledge of experience with implementing containers using AWS container services cloud-native container orchestrators in AWS.
Strong working experience with AWS services as EC2, RDS, API Gateway, Lambda, DynamoDB, Elastic Cache, ECS, ALB/NLB Load Balancers, S3, EBS, VPC Networking, Secret Manager, Parameter Store, etc.
Ability to participate in fast-paced DevOps and System Engineering teams within Scrum agile processes.
Experience with DevOps tools will be an added advantage.
Know-how of working with Python / Bash scripting will help.
","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']",[None],999,"['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']","['analyt', 'ml', 'aw', 'python', 'engin']","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'analyt', 'ml', 'aw', 'python', 'engin']"
DE,"Job Title: Data EngineerCompanyWork matters.
And the workplace of the future is going to be a great place.
The team is fast-paced while managing highly accurate detailed information.
* Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)* Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools* Working knowledge Tableau, Power BI is a plus* Strong analytical and problem-solving ability and be able dive into technical details and design analytics solutions* Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively* Must have good communication, presentation, and documentation skills* Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products* Business process design, project management, and/or Agile SDLC experience a plus* 1-2 years of SAP HANA experience is a plus
","['sql', 'excel', 'bi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'power bi']","['dashboard', 'optim', 'commun', 'analyz']",999,"['sql', 'excel', 'powerbi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'optim', 'commun', 'analyz']","['analyt', 'program', 'power', 'optim', 'bi', 'python', 'comput', 'sourc']","['sql', 'excel', 'powerbi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'optim', 'commun', 'analyz', 'analyt', 'program', 'power', 'optim', 'bi', 'python', 'comput', 'sourc']"
DE,"Experience with Spark streaming, building real time data pipelines is preferred middot 2+ years of experience working with AWS platform.
Experience with solutioning on AWS infrastructure using services like AWS S3, Lambda, EMR, Redshift (or Snowflake) middot Experience with automating and orchestrating jobs on a big data platform using Oozie, Airflow, Jenkins or something similar middot Good understanding and experience working with various products in the Big data ecosystem like Hive, HDFS, Presto, NoSQL databases like Cassandra, DynamoDB middot Experience with setting up and using Kafka for real time streaming is a big plus middot Has to be a team player and open to working with newer technologies as well as supporting legacy systems middot Prior experience with working in a SQL server based environment and using SSIS, SSRS, TSQL is a plus.
middot Prior experience with traditional ETL tools like Talend Open Studio, Pentaho or something similar is a plus
","['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho']","['etl', 'pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'big', 'etl']","['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'big', 'etl']"
DE,"The combined worldwide revenue of independent member firms is $3.6 billion.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status, gender identity, sexual orientation, or any other legally protected basis, in accordance with applicable federal, state or local law.
* You enjoy supporting a variety of industries and embedding yourself with client teams to work together to find a solution.
* You enjoy being face to face with clients, understand who the key stakeholders on projects are, and positively influence the business need behind the use of data.
* You are constantly looking to grow your education in technology and staying up to date with the latest trends.
* You are a team player that encourages collaboration and has an entrepreneurial mind.
* You enjoy sharing what you learned with the team and are willing to be a mentor to others.
* You love to learn and enjoy putting yourself out of your comfort zone and have done or at least entertained the idea of speaking at tech events.
* Enjoy building relationships with your colleagues through social activities and team outings supporting work-life balance.
* You have and are interested in maintaining different technical certifications.QualificationsWhat you will do:* You will be responsible for working within an agile environment to aid in the delivery of a managed service defined by the Architect of Project Manager.
* Have strong experience building out data warehouses.
* Lead or support the day to day sprint activities provided to you by your pod leader.
* Work to understand business processes and possible improvements across an array of industries.
* Exhibit responsibility and accountability towards quality completion of projects and consistently hitting project timelines.
* Strong verbal and written communication skills and are not ashamed to ask questions or raise concern on projects.
* Outstanding customer service skills following proper business requirements and human resources expectations.
* Disciplined to be able to work in a variety of business environments.
* Ability to travel potentially up to 50% of the time.
* Maintained a Bachelor's degree in Computer Science, Engineering, Math, Information Technology, or other related discipline.
",[None],"['information technology', 'commun', 'math', 'account']",1,"['information technology', 'commun', 'math', 'account']","['day', 'basi', 'learn', 'human', 'provid', 'comput', 'warehous', 'relat', 'engin']","['information technology', 'commun', 'math', 'account', 'day', 'basi', 'learn', 'human', 'provid', 'comput', 'warehous', 'relat', 'engin']"
DE,"Mandatory Skills:Â
Knowledge and experience in API development, AI/ML, SageMaker, Data Lake, Data Analytics, Cloud Monitoring and Analytics,Â
Strong cloud programming skill with experience in API and AWS Lambda functions or any other scripting languages like Python / Bash using Python & Node.js
Good understanding in using AWS CLI, Cloud formation, Terraform, Ansible with troubleshooting experiences
Strong knowledge of Cloud Security practices and IAM Policy preparation for AWS
Good to have:Â
Knowledge on Experience with implementing containers using AWS container services cloud native container orchestrators in AWS
Strong working experience with AWS services as EC2, RDS, API Gateway, Lambda, DynamoDB, Elastic Cache, ECS, ALB/NLB Load Balancers, S3, EBS, VPC Networking , Secret Manager, Parameter Store âetc.
Ability to participate in fast-paced DevOps and System Engineering teams within Scrum agile processes
Experience with DevOps tools will be an added advantage
Know-how of working with Python / Bash scripting will help
","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']",[None],999,"['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']","['analyt', 'program', 'engin', 'ml', 'aw', 'python', 'â']","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'analyt', 'program', 'engin', 'ml', 'aw', 'python', 'â']"
DE,"As a Senior Data Engineer in Chicago, you will...
Design and build data pipelines to support data science projects following software engineering best practices
Use state of the art technologies to acquire, ingest and transform big datasets
Map data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics models
Create and manage data environments in the cloud or on premise
Ensure information security standards are maintained at all time
Contribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clients
Have the opportunity to contribute to R&D and internal asset development projects
What you'll benefit from
Real-World Impact– No project is ever the same.
Innovative Work Culture– Creativity, insight and passion come from being balanced.
","['cloud', 'r']","['pipelin', 'big data']",999,"['cloud', 'r', 'pipelin', 'big data']","['analyt', 'asset', 'pipelin', 'big', 'engin']","['cloud', 'r', 'pipelin', 'big data', 'analyt', 'asset', 'pipelin', 'big', 'engin']"
DE,"The overarching responsibilities for this role are (i) on-boarding successfully and efficiently new clients, (ii) maintaining the data and ecosystem on an ongoing basis and introducing & implementing improvements as needed.
The successful applicant will be joining an open and diverse team with a ""Can Do"" attitude, and a strong desire to make an impact in a start-up organization.
RESPONSIBILITIES & TASKS
Optimize and manage existing clients data pipelines
Monitor, maintain, and, if needed rectify, various clients data integrity
Develop and/or enhance automated processes to proactively identify any data related issues and/or simplify process
Gather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution base
Collaborate with team members to automate queries as needed
REQUIRED QUALIFICATIONS
Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field.
Working and practical knowledge of programming principles, techniques, standards and analytical ability
Strong proficiency in Java
Proven experience with complex SQL query design and optimization
Experience with Bash scripting
Experience in data cleansing, curation, parsing, integration, semantic mapping, or editing
Experience with analytics systems (data warehouses, dimensional models, etc.)
Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environment
Strong team player with a passion for data and problem solving
Excellent oral and written communication skills
PREFERRED QUALIFICATIONS
Degree in Computer Science, Computer Engineering, Management Information Systems or related field
1-3 years of applied data engineering-related experience
Strong competence in Python
Experience with Google Cloud
Experience with Linux/Unix
PDIs employee-oriented culture provides a supportive and dynamic work environment for high achievers.
Powered by JazzHR
","['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel']","['pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math']",1,"['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel', 'pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math']","['basi', 'analyt', 'techniqu', 'pipelin', 'relat', 'divers', 'power', 'optim', 'employe', 'python', 'comput', 'appli', 'statist', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'google cloud', 'linux', 'unix', 'cloud', 'java', 'python', 'excel', 'pipelin', 'optim', 'problem solving', 'statist', 'commun', 'math', 'basi', 'analyt', 'techniqu', 'pipelin', 'relat', 'divers', 'power', 'optim', 'employe', 'python', 'comput', 'appli', 'statist', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R14533 Data Engineering Manager (Open)
Summary:
Leads the development of data processing systems and interfaces, which support data science efforts, with a focus on enabling scalable, high performance, distributed computing environments.
Researches new data processing technologies, building prototypes as needed, and participating in related leadership activities.
Contributes to the implementation of machine learning and statistical algorithms, including making them more efficient and scalable.
As a leader of the engineering team and in collaboration with the Data Science team, works closely with scientists and contributes to the modeling and data mining efforts.
Primary Accountabilities
Data Science Infrastructure Design, Development, and Operations
Lead architectural planning, design, development, deployment, and management of analytical environments capable of ingesting, processing, and analyzing large, diverse data sets containing structured and unstructured elements.
Determines high level system integrations, primary dependent systems and infrastructure needed to implement the proposed business idea.
Analyzes architecture portfolios to assess opportunities for improvement and identify changes needed to meet strategic business needs.
Modify analytical architectures over time as technologies and DSAL needs evolve.
Data Science Research Support
Participate in the definition and planning of analytical projects to solve complex business problems as the key technical resource in the areas of data processing and scalable analytical and computational platforms.
Support quick turnaround for data science projects that provide time sensitive insights by facilitating ingestion, transformation, and processing of data, by developing software or scripting for efficient analytical processing, by rapidly deploying visualization or business intelligence tools and dashboards, or by contributing other technical skills, including software development, as appropriate.
Maximize the predictability, efficiency, effectiveness, and maintainability of data science-related infrastructure elements with a focus on analytical compute environments.
Develop means for automating data- and analytics-related systems and processes, as appropriate, to support data science activities.
Lab Environment Operational Oversight
Establishes and federates the standards and processes for the effective governance, quality, integrity, and timeliness of all data accessed, ingested and exported within the Lab infrastructural systems.
Manages contractual terms and fulfillment from third party vendors for both technical products and services.
Technology Strategy
Analyzes business and technology trends related to data architecture, data engineering, data management, and high performance computing to provide expertise to management and resources within DSAL, the Venture Capital group, and other areas of the Business Development division.
Participates in the development of policies, standards, and guidelines that direct the selection, development, implementation, and use of architecture technologies within the unit.
Function in a collaborative technical consultancy role to routinely contribute technical expertise in discussions with senior management.
Strategies Linked to the Division’s Business Goals/Results
Establishes, communicates, and implements departmental plans, objectives, and strategies.
Participates as a member of the Management Team.
Management/Leadership for Department or Unit
Prepares and analyzes department/unit plans and reports.
Provides leadership by exhibiting influence and expertise, thus affecting the results of the operating area.
Creates an effective work environment by developing a common vision, setting clear objectives, expecting teamwork, recognizing outstanding performance, and maintaining open communications.
Develops staff through coaching, providing performance feedback, providing effective performance assessments, and establishing performance & development plans.
Specialized Knowledge & Skill Requirements
Demonstrated experience providing customer-driven solutions, support or service
Advanced knowledge and experience using a variety of programming languages to implement and operate data science infrastructure assets (e.g., Scala, Python, Java, R, C/C++).
Demonstrated entrepreneurial experience working with technology focused startups.
Demonstrated experience developing and managing complex technical projects involving parallel or distributed computing, including Hadoop, the Apache Stack (Spark, Storm, Kafka, etc.
), and related technologies.
Demonstrated experience working across a broad range of information technology domains, including application, data, infrastructure, and security with specific expertise in design, development, and deployment of data- and analytics-related platforms and tools.
Demonstrated experience working with or on a Data Science team and familiarity with data mining and machine learning techniques.
Extensive knowledge and understanding of data management strategies and roadmaps as well as data architectural design models.
Familiarity with a wide variety of data formats and processing methodologies.
Familiarity with relational database design and SQL scripting.
Strong willingness to adapt, pivot, and learn as needed to address emerging opportunities and challenges.
Willingness to contribute technically in whatever manner is needed to get the job done.
Travel Requirements
This position requires travel up to 5% of the time.
Additional Job Information:
This role will require candidates to be strong technically and have had some prior leadership experience.
Candidates should be able to demonstrate ability in Python, SQL, and knowledge of operating in a cloud environment.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R14533 Data Engineering Manager (Open)
Summary:
Leads the development of data processing systems and interfaces, which support data science efforts, with a focus on enabling scalable, high performance, distributed computing environments.
Researches new data processing technologies, building prototypes as needed, and participating in related leadership activities.
Contributes to the implementation of machine learning and statistical algorithms, including making them more efficient and scalable.
As a leader of the engineering team and in collaboration with the Data Science team, works closely with scientists and contributes to the modeling and data mining efforts.
Primary Accountabilities
Data Science Infrastructure Design, Development, and Operations
Lead architectural planning, design, development, deployment, and management of analytical environments capable of ingesting, processing, and analyzing large, diverse data sets containing structured and unstructured elements.
Determines high level system integrations, primary dependent systems and infrastructure needed to implement the proposed business idea.
Analyzes architecture portfolios to assess opportunities for improvement and identify changes needed to meet strategic business needs.
Modify analytical architectures over time as technologies and DSAL needs evolve.
Data Science Research Support
Participate in the definition and planning of analytical projects to solve complex business problems as the key technical resource in the areas of data processing and scalable analytical and computational platforms.
Support quick turnaround for data science projects that provide time sensitive insights by facilitating ingestion, transformation, and processing of data, by developing software or scripting for efficient analytical processing, by rapidly deploying visualization or business intelligence tools and dashboards, or by contributing other technical skills, including software development, as appropriate.
Maximize the predictability, efficiency, effectiveness, and maintainability of data science-related infrastructure elements with a focus on analytical compute environments.
Develop means for automating data- and analytics-related systems and processes, as appropriate, to support data science activities.
Lab Environment Operational Oversight
Establishes and federates the standards and processes for the effective governance, quality, integrity, and timeliness of all data accessed, ingested and exported within the Lab infrastructural systems.
Manages contractual terms and fulfillment from third party vendors for both technical products and services.
Technology Strategy
Analyzes business and technology trends related to data architecture, data engineering, data management, and high performance computing to provide expertise to management and resources within DSAL, the Venture Capital group, and other areas of the Business Development division.
Participates in the development of policies, standards, and guidelines that direct the selection, development, implementation, and use of architecture technologies within the unit.
Function in a collaborative technical consultancy role to routinely contribute technical expertise in discussions with senior management.
Strategies Linked to the Division’s Business Goals/Results
Establishes, communicates, and implements departmental plans, objectives, and strategies.
Participates as a member of the Management Team.
Management/Leadership for Department or Unit
Prepares and analyzes department/unit plans and reports.
Provides leadership by exhibiting influence and expertise, thus affecting the results of the operating area.
Creates an effective work environment by developing a common vision, setting clear objectives, expecting teamwork, recognizing outstanding performance, and maintaining open communications.
Develops staff through coaching, providing performance feedback, providing effective performance assessments, and establishing performance & development plans.
Specialized Knowledge & Skill Requirements
Demonstrated experience providing customer-driven solutions, support or service
Advanced knowledge and experience using a variety of programming languages to implement and operate data science infrastructure assets (e.g., Scala, Python, Java, R, C/C++).
Demonstrated entrepreneurial experience working with technology focused startups.
Demonstrated experience developing and managing complex technical projects involving parallel or distributed computing, including Hadoop, the Apache Stack (Spark, Storm, Kafka, etc.
), and related technologies.
Demonstrated experience working across a broad range of information technology domains, including application, data, infrastructure, and security with specific expertise in design, development, and deployment of data- and analytics-related platforms and tools.
Demonstrated experience working with or on a Data Science team and familiarity with data mining and machine learning techniques.
Extensive knowledge and understanding of data management strategies and roadmaps as well as data architectural design models.
Familiarity with a wide variety of data formats and processing methodologies.
Familiarity with relational database design and SQL scripting.
Strong willingness to adapt, pivot, and learn as needed to address emerging opportunities and challenges.
Willingness to contribute technically in whatever manner is needed to get the job done.
Travel Requirements
This position requires travel up to 5% of the time.
Additional Job Information:
This role will require candidates to be strong technically and have had some prior leadership experience.
Candidates should be able to demonstrate ability in Python, SQL, and knowledge of operating in a cloud environment.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
","['sql', 'spark', 'scala', 'hadoop', 'cloud', 'java', 'python', 'c', 'r', 'kafka']","['research', 'data mining', 'dashboard', 'visual', 'predict', 'machine learning', 'statist', 'analyz', 'information technology', 'commun', 'account']",999,"['sql', 'spark', 'scala', 'hadoop', 'cloud', 'java', 'python', 'c', 'r', 'kafka', 'research', 'data mining', 'dashboard', 'visual', 'predict', 'machine learning', 'statist', 'analyz', 'information technology', 'commun', 'account']","['techniqu', 'visual', 'predict', 'provid', 'python', 'integr', 'common', 'algorithm', 'analyt', 'challeng', 'hadoop', 'statist', 'asset', 'learn', 'primari', 'infrastructur', 'parti', 'set', 'engin', 'particip', 'machin', 'spark', 'divers', 'comput', 'scientist', 'relat']","['sql', 'spark', 'scala', 'hadoop', 'cloud', 'java', 'python', 'c', 'r', 'kafka', 'research', 'data mining', 'dashboard', 'visual', 'predict', 'machine learning', 'statist', 'analyz', 'information technology', 'commun', 'account', 'techniqu', 'visual', 'predict', 'provid', 'python', 'integr', 'common', 'algorithm', 'analyt', 'challeng', 'hadoop', 'statist', 'asset', 'learn', 'primari', 'infrastructur', 'parti', 'set', 'engin', 'particip', 'machin', 'spark', 'divers', 'comput', 'scientist', 'relat']"
DE,"Where good people build rewarding careers.Think that working in the insurance field can't be exciting, rewarding and challenging?
Think again.
And you'll have fun doing it.
",[None],[None],999,[],[None],[None]
DE,"Job Title: Data Engineer
Work matters.
And the workplace of the future is going to be a great place.
People matter.
Team
This is a new team within the Data and Analytics organization.
The team is fast-paced while managing highly accurate detailed information.
Role
The role entails collaborative engagements with Field & Product Line Sales Teams, Global Services, Alliances & Channels and FP & A to deploy insightful analytic products, establish alignment on processes teams and deliver strategic metrics for current and future business initiatives.
What You Get To Do In This Role
You will provide insights and deep analysis being sought by users/business stakeholders
Work with Cross Functional Analytics team members to curate and assimilate insights
Grow into being SME on business functions
Gather business requirements from stakeholders on various analytics initiatives
Analyze requirements, determine optimal solutions and determine gap from current state, dependencies and ways to mitigate risks
Develop business requirements documentation, process workflow diagrams, functional specifications, user acceptance test scripts and other supporting documentation for Business Intelligence and Analytics initiatives
Assist stakeholders with data analysis, design data models & develop DB Views, procs, models in SAP HANA to meet business need
Develop dashboard and report prototypes and mock-ups with respect to the UX/UI Best Practices and have impactful UI Design
Communicate status regularly with stakeholders
Define required data integration requirements between various systems and work with extended team to get them created
Collaborate with India Development Center BI team to translate business requirements and get appropriate data solutions developed to meet business need
Partner with Global BI team to help implement solutions for end user adoption
Bachelor's Degree in Information System, Analytics, Business Intelligence or related field required
1 to 3+ years of documented experience in writing strong SQL, PLSQL in data warehouse technologies (Hana, Snowflake or any modern database).
Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)
Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools
Working knowledge Tableau, Power BI is a plus
Strong analytical and problem-solving ability and be able dive into technical details and design analytics solutions
Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively
Must have good communication, presentation, and documentation skills
Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products
Business process design, project management, and/or Agile SDLC experience a plus
1-2 years of SAP HANA experience is a plus
","['sql', 'excel', 'bi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'power bi']","['dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun']",1,"['sql', 'excel', 'powerbi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun']","['analyt', 'program', 'relat', 'line', 'power', 'optim', 'bi', 'python', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'excel', 'powerbi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun', 'analyt', 'program', 'relat', 'line', 'power', 'optim', 'bi', 'python', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"By clicking continue you agree to Built In's Privacy Policy and Terms of Use.
The platform gives businesses of all sizes access to hundreds of pre-built automations that combine email marketing, marketing automation, CRM, and machine learning for powerful orchestration, segmentation and personalization across social, email, messaging, chat, and text.
What your day could consist of:
Manage and improve existing ETLs and create ETLs to ingest new sources of data
Programmatically unify data from various sources
Promote and execute the best practices of data management, at both team-wide and company-wide
Besides the strong technical skills, the ideal candidate should be a continuous learner, comfortable at working with various teams, have a natural curiosity of how things work, and always committed to delivering top quality work
What is needed:
Advanced SQL knowledge
Proficiency with Python (3+ years of experience)
Hands-on Linux experience
Working experience processing 500GB to 5TB of data
Experience with both batch and streaming data processing
Experience working with structured, semi-structured, and unstructured data
Experience in interacting with and optimizing usage of cloud data warehouses (Snowflake)
Optional: AWS, Terraform
& lever-source%5B%5D=BuiltInChicago
First Name
Last Name
Email Address
","['sql', 'linux', 'aw', 'snowflak', 'python', 'cloud']","['machine learning', 'etl', 'segment']",999,"['sql', 'linux', 'aw', 'snowflak', 'python', 'cloud', 'machine learning', 'etl', 'segment']","['day', 'machin', 'learn', 'power', 'aw', 'python', 'warehous', 'etl', 'sourc']","['sql', 'linux', 'aw', 'snowflak', 'python', 'cloud', 'machine learning', 'etl', 'segment', 'day', 'machin', 'learn', 'power', 'aw', 'python', 'warehous', 'etl', 'sourc']"
DE,"Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).
Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.
Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation.
","['java', 'javascript', 'python', 'salesforc']","['recommend', 'healthcar', 'machine learning', 'risk']",999,"['java', 'javascript', 'python', 'salesforc', 'recommend', 'healthcar', 'machine learning', 'risk']","['analyt', 'machin', 'python', 'set', 'sourc', 'integr']","['java', 'javascript', 'python', 'salesforc', 'recommend', 'healthcar', 'machine learning', 'risk', 'analyt', 'machin', 'python', 'set', 'sourc', 'integr']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Mandatory Skills
Knowledge and experience in API development, AI/Client, SageMaker, Data Lake, Data Analytics, Cloud Monitoring and Analytics,
Strong cloud programming skill with experience in API and AWS Lambda functions or any other scripting languages like Python / Bash using Python & Node.js
Good understanding in using AWS CLI, Cloud formation, Terraform, Ansible with troubleshooting experiences
Strong knowledge of Cloud Security practices and IAM Policy preparation for AWS
Good to Have
Knowledge on Experience with implementing containers using AWS container services cloud native container orchestrators in AWS
Strong working experience with AWS services as EC2, RDS, API Gateway, Lambda, DynamoDB, Elastic Cache, ECS, ALB/NLB Load Balancers, S3, EBS, VPC Networking , Secret Manager, Parameter Store etc.
Ability to participate in fast-paced DevOps and System Engineering teams within Scrum agile processes
Experience with DevOps tools will be an added advantage
Know-how of working with Python / Bash scripting will help
","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']",[None],999,"['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud']","['analyt', 'program', 'aw', 'python', 'engin']","['sagemak', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'analyt', 'program', 'aw', 'python', 'engin']"
DE,"Although OCD affects over 181 million people globally, is ranked by the W.H.O.
as a top 10 most disabling condition, and costs 3X more to treat than depression, over 90% of the OCD population isn’t able to access effective care due to stigma and misdiagnosis.
As a result, OCD patients often go untreated and suffer in silence, which is why it’s nicknamed ""the secret illness.”
In between each session, members can access moderated support communities and tech-enabled therapeutic tools to ensure they have constant support whenever their provider isn’t around.
Collaborate with the product development cycle to measure effectiveness of features and contribute to the direction of the product.
Help sales teams understand what is important to share and show how data can be used to shed light on the misunderstood nature of OCD.
Present findings to team
Qualifications
Expert in SQL and high-level languages such as Python
Strong computer science fundamentals, algorithms, data structures
Excellent communication skills
Authorized to work in the U.S
Additional Information
Casual, challenging, and engaging startup environment with an outstanding mission driven team atmosphere
Competitive compensation
Comprehensive benefits package, including medical, dental, vision coverage, and 401(k).
Flexible PTO policy
Awesome office on Michigan Avenue, ability to work remotely
Onsite amenities including a fitness center
","['sql', 'python', 'excel']",['commun'],999,"['sql', 'python', 'excel', 'commun']","['challeng', 'provid', 'comput', 'python', 'packag']","['sql', 'python', 'excel', 'commun', 'challeng', 'provid', 'comput', 'python', 'packag']"
DE,"Strong Programming experience with object-orientedobject function scripting languages Python.
Experience with big data tools Hadoop, Apache Spark etc.
Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc.
(Nice to have) Experience with relational SQL, Snowflake and NoSQL databases NO OPTCPT please
","['sql', 'spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'cloud']",['big data'],999,"['sql', 'spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'cloud', 'big data']","['stream', 'spark', 'hadoop', 'aw', 'python', 'big', 'relat']","['sql', 'spark', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'cloud', 'big data', 'stream', 'spark', 'hadoop', 'aw', 'python', 'big', 'relat']"
DE,"Sr. Data Engineer
Chicago, IL
Six month contract to hire (must be GC or USC)
SSIS
SSRS
Power Shell]
MS Sql, T-Sql
No Oracle
Be the go to person
Get up to speed quickly
As a Senior Data Engineer you will contribute to database management of large scale web-based applications through the use of SQL, C#, and .NET tools.
Responsibilities:
Data Engineer's work in conjunction with Software Engineers, DBA's, Business Analysts, Quality Assurance and business owners
Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions
Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.
Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.
Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.
Perform code reviews and provide feedback in a timely manner.
Promote collective code ownership for everyone to have visibility into the feature codebase.
Present technical ideas and concepts in business-friendly language.
Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.
Identify and correct performance bottlenecks related to SQL code.
Support timely production releases and adherence to release activities.
Contribute to data retention strategy.
Position Requirements
1-3 years in commercial-grade business applications environment leveraging the following:
SQL Server, T-SQL, SSIS, stored procedures, user-defined functions and table functions
Managing design risk
1-3 years leveraging OO programming techniques
Software development lifecycles, Unit test techniques, debugging/analytical techniques
Working with an organization with defined market goals, products, customers, revenue, and development teams
Experienced mentors to learn and adopt new practices
Work in wide variety of data management
Ability to constantly enhance and improve applications
Have a clearly defined career growth track with enough flexibility to pave your own way
","['sql', 'c', 'oracl', 'ssr']","['recommend', 'optim', 'risk']",999,"['sql', 'c', 'oracl', 'ssr', 'recommend', 'optim', 'risk']","['analyt', 'techniqu', 'engin', 'challeng', 'power', 'provid', 'optim', 'releas', 'relat', 'collect', 'evalu']","['sql', 'c', 'oracl', 'ssr', 'recommend', 'optim', 'risk', 'analyt', 'techniqu', 'engin', 'challeng', 'power', 'provid', 'optim', 'releas', 'relat', 'collect', 'evalu']"
DE,"Title: Python Data Engineer - Trading
Salary: 120K plus DOE Competitive
Key Points:
3+ years' Python experience in a Linux environment
Spark, Hadoop and other distributed data processing technologies experience.
Background managing/reusing calculated data both in the cloud and on prem.
Background utilizing data caching strategies for optimal pipelining
This person will be collaborating with internal researchers/developers to automate research pipelines that create production models.
Overall this person will be responsible for driving decisions on processing/storage formats and locality The Python developer will be integral in exploring data sources to understand quality, and measure processes for fault tolerance and optimization possibilities.
If applying for this role - Please take each key point and provide number of years experience and what you would rate yourself, 1 thru 10 (10 being expert) for each key point.
","['linux', 'spark', 'hadoop', 'cloud', 'python']","['research', 'optim', 'pipelin']",999,"['linux', 'spark', 'hadoop', 'cloud', 'python', 'research', 'optim', 'pipelin']","['spark', 'pipelin', 'hadoop', 'optim', 'python', 'integr', 'sourc', 'engin']","['linux', 'spark', 'hadoop', 'cloud', 'python', 'research', 'optim', 'pipelin', 'spark', 'pipelin', 'hadoop', 'optim', 'python', 'integr', 'sourc', 'engin']"
DE,"My client is looking for a strong Data Engineer who has experience working with Enterprise data platforms, managing data lakes using big data technologies.
My client is only considering W2 candidates at this time.
Sponsorship is not available at this time.
Job Requirements middot 5+ years of experience working with enterprise data platforms, building and managing data lakes and using big data technologies middot 2+ years of experience with Spark using PythonScala.
Experience with Spark streaming, building real time data pipelines is preferred middot 2+ years of experience working with AWS platform.
Experience with solutioning on AWS infrastructure using services like AWS S3, Lambda, EMR, Redshift (or Snowflake) middot Experience with automating and orchestrating jobs on a big data platform using Oozie, Airflow, Jenkins or something similar middot Good understanding and experience working with various products in the Big data ecosystem like Hive, HDFS, Presto, NoSQL databases like Cassandra, DynamoDB middot Experience with setting up and using Kafka for real time streaming is a big plus middot Has to be a team player and open to working with newer technologies as well as supporting legacy systems middot Prior experience with working in a SQL server based environment and using SSIS, SSRS, TSQL is a plus.
middot Prior experience with traditional ETL tools like Talend Open Studio, Pentaho or something similar is a plus
","['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho']","['etl', 'pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'avail', 'big', 'etl', 'engin']","['sql', 'spark', 'airflow', 'cassandra', 'ssr', 'aw', 'lambda', 'redshift', 's3', 'snowflak', 'nosql', 'hive', 'kafka', 'pentaho', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'infrastructur', 'aw', 'avail', 'big', 'etl', 'engin']"
DE,"Analytics
Chicago
Qualifications
Degree educated in Computer Science, Engineering, Mathematics, or equivalent experience
Previous commercial experience in a data-driven role
Ability to write clean, maintainable, and robust code in Python, Scala, Java or similar languages
Knowledge of software engineering concepts and best practices
Familiarity with the latest OSS, cloud, container, query and database technologies as well as query languages
Confirmed experience building data pipelines in production and ability to work across structured, semi-structured and unstructured data
Experience preparing data for analytics and following a data science workflow
Commercial client-facing or senior stakeholders management experience
Who You'll Work With
You will be part of a global Data Engineering community and you will work in cross-functional and Agile project teams alongside Project Managers, Data Scientists, Machine Learning Engineers, other Data Engineers, and industry experts.
Who you are
You are a highly collaborative individual and enjoy solving problems that focus on adding business value.
You have a sense of ownership and enjoy hands-on technical work.
What You'll Do
As a Senior Data Engineer in Chicago, you will...
Design and build data pipelines to support data science projects following software engineering best practices
Use state of the art technologies to acquire, ingest and transform big datasets
Map data fields to hypothesis, curate, wrangle and prepare data to be used in advanced analytics models
Create and manage data environments in the cloud or on premise
Ensure information security standards are maintained at all time
Contribute to cross-functional problem-solving sessions with your team and deliver presentations to colleagues and clients
Have the opportunity to contribute to R&D and internal asset development projects
What you'll benefit from
Real-World Impact– No project is ever the same.
Innovative Work Culture– Creativity, insight and passion come from being balanced.
Industries
High Tech
Functions
Technology
FOR U.S.
All qualified applicants will receive consideration for employment without regard to sex, gender
identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran
status, age, or any other characteristic protected by applicable law.
FOR NON-U.S.
For additional details
McKinsey Careers and
Diversity & Inclusion sites.
","['scala', 'cloud', 'java', 'python', 'r']","['pipelin', 'machine learning', 'clean', 'big data', 'commun']",1,"['scala', 'cloud', 'java', 'python', 'r', 'pipelin', 'machine learning', 'clean', 'big data', 'commun']","['analyt', 'asset', 'machin', 'learn', 'pipelin', 'divers', 'python', 'scientist', 'comput', 'big', 'engin']","['scala', 'cloud', 'java', 'python', 'r', 'pipelin', 'machine learning', 'clean', 'big data', 'commun', 'analyt', 'asset', 'machin', 'learn', 'pipelin', 'divers', 'python', 'scientist', 'comput', 'big', 'engin']"
DE,"The combined worldwide revenue of independent member firms is $3.6 billion.
* Create and maintain optimal data pipeline architecture* Assemble large, complex data sets that meet functional / non-functional business requirements* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
* Experience with big data tools: Hadoop, Spark, Kafka, etc.
* Query authoring (SQL) as well as working familiarity with a variety of databases* Experience building and optimizing 'big data' data pipelines, architectures and data sets* Deep experience with AWS cloud services: EC2, EMR, RDS, Redshift* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement* Strong analytic skills related to working with unstructured datasets* Build processes supporting data transformation, data structures, metadata, dependency and workload management* Experience with relational SQL and NoSQL databases, including PostgreSQL and Cassandra* A successful history of manipulating, processing and extracting value from large disconnected datasets* Working knowledge of message queuing, stream processing and highly scalable 'big data' data stores* Experience with stream-processing systems: Storm, Spark-Streaming, etc.
* Strong project management and organizational skills required* Experience supporting and working with cross-functional teams in a dynamic environment* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow preferred
","['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'big', 'set']","['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'big', 'set']"
DE,"This might be you if* Have AWS, Azure, and/or Google Cloud Platform engineering experience and experience working within Big Data (Hadoop, Cloudera/Hortonworks, MapReduce, Amazon AWS EMR) environments* You design awesome, next-generation cloud-based data environments using Python, Scala and AWS deployment tools, etc.
","['mapreduc', 'google cloud', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python']",['big data'],999,"['mapreduc', 'google cloud', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'big data']","['azur', 'hadoop', 'aw', 'python', 'big', 'engin']","['mapreduc', 'google cloud', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'big data', 'azur', 'hadoop', 'aw', 'python', 'big', 'engin']"
DE,"Cloud Data Engineer
If you have experience as a data pipeline builder and data wrangler and enjoy optimizing data systems and building them from the ground up, this is a great role for you.
You are self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities for Cloud Data Engineer
Create and maintain optimal data pipeline architecture with AWS
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Qualifications for Cloud Data Engineer
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
They should also have experience using the following software/tools:
Experience with big data tools: Kinesis, Glue, S3, etc.
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools: AWS Glue, Snaplogic
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Internal ID: 103198
","['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Would you like to be part of a team focused on helping customers in a ""once in a generation"" shift to the cloud and AWS.
This includes assessing customer needs, re-engineering business intelligence processes, designing and developing data models, and sharing your expertise throughout the deployment process.
Responsibilities Include but Not Limited to:
Strong Development Experience in either one of the Distributed Big Data processing (bulk) engines preferably using Spark on EMR or related (Must Have)
Strong Development Experience on at least one or more event driven streaming platforms prefer Kinesis, Firehose, Kafka or related (Must Have)
Strong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Airflow or related (Must Have)
Strong experience on either one or more MPP Data Warehouse Platforms prefer AWS RedShift, PostgreSQL, Teradata or similar (Must Have)
Strong Data Cataloging experience preferably using AWS Glue (Nice to Have)
Strong Development Experience on at least one NoSQL OR Document databases (Nice to Have)
Experience on at least one or More Ingestion Integration tools Like Apache NIFI or Streamset or related (Nice to Have)
Strong Development Experience on at least one Caching Tools like Redis, Lucene, Memcached (Nice to Have)
Strong Understanding of at least one or more Cluster Managers (Yarn, Hive, Pig, etc) (Nice to Have)Interface with client project sponsors to gather, assess and interpret client needs and requirements
Advising on database performance, altering the ETL process, providing SQL transformations, discussing API integration, and deriving business and technical KPIs
Develop a data model around stated use cases to capture client’s KPIs and data transformations
Assess, document and translate goals, objectives, problem statements, etc.
Document and communicate product feedback in order to improve user experience
Qualifications:
A passion for exploring data and extracting valuable insights.
Proven analytical, problem solving, and troubleshooting expertise.
Exposure to developer tools/workflow (e.g., git/github, *nix, SSH)Experience optimizing database/query performance.
Experience with AWS ecosystem (EC2, S3, RDS, Redshift).Experience with business intelligence tools with a physical model (e.g., MicroStrategy, Business Objects, Cognos).Experience with data warehousing.
Exposure to NoSQL-based, SQL-like technologies (e.g., Hive, Pig, Spark SQL/Shark, Impala, BigQuery)
Excellent verbal and written communication skills
Ability to travel up to 70-80%
Education and Experience:
Bachelor’s Degree in Computer Science or Equivalent
Minimum five years of Big Data Engineering on AWS experience
This position can be located in Columbus or Cincinnati, OH or Detroit or Chicago.
","['sql', 'cogno', 'redshift', 's3', 'nosql', 'kafka', 'pig', 'bigqueri', 'git', 'lambda', 'hive', 'ec2', 'aw', 'excel', 'spark', 'airflow', 'cloud', 'postgresql', 'github']","['pipelin', 'data warehousing', 'problem solving', 'big data', 'etl', 'commun', 'cluster', 'kpi']",1,"['sql', 'cogno', 'redshift', 's3', 'nosql', 'kafka', 'pig', 'bigqueri', 'git', 'lambda', 'hive', 'ec2', 'aw', 'excel', 'spark', 'airflow', 'cloud', 'postgresql', 'github', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'etl', 'commun', 'cluster', 'kpi']","['analyt', 'spark', 'pipelin', 'relat', 'aw', 'interpret', 'warehous', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'cogno', 'redshift', 's3', 'nosql', 'kafka', 'pig', 'bigqueri', 'git', 'lambda', 'hive', 'ec2', 'aw', 'excel', 'spark', 'airflow', 'cloud', 'postgresql', 'github', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'etl', 'commun', 'cluster', 'kpi', 'analyt', 'spark', 'pipelin', 'relat', 'aw', 'interpret', 'warehous', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"Title: Research Data Engineer
Their world-class capabilities position them to meet the liquidity demands of their diverse group of institutional clients in all market conditions.
In partnering, their clients, including asset managers, banks, broker-dealers, hedge funds, government agencies and public pension programs are able to gain a powerful trading advantage and are better positioned to meet investment goals.
The team makes its mark every day from offices around the world.
Data Engineers are tasked with building next generation data analysis platforms.
Key Responsibilities:
Partner with business leaders, quantitative researchers and technologists to define priorities and deliver custom solutions
Skillset Requirements:
A deep passion for working with data and developing software to address data processing challenges
Minimum of a bachelor’s degree in Computer Science or equivalent experience with good software design and engineering skills
Proficiency within one or more programming languages including Python, C, C++, R and/or JavaScript is a plus
Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop
Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development
Good analytical and quantitative abilities
Demonstrated ability to quickly learn new technologies and skills
Ability to manage multiple tasks and thrive in a fast-paced team environment
","['mongodb', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'c', 'r', 'nosql']","['research', 'natural language processing', 'machine learning']",1,"['mongodb', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'c', 'r', 'nosql', 'research', 'nlp', 'machine learning']","['day', 'analyt', 'asset', 'program', 'learn', 'machin', 'challeng', 'spark', 'divers', 'power', 'public', 'hadoop', 'python', 'comput', 'quantit', 'engin']","['mongodb', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'c', 'r', 'nosql', 'research', 'nlp', 'machine learning', 'day', 'analyt', 'asset', 'program', 'learn', 'machin', 'challeng', 'spark', 'divers', 'power', 'public', 'hadoop', 'python', 'comput', 'quantit', 'engin']"
DE,"Data Engineer
Contribute to all aspects of the project lifecycle, including requirements gathering, design, development, testing, and deployment.
Learn quickly and juggle multiple tasks for various customers.
Communicate complex technical problems effectively to management and to the business.
Collaborate with other technical team members on solutions.
Effectively prioritize support tasks with project work.
Specific duties include:
Design, develop and maintain data models, database architectures, and associated database objects in Snowflake, Oracle, and other database solutions such as Azure.
Design, develop, and maintain data integrations using Informatica Power Center, Informatica Integrated Cloud Services, and data prep tools.
Participate in or drive project activities such as requirements gathering, design, develop, test, and deploy.
Create and maintain necessary technical documentation, including requirements, design, and test documents.
Must have's:
A Master’s or Bachelor’s degree in Computer Science, MIS, engineering, or a related technical discipline is required.
5+ years of experience in data engineering, data warehousing, business intelligence, ETL on databases such as Oracle or SQL Server, and/or big data is required.
3+ years of experience in ETL/ data integration is required with 2+ years of experience in Informatica PowerCenter, job scheduling tools is required.
Working experience in Python/R/Scala, Snowflake is required.
Hands on experience in writing and understanding complex SQL (e.g.
CTE’s others).
Thorough understanding of relational database design and best practices, including dimensional (star, snowflake) models is required.
A collaborative working style and ability to work well within the team and with business consumers is required.
Ability to clearly communicate to technical and non-technical audience by written and verbal is required.
Independent analytical, critical thinking, and problem-solving ability in complex technical environments is required.
Nice to have's:
Production experience in OBIEE, Oracle Analytics Cloud (OAC) and Tableau is nice to have.
Familiarity with big data technologies such as Microsoft Azure Data or AWS is nice to have.
EOE/M/F/D/V/SO
","['sql', 'azur', 'scala', 'aw', 'snowflak', 'python', 'tableau', 'r', 'cloud', 'microsoft', 'oracl']","['data warehousing', 'etl', 'commun', 'big data']",1,"['sql', 'azur', 'scala', 'aw', 'snowflak', 'python', 'tableau', 'r', 'cloud', 'microsoft', 'oracl', 'data warehousing', 'etl', 'commun', 'big data']","['analyt', 'learn', 'azur', 'relat', 'power', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'particip', 'integr']","['sql', 'azur', 'scala', 'aw', 'snowflak', 'python', 'tableau', 'r', 'cloud', 'microsoft', 'oracl', 'data warehousing', 'etl', 'commun', 'big data', 'analyt', 'learn', 'azur', 'relat', 'power', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'particip', 'integr']"
DE,"Responsibility:
The role will be responsible for deploying the
model that is designed by the Data Scientist.
The deployment will use Azure
Platform and is expected to have the scalability and performance of the cloud
deployment.
Requirement:
1.
At least continuous 4 years
of experience with Spark, especially
pySpark implementation
2.
Experience with Azure Deployment
using AKS and Container Services
3.
MUST HAVE experience with Azure
Data Bricks solution that includes
a.
Azure ML library
b.
Azure Kafka
c.
Azure CosmosDB
d.
Azure BlobStorage
4.
At least a total 8 years in a
career related to data analytics
5.
Must have a high level
understanding of analytics use cases mentioned in one or more of the following:
a.
Assortment optimization
b.
Promotional planning and
effectiveness
c.
Dynamic pricing
d.
Markdown optimization
e.
Labor scheduling &
optimization
f.
Various types of forecasting &
large scale predictions
g.
Large-scale experimental
frameworks (multi-arm bandits, etc.)
h.
Channel optimization;
i.
Fraud detection
Responsibility:
The role will be responsible for deploying the model that is designed by the Data Scientist.
The deployment will use Azure Platform and is expected to have the scalability and performance of the cloud deployment.
Requirement:
1.
At least continuous 4 years of experience with Spark, especially pySpark implementation
2.
Experience with Azure Deployment using AKS and Container Services
3.
MUST HAVE experience with Azure Data Bricks solution that includes
a. Azure ML library
b. Azure Kafka
c. Azure CosmosDB
d. Azure BlobStorage
4.
At least a total 8 years in a career related to data analytics
5.
Must have a high level understanding of analytics use cases mentioned in one or more of the following:
a.
Assortment optimization
b.
Promotional planning and effectiveness
c. Dynamic pricing
d. Markdown optimization
e. Labor scheduling & optimization
f. Various types of forecasting & large scale predictions
g. Large-scale experimental frameworks (multi-arm bandits, etc.)
h. Channel optimization;
i.
Fraud detection
","['cloud', 'kafka', 'spark', 'azur']","['optim', 'bandit', 'predict']",999,"['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict']","['analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat']","['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict', 'analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat']"
DE,"They enable clients in more than 50+ countries to create and execute strategies for their digital transformation.
Their team of 200,000+ innovators, across the globe, is differentiated by the imagination, knowledge, and experience, across industries and technologies that they bring to every project they undertake.
",[None],[None],999,[],['digit'],['digit']
DE,"Are you interested in building the future of healthcare and transforming the patient experience?
Are you hopeful about what data and medical research can do to improve medicine?
Your work will involve coming up with new software features, defining metrics, streamlining existing data processes, and mentoring other team members.
Your work will be impactful across the entire organization.
Role Responsibilities
Design, develop, and maintain ETL infrastructure to support the ingestion of external data sources
Understand data requirements and implement solutions for data science applications
Perform unit and integration testing
Mentor junior team members
Requirements
Ideal Qualifications
Experience designing, building, and maintaining ETL infrastructure in a production setting
BS/MS in Computer Science, Engineering, Mathematics, or related field
Proficient in Python or another object oriented programming language
Deep knowledge of SQL and at least one database technology
Experience with software development lifecycle processes and using version control systems (git), either from prior data engineering work or in a more traditional software engineering setting
Highly self-motivated with strong analytical problem-solving skills and attention to detail
Nice to Haves
Experience with workflow management systems such as luigi or airflow
Experience in machine learning and/or business intelligence
Experience with cloud technologies such as AWS, Google Cloud Platform, or Azure
Experience with ETL tools like Apache Kafka, Logstash, Segment, Informatica
Experience with automated machine learning technologies such as Amazon SageMaker or Google Cloud AutoML
Experience with cloud data warehouse platforms such as Snowflake, Qubole, etc.
Experience working in Healthcare, Finance or another regulated industry
Benefits
Great Benefits - top-notch health, dental and vision insurance.
Additional perks available including 401K.
True Idea Meritocracy - great ideas win out.
No need to track it or save up.
","['sql', 'google cloud', 'airflow', 'azur', 'sagemak', 'aw', 'snowflak', 'python', 'cloud', 'kafka']","['research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl']",1,"['sql', 'google cloud', 'airflow', 'azur', 'sagemak', 'aw', 'snowflak', 'python', 'cloud', 'kafka', 'research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl']","['analyt', 'machin', 'program', 'learn', 'azur', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin', 'integr']","['sql', 'google cloud', 'airflow', 'azur', 'sagemak', 'aw', 'snowflak', 'python', 'cloud', 'kafka', 'research', 'healthcar', 'segment', 'machine learning', 'financ', 'etl', 'analyt', 'machin', 'program', 'learn', 'azur', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin', 'integr']"
DE,"Fractal's Overview:
Fractal Analytics is a strategic AI partner to Fortune 500 companies with a vision to power every human decision in the enterprise.
Fractal is building a world where individual choices, freedom, and diversity are the greatest assets; an ecosystem where human imagination is at the heart of every decision.
Where no possibility is written off, only challenged to get better.
Fractal has been featured as a Great Place to Work by The Economic Times in partnership with the Great Place to Work® Institute and recognized as a 'Cool Vendor' and a 'Vendor to Watch' by Gartner.
Data engineering services required:
Build data products and processes alongside the core engineering and technology team
Collaborate with senior data scientists to curate, wrangle, and prepare data for use in their advanced analytical models
Integrate data from a variety of sources, assuring that they adhere to data quality and accessibility standards
Modify and improve data engineering processes to handle ever larger, more complex, and more types of data sources and pipelines
Use Hadoop architecture and HDFS commands to design and optimize data queries at scale
Evaluate and experiment with novel data engineering tools and advises information technology leads and partners about new capabilities to determine optimal solutions for particular technical problems or designated use cases
Big data engineering skills:
5+ years of hands-on experience in one or more modern Object-Oriented Programming languages (Java, Scala, Python) including the ability to code in more than one programming language.
5+ years of hands-on experience applying principles, best practices, and trade-offs of schema design to different database systems, including relational (Oracle, MSSQL, Postgres, MySQL) and NoSQL (HBase, Cassandra, MongoDB)
2+ years of hands-on experience implementing batch and real-time data integration frameworks and/or applications in private or public cloud environments (AWS, Azure, GCP, etc.)
using various technologies (Hadoop, Spark, Impala, etc.
), including assessing performance, debugging, and fine-tuning those systems
Deep understanding of the latest data science and data engineering methods and processes to develop impactful and reusable patterns and abstractions from enterprise-level data assets
3+ years of hands-on experience in all phases of data modeling from conceptualization to database optimization
Demonstrated ability to perform the engineering necessary to acquire, ingest, cleanse, integrate, and structure massive volumes of data from multiple sources and systems into enterprise analytics platforms
Proven ability to design and optimize queries to build scalable, modular, efficient data pipelines
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Proven experience delivering production-ready data engineering solutions, including requirements definition, architecture selection, prototype development, debugging, unit-testing, deployment, support, and maintenance
Ability to operate with a variety of data engineering tools and technologies; vendor agnostic candidates preferred
Domain and industry knowledge:
Strong collaboration and communication skills to work within and across technology teams and business units
Demonstrates the curiosity, interpersonal abilities, and organizational skills necessary to serve as a consulting partner, includes the ability to uncover, understand, and assess the needs of various business stakeholders
Experience with problem discovery, solution design, and insight delivery that involves frequent interaction, education, engagement, and evangelism with senior executives
Ideal candidate will have extensive experience with the creation and delivery of advanced analytics solutions for healthcare payers or insurance companies, including anomaly detection, provider optimization, studies of sources of fraud, waste, and abuse, and analysis of clinical and economic outcomes of treatment and wellness programs involving medical or pharmacy claims data, electronic medical record data, or other health data
Experience with healthcare providers, pharma, or life sciences is a plus
Experience with Kafka is required
","['mongodb', 'gcp', 'spark', 'cassandra', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'postgr', 'mysql', 'kafka', 'oracl']","['healthcar', 'pipelin', 'anomali', 'cleans', 'data modeling', 'optim', 'big data', 'information technology', 'commun', 'econom']",2,"['mongodb', 'gcp', 'spark', 'cassandra', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'sql', 'kafka', 'oracl', 'healthcar', 'pipelin', 'anomali', 'cleans', 'data modeling', 'optim', 'big data', 'information technology', 'commun', 'econom']","['program', 'provid', 'python', 'integr', 'sourc', 'evalu', 'analyt', 'challeng', 'azur', 'human', 'hadoop', 'optim', 'clinic', 'asset', 'public', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'power', 'scientist', 'relat']","['mongodb', 'gcp', 'spark', 'cassandra', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'hbase', 'nosql', 'sql', 'kafka', 'oracl', 'healthcar', 'pipelin', 'anomali', 'cleans', 'data modeling', 'optim', 'big data', 'information technology', 'commun', 'econom', 'program', 'provid', 'python', 'integr', 'sourc', 'evalu', 'analyt', 'challeng', 'azur', 'human', 'hadoop', 'optim', 'clinic', 'asset', 'public', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'power', 'scientist', 'relat']"
DE,"Each employee is hand-picked not only for their skills, but for their personality and broad expertise.
Build and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).
Uncover and recommend remediations for data quality anomalies.
Investigate, recommend and implement data ingestion and ETL performance improvements.
Document data ingestion and ETL program designs, present findings, conduct peer code reviews.
Develop and execute test plans to validate code.
Your Expertise:
4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.
3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.
Experience configuring big data solutions in a cloud environment (AWS, Azure or GCP).
Experience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.
Experience developing complex technical and ETL programs within a Hadoop ecosystem.
Must have a bachelors degree in Computer Science, Technology, Computer Information Systems, Computer Applications, Engineering, or a related field.
Your X-Factor:
Aptitude - You have an innate capacity to transition from project to project without skipping a beat.
Communication - You have excellent written and verbal communication skills for coordination across projects and teams.
Impact - You are a critical thinker with an emphasis on creativity and innovation.
Passion - You have the drive to succeed paired with a continuous hunger to learn.
Leadership - You are trusted, empathetic, accountable, and empower others around you.
Why Were Proud To Be Mavens!
Google Cloud North America Services Partner of the Year 2019, 2018
#21 Best Workplaces in Chicago, FORTUNE, 2018
Great Place To Work Certification, Great Place to Work, 2017 & 2018
Fast Fifty, Crain's Chicago Business
101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)
Top Google Cloud Partner, Clutch
Fastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine
Top IT Services Companies, Clutch
Google Global Rising Star Partner of the Year
Ready to Learn More?
Check out the Data Team
See what Glassdoor has to say
Real Customer Stories
","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel']","['recommend', 'anomali', 'big data', 'etl', 'commun', 'account']",1,"['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account']","['program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account', 'program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']"
DE,"Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists.
Seeks to understand the data being worked with as its often unstructured data sets.
Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.
Job Summary Wording
You’ll also be responsible for integrating these applications with the architecture used across the organization.
Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset.
Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems.
Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases.
Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g.
RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions.
Able to employ design patterns and generalize code to address common use cases.
Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g.
Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Applied knowledge of data modeling principles (e.g.
dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g.
Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e.
Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience.
This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
Every dream is a journey that starts with a single step.
Start your journey right here.
Bring your dreams.
Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists.
Seeks to understand the data being worked with as its often unstructured data sets.
Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.
Job Summary Wording
You’ll also be responsible for integrating these applications with the architecture used across the organization.
Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset.
Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems.
Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases.
Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g.
RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions.
Able to employ design patterns and generalize code to address common use cases.
Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g.
Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Applied knowledge of data modeling principles (e.g.
dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g.
Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e.
Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience.
This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
LI:DB1
","['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker']","['pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account']",1,"['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker', 'pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account']","['visual', 'python', 'etl', 'common', 'sourc', 'analyt', 'challeng', 'azur', 'hadoop', 'optim', 'appli', 'primari', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'gcp', 'spark', 'azur', 'sklearn', 'h2o', 'git', 'hadoop', 'aw', 'cloud', 'python', 'nosql', 'docker', 'pipelin', 'visual', 'data modeling', 'machine learning', 'optim', 'big data', 'exploratori', 'etl', 'account', 'visual', 'python', 'etl', 'common', 'sourc', 'analyt', 'challeng', 'azur', 'hadoop', 'optim', 'appli', 'primari', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Bachelor's degree or higher in Computer Science or closely related field 4 years of experience using ""big data"" technologies 2+ years of experience in design and implementing AWS Cloud Solutions at enterprise level Experience with functional or object-oriented languages such as Python, C++, REST, Scala Handson experience on Complex SQl queries Experience with Airflow, Argo, Luigi, or similar orchestration tool Experience performing root cause analysis on Spark jobs to identify areas for improvement Experience with No-SQL databases such as HBase, Cassandra, or Redis.
Experience with streaming technologies such as Kafka, Flink, or Spark Streaming Experience with DevOps principals and CICD and Containers and Kubernetes
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'aw', 'cloud', 'python', 'hbase', 'kubernet', 'kafka']",['big data'],1,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'aw', 'cloud', 'python', 'hbase', 'kubernet', 'kafka', 'big data']","['stream', 'spark', 'aw', 'python', 'comput', 'big', 'relat']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'aw', 'cloud', 'python', 'hbase', 'kubernet', 'kafka', 'big data', 'stream', 'spark', 'aw', 'python', 'comput', 'big', 'relat']"
DE,"The Senior Data Engineer will work within the data and the analytics team and partner with multiple businesses and various engineering teams (Platform, Security) to build high quality data pipelines.
This individual will be responsible to integrate data from a variety of data sources utilizing cloud-based data structures (AWS) and determine/enhance existing data sources between internal and external stakeholders.
The things that you will tackle:
Advise, consult and coach other data and analytic professionals on data standards and practices
Develop, implement and optimize streaming, data lake and analytics big data solutions
Create and execute testing including unit, integration and end-to-end tests of data pipelines
Foster a culture of sharing, re-use, design for scale stability and operational efficiency of data and analytical solutions
Adapt and learn new technologies in a quickly changing field
Create data products for analytics and engineering team members to improve their productivity
Recommend and implement best tools to ensure optimized data performance
Bachelor’s degree in computer science or related field.
Minimum of five (5) years of application development and implementation experience.
Experience with SQL, Big Data, ETL programming and development within
Competitive, comprehensive healthcare coverage.
Generous paid time off, including time off to volunteer!
401K with employer match and profit sharing.
Be Well!
Lifestyle reimbursement program.
Tuition reimbursement and professional development programs.
Are you a Changemaker?
","['sql', 'aw', 'cloud']","['recommend', 'healthcar', 'pipelin', 'optim', 'big data', 'etl']",1,"['sql', 'aw', 'cloud', 'recommend', 'healthcar', 'pipelin', 'optim', 'big data', 'etl']","['analyt', 'program', 'pipelin', 'relat', 'optim', 'aw', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']","['sql', 'aw', 'cloud', 'recommend', 'healthcar', 'pipelin', 'optim', 'big data', 'etl', 'analyt', 'program', 'pipelin', 'relat', 'optim', 'aw', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"The role will be responsible for deploying the model that is designed by the Data Scientist.
The ideal candidate should have worked with Azure in the past especially with Azure Data Bricks solution.
What will you do?
Prepare the raw data to be ready for analytics including cleaning, imputing and standarding data format Write a model as designed by the Data Scientist.
Bachelor degree in Computer Science, Mathematics, or related fields At least continuous 4 years of experience with Spark, especially pySpark implementation.
Experience with Azure Deployment using AKS and Container Services.
Must have an experience with Azure Data Bricks solution that includes Azure ML library, Azure Kafka, Azure CosmosDB and Azure BlobStorage At least a total 8 years in a career related to data analytics.
Must have a high-level understanding of analytics use cases mentioned in one or more of the following Assortment optimization, Promotional planning and effectiveness, Dynamic pricing, Markdown optimization, Labor scheduling optimization
","['kafka', 'spark', 'pyspark', 'azur']",['optim'],1,"['kafka', 'spark', 'pyspark', 'azur', 'optim']","['analyt', 'spark', 'ml', 'azur', 'optim', 'comput', 'scientist', 'relat']","['kafka', 'spark', 'pyspark', 'azur', 'optim', 'analyt', 'spark', 'ml', 'azur', 'optim', 'comput', 'scientist', 'relat']"
DE,"It is estimated each support analyst spends about an hour every day answering client's questions.
The goal of the project is to provide clients access to the data so they can access the data themselves.
Role to be played by the consultant/contractor:
Work with the business, understand and review technical requirements
Design and develop end to end solution
Ability to pick right technology for the execution of project
Profile
Hands on experience working with tools in Big Data ecosystem
Hands on experience in building Kafka streaming application in Java
Hands on experience in building Spark applications in Java or Scala
Hands on experience in working with NoSQL databases
Full stack Java developer
Hands on experience in building production applications using Node.js and React
Environment History of Project:
","['spark', 'scala', 'java', 'nosql', 'kafka', 'react']",['big data'],999,"['spark', 'scala', 'java', 'nosql', 'kafka', 'react', 'big data']","['day', 'big', 'spark']","['spark', 'scala', 'java', 'nosql', 'kafka', 'react', 'big data', 'day', 'big', 'spark']"
DE,"You will be responsible for working closely with Data Architect's and Big Data Engineers and their vision to help design, build and provide governance of large-scale streaming architecture solution that delivers business value across the organization.
You will have the opportunity to both lead and execute on projects and always consider the bigger picture proactively with anticipating any performance issues, troubleshooting, monitoring, quality, etc This role will require experience with hybrid platforms, cloud migration, publishing, development with newer technologies such as Spring Boot, KSQL/Stream Processing, data connectors such as Kafka, performance tuning, data quality and data visualization knowledge.
* Revenue generated* Budget responsibilities* Leads the delivery, support and maintenance of solutions with one or more business and technology areas.
* Organizational impact results from mid-large sized projectsRequired Job Qualifications* Bachelor's degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field* 5+ years of experience in Technology related field including prior lead experience* Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools* Strong organizational, analytical, critical thinking and leadership skills* Demonstrated leadership on mid-large-scale project impacting strategic partnersThrivent provides Equal Employment Opportunity (EEO) without regard to race, religion, color, sex, gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state, or federal law.
",['kafka'],"['big data', 'visual', 'statist', 'predict']",1,"['kafka', 'big data', 'visual', 'statist', 'predict']","['analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']","['kafka', 'big data', 'visual', 'statist', 'predict', 'analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']"
DE,"For this position, Ideal candidate should have Knowledge and experience in API development, AI/ML, SageMaker, Data Lake, Data Analytics, Cloud Monitoring and Analytics.
The candidate must possess Strong cloud programming skill with experience in API and AWS Lambda functions or any other scripting languages like Python / Bash using Python & Node.js.
Good understanding in using AWS CLI, Cloud formation, Terraform, Ansible with troubleshooting experiences is necessary.
For more details, please contact:
Shubham Nigam
(972) 753-6500 x409
jobs@primusglobal.com
","['sagemak', 'aw', 'lambda', 'python', 'cloud']",[None],999,"['sagemak', 'aw', 'lambda', 'python', 'cloud']","['analyt', 'program', 'ml', 'aw', 'python']","['sagemak', 'aw', 'lambda', 'python', 'cloud', 'analyt', 'program', 'ml', 'aw', 'python']"
DE,"This is a 6 month contract for hire role located in down town Chicago.
The Data Catalog Engineer will be responsible for the construction of an enterprise data catalog which will connect to data assets throughout the organization, tag the data, house technical and business meta data, intelligently discover relationships between the data, and enable usage by a wide variety of enterprise users.
Responsibilities:
Continually add to the data catalog by Ingestion of data from a wide variety of data sources, including relational databases, flat files, unstructured data, NoSQL data, ETL repositories, and reporting repositories.
Configure data sources, data domains, and their relationships.
Meet data definition, data quality, data profiling, and data lineage expectations by utilizing the data catalog functionality
Create and administer a data catalog security model to ensure the proper groups can access the right information; create a foundation for future data governance
Meet data privacy requirements by proper data classification and remediation of data privacy use cases
Work closely with the Lead Data Project Manager and Lead Data Business Analyst to ensure that the Data Catalog and Data Privacy programs are meeting expectations.
Collect data issues across the meta data ecosystem, work with business owners and data stewards to remediate issues.
Create a highly structured and comprehensive control environment so that data catalog content remains in high quality
Required Skills:
Bachelor’s Degree in Computer Science, Statistics, Information Systems or other related field
At least 1 year of experience with data catalog systems, technologies, and workflows.
Broad technical skills necessary, with a solid understanding of data systems and infrastructure.
Strong SQL and data analysis skills to query a wide variety of data sources.
Ability to analyze data models, data taxonomy, and deep appreciation of data standards and quality.
Previous hands-on experience with ETL, data profiling, and data lineage tools.
Ability to work with cross functional teams, perform root cause analysis and resolve complex issues.
Strong written and verbal communication skills, including the ability to articulate ideas concisely and clearly to both technical and non-technical audiences
#IND123
","['sql', 'nosql']","['classif', 'commun', 'statist', 'etl']",1,"['sql', 'nosql', 'classif', 'commun', 'statist', 'etl']","['asset', 'program', 'engin', 'relat', 'infrastructur', 'comput', 'statist', 'etl', 'sourc', 'collect']","['sql', 'nosql', 'classif', 'commun', 'statist', 'etl', 'asset', 'program', 'engin', 'relat', 'infrastructur', 'comput', 'statist', 'etl', 'sourc', 'collect']"
DE,"Ability to participate in fast-paced DevOps and System Engineering teams within Scrum agile processes Experience with DevOps tools will be an added advantage Know-how of working with Python Bash scripting will help
",['python'],[None],999,['python'],"['python', 'engin']","['python', 'python', 'engin']"
DE,"Position Summary:
The Data Engineer is responsible for designing, developing, and supporting data management solutions.
The position will develop data models, perform data analysis, construct technical designs, develop data integration solutions, collaborate with team members and business stakeholders, and support existing data solutions.
This position will also lead and coordinate the work activities of offshore development and support resources.
Position Responsibilities may include, but not limited to:
Responsible for the solution architecture, design, development, and support of data management applications
Drive data sourcing and integration solution design and development on hybrid (cloud & on-prem) data solutions
Perform data analysis and architect data models for analytics
Providing guidance and direction to offshore ETL support/development resources
Collaborate with cross functional teams such as Infrastructure, Support, DBA and Business team
Assist with task identification and effort estimates for ETL development
Assist with risk and issue identification and resolution
Provide off-hour/weekend ETL support (on a rotating basis)
Other duties as assigned
",['cloud'],"['etl', 'risk']",999,"['cloud', 'etl', 'risk']","['basi', 'analyt', 'etl', 'infrastructur', 'provid', 'integr', 'sourc', 'engin']","['cloud', 'etl', 'risk', 'basi', 'analyt', 'etl', 'infrastructur', 'provid', 'integr', 'sourc', 'engin']"
DE,"Cloud Data Engineer (Azure)
Chicago, IL
Responsibility:
The role will be responsible for deploying the model that is designed by the Data Scientist.
The deployment will use Azure Platform and is expected to have the scalability and performance of the cloud deployment.
Requirement:
1.
At least continuous 4 years of experience with Spark, especially pySpark implementation
2.
Experience with Azure Deployment using AKS and Container Services
3.
MUST HAVE experience with Azure Data Bricks solution that includes
a. Azure ML library
b. Azure Kafka
c. Azure CosmosDB
d. Azure BlobStorage
4.
At least a total 8 years in a career related to data analytics
5.
Must have a high level understanding of analytics use cases mentioned in one or more of the following:
a.
Assortment optimization
b.
Promotional planning and effectiveness
c. Dynamic pricing
d. Markdown optimization
e. Labor scheduling & optimization
f. Various types of forecasting & large scale predictions
g. Large-scale experimental frameworks (multi-arm bandits, etc.)
h. Channel optimization;
i.
Fraud detection
","['cloud', 'kafka', 'spark', 'azur']","['optim', 'bandit', 'predict']",999,"['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict']","['analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat', 'engin']","['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict', 'analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat', 'engin']"
DE,"As a Senior Data Engineer, you will contribute to database management of large-scale web-based applications through the use of SQL, Cloud (AWS or Azure), and the Microsoft BI stack.
These technologies enable the client's business while supporting architectural vision of quality, scalability, performance and function.
Responsibilities:
Data Engineer's work in conjunction with Software Engineers, DBA's, Business Analysts, Quality Assurance and business owners
Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions
Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.
Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.
Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.
Perform code reviews and provide feedback in a timely manner.
Promote collective code ownership for everyone to have visibility into the feature codebase.
Present technical ideas and concepts in business-friendly language.
Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.
Identify and correct performance bottlenecks related to SQL code.
Support timely production releases and adherence to release activities.
Contribute to data retention strategy.
Position Requirements
3+ years in commercial-grade business applications environment leveraging the following:
SQL Server, T-SQL, SSIS, stored procedures, user-defined functions and table functions
Managing design risk
3+ years leveraging OO programming techniques
Software development lifecycles, Unit test techniques, debugging/analytical techniques
2+ years of Cloud Platform experience with either AWS (preferred) or Azure
","['sql', 'azur', 'bi', 'aw', 'cloud', 'microsoft']","['recommend', 'optim', 'risk']",999,"['sql', 'azur', 'powerbi', 'aw', 'cloud', 'microsoft', 'recommend', 'optim', 'risk']","['analyt', 'techniqu', 'engin', 'challeng', 'azur', 'provid', 'optim', 'bi', 'aw', 'releas', 'relat', 'collect', 'evalu']","['sql', 'azur', 'powerbi', 'aw', 'cloud', 'microsoft', 'recommend', 'optim', 'risk', 'analyt', 'techniqu', 'engin', 'challeng', 'azur', 'provid', 'optim', 'bi', 'aw', 'releas', 'relat', 'collect', 'evalu']"
DE,"The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
You should be able to not take yourself too seriously as well.
What you'll do:
Plan and deliver data engineering projects
Manage performance of the team including recurring 1:1s, performance reviews, goal setting and professional development
Identify roadblocks and remove them proactively
Implement effective metrics and monitoring processes
Driving a culture of continuous improvement
Stay informed about new technologies and trends in data engineering
B.S.
or B.A.
Proficiency in at least one object oriented language (Java, Python or Scala preferred)
Proficiency in SQL and comfortable working with large datasets to identify insights
Experience with open source big data technologies (HDFS, Hive, Presto, Pig, MapReduce)
Experience designing, developing and deploying data pipelines
Strong problem solving skills
Experience managing and mentoring a small team of data engineers
Strong communication skills
What puts you over the top (one of the following):
Expertise in Apache Spark
Expertise implementing streaming data pipelines using open source technologies
Experience with AWS ecosystem
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'spark', 'scala', 'pig', 'aw', 'python', 'hive', 'java']","['problem solving', 'commun', 'pipelin', 'big data']",1,"['sql', 'mapreduc', 'spark', 'scala', 'pig', 'aw', 'python', 'hive', 'java', 'problem solving', 'commun', 'pipelin', 'big data']","['engin', 'spark', 'pipelin', 'aw', 'python', 'big', 'set', 'sourc', 'collect']","['sql', 'mapreduc', 'spark', 'scala', 'pig', 'aw', 'python', 'hive', 'java', 'problem solving', 'commun', 'pipelin', 'big data', 'engin', 'spark', 'pipelin', 'aw', 'python', 'big', 'set', 'sourc', 'collect']"
DE,"Senior Applied Data Engineers are experts in creating elegant, maintainable, and extensible data pipelines and database architectures.
As a Senior Applied Data Engineer, you will be the go-to person for questions about how to handle new datasets.
You will build maintainable and extensible ETL solutions for diverse data sources.
**This is a temporary, full-time, salaried position with benefits slated to end Dec. 31, 2020.
There is a possibility of extension if the business requires it.
**
Responsibilities
Transform data based on business requirements
Create pipelines that deliver data error-free and on-time with features such as logging, fault tolerance, notifications, and scalability
Document and train others on data pipelines
Serve as a technical resource in resolving issues related to data pipelines
Work closely with client-facing teams
Mentor junior members of the Applied Data Engineering team
Project manage data pipeline construction and maintenance work
Minimum Requirements
Bachelor’s degree in an analytical subject (statistics, math, economics, physics, engineering, business, political or social science, computer science, etc)
5+ years of experience with database and ETL pipeline design
Experience leading data and/or software projects
Proficiency in Python and SQL
Ability to work as part of a cross-functional team to solve problems and build solutions
Strong communication and teamwork skills
Eagerness to constantly learn and teach others
Preferred Qualifications
Significant experience with Python and SQL
Experience with software development practices including unit testing, version control, code review, and Agile development
Experience with technical project management
Mentorship and training experience
Familiarity with data and database technologies such as Redshift, Spark, S3, postgres, HDFS, etc., and with issues related to distributed systems.
The opportunity to be part of a growing tech startup focused on solving interesting and meaningful problems, invested in internal promotion, and committed to fostering a diverse, equal and inclusive workplace.
Competitive benefits, including unlimited PTO, 401K match with immediate vesting, health, dental, and vision benefits, paid parental leave, breastfeeding support including breastmilk shipping services for traveling moms, flexible work from home policy, commuter benefits, wellness initiatives including weekly group meditations, monthly on-site massage therapy, and pet insurance.
If you have a disability or special need that requires accommodation, please contact internalrecruiting@civisanalytics.com
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States.
EEO IS THE LAW
EEO Supplement
Pay Transparency
","['sql', 'spark', 'python', 's3', 'redshift']","['pipelin', 'statist', 'etl', 'commun', 'econom']",1,"['sql', 'spark', 'python', 's3', 'redshift', 'pipelin', 'statist', 'etl', 'commun', 'econom']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'python', 'comput', 'appli', 'statist', 'etl', 'sourc', 'engin']","['sql', 'spark', 'python', 's3', 'redshift', 'pipelin', 'statist', 'etl', 'commun', 'econom', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'python', 'comput', 'appli', 'statist', 'etl', 'sourc', 'engin']"
DE,"If your Interested Kindly Call me Or Mail me ASAP...
Position: Data Engineer
Duration: 6+ Month
Contract
Visa: Only GC , USC or H4 EAD
Key Words: SQL, Python, SQL Server, Data Engineer, Airflow, Spark
Required technologies: SQL and Python.
This person should be well-versed with both
Experience building and modifying data pipelines
NOT REQUIRED, but it is a plus if candidates have Apache Airflow, Spark, or Google Services
Thanks & Regards,
Raj - Rajesh Kumar
317-648-1580 Ext -504 (Voice)
rajeshk@avtechsol.com
","['sql', 'python', 'spark', 'airflow']",['pipelin'],999,"['sql', 'python', 'spark', 'airflow', 'pipelin']","['python', 'spark', 'engin', 'pipelin']","['sql', 'python', 'spark', 'airflow', 'pipelin', 'python', 'spark', 'engin', 'pipelin']"
DE,"Position Summary:
The Data Engineer is responsible for designing, developing, and supporting data management solutions.
The position will develop data models, perform data analysis, construct technical designs, develop data integration solutions, collaborate with team members and business stakeholders, and support existing data solutions.
This position will also lead and coordinate the work activities of offshore development and support resources.
Position Responsibilities may include, but not limited to:
Responsible for the solution architecture, design, development, and support of data management applications
Drive data sourcing and integration solution design and development on hybrid (cloud & on-prem) data solutions
Perform data analysis and architect data models for analytics
Providing guidance and direction to offshore ETL support/development resources
Collaborate with cross functional teams such as Infrastructure, Support, DBA and Business team
Assist with task identification and effort estimates for ETL development
Assist with risk and issue identification and resolution
Provide off-hour/weekend ETL support (on a rotating basis)
Other duties as assigned
Position Requirements:
Required Skills and Experience:
4 year degree in Computer Science, Information Systems
5+ years of relevant Business Intelligence/Data Warehousing/Data Integration work experience
4+ years of Hands-on experience developing data management solutions using MSBI (SSIS)
2+ years Data modelling (logical/physical, relational and document/object) and 2+ years of Data Integration solution design experience
1-3 years of developing solutions using Windows Server OS
Strong understanding of Dimensional Modeling techniques
Experience leading other developers
Strong written and verbal communication skills
Strong problem solving and analytical skills
Strong understanding of SDLC best practices with an emphasis on DW/BI practices
This position must pass a post-offer background and drug test
Preferred Skills and Experience:
Experience with one of the ETL tools - MS SSIS, Informatics on SQL Server DB
1+ year of Public cloud experience (Azure, AWS)
Proficiency in scripting languages such as Power Shell
1+ year of experience leveraging Cloud Services (IaaS, PaaS)
1+ year hands-on development experience using open source data integration tool such as Talend, Azure Data Factory
Experience leading offshore resources
ITIL certification
Experience in Agile (SCRUM) and Waterfall methodologies
Distribution and Logistics Industry experience
Physical Demands and Work Environment:
Should an individual in this classification not be able to adhere to this requirement due to a disability, they should contact their Human Resources department to see what, if any, reasonable accommodation may be
As an Equal Opportunity Employer, Reyes Holdings companies will recruit and select applicants for employment solely on the basis of their qualifications.
Drug Free Employer.
jobDetails
","['sql', 'azur', 'bi', 'aw', 'cloud', 'db']","['classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun']",1,"['sql', 'azur', 'powerbi', 'aw', 'cloud', 'db', 'classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun']","['basi', 'analyt', 'techniqu', 'azur', 'relat', 'power', 'human', 'public', 'infrastructur', 'provid', 'bi', 'aw', 'comput', 'etl', 'sourc', 'engin', 'integr']","['sql', 'azur', 'powerbi', 'aw', 'cloud', 'db', 'classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun', 'basi', 'analyt', 'techniqu', 'azur', 'relat', 'power', 'human', 'public', 'infrastructur', 'provid', 'bi', 'aw', 'comput', 'etl', 'sourc', 'engin', 'integr']"
DE,"Escalates product bugs to the vendor and follows issues through to conclusion bull Fulfills responsibilities for on call support, including potential coverage of on-call support rotation for off office hours bull Maintains and reports operational Key Performance Indicators (KPIs) on items such as performance, service incidents and tickets set forth by the organization leadership bull Creates and maintains documentation supporting training, system administration, deployment, and operational processes and procedures.
bull Contributes to support deployment, including planning and execution, data conversion approach, script development and execution, warranty period and transition of the solution to the platform's operational context bull Learns to operate effectively in both waterfall and Agile Software Development Lifecycle (SDLC) processes and methodologies bull Must have experience in building complex dashboards, reports using complex data sets on the Incorta platform bull Must have strong SQL skills and be able to quickly ldquovisualizerdquo the SQL getting passed to the database bull Desired experience with latest Business Intelligence and analytical technologies (Ex Analytics, Big Data, Statistics, Algorithms, and Cloud computing etc...) EXPERIENCE SKILLS bull 5+ years of overall industry experience bull 1+ years of Incorta experience Required Skills bull Expertise with developing on and supporting the Incorta platform bull Advanced report and SQL Query development skills bull Good understanding of ETL concepts and data modeling bull Expertise in query writing and scripting skills is a plus ndash PLSQL, Unix, windows scripting, Python bull Working knowledge of Apache Spark and Python scripting is a huge plus bull Working knowledge of Power BI or Cognos would be a plus bull Bachelor degree in Computer Science or a related discipline Please send your resume to sjayakumarprairieinc.com for immediate consideration.
","['sql', 'spark', 'cogno', 'unix', 'bi', 'cloud', 'python', 'power bi']","['dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi']",1,"['sql', 'spark', 'cogno', 'unix', 'powerbi', 'cloud', 'python', 'dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi']","['analyt', 'learn', 'spark', 'set', 'relat', 'power', 'bi', 'python', 'comput', 'statist', 'big', 'etl', 'algorithm']","['sql', 'spark', 'cogno', 'unix', 'powerbi', 'cloud', 'python', 'dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi', 'analyt', 'learn', 'spark', 'set', 'relat', 'power', 'bi', 'python', 'comput', 'statist', 'big', 'etl', 'algorithm']"
DE,"Data Engineer
YOUR OPPORTUNITY:
Develop solutions that enable investment professionals to efficiently extract insights from data.
This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
YOUR SKILLS & TALENTS:
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker']","['research', 'commun']",1,"['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun']","['program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']","['sql', 'spark', 'dask', 'airflow', 'azur', 'jupyt', 'hadoop', 'aw', 'snowflak', 'java', 'python', 's3', 'nosql', 'kubernet', 'kafka', 'docker', 'research', 'commun', 'program', 'engin', 'spark', 'azur', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'quantit', 'collect']"
DE,"Want to be a part of a team of diverse collaborators in an authentically fun culture?
More About the Role
Data Engineer will be responsible for building efficient data pipelines that transform raw data into a format usable by downstream applications that serve both analytical and operational use cases.
Working with high volumes of data to efficiently process and expose for analysis
Collaborating with other engineering teams on strategies for data
Work with cutting edge data processing technologies
Analyze data to measure impacts of data schemas and use it to iterate on improvements
Translate from technical to business, and vice versa.
You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them.
Then turn around and do it the other way
Excellent knowledge on SQL, data modelling and patterns.
5-7 years experience with Python or another general purpose programming language
Background in writing ETL jobs within a Business Intelligence context
A bachelor's degree, preferably in a computer-related discipline.
Enthusiasm for the job.
Are you excited about data?
Do you love your users?
Got These?
Even Better:
Experience big data processing with Spark and other big data tools a plus
Excellent communication skills, including the ability to crystallize and broadly socialize insights
Problem analysis and problem-solving skills
Rigorous attention to detail and accuracy
Exposure to Amazon AWS or another cloud provider
Adaptability and collaborative skills
Flexible PTO.
Health and Wellness.
Learning and Career Growth.
MealPerks.
Who’s ready for some lunch?
Fun.
Social Impact.
The EEO is the Law poster is available here: DOL Poster.
Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address.
","['sql', 'spark', 'aw', 'cloud', 'python', 'excel']","['pipelin', 'analyz', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'aw', 'cloud', 'python', 'excel', 'pipelin', 'analyz', 'big data', 'etl', 'commun']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'provid', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']","['sql', 'spark', 'aw', 'cloud', 'python', 'excel', 'pipelin', 'analyz', 'big data', 'etl', 'commun', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'provid', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']"
DE,"Position Summary:
The Data Engineer is responsible for designing, developing, and supporting data management solutions.
The position will develop data models, perform data analysis, construct technical designs, develop data integration solutions, collaborate with team members and business stakeholders, and support existing data solutions.
This position will also lead and coordinate the work activities of offshore development and support resources.
Position Responsibilities may include, but not limited to:
Responsible for the solution architecture, design, development, and support of data management applications
Drive data sourcing and integration solution design and development on hybrid (cloud & on-prem) data solutions
Perform data analysis and architect data models for analytics
Providing guidance and direction to offshore ETL support/development resources
Collaborate with cross functional teams such as Infrastructure, Support, DBA and Business team
Assist with task identification and effort estimates for ETL development
Assist with risk and issue identification and resolution
Provide off-hour/weekend ETL support (on a rotating basis)
Other duties as assigned
Position Requirements:
Required Skills and Experience:
4 year degree in Computer Science, Information Systems
5+ years of relevant Business Intelligence/Data Warehousing/Data Integration work experience
4+ years of Hands-on experience developing data management solutions using MSBI (SSIS)
2+ years Data modelling (logical/physical, relational and document/object) and 2+ years of Data Integration solution design experience
1-3 years of developing solutions using Windows Server OS
Strong understanding of Dimensional Modeling techniques
Experience leading other developers
Strong written and verbal communication skills
Strong problem solving and analytical skills
Strong understanding of SDLC best practices with an emphasis on DW/BI practices
This position must pass a post-offer background and drug test
Preferred Skills and Experience:
Experience with one of the ETL tools - MS SSIS, Informatics on SQL Server DB
1+ year of Public cloud experience (Azure, AWS)
Proficiency in scripting languages such as Power Shell
1+ year of experience leveraging Cloud Services (IaaS, PaaS)
1+ year hands-on development experience using open source data integration tool such as Talend, Azure Data Factory
Experience leading offshore resources
ITIL certification
Experience in Agile (SCRUM) and Waterfall methodologies
Distribution and Logistics Industry experience
Physical Demands and Work Environment:
Should an individual in this classification not be able to adhere to this requirement due to a disability, they should contact their Human Resources department to see what, if any, reasonable accommodation may be
As an Equal Opportunity Employer, Reyes Holdings companies will recruit and select applicants for employment solely on the basis of their qualifications.
Drug Free Employer.
jobDetails
","['sql', 'azur', 'bi', 'aw', 'cloud', 'db']","['classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun']",1,"['sql', 'azur', 'powerbi', 'aw', 'cloud', 'db', 'classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun']","['basi', 'analyt', 'techniqu', 'azur', 'relat', 'power', 'human', 'public', 'infrastructur', 'provid', 'bi', 'aw', 'comput', 'etl', 'sourc', 'engin', 'integr']","['sql', 'azur', 'powerbi', 'aw', 'cloud', 'db', 'classif', 'risk', 'etl', 'data warehousing', 'problem solving', 'logist', 'commun', 'basi', 'analyt', 'techniqu', 'azur', 'relat', 'power', 'human', 'public', 'infrastructur', 'provid', 'bi', 'aw', 'comput', 'etl', 'sourc', 'engin', 'integr']"
DE,"Escalates product bugs to the vendor and follows issues through to conclusion bull Fulfills responsibilities for on call support, including potential coverage of on-call support rotation for off office hours bull Maintains and reports operational Key Performance Indicators (KPIs) on items such as performance, service incidents and tickets set forth by the organization leadership bull Creates and maintains documentation supporting training, system administration, deployment, and operational processes and procedures.
bull Contributes to support deployment, including planning and execution, data conversion approach, script development and execution, warranty period and transition of the solution to the platform's operational context bull Learns to operate effectively in both waterfall and Agile Software Development Lifecycle (SDLC) processes and methodologies bull Must have experience in building complex dashboards, reports using complex data sets on the Incorta platform bull Must have strong SQL skills and be able to quickly ldquovisualizerdquo the SQL getting passed to the database bull Desired experience with latest Business Intelligence and analytical technologies (Ex Analytics, Big Data, Statistics, Algorithms, and Cloud computing etc...) EXPERIENCE SKILLS bull 5+ years of overall industry experience bull 1+ years of Incorta experience Required Skills bull Expertise with developing on and supporting the Incorta platform bull Advanced report and SQL Query development skills bull Good understanding of ETL concepts and data modeling bull Expertise in query writing and scripting skills is a plus ndash PLSQL, Unix, windows scripting, Python bull Working knowledge of Apache Spark and Python scripting is a huge plus bull Working knowledge of Power BI or Cognos would be a plus bull Bachelor degree in Computer Science or a related discipline Please send your resume to sjayakumarprairieinc.com for immediate consideration.
","['sql', 'spark', 'cogno', 'unix', 'bi', 'cloud', 'python', 'power bi']","['dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi']",1,"['sql', 'spark', 'cogno', 'unix', 'powerbi', 'cloud', 'python', 'dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi']","['analyt', 'learn', 'spark', 'set', 'relat', 'power', 'bi', 'python', 'comput', 'statist', 'big', 'etl', 'algorithm']","['sql', 'spark', 'cogno', 'unix', 'powerbi', 'cloud', 'python', 'dashboard', 'data modeling', 'statist', 'big data', 'etl', 'kpi', 'analyt', 'learn', 'spark', 'set', 'relat', 'power', 'bi', 'python', 'comput', 'statist', 'big', 'etl', 'algorithm']"
DE,"AddThisShare
Data Engineer
Data Engineering Job Overview
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Hadoop ‘big data’, and cloud native technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights
Work with executive, functional, and technical stakeholders regarding data-related technical design, development, and architecture initiatives
Create data tools for analytics and data scientist team members
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Ability to design and build data models which facilitate end user reporting and analysis
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Willingness to travel.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Education: Bachelor’s degree.
#LI-BB1
","['sql', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka']","['optim', 'end user', 'pipelin', 'big data']",1,"['sql', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'end user', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'python', 'scientist', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka', 'optim', 'end user', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'python', 'scientist', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Sr.
Cloud Data Engineer
If you have experience as a data pipeline builder and data wrangler and enjoy optimizing data systems and building them from the ground up, this is a great role for you.
You are self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities for Cloud Data Engineer
Create and maintain optimal data pipeline architecture with AWS
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Qualifications for Cloud Data Engineer
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
They should also have experience using the following software/tools:
Experience with big data tools: Kinesis, Glue, S3, etc.
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools: AWS Glue, Snaplogic, Fivetran
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Internal ID: 103197
","['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'scala', 'ec2', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'action', 'big', 'set', 'sourc', 'engin']"
DE,"Azure Data Engineer
Photon/wellgreen
$60-65/hr
Visa: H1B, USC, GC, TN Visa, L2 EAD
Must - Azure Data Bricks, Azure ML library, Azure Kafka, Spark, especially pySpark
Responsibility:
The role will be responsible for deploying the model that is designed by the Data Scientist.
The
deployment will use Azure Platform and is expected to have the scalability and performance of
the cloud deployment.
Requirement:
1.
At least continuous 4 years of experience with Spark, especially pySpark
implementation
2.
Experience with Azure Deployment using AKS and Container Services
3.
MUST HAVE experience with Azure Data Bricks solution that includes
a. Azure ML library
b. Azure Kafka
c. Azure CosmosDB
d. Azure BlobStorage
4.
At least a total 8 years in a career related to data analytics
5.
Must have a high level understanding of analytics use cases mentioned in one or more
of the following:
a.
Assortment optimization
b.
Promotional planning and effectiveness
c. Dynamic pricing
d. Markdown optimization
e. Labor scheduling & optimization
f. Various types of forecasting & large scale predictions
g. Large-scale experimental frameworks (multi-arm bandits, etc.)
h. Channel optimization;
i.
Fraud detection
Contact:
Su@eninsystems.com
615-710-8582
","['cloud', 'kafka', 'spark', 'azur']","['optim', 'bandit', 'predict']",999,"['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict']","['analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat', 'engin']","['cloud', 'kafka', 'spark', 'azur', 'optim', 'bandit', 'predict', 'analyt', 'spark', 'ml', 'azur', 'predict', 'optim', 'scientist', 'relat', 'engin']"
DE,"This is a 6 month contract for hire role located in down town Chicago.
The Data Catalog Engineer will be responsible for the construction of an enterprise data catalog which will connect to data assets throughout the organization, tag the data, house technical and business meta data, intelligently discover relationships between the data, and enable usage by a wide variety of enterprise users.
Responsibilities:
Continually add to the data catalog by Ingestion of data from a wide variety of data sources, including relational databases, flat files, unstructured data, NoSQL data, ETL repositories, and reporting repositories.
Configure data sources, data domains, and their relationships.
Meet data definition, data quality, data profiling, and data lineage expectations by utilizing the data catalog functionality
Create and administer a data catalog security model to ensure the proper groups can access the right information; create a foundation for future data governance
Meet data privacy requirements by proper data classification and remediation of data privacy use cases
Work closely with the Lead Data Project Manager and Lead Data Business Analyst to ensure that the Data Catalog and Data Privacy programs are meeting expectations.
Collect data issues across the meta data ecosystem, work with business owners and data stewards to remediate issues.
Create a highly structured and comprehensive control environment so that data catalog content remains in high quality
Required Skills:
Bachelor’s Degree in Computer Science, Statistics, Information Systems or other related field
At least 1 year of experience with data catalog systems, technologies, and workflows.
Broad technical skills necessary, with a solid understanding of data systems and infrastructure.
Strong SQL and data analysis skills to query a wide variety of data sources.
Ability to analyze data models, data taxonomy, and deep appreciation of data standards and quality.
Previous hands-on experience with ETL, data profiling, and data lineage tools.
Ability to work with cross functional teams, perform root cause analysis and resolve complex issues.
Strong written and verbal communication skills, including the ability to articulate ideas concisely and clearly to both technical and non-technical audiences
#IND123
","['sql', 'nosql']","['classif', 'commun', 'statist', 'etl']",1,"['sql', 'nosql', 'classif', 'commun', 'statist', 'etl']","['asset', 'program', 'engin', 'relat', 'infrastructur', 'comput', 'statist', 'etl', 'sourc', 'collect']","['sql', 'nosql', 'classif', 'commun', 'statist', 'etl', 'asset', 'program', 'engin', 'relat', 'infrastructur', 'comput', 'statist', 'etl', 'sourc', 'collect']"
DE,"Ready to discover the possibilities that live in technology?
Street-Smart - Thriving in Dynamic Times
Juice - The ""Stuff"" it takes to be a Needle Mover
Teamwork - Humble, Hungry and Smart
About the Role:
What You'll Do:
The Data Engineer will act as an expert and trusted advisor who develops, implements, troubleshoots, and optimizes data solutions across many platforms.
This role will work closely with clients, partners and other business units to ensure consulting engagements are successful.
SUMMARY OF ESSENTIAL JOB FUNCTIONS:
Understand customers' overall data estate, IT and business priorities and success measures to design implementation architectures and solutions using advanced analytics and artificial intelligence
Maintain and advance deep technical skills and knowledge, keeping up to date with market trends and competitive insights, and share within the technical community
Be a Voice of Customer to share insights and best practices, connect with Engineering team to remove key blockers
Assess the Customers' knowledge of Azure platform and overall cloud readiness to support customers through a structured learning plan and ensure its delivery through partners
Responsible for design, development, and hands-on implementation of data intelligence solutions including data platform build-up, proof of concepts or pilot implementation, software development, software integration, and documentation
Perform hands on development of apache, big data technologies, and framework
Align solutions with standards and best practices working with cross-functional engineering and consulting teams
Collaborate and communicate with Sales and Account Management team to ensure smooth and successful delivery and assist with the identification of additional Advanced Services and Sales opportunities within the customer's environment
Establish strong and lasting relationships with key stakeholders and decision makers in client organizations
REQUIRED SKILLS AND EXPERIENCE:
Bachelor's degree from an accredited university required
Understanding and hands on experience with modern distributed data systems(Hadoop ecosystem, public cloud, etc).
Experience building applications in c# or java.
Understanding of BI technologies from traditional data warehousing to SaaS solutions in the cloud.
Experience in designing data and analytics architectures in Microsoft Azure cloud.
Well informed on cloud native technologies that enable batch and streaming data ingestion into cloud (For example: Azure Data Factory, Azure Event Hubs, Azure IoT Hubs etc)
Experienced in designing data lakes in Azure cloud for serving big data analytical workloads.
Proven track record of driving decisions collaboratively, resolving conflicts and ensuring follow through with exceptional verbal and written communication skills
Microsoft Certified Azure Solutions Architect Expert certification a plus.
Previous experience working for a consulting or services organization strongly preferred
5+ years of software development experience in distributed systems and building large-scale applications
5+ years of experience in building large scale, high performance, high availability systems and Strong Computer Science fundamentals (algorithms, data structures)
Hadoop, NoSQL or other Big Data certifications are a huge plus
Experience with Big Data technologies (SPARK, HDFS, HBase, Cloudera, MAPR, Hadoop and other frameworks in Hadoop ecosystem
Deep knowledge of Hadoop tools (MapReduce, SPARK, Oozie, ELK, KAFKA, HUE, HBase)
Fluency in several programming languages such as Python, Scala, or Java, with the ability to pick up new languages and technologies quickly
Intermediate knowledge with software engineering best practices
Must be able to quickly understand technical and business requirements and be able to translate them into technical implementations
Ability to mix deep technical expertise with simple, everyday language to deliver a story that is memorable, educational and useful
Highly organized, detail-oriented, excellent time management skills and able to effectively prioritize tasks in a fast-paced, high-volume, and evolving work environment
Ability to approach customer and sales requests with a proactive and consultative manner; listen and understand user requests and needs and effectively deliver
Comfortable managing multiple and changing priorities, and meeting deadlines in an entrepreneurial environment
Motivated self-starter who loves to troubleshoot and solve challenging problems and feels comfortable working directly with customers
The Perks:
Comprehensive medical, dental and vision plans for you and your dependents
401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term Disability
Competitive Compensation
Training and development programs
Stocked kitchen with snacks and beverages
Collaborative and cool office culture
Work-life balance and generous paid time off
","['mapreduc', 'spark', 'azur', 'scala', 'hadoop', 'bi', 'cloud', 'python', 'java', 'c', 'nosql', 'microsoft', 'hbase', 'kafka', 'excel']","['data warehousing', 'commun', 'big data', 'account']",1,"['mapreduc', 'spark', 'azur', 'scala', 'hadoop', 'powerbi', 'cloud', 'python', 'java', 'c', 'nosql', 'microsoft', 'hbase', 'kafka', 'excel', 'data warehousing', 'commun', 'big data', 'account']","['analyt', 'essenti', 'program', 'learn', 'spark', 'azur', 'public', 'hadoop', 'bi', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['mapreduc', 'spark', 'azur', 'scala', 'hadoop', 'powerbi', 'cloud', 'python', 'java', 'c', 'nosql', 'microsoft', 'hbase', 'kafka', 'excel', 'data warehousing', 'commun', 'big data', 'account', 'analyt', 'essenti', 'program', 'learn', 'spark', 'azur', 'public', 'hadoop', 'bi', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Innovative, adaptable, results-driven.
The role of the Data Engineer is to work with Data Architect to build Enterprise Data Management system from the ground up.
The Data Engineer also works as a liaison between the subject matter experts in other departments and the Information Systems Department to understand the business requirements, needs and gaps in order to identify the appropriate datasets to perform analysis and develop insightful reports and dashboards.
You should be open to new and different ways to accomplish your work and be comfortable with new processes, initiatives and changes in priorities.
Responsibilities
Work with Data Architect to develop a data lake, data warehouse in local and/or Azure cloud environment
Integrate disparate data models into coherent enterprise data models
Develop ETL data pipelines to populate data lake and warehouse
Actively participate in Data Governance Program to maintain metadata and data definitions
Work in a team environment with other departments to develop reports, KPIs and dashboards (very strong communication skills)
Interpret business requirements to identify proper tools and methods to analyze, identify and report data trends and variances
Education and Experience
Bachelor’s Degree in Computer Science, Computer Engineering or Information Systems with university level programming courses
3+ years of data engineer experience
3+ years of recent experience in ETL and data warehouse development or maintenance
3+ years of experience in KPI, reports and dashboard development
Experience in Azure and SharePoint is a plus
Experience in agile software development environment is a plus
Skills and Abilities
Proficiency in Power BI, Azure Cloud, C#, ETL (SSIS preferred), T-SQL, Excel
Ability to develop data dictionaries of an existing database
Ability to write, analyze and debug SQL queries
Ability to develop dashboards and data models in Power BI; must be familiar with DAX and Power Pivot, and be willing to get proficient at them
Ability to develop business models and perform analysis in MS Excel
Proficiency in R and Python preferred
Working at April Health
You’ll work in a modern office alongside dedicated colleagues who work to consistently provide a superior patient experience.
","['sql', 'azur', 'bi', 'cloud', 'python', 'c', 'r', 'excel', 'power bi']","['pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi']",1,"['sql', 'azur', 'powerbi', 'cloud', 'python', 'c', 'r', 'excel', 'pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi']","['program', 'pipelin', 'azur', 'power', 'bi', 'python', 'interpret', 'comput', 'warehous', 'etl', 'engin', 'integr']","['sql', 'azur', 'powerbi', 'cloud', 'python', 'c', 'r', 'excel', 'pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi', 'program', 'pipelin', 'azur', 'power', 'bi', 'python', 'interpret', 'comput', 'warehous', 'etl', 'engin', 'integr']"
DE,"Country:
United States
Cities:
Houston
Area of expertise:
Analytics
Love living in the cloud?
As an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions.
You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools.
You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.
Day-to-day, you will:
• Translate business requirements to technical solutions using strong business insight.
• Analyzes current business practices, processes and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
• Support the planning and implementation of data design services, providing sizing and configuration assistance, and performing needs assessments.
• Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
• Design and Build Modern Data Pipelines and Data Streams.
• Design and Build Data Service APIs.
• Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
• Expose data to end users using Power BI, Azure API Apps or other modern visualization platform or experience.
• Implement effective metrics and monitoring processes.
• Able to travel approximately 80%
Your technical/non-technical skills include:
• Demonstrable experience of turning business use cases and requirements to technical solutions.
• Experience in business processing mapping of data and analytics solutions.
• Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
• T-SQL is required.
• Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.
• Experience preparing data for Data Science and Machine Learning.
• Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
• Strong collaboration ethic and experience working with remote teams.
• Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
(nice to have)
• Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
(nice to have)
• Knowledge of Lambda and Kappa architecture patterns.
(nice to have)
Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field.
An equivalent combination of education and experience will also suffice.
Preferred Years of Work Experience:
You likely have about 3-5+ years of relevant professional experience.
• 14-time winner of Microsoft Partner of the Year
• 24,000+ certifications in Microsoft technology
• 90+ Microsoft partner awards
• 17 Gold Competencies
• 3,500 analytics professionals worldwide
• 1,000 data engineers
• Implemented analytics systems for more than 550 clients
• 400 AI practitioners
• 300 cognitive service experts
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['sql', 'azur', 'bi', 'lambda', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']",1,"['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']","['day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']","['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology', 'day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']"
DE,"Data Engineer
ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow
Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle
Experience with cloud-based platforms and tools
Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps
Experience with integrating to both database systems and APIs
Experience with documenting technical requirements, designs and systems
Extensive experience building scalable and resilient data pipelines
Extensive experience writing SQL
Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R
Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
Requirements
3-5+ years of experience in technology and/or consulting
Bachelor’s Degree in CS, MIS, CIS, or a comparable technical degree
Benefits
","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud']","['data warehousing', 'etl', 'pipelin', 'big data']",1,"['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']"
DE,"Learn more at slalom.com.About the RoleAs a Data Engineer, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies.
In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.Responsibilities* Work as part of a team to develop Cloud Data and Analytics solutions* Participate in development of cloud data warehouses and business intelligence solutions* Data wrangling of heterogeneous data and explore and discover new insights* Gain hands-on experience with new data platforms and programming languages (e.g.
","['google cloud', 'spark', 'azur', 'hadoop', 'cloud', 'amazon web services']",['big data'],999,"['google cloud', 'spark', 'azur', 'hadoop', 'cloud', 'aw', 'big data']","['analyt', 'big', 'program', 'stream', 'spark', 'azur', 'hadoop', 'warehous', 'relat', 'engin', 'particip']","['google cloud', 'spark', 'azur', 'hadoop', 'cloud', 'aw', 'big data', 'analyt', 'big', 'program', 'stream', 'spark', 'azur', 'hadoop', 'warehous', 'relat', 'engin', 'particip']"
DE,"Responsibilities:
• Collaborate with data scientists, product management, and web engineers to deliver value and project outcomes
• Convert prototype models and data pipelines built by data scientists for use in production.
• Evaluation and debugging of model performance to ensure parity with prototype (Spark/Java/Python).
• Develop low-latency, real-time predictive models in a microservice environment (Java).
• Balance long-term code health and maintainability with business needs.
• Profiling and performance tuning of production code.
• ML Ops: support of Dataproc, Zeppelin, Gitlab, continuous integration systems, monitoring, alerting, etc.
Qualifications:
• Experience and interest in Big Data technologies (Hadoop/ Spark/ NoSQL DB)
• Experience working on projects within the cloud ideally Azure/AWS
• Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc.
SQL Python, Java, Scala, R
• Experience in at least one ETL tool
• Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Please send your resume to Andrew Butler, Senior Technical Recruiter for immediate consideration.
Job Requirements:
","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'r', 'nosql', 'cloud']","['pipelin', 'tune', 'predict', 'big data', 'etl']",999,"['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'r', 'nosql', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl']","['program', 'spark', 'pipelin', 'ml', 'azur', 'set', 'integr', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'r', 'nosql', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl', 'program', 'spark', 'pipelin', 'ml', 'azur', 'set', 'integr', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']"
DE,"Data Engineer
Vacancy Number: 26110
Date Posted: May 18, 2020
Data Engineer
About the role
This role is responsible for the extraction, collection warehousing and preparation of diverse data sets to support building data science pipelines.
This role requires the ability to quickly build and deploy data collection, ingestion and delivery pipelines to facilitate data use by analytics and advanced machine learning/AI engines.
In this role you will:
Work under the supervision of data science lead, geoscientists and subject matter experts.
Collect, aggregate and wrangle large volumes of data from multiple sources using advanced methodologies in data engineering.
Develop data ingestion and preparation workflows for data science pipelines for high and low velocity data.
Perform descriptive analytics on large heterogeneous data sets and create metrics to measure data quality and readiness for analytics.
Collaborate with other team members and the business to improve data models that feed BI tools and data science pipelines.
Assist with the build out and maintenance of an AWS development environment
Research and identify potential data engineering solutions from external partners
Attend relevant industry and technology conferences/seminars and bring back learnings for sharing to the broader Innovation Team and Petroleum Business
Help to build and personally model capabilities and behaviours that value and promote innovation
About you
Bachelors (Master is preferred) degree in STEM major from accredited institution.
5+ years’ experience in data engineering and managing large, complex, disparate data sets.
Good understanding of descriptive analytics and data engineering techniques for data science.
Demonstrated skills in data collection, cleansing, visualization, data quality assessment, and the use of analytics to build minimum viable data (MVD)
Demonstrated expertise in the development of data engineering pipelines for machine learning and artificial intelligence applications using structured, unstructured and semi-structured data sets.
Must have experience building data models to integrate diverse and high dimensional data sets.
Excellent problem solving and critical thinking skills with a thirst to learn new areas.
Experience with real-time data ingestion.
Good understanding of analytics and machine learning project lifecycle.
Interdisciplinary mind, i.e.
demonstrated ability to map experiences across different domains.
Demonstrated skills in the use of one or more analytics software tools and languages (e.g.
Python, R, Matlab, Java, Scala)
Experience in working with and analysing complex geospatial data sets, and knowledgeable in Geospatial analytic tools such as ArcGIS, ArcPRO, ArcPy, etc.
Good understanding of distributed computing, virtualization, and cloud technologies.
Excellent oral and written communication skills, able to effectively explain technical information to various audiences
Experience in Oil & Gas is a plus
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status, or other status protected by law or regulation.
","['scala', 'bi', 'aw', 'python', 'java', 'r', 'cloud', 'excel', 'matlab']","['research', 'pipelin', 'visual', 'machine learning', 'problem solving', 'supervis', 'commun', 'geospati']",1,"['scala', 'powerbi', 'aw', 'python', 'java', 'r', 'cloud', 'excel', 'matlab', 'research', 'pipelin', 'visual', 'machine learning', 'problem solving', 'supervis', 'commun', 'geospati']","['analyt', 'machin', 'techniqu', 'learn', 'engin', 'pipelin', 'visual', 'divers', 'bi', 'aw', 'python', 'set', 'sourc', 'collect']","['scala', 'powerbi', 'aw', 'python', 'java', 'r', 'cloud', 'excel', 'matlab', 'research', 'pipelin', 'visual', 'machine learning', 'problem solving', 'supervis', 'commun', 'geospati', 'analyt', 'machin', 'techniqu', 'learn', 'engin', 'pipelin', 'visual', 'divers', 'bi', 'aw', 'python', 'set', 'sourc', 'collect']"
DE,"This position will be an integral part of building and maintaining a net new data streaming platform to unify and govern reporting and data access across various products.
Additional responsibilities may include design, construction, testing, optimization, and deployment of Data Engineering & Analytics technologies including but not limited to Relational Databases (Oracle), Kafka, Python as well as AI/ML & advanced analytics skills like Regression & Classification modeling in R or Python.The Data Engineer will need to work effectively with a globally distributed team of developers & product owners/stakeholders to implement reporting & analytics solutions leveraging the JPMC Legal generated data.
Overall, the ideal candidate for this position will be highly skilled in data warehouse, streaming data, data manipulation & advanced analytics tools and have knowledge of visualization and presentation of enterprise data.This role requires a wide variety of strengths and capabilities, including:* BS/BA degree or equivalent experience* Advanced knowledge of application, data, and infrastructure architecture disciplines* Understanding of architecture and design across all systems* Working proficiency in developmental toolsets* Knowledge of industry-wide technology trends and best practices* Ability to work in large, collaborative teams to achieve organizational goals* Passionate about building an innovative culture* Proficiency in one or more modern programming languages* Understanding of software skills such as business analysis, development, maintenance, and software improvement* 6+ years of experience in a Data Engineering role, with a focus on data & analytics technologies* Experience delivering product with Agile / Scrum methodologies* Ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences* Applied experience with Kafka streaming, JMS and other messaging technologies* Familiar with Docker, Kubernetes and other container technologies* Knowledge and applied experience in Java with Spring Framework, Spring Boot and other Spring technologies* Proficient in data analytics skills for more than one data engineering & analytics technologies including data warehouse (Oracle) and advanced analytics tools & methodologies (NLP, Classification models) Comprehensive analysis & design experience with demonstrated knowledge of Oracle based data warehouse / database structures.
* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
* Previous working experience in SQL, PL/SQL a must.
* Proficiency in one or more modern programming languages- experience in Python, R etc.
* Experience with Tableau & similar visualization tools a plus.
* Strong data analysis skills and problem solving ability* Working proficiency in a selection of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of software.
You're an integral part of one of the world's biggest tech companies.
","['sql', 'net', 'tableau', 'python', 'java', 'r', 'kubernet', 'kafka', 'docker', 'oracl']","['classif', 'visual', 'optim', 'problem solving', 'nlp', 'regress']",1,"['sql', 'net', 'tableau', 'python', 'java', 'r', 'kubernet', 'kafka', 'docker', 'oracl', 'classif', 'visual', 'optim', 'problem solving', 'nlp', 'regress']","['analyt', 'program', 'ml', 'relat', 'visual', 'infrastructur', 'optim', 'python', 'appli', 'warehous', 'integr', 'engin']","['sql', 'net', 'tableau', 'python', 'java', 'r', 'kubernet', 'kafka', 'docker', 'oracl', 'classif', 'visual', 'optim', 'problem solving', 'nlp', 'regress', 'analyt', 'program', 'ml', 'relat', 'visual', 'infrastructur', 'optim', 'python', 'appli', 'warehous', 'integr', 'engin']"
DE,"They are looking for a talented Data Engineer to be a key player in building their next generation, artificial intelligence platform, serving as their core product!
In this role you will immediately have the opportunity to work on key product features for this energy related SaaS offering.
Other responsibilities will include:
Working with big data and artificial intelligence
Architecting and implementing data solutions across the entire platform
Working in a cloud-centric environment
Required Skills:
Experience working with the Azure stack
Database experience using MongoDB and SQL
Python scripting is a PLUS
http://www.walkerelliott.com/candidates/jobs/jobDetail/default.aspx?GUID=11321&Apply=true
","['sql', 'mongodb', 'azur', 'cloud', 'python']",['big data'],1,"['sql', 'mongodb', 'azur', 'cloud', 'python', 'big data']","['azur', 'python', 'big', 'relat', 'engin']","['sql', 'mongodb', 'azur', 'cloud', 'python', 'big data', 'azur', 'python', 'big', 'relat', 'engin']"
DE,"Search Jobs
Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?
You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.
Responsibilities:
· Understand the business capability/requirements and transform them into robust design solutions
· Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed
· Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.
· Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.
· Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLT
· Protect data integrity and accuracy.
Perform root cause analysis of issues that hinder the data quality.
Work with data source owner to increase quality and accuracy of the source data.
· Help data consumers to correctly understand and use the data.
Qualifications:
· 8+ years of experience in as a Data Engineer handling large volumes of data.
· Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.
· Expertise in writing advanced SQL queries.
· Experience working with Informatica, SAP SDI/SLT
· Expertise in SAP HANA, Hive/Hadoop/Hawq/Spark
· Working knowledge of BI Reporting tools like BOBJ and Tableau
· Experience in Python Scripting
· Strong analytical and troubleshooting skills
· Excellent verbal and written communication skills
· Bachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.
Search Jobs
","['sql', 'spark', 'excel', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'hana']","['etl', 'commun', 'statist']",1,"['sql', 'spark', 'excel', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'hana', 'etl', 'commun', 'statist']","['analyt', 'spark', 'set', 'hadoop', 'infrastructur', 'bi', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'excel', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'hana', 'etl', 'commun', 'statist', 'analyt', 'spark', 'set', 'hadoop', 'infrastructur', 'bi', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin', 'integr']"
DE,"Position: Data EngineerLocation: Houston, TX 77002
Duration: 6+ months contract
As part of the Data Engineering team, the Data Platform Engineer will work closely with Data Engineers and IT Infrastructure team to ensure the data platform is highly available, reliable, and stable.
This individual will provide technical and thought leadership to the team to streamline the delivery of analytics to the business.
Must Have:
Kubernetes
Linux
MapR
Responsibilities
Implementation and ongoing administration of the platform
Perform incident investigation, diagnosis and provide resolution
Manage cluster provisioning, performance tuning, and security configuration
System monitoring and remediation of any production issues
Identify recurring problems and perform root cause analysis
Provide and implement sustainable solutions
Collaborate with the application teams to install updates, fixes, and patches
Coordinate and perform version upgrades
File system management and monitoring
Act as a primary contact for the platform
Point of contact for hardware and vendor escalation
Document all service levels
Perform application deployments acting as a gatekeeper to the production environment.
Regards,
Hari Haran.S
O: (281) 957-5888 Ext: 192
C: (754) 205-1604
Email: hari.haran@wisemen.com
www.wisemen.com
","['kubernet', 'linux', 'c']","['tune', 'hardwar', 'cluster']",999,"['kubernet', 'linux', 'c', 'tune', 'hardwar', 'cluster']","['analyt', 'primari', 'infrastructur', 'provid', 'avail', 'engin']","['kubernet', 'linux', 'c', 'tune', 'hardwar', 'cluster', 'analyt', 'primari', 'infrastructur', 'provid', 'avail', 'engin']"
DE,"Must Have Skills Python Spark SQL Responsibilities Architect end to end data solutions including data collection and storage, data modeling, and data consumption Work independently on data projects for multiple business functions Implement data flows connecting operational systems, BI systems, and the big data platform Design and implement an Enterprise Data Warehouse Automate manual data flows for repeated use and scalability Develop data-intensive applications with API and streaming data pipelines Prepare and transform data into a usable state for analytics Document and maintain source-to-target mappings and data lineage Productionize mathematical models and machine learning models Assists data analysts and data scientists with query optimization, performance tuning, and data processing Identify opportunities for data improvements and presents recommendations to management
","['sql', 'bi', 'python', 'spark']","['recommend', 'pipelin', 'tune', 'data modeling', 'machine learning', 'optim', 'big data']",999,"['sql', 'powerbi', 'python', 'spark', 'recommend', 'pipelin', 'tune', 'data modeling', 'machine learning', 'optim', 'big data']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'optim', 'bi', 'python', 'warehous', 'scientist', 'big', 'sourc', 'collect']","['sql', 'powerbi', 'python', 'spark', 'recommend', 'pipelin', 'tune', 'data modeling', 'machine learning', 'optim', 'big data', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'optim', 'bi', 'python', 'warehous', 'scientist', 'big', 'sourc', 'collect']"
DE,"Innovative, adaptable, results-driven.
The role of the Data Engineer is to work with Data Architect to build Enterprise Data Management system from the ground up.
The Data Engineer also works as a liaison between the subject matter experts in other departments and the Information Systems Department to understand the business requirements, needs and gaps in order to identify the appropriate datasets to perform analysis and develop insightful reports and dashboards.
You should be open to new and different ways to accomplish your work and be comfortable with new processes, initiatives and changes in priorities.
This role requires a lot of collaboration with the Alphascript team, so you should be able to convey facts and information clearly (both verbally and written) and be comfortable sharing your ideas and proactively contributing to group objectives.
Responsibilities
Work with Data Architect to develop a data lake, data warehouse in local and/or Azure cloud environment
Integrate disparate data models into coherent enterprise data models
Develop ETL data pipelines to populate data lake and warehouse
Actively participate in Data Governance Program to maintain metadata and data definitions
Work in a team environment with other departments to develop reports, KPIs and dashboards (very strong communication skills)
Interpret business requirements to identify proper tools and methods to analyze, identify and report data trends and variances
Education and Experience
Bachelor’s Degree in Computer Science, Computer Engineering or Information Systems with university level programming courses
3+ years of data engineer experience
3+ years of recent experience in ETL and data warehouse development or maintenance
3+ years of experience in KPI, reports and dashboard development
Experience in Azure and Sharepoint a plus
Experience in agile software development is a plus
Skills and Abilities
Proficiency in Power BI, Azure Cloud, C#, ETL (SSIS preferred), T-SQL, Excel
Ability to develop data dictionaries of an existing database
Ability to write, analyze and debug SQL queries
Ability to develop dashboards and data models in Power BI; must be familiar with DAX and Power Pivot, and be willing to get proficient at them
Ability to develop business models and perform analysis in MS Excel
Proficiency in R and Python preferred
Occasional travel is required.
Availability
This is a full-time position.
","['sql', 'azur', 'bi', 'cloud', 'python', 'c', 'r', 'excel', 'power bi']","['pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi']",1,"['sql', 'azur', 'powerbi', 'cloud', 'python', 'c', 'r', 'excel', 'pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi']","['program', 'pipelin', 'azur', 'power', 'bi', 'python', 'avail', 'interpret', 'comput', 'warehous', 'etl', 'engin', 'integr']","['sql', 'azur', 'powerbi', 'cloud', 'python', 'c', 'r', 'excel', 'pipelin', 'dashboard', 'analyz', 'etl', 'commun', 'kpi', 'program', 'pipelin', 'azur', 'power', 'bi', 'python', 'avail', 'interpret', 'comput', 'warehous', 'etl', 'engin', 'integr']"
DE,"The ideal candidate will have experience cleaning data, reporting and dash boarding, ETL, monster SQL skills, performance tuning, database architecture and design and all the other random lessons learned in a long career working with data.
Rock star Hadoop and Spark skills are preferred for this role.
Responsibilities:
Collaborate with data scientists, product management, and web engineers to deliver value and project outcomes
Convert prototype models and data pipelines built by data scientists for use in production.
Evaluation and debugging of model performance to ensure parity with prototype (Spark/Java/Python).
Develop low-latency, real-time predictive models in a microservice environment (Java).
Balance long-term code health and maintainability with business needs.
Profiling and performance tuning of production code.
ML Ops: support of Dataproc, Zeppelin, Gitlab, continuous integration systems, monitoring, alerting, etc.
""
Experience and interest in Big Data technologies (Hadoop/ Spark/ NoSQL DB)
Experience working on projects within the cloud ideally Azure/AWS
Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc.
SQL Python, Java, Scala, R
Experience in at least one ETL tool
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Benefits:
If you are seeking a remarkable opportunity within a dynamic, growing organization, please email your resume and salary requirements for immediate consideration
""
","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'r', 'cloud']","['pipelin', 'tune', 'predict', 'big data', 'etl']",999,"['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'r', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl']","['program', 'learn', 'spark', 'pipelin', 'ml', 'azur', 'set', 'integr', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'r', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl', 'program', 'learn', 'spark', 'pipelin', 'ml', 'azur', 'set', 'integr', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']"
DE,"Responsibilities for Data Engineer:
â Create and maintain optimal data pipeline.
â Assemble large, complex data sets that meet functional / non-functional business requirements.
â Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
â Build analytics tools/dashboards that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics using Tableau.
â Work with stakeholders including the business owners, Data and Design teams to assist with data-related technical issues and support their data needs.
â Create data tools for analytics and data scientist team members that assist them in building and optimizing models/visualizations.
Required Qualifications for Data Engineer
They should also have experience in following:
â Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL).
â Experience building and optimizing 'big data' data pipelines, architectures and data sets.
â knowledge with data visualization best practices.
â Minimum 1-year experience in Tableau development
â Experience with various data prep/pipeline/integration tools like Alteryx, Azure Data Factory etc.
â In-depth knowledge of relational databases (e.g.
Oracle and SQL Server), including data warehousing concepts and best practices
â Proficient is SQL
â Experience in at least one the following languages â R, Python, SCALA
â Experience working in an agile or Scrum based environment
â Ability to test and document end-to-end processes.
Experience working with Microsoft Azure or AWS platform will be preferred
","['sql', 'azur', 'scala', 'aw', 'tableau', 'python', 'r', 'microsoft', 'oracl']","['pipelin', 'dashboard', 'visual', 'optim', 'data warehousing', 'big data']",999,"['sql', 'azur', 'scala', 'aw', 'tableau', 'python', 'r', 'microsoft', 'oracl', 'pipelin', 'dashboard', 'visual', 'optim', 'data warehousing', 'big data']","['analyt', 'engin', 'pipelin', 'azur', 'set', 'relat', 'visual', 'optim', 'aw', 'python', 'scientist', 'action', 'integr', 'â']","['sql', 'azur', 'scala', 'aw', 'tableau', 'python', 'r', 'microsoft', 'oracl', 'pipelin', 'dashboard', 'visual', 'optim', 'data warehousing', 'big data', 'analyt', 'engin', 'pipelin', 'azur', 'set', 'relat', 'visual', 'optim', 'aw', 'python', 'scientist', 'action', 'integr', 'â']"
DE,"Data Engineer
Duration:-10+ Months
Rate:-DOE
Visa:- USC,GC,GC-EAD,H4EAD,TN
6 years of Experience in Data Engineering.
Greenplum, Hadoop (for their Data Lake) and Teradata (for their Datawarehouse).
Spark is used for Data transformations.
Experience with MongoDB and any type of Graph databases(Neo4j, Amazon Neptune, Cassandra) is a plus.
","['mongodb', 'spark', 'cassandra', 'hadoop']",['graph'],999,"['mongodb', 'spark', 'cassandra', 'hadoop', 'graph']","['spark', 'engin', 'hadoop']","['mongodb', 'spark', 'cassandra', 'hadoop', 'graph', 'spark', 'engin', 'hadoop']"
DE,"What you'll do
Level: Advanced/Expert
Banking
What you bring
For more, click: https://www.mindteck.com/career/life-at-mindteck.html
All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as a qualified individual with a disability, or any other trait protected by law.
",[None],[None],999,[],[None],[None]
DE,"alliantgroup's national office is in Houston, Texas with regional offices in Orange County (CA), Chicago, New York, Miami, Seattle, San Francisco, Atlanta, Los Angeles, Denver, San Diego, Phoenix, Washington D.C., and Boston.alliantgroup is currently in search of a full-time employee to execute strategic projects to create competitive edge solutions combining design, data, and software engineering.
The ideal candidate will have experience cleaning data, reporting and dash boarding, ETL, monster SQL skills, performance tuning, database architecture and design and all the other random lessons learned in a long career working with data.
Rock star Hadoop and Spark skills are preferred for this role.Responsibilities:* Collaborate with data scientists, product management, and web engineers to deliver value and project outcomes* Convert prototype models and data pipelines built by data scientists for use in production.
* Evaluation and debugging of model performance to ensure parity with prototype (Spark/Java/Python).
* Develop low-latency, real-time predictive models in a microservice environment (Java).
* Balance long-term code health and maintainability with business needs.
* Profiling and performance tuning of production code.
* ML Ops: support of Dataproc, Zeppelin, Gitlab, continuous integration systems, monitoring, alerting, etc.
* Experience and interest in Big Data technologies (Hadoop/ Spark/ NoSQL DB)* Experience working on projects within the cloud ideally Azure/AWS* Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc.
SQL Python, Java, Scala, R* Experience in at least one ETL tool* Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data setsBenefits:alliantgroup offers excellent benefits including, but not limited to, the following: Medical, Dental, Vision, Life, 401(k), FSA, Holidays, PTO, Training and Mentoring programs.If you are seeking a remarkable opportunity within a dynamic, growing organization, please email your resume and salary requirements for immediate consideration
","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'dash', 'nosql', 'r', 'db', 'excel']","['pipelin', 'tune', 'predict', 'big data', 'etl']",999,"['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'dash', 'nosql', 'r', 'db', 'excel', 'pipelin', 'tune', 'predict', 'big data', 'etl']","['program', 'learn', 'spark', 'pipelin', 'ml', 'azur', 'integr', 'predict', 'hadoop', 'aw', 'employe', 'python', 'scientist', 'texa', 'big', 'etl', 'engin', 'evalu']","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'dash', 'nosql', 'r', 'db', 'excel', 'pipelin', 'tune', 'predict', 'big data', 'etl', 'program', 'learn', 'spark', 'pipelin', 'ml', 'azur', 'integr', 'predict', 'hadoop', 'aw', 'employe', 'python', 'scientist', 'texa', 'big', 'etl', 'engin', 'evalu']"
DE,"Data Engineer
Share
Job ID: FA-0100-322
Open Since: 2019-05-28
City: Houston
State: Texas
Country: United States of America
Need for a well-demonstrated Data Engineer who will work with the Data Science team to complete a major data project.
Mode of Interview : Telephonic/F2F
Job Skills:
Experience working on Hadoop platform components
Knowledge of Big Data tools, such as zookeeper, Kafka Streaming.
Shell scripting experience
Experience with integration of data from multiple data sources (NoSQL, Mongo, SQL)
Experience working with Structured/Unstructured data.
Experience creating ETL pipelines
Experience in Docker builds and Git file versioning
Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.
Knowledge of programming in Python
Knowledge of MapR
Knowledge of Scala framework
Experience with Spark, Storm or Flink
Minimum Experience: 8 Yrs
Roles & Responsibilities:
Integrate Data from multiple data sources
Create ETL Pipelines
Work under the guidance of Lead to develop based on design/architecture.
Education:
Bachelor’s Degree in Computer Science or equivalent work experience.
Masters preferred
","['sql', 'spark', 'scala', 'git', 'hadoop', 'python', 'nosql', 'kafka', 'docker']","['etl', 'pipelin', 'big data']",1,"['sql', 'spark', 'scala', 'git', 'hadoop', 'python', 'nosql', 'kafka', 'docker', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'texa', 'big', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'scala', 'git', 'hadoop', 'python', 'nosql', 'kafka', 'docker', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'texa', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"This role will be responsible for designing, developing, optimizing, standardizing data engineering pipelines along with creating robust data models for data publishing while complying with, and adding value to, the data architecture.
The Data Engineer will also be responsible for guiding the existing data engineering team and developing cloud-native solutions with low time to market by leveraging DevOps methodologies.
This individual will evangelize and implement the modern practices in data engineering that address scale and are essential for digital transformation through high value driven projects.Roles & ResponsibilitiesWork closely with data scientists, platform engineers, data architects, and data source owners to deliver foundational data sets, enabling analytics solutions driving successful business outcomes.Provide leadership in designing and implementing best practices for efficient sourcing and processing large data sets from Analytics data stores.Build real-time, reliable, scalable, high-performing, distributed, fault tolerant systems.Design and develop code, scripts and data pipelines that leverage structured and unstructured data.Implement measures to address data privacy, security, compliance and ensure robust data governance.Industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scaleMonitor, maintain and optimize production systems.
Investigate and resolve incidents reported by users.
Identify opportunities to automate, consolidate and simplify platform.Work with Enterprise Architecture, Digital, and other IT teams to develop and maintain data integrity, integration and governance standards.Contribute to the selection of platforms, data management, libraries, tool chain and OSS for software development.
Stay on top of evolving technology to suggest and prototype and implement improvements to the data architecture.Collaborate with cross-functional teams to help utilize and drive adoption of new big data tools / models.Guide business stakeholder and mentor members of the data and analytics teams regarding technology and best practices.Manage assigned activities within time, cost and technical objectives.
May manage small projects with internal or external resources.Min.
QualificationsBachelor's degree required10+ years hands-on experience in architecting, developing, and successfully operationalizing complex/large scale data management projectsAt least 5+ years of data warehouse and ETL design and development using SQL on RDBMS and MPP databasesExperience in big data development on Hadoop or Spark frameworks using Python, Java, noSQL, TimeSeries DBs, HDFSExperience in development on Azure Cloud (PaaS) Data SolutionsExperience working with LAMBDA architecture using real-time Kafka ingestion and high-volume batch loadsPreferred QualificationsDegree in Computer Science, Engineering, Technical Science or related disciplines, preferred.Experience building data management (metadata, lineage, tracking etc.)
","['sql', 'spark', 'azur', 'hadoop', 'cloud', 'lambda', 'python', 'java', 'db', 'nosql', 'kafka']","['optim', 'etl', 'pipelin', 'big data']",1,"['sql', 'spark', 'azur', 'hadoop', 'cloud', 'lambda', 'python', 'java', 'db', 'nosql', 'kafka', 'optim', 'etl', 'pipelin', 'big data']","['python', 'etl', 'integr', 'sourc', 'analyt', 'essenti', 'azur', 'hadoop', 'optim', 'warehous', 'set', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'spark', 'azur', 'hadoop', 'cloud', 'lambda', 'python', 'java', 'db', 'nosql', 'kafka', 'optim', 'etl', 'pipelin', 'big data', 'python', 'etl', 'integr', 'sourc', 'analyt', 'essenti', 'azur', 'hadoop', 'optim', 'warehous', 'set', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Through working closely with other Data Engineers, Data Scientists, business domain experts, and IT, the Sr. Data Engineer will have the opportunity to provide significant tangible business value through developing and deploying scalable Data Engineering capabilities on high value problems.
QualificationsBS in Computer Science, Software Engineering, Computer Engineering, or related technical field.
",[None],[None],999,[],"['relat', 'comput', 'scientist', 'engin']","['relat', 'comput', 'scientist', 'engin']"
DE,"DATA ENGINEER – HOUSTON, TX
This will require close communication with product, design, and other engineers on the team to coordinate deliverables and navigate changes in priorities.
The ideal candidate will have experience working with a early-stage SaaS start-ups and/or in fluid, dynamic, and fast-paced environments.
What will you be doing?
Architecting, implementing and deploying data solutions and pipelines
Working with business team members to understand problems and propose technical solutions
Automating solutions to improve data quality and improve overall productivity of end users
Delivering on-time release of well-tested and reliable code
Development of ETL logic, logging, monitoring, and maintenance of the central database
Integrating multiple data storage solutions using databases such as SQL Server, Mongo DB, and Azure Cognitive Search
Most importantly
Job Requirements
BS/BA in Computer Science, Engineering, or equivalent preferred
3+ years of experience in Azure cloud architecture and relational database management (SQL Server)
Experience on Azure Data Factory (creating pipelines, loading and moving data) , Logic Apps, and Kusto preferred
Familiarity with Python and Jupyter notebooks a plus
Experience building large scale applications within oil and gas sector preferred
Familiarity with build systems, such as Github/Gitlab, Azure DevOps
Experience implementing, testing, and deploying code to a production environment
Ability to work effectively in a team environment
Capable of knowledgeably discussing performance trade-offs when evaluating different approaches
Great at solving problems, debugging, troubleshooting, and designing & implementing solutions to complex technical issues
Ability and eagerness to independently learn new technologies, prototype and propose solutions to the team
Competitive package (salary & equity)
Flexible work environment
Unlimited PTO
Volunteer time off
Vacation stipend (after 1yr)
High growth potential with an enterprising and energetic team
Notes
For the purposes of the interviewing process, you can expect to be rated on the results of a coding challenge that will be sent to you and both technical and team oriented in-person interviews.
Job Type: Full-time
Pay: $70,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Visa Sponsorship Potentially Available:
No: Not providing sponsorship for this job
","['sql', 'azur', 'jupyt', 'db', 'cloud', 'python', 'github']","['etl', 'commun', 'pipelin', 'end user']",1,"['sql', 'azur', 'jupyt', 'db', 'cloud', 'python', 'github', 'etl', 'commun', 'pipelin', 'end user']","['challeng', 'azur', 'pipelin', 'relat', 'python', 'avail', 'packag', 'comput', 'releas', 'etl', 'engin', 'integr']","['sql', 'azur', 'jupyt', 'db', 'cloud', 'python', 'github', 'etl', 'commun', 'pipelin', 'end user', 'challeng', 'azur', 'pipelin', 'relat', 'python', 'avail', 'packag', 'comput', 'releas', 'etl', 'engin', 'integr']"
DE,"Country:
United States
Cities:
Houston
Area of expertise:
Analytics
Love living in the cloud?
As an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions.
You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools.
You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.
Day-to-day, you will:
• Translate business requirements to technical solutions using strong business insight.
• Analyzes current business practices, processes and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
• Support the planning and implementation of data design services, providing sizing and configuration assistance, and performing needs assessments.
• Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
• Design and Build Modern Data Pipelines and Data Streams.
• Design and Build Data Service APIs.
• Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
• Expose data to end users using Power BI, Azure API Apps or other modern visualization platform or experience.
• Implement effective metrics and monitoring processes.
• Able to travel approximately 80%
Your technical/non-technical skills include:
• Demonstrable experience of turning business use cases and requirements to technical solutions.
• Experience in business processing mapping of data and analytics solutions.
• Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
• T-SQL is required.
• Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.
• Experience preparing data for Data Science and Machine Learning.
• Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
• Strong collaboration ethic and experience working with remote teams.
• Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
(nice to have)
• Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
(nice to have)
• Knowledge of Lambda and Kappa architecture patterns.
(nice to have)
Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field.
An equivalent combination of education and experience will also suffice.
Preferred Years of Work Experience:
You likely have about 3-5+ years of relevant professional experience.
• 14-time winner of Microsoft Partner of the Year
• 24,000+ certifications in Microsoft technology
• 90+ Microsoft partner awards
• 17 Gold Competencies
• 3,500 analytics professionals worldwide
• 1,000 data engineers
• Implemented analytics systems for more than 550 clients
• 400 AI practitioners
• 300 cognitive service experts
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['sql', 'azur', 'bi', 'lambda', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']",1,"['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']","['day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']","['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology', 'day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']"
DE,"As a Data Engineer you will develop and maintain scalable data pipelines while collaborating with analytical and business teams to improve data models.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Qualifications:
• Preferable to have a Degree in an analytical field (e.g.
Computer Science, Mathematics, Statistics, Engineering, Operations Research, Management Science) and 4+ years of professional experience.
• At least 4 years of data analytics experience in a distributed computing environment
• Strong experience with products or similar products like Apache Spark, Big Data
• Building and analyzing dashboards and reports
• Empower and assist operation and product teams through building key data sets and data-based recommendations
• Automating analyses and authoring pipelines via SQL/python based ETL framework
• Superb SQL programming skill with understanding of ETL tools and database architecture.
• Demonstrable familiarity with code and programming concepts.
Experience with Python is highly preferred but not required.
**For confidential searches, please send resume directly to jelle.dejong@rht.com***
Scala, Python, Apache Spark
All applicants applying for U.S. job openings must be authorized to work in the United States.
All applicants applying for Canadian job openings must be authorized to work in Canada.
An Equal Opportunity Employer M/F/Disability/Veterans.
","['sql', 'python', 'spark', 'scala']","['recommend', 'research', 'pipelin', 'dashboard', 'statist', 'big data', 'etl']",1,"['sql', 'python', 'spark', 'scala', 'recommend', 'research', 'pipelin', 'dashboard', 'statist', 'big data', 'etl']","['analyt', 'spark', 'pipelin', 'set', 'relat', 'avail', 'python', 'statist', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'python', 'spark', 'scala', 'recommend', 'research', 'pipelin', 'dashboard', 'statist', 'big data', 'etl', 'analyt', 'spark', 'pipelin', 'set', 'relat', 'avail', 'python', 'statist', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"Big Data Engineer
JPMC
Houston , TX
6-12 Months CTH role.
Rate: $66/hr w2 Max or 115K Salary range.
JOB :
Level: Advanced/Expert
Thanks
Farzan
",[None],['big data'],999,['big data'],"['big', 'engin']","['big data', 'big', 'engin']"
DE,"*NEW* Sr. Data Engineer
Well branded and innovative industry leader is seeking a well rounded candidate who will have deep and broad experience in all aspects of data which include cleaning data, reporting and dash boarding, ETL, performance tuning, database architecture and design.
Responsibilities:
Ths individual will work closely and collaborate with data scientists, product management, and web engineers to deliver value and project outcomes.
Convert prototype models and data pipelines built by data scientists for use in production.
Evaluation and debugging of model performance to ensure parity with prototype.
Develop low-latency, real-time predictive models in a microservice environment.
Profiling and performance tuning of production code.
Qualifications:
Experience and interest in Big Data technologies (Hadoop/ Spark/ NoSQL DB).
Experience working on projects within the cloud ideally Azure/AWS.
Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc.
SQL Python, Java, Scala, R.
Experience in at least one ETL tool.
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets.
MUST be able to clearly articulate strategic, functional and technical concepts to all levels of stakeholders, technical teams and executive management.
Please send your resume to Colin Crane, Sr Technical Recruiter for immediate consideration.
","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'cloud']","['pipelin', 'tune', 'predict', 'big data', 'etl']",999,"['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl']","['program', 'spark', 'pipelin', 'azur', 'set', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']","['sql', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'db', 'python', 'java', 'dash', 'nosql', 'cloud', 'pipelin', 'tune', 'predict', 'big data', 'etl', 'program', 'spark', 'pipelin', 'azur', 'set', 'predict', 'hadoop', 'aw', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']"
DE,"Work Shift: DAY
Work Week: M - F
Job Summary
The primary focus of this position will be to provide support for assigned sub-projects within a large-scale project involving cross institutional big data and artificial intelligence.
Prior experience with AWS technologies and Epic is desirable.
More specifically, experience with various data storage technologies on cloud such as S3 Buckets, Glacier, DynamoDB, Cassandra, MangoDB, Redis and other relational DBs is desirable.
Experience with Kafka and Hadoop Ecosystem, modeling for relational and NoSQL schemas and development with Java, Python/pySpark and scala languages is also a plus.
Prior experience in healthcare is highly desirable.
The candidate is expected to be responsible for intermediate level support and configuration of assigned applications.
Among others, responsibilities will include to build and manage infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using big data technologies such as AWS and SQL, build and manage analytics tools that utilize the data pipeline for various purposes such as research and achieving key clinical and operation goals.
The position also entails actively participating in design, cataloguing, and documentation of data systems and associated processing.
Further roles include assisting, training and supporting customers with the operation and administration of systems.
Candidate will be required to have active engagement with diverse functional teams (technical and clinical) and evidence of working with teams and strong communication skills (written and verbal) are preferred.
The Application Analyst troubleshoots issues and may also work with vendors on issue resolution.
The Application Analyst will partner with end users to increase their understanding of available tools to enhance research and operational productivity.
Responsibilities will include creating, documentation and assisting other IT groups and teams with issues and problems.
The Application Analyst ensures the services provided contribute to the successful accomplishment of department and organizational goals.
The Application Analyst possess intermediate technical expertise to support specific healthcare applications.
The Analyst will cross train to become proficient in multiple applications including AWS technology.
PATIENT AGE GROUP(S) AND POPULATION(S) SERVED
Focuses on patient/customer safety
Intentionally rounds with patients/customers to ensure their needs are being met
Involves patients (customers) in shift/handoff reports by enabling their participation in their plan of care as applicable to the given job
PRIMARY JOB RESPONSIBILITIES
Job responsibilities labeled EF capture those duties that are essential functions of the job.
PEOPLE -25%
Accurately prepares written business correspondence that is coherent, grammatically correct, effective and professional.
(EF)
Establishes and maintains effective working relationship with clients.
Shares acquired skills with team members through documentation and training.
(EF)
Assists, trains and supports customers with the operation and administration of systems.
(EF)
SERVICE - 30%
Debugs, programs and tests systems and applications.
Proposes solutions to problems and considers timeliness, effectiveness, and practicality in addressing client needs.
(EF)
Utilizes working knowledge of assigned applications to effectively complete assigned tasks.
Applies workflow to and from clinical and business applications.
(EF)
Effectively leads and facilitates meetings.
Develops meeting objectives, agendas and action items.
Supports and installs software applications, including on call support.
(EF)
QUALITY/SAFETY - 20%
Provides high quality technical support for assigned application(s).
Creates custom solutions or configuration options to solve operational or workflow issues.
(EF)
(EF)
FINANCE - 10%
Establishes responsible deadlines and personal work plans and manages time effectively.
Accurately completes and submits assigned work and status reports according to project timelines and expectations.
(EF)
Prioritizes issue resolution, work requests and tasks effectively.
(EF)
GROWTH/INNOVATION - 15%
Anticipates client needs and communicates to department leadership for solution development.
(EF)
Resolves problems of moderate to advanced complexity using strong analytical and logic skills.
Generates innovative solutions in partnership with customers, vendors cross and functional IT teams(EF)
Seeks out opportunities to cross train and become proficient in multiple applications (EF)
Participates in professional development.
Completes Individual Development Plan.
(IDP) (EF)
EDUCATION REQUIREMENTS
Bachelors Degree in Information Technology, Business Administration or related field or experience working as a licensed clinical or certified IT professional.
An additional four years experience in addition to the experience listed below in lieu of Bachelors or licensed clinical or certified IT professional experience
EXPERIENCE REQUIREMENTS
Three years experience in IT or clinical or business workflow required
Experience supporting clinical, ancillary or business environments
CERTIFICATIONS, LICENSES AND REGISTRATIONS REQUIRED
Certification within six months of hire if appropriate to assigned application.
(i. e. Epic) Epic Certification must be maintained.
KNOWLEDGE, SKILLS AND ABILITIES REQUIRED
Demonstrates the skills and competencies necessary to safely perform the assigned job, determined through on-going skills, competency assessments, and performance evaluations
Sufficient proficiency in speaking, reading, and writing the English language necessary to perform the essential functions of this job, especially with regard to activities impacting patient or employee safety or security
Ability to effectively communicate with patients, physicians, family members and co-workers in a manner consistent with a customer service focus and application of positive language principles
Understanding of business processes and requirements as related to the assigned (clinical or business) environment.
Ability to support large scale clinical and ancillary systems
Familiar with current database and operating systems as required for assigned applications
Technical skills to support multiple applications
Intermediate level competency in multiple applications or areas of clinical workflow.
Demonstrated project management skills.
Ability to problem solve and generate innovative solutions in conjunction with customers, vendors, and Information Technology
SUPPLEMENTAL REQUIREMENTS
Work Attire Yes/No
Uniform No
Scrubs No
Business professional Yes
Other (dept approved) No
On-Call* Yes, on a regular basis (for Non-Exempt or Exempt jobs)
Disaster, Severe Weather Event, etc) regardless of selection above.
Travel**
May require travel within Yes
May require travel outside Yes
**Travel specifications may vary by department.
Please note any other special considerations to this job: __________________________
Equal Employment Opportunity
VEVRAA Federal Contractor priority referral Protected Veterans requested.
The research institute was created to provide the infrastructure and support for these endeavors, and to house the technology and resources needed to make innovative breakthroughs in important areas of human disease.
A 540,000 square foot building dedicated to research, the research institute houses over 1,500 staff and trainees, 277 principal investigators and has more than 840 ongoing clinical trials.
","['sql', 'pyspark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'db', 'kafka']","['research', 'healthcar', 'big data', 'pipelin', 'end user', 'optim', 'financ', 'information technology', 'commun']",1,"['sql', 'pyspark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'db', 'kafka', 'research', 'healthcar', 'big data', 'pipelin', 'end user', 'optim', 'financ', 'information technology', 'commun']","['day', 'basi', 'program', 'provid', 'python', 'sourc', 'evalu', 'analyt', 'essenti', 'human', 'hadoop', 'optim', 'avail', 'appli', 'action', 'clinic', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'particip', 'pipelin', 'divers', 'relat']","['sql', 'pyspark', 'cassandra', 'scala', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'db', 'kafka', 'research', 'healthcar', 'big data', 'pipelin', 'end user', 'optim', 'financ', 'information technology', 'commun', 'day', 'basi', 'program', 'provid', 'python', 'sourc', 'evalu', 'analyt', 'essenti', 'human', 'hadoop', 'optim', 'avail', 'appli', 'action', 'clinic', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'particip', 'pipelin', 'divers', 'relat']"
DE,"This includes the creation of analytical databases and tools, provisioning datasets to be used for advanced analytics, implementation of information security best practices & policies, and building out automated data visualization and reporting tools.The Senior Data Engineer will have an eye for building and optimizing data systems and will work closely with various stakeholders such as actuaries, underwriters, IT, and data scientists to develop data pipelines and ensure consistency and compatibility of data development across multiple projects.
* Experience working closely with actuaries and data scientists in designing datasets and analytical and visualization tools and processes* Strong interpersonal skills and ability to project manage and work with cross-functional teams* Flexible and responsive with ability to adapt to rapid change in direction or business priority* Ability to work independently with limited supervision as well as contribute to team efforts is required* Ability and desire to communicate technical matters in order to train others and lead adoption of new tools and technologies* Demonstrated experience in working on development processes and agile methodologies* Sound analytical skills, as well as strong problem-solving aptitude.
Able to proactively respond to a changing diverse business environment* Skilled at optimizing of work processes.
",[None],"['supervis', 'visual', 'pipelin']",999,"['supervis', 'visual', 'pipelin']","['analyt', 'pipelin', 'divers', 'visual', 'scientist', 'engin']","['supervis', 'visual', 'pipelin', 'analyt', 'pipelin', 'divers', 'visual', 'scientist', 'engin']"
DE,"No need to be in Houston or relocate!
Herersquos the deal The Data Engineer will be responsible for transforming data into a format that can be easily analyzed.
Principle Accountabilities Installation of clusters across multiple nodes Security assessment and guidance.
Skills Needed MapR cluster administration (preferably with the latest version of MapR + MapR 5.2) MapR security, monitoring, DB and Hbase administration MapR ES and Kafka administration MapR Data Governance story Hive, Drill, Spark, Sqoop, Oozie installation, administration, and tuning Automationscripting using Ansible, Puppet, Chef or Salt Experience with NFS or FUSE Basic ETL experience Cloud experience Docker and Kubernetes experience a big plus Qualifications and Requirements Strong work ethic, attitude and follow through ability.
Excellent communicative, presentation and interpersonal skills.
Makes compelling presentations to a variety of audiences using visual aids, PowerPoint presentations and software demos.
Is adept at getting the attention and involvement of the most sophisticated and difficult audiences.
Takes initiative and pursues opportunities.
Self-motivated and excellent multi-tasking skills.
Prioritizes and performs a variety of concurrent tasks with minimal direction.
Projects a professional and polished image that inspires confidence and trust.
","['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker']","['visual', 'analyz', 'etl', 'commun', 'cluster', 'account']",999,"['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker', 'visual', 'analyz', 'etl', 'commun', 'cluster', 'account']","['spark', 'visual', 'big', 'etl', 'engin']","['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker', 'visual', 'analyz', 'etl', 'commun', 'cluster', 'account', 'spark', 'visual', 'big', 'etl', 'engin']"
DE,"Azure Data Engineer (Need senior profiles)
Houston, TX
Coordinate with teams to understand the current landscape
Architect data ingestion solution using Azure DataFactory and DataBricks components for project requirements
Requirement clarification sessions with business analysts, and meetings with stakeholders
Work closely with project team to ensure that impediments are identified and cleared on-time
Report status on a daily and weekly basis & engage with stakeholders to finalize the architecture
Enable reusable components during design process
Desired Skills include Azure DataFactory, Azure Databricks, MS SQL, CosmosDB, Azure Functions, Azure Kubernetes Server
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'kubernet', 'azur']",[None],999,"['sql', 'kubernet', 'azur']","['basi', 'engin', 'azur', 'evalu']","['sql', 'kubernet', 'azur', 'basi', 'engin', 'azur', 'evalu']"
DE,"Role: Azure Data Engineer
Contract Position
Required:
Coordinate with teams to understand the current landscape
Architect data ingestion solution using Azure DataFactory and DataBricks components for project requirements
Requirement clarification sessions with business analysts, and meetings with stakeholders
Work closely with project team to ensure that impediments are identified and cleared on-time
Report status on a daily and weekly basis & engage with stakeholders to finalize the architecture
Enable reusable components during design process
Desired Skills include Azure DataFactory, Azure Databricks, MS SQL, CosmosDB, Azure Functions, Azure Kubernetes Server
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'kubernet', 'azur']",[None],999,"['sql', 'kubernet', 'azur']","['basi', 'engin', 'azur', 'evalu']","['sql', 'kubernet', 'azur', 'basi', 'engin', 'azur', 'evalu']"
DE,"This includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems.
Data Engineers work together with data consumers to determine, create, and populate optimal data architectures, structures, and systems.
Data Engineers must also plan, design, and optimize for data throughput and query performance issues.
This requires constantly updating expertise in areas such as platform, network and storage technologies, bandwidth management, data bus implications, and design.
Additionally, you will play a key role in the selection of backend database technologies (SQL, NoSQL, HPC, etc), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.
DDI focuses on developing the workforce with cutting-edge skills, investing in IT infrastructure, and modernizing the way the Agency does business.
Learn more about the Directorate of Digital Innovation.
Qualifications
Minimum Qualifications:
Bachelor's degree in one of the following fields or related studies:
Mathematics
Computer Science
Engineering
Management Information Systems
GPA of at least 3.0 on a 4-point scale
Knowledge of the following:
data manipulation
databases
data structures
data management
best engineering practices
All applicants must successfully complete:
A thorough medical and psychological exam
A polygraph interview
A comprehensive background investigation
To be considered suitable for Agency employment, applicants must generally not have used illegal drugs within the last 12 months.
The issue of illegal drug use prior to 12 months ago is carefully evaluated during the medical and security processing.
You can add up to four (4) positions.
The positions will appear in the cart once you have created an account.
DO NOT submit multiple applications; this will only slow the review of your application and delay processing.
Please read the Application Instructions carefully before you begin the online application process.
Important application instructions for this position:
The following items must be attached to your on-line application (PDF format preferred):
Resume
Cover letter in which you specify your qualifications for one or more positions.
Please address why you want to work in this role and what differentiates you from other applicants.
Unofficial transcripts for all degrees
Their interest, however, may not be benign or in your best interest.
Once you reveal your interest you lose control of that information.
","['sql', 'nosql']","['optim', 'etl', 'pipelin', 'account']",1,"['sql', 'nosql', 'optim', 'etl', 'pipelin', 'account']","['digit', 'learn', 'pipelin', 'relat', 'line', 'evalu', 'optim', 'infrastructur', 'comput', 'etl', 'engin', 'integr']","['sql', 'nosql', 'optim', 'etl', 'pipelin', 'account', 'digit', 'learn', 'pipelin', 'relat', 'line', 'evalu', 'optim', 'infrastructur', 'comput', 'etl', 'engin', 'integr']"
DE,"NEW Sr. Data Engineer Well branded and innovative industry leader is seeking a well rounded candidate who will have deep and broad experience in all aspects of data which include cleaning data, reporting and dash boarding, ETL, performance tuning, database architecture and design.
Responsibilities Ths individual will work closely and collaborate with data scientists, product management, and web engineers to deliver value and project outcomes.
Convert prototype models and data pipelines built by data scientists for use in production.
Evaluation and debugging of model performance to ensure parity with prototype.
Develop low-latency, real-time predictive models in a microservice environment.
Profiling and performance tuning of production code.
Qualifications Experience and interest in Big Data technologies (Hadoop Spark NoSQL DB).
Experience working on projects within the cloud ideally AzureAWS.
Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc.
SQL Python, Java, Scala, R. Experience in at least one ETL tool.
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets.
MUST be able to clearly articulate strategic, functional and technical concepts to all levels of stakeholders, technical teams and executive management.
Please send your resume to Colin Crane, Sr Technical Recruiter for immediate consideration.
","['sql', 'spark', 'scala', 'hadoop', 'db', 'cloud', 'python', 'java', 'dash', 'nosql']","['pipelin', 'tune', 'predict', 'big data', 'etl']",999,"['sql', 'spark', 'scala', 'hadoop', 'db', 'cloud', 'python', 'java', 'dash', 'nosql', 'pipelin', 'tune', 'predict', 'big data', 'etl']","['program', 'spark', 'pipelin', 'set', 'predict', 'hadoop', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']","['sql', 'spark', 'scala', 'hadoop', 'db', 'cloud', 'python', 'java', 'dash', 'nosql', 'pipelin', 'tune', 'predict', 'big data', 'etl', 'program', 'spark', 'pipelin', 'set', 'predict', 'hadoop', 'python', 'scientist', 'big', 'etl', 'engin', 'evalu']"
DE,"Senior Applied Data Engineers are experts in creating elegant, maintainable, and extensible data pipelines and database architectures.
As a Senior Applied Data Engineer, you will be the go-to person for questions about how to handle new datasets.
You will build maintainable and extensible ETL solutions for diverse data sources.
**This is a temporary, full-time, salaried position with benefits slated to end Dec. 31, 2020.
There is a possibility of extension if the business requires it.
**
Responsibilities
Transform data based on business requirements
Create pipelines that deliver data error-free and on-time with features such as logging, fault tolerance, notifications, and scalability
Document and train others on data pipelines
Serve as a technical resource in resolving issues related to data pipelines
Work closely with client-facing teams
Mentor junior members of the Applied Data Engineering team
Project manage data pipeline construction and maintenance work
Minimum Requirements
Bachelor’s degree in an analytical subject (statistics, math, economics, physics, engineering, business, political or social science, computer science, etc)
5+ years of experience with database and ETL pipeline design
Experience leading data and/or software projects
Proficiency in Python and SQL
Ability to work as part of a cross-functional team to solve problems and build solutions
Strong communication and teamwork skills
Eagerness to constantly learn and teach others
Preferred Qualifications
Significant experience with Python and SQL
Experience with software development practices including unit testing, version control, code review, and Agile development
Experience with technical project management
Mentorship and training experience
Familiarity with data and database technologies such as Redshift, Spark, S3, postgres, HDFS, etc., and with issues related to distributed systems.
The opportunity to be part of a growing tech startup focused on solving interesting and meaningful problems, invested in internal promotion, and committed to fostering a diverse, equal and inclusive workplace.
Competitive benefits, including unlimited PTO, 401K match with immediate vesting, health, dental, and vision benefits, paid parental leave, breastfeeding support including breastmilk shipping services for traveling moms, flexible work from home policy, commuter benefits, wellness initiatives including weekly group meditations, monthly on-site massage therapy, and pet insurance.
If you have a disability or special need that requires accommodation, please contact internalrecruiting@civisanalytics.com
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States.
EEO IS THE LAW
EEO Supplement
Pay Transparency
","['sql', 'spark', 'python', 's3', 'redshift']","['pipelin', 'statist', 'etl', 'commun', 'econom']",1,"['sql', 'spark', 'python', 's3', 'redshift', 'pipelin', 'statist', 'etl', 'commun', 'econom']","['analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'python', 'comput', 'appli', 'statist', 'etl', 'sourc', 'engin']","['sql', 'spark', 'python', 's3', 'redshift', 'pipelin', 'statist', 'etl', 'commun', 'econom', 'analyt', 'learn', 'spark', 'pipelin', 'relat', 'divers', 'python', 'comput', 'appli', 'statist', 'etl', 'sourc', 'engin']"
DE,"Hi All,
Hope you are doing well!!
Azure Focused Data Engineer
GC / USC
Duration: 6 months, extension likely
Interview Process: Standard (phone - or Skype/WebEx)
Need Azure focused Data Engineers with Web App experience.
Tableau is huge as well, but not necessary.
please provide details requested below:
FULL LEGAL NAME (As it appears on the I-9 form):
AVAILABLE:
EMAIL:
PHONE#
Expected RATE:
WORK STATUS:
Rachna Gaur
Technical Recruiter
Direct:
Mob;
Email:
Add: 18756 Stone Oak Park Way,Suite200,San Antonio TX 78258
","['tableau', 'azur']",[None],999,"['tableau', 'azur']","['avail', 'engin', 'azur']","['tableau', 'azur', 'avail', 'engin', 'azur']"
DE,"As part of the Data Engineering team, the Data Platform Engineer will work closely with Data Engineers and IT Infrastructure team to ensure the data platform is highly available, reliable, and stable.
This individual will provide technical and thought leadership to the team to streamline the delivery of analytics to the business.
Must Have Kubernetes Linux MapR Responsibilities Implementation and ongoing administration of the platform Perform incident investigation, diagnosis and provide resolution Manage cluster provisioning, performance tuning, and security configuration System monitoring and remediation of any production issues Identify recurring problems and perform root cause analysis Provide and implement sustainable solutions Collaborate with the application teams to install updates, fixes, and patches Coordinate and perform version upgrades File system management and monitoring Act as a primary contact for the platform Point of contact for hardware and vendor escalation Document all service levels Perform application deployments acting as a gatekeeper to the production environment.
","['kubernet', 'linux']","['tune', 'hardwar', 'cluster']",999,"['kubernet', 'linux', 'tune', 'hardwar', 'cluster']","['analyt', 'primari', 'infrastructur', 'provid', 'avail', 'engin']","['kubernet', 'linux', 'tune', 'hardwar', 'cluster', 'analyt', 'primari', 'infrastructur', 'provid', 'avail', 'engin']"
DE,"Need Data Engineer with
ETL
Data modelling
</li>
",[None],['etl'],999,['etl'],"['etl', 'engin']","['etl', 'etl', 'engin']"
DE,"Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction.
Defines scope, plans, and deliverables for assigned components.
Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems.
Applies metrics to monitor performance and measure key project parameters.
Prepares system documentation.
Conforms to security and quality standards.
Stays current on emerging tools, techniques, and technologies.Responsibilities:+ Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives+ Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams+ Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.+ Develop visualization, user experience and configuration elements of solution design.+ Execute and coordinate requirements management and change management processes.
Participates as a member of and leads development teams.+ Designs units for others.+ Completes development to implement complex components.+ Designs solutions for others to develop.+ Participates in cross-functional teams.+ Leads design activities and provides mentoring and guidance to developers.+ Designs, prepares and executes unit tests.+ Demonstrates technical leadership and exerts influence outside of immediate team.+ Develops innovative team solutions to complex problems.+ Contributes to strategic direction for teams.+ Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g.
Determination on requests for reasonable accommodation are considered on a case-by-case basis.
This contact information (email and phone) is intended for application assistance and accommodation requests only.
",[None],['visual'],999,['visual'],"['basi', 'analyt', 'techniqu', 'corpor', 'visual', 'appli', 'engin', 'particip']","['visual', 'basi', 'analyt', 'techniqu', 'corpor', 'visual', 'appli', 'engin', 'particip']"
DE,"Should have at least 6-7 years of minimum relevant experience in managing Cisco/Juniper Infrastructure.
Technical/Functional Skills must.
Manage and Troubleshoot Layer 2 and Layer 3 Datacentre (DC) network infrastructure
Manage and Troubleshoot switch network configured with MST, RPVST, Ether-channel, Dot1Q Trunking, Pruning, etc
Knowledge on OTV & ACI is mandate.
Manage and Troubleshoot complex enterprise routing with MPLS, BGP, OSPF and QoS
Handle and troubleshoot Cisco NEXUS switches (5000, 7000, 6000 , 9000 series) configured with VDC, VPC, FEX , OTV
Handle and Troubleshoot DC setup with VPN (DMVPN, L2L and AnyConnect) tunnels configured on Cisco Routers and ASA boxes- Cisco ASR 1000 Service Aggregation routers, 4000 ISR Gen
Added advantages.
Cisco WAN Accelerator (WAAS)
Handle and troubleshoot Cisco load balancing solutions F5 Load Balancer
Experience on firewalls.
Hands on experience over Cisco Client or Client
Should know about ACI and SDWAN technologies.
It will be added advantage if working experience.
Certifications:
CCNA Must and CCNP preferred.
",[None],[None],999,[],"['handl', 'infrastructur']","['handl', 'infrastructur']"
DE,"Python Data Engineer - RT
Houston, Texas, United States, 77002
Contract, Contract - Option to Extend
This is a 6-month contract opportunity with the possibility of extension.
Essential Duties/Responsibilities:
Design and develop data pipelines in Python
Abstract, refactor, and package reusable code for broad use
Develop test suites to ensure code works as planned and enable fast edits as business requirements change
Additional Knowledge, Skills and Abilities:
Desire to automate everything
Good communication skills
Independent and self-driven worker
Creative problem solver
Required Knowledge, Skills, and Abilities: (Submission Summary):
1.
Degree (BS, MS, or PhD) in Computer Science, Electrical and Computer Engineering, Software Engineering, or Information Systems preferred; any STEM BS, MS or PhD with appropriate work experience will be considered.2+ years of professional Python software design and development
2.
Linux administration experience a plus
3.
Expert-level Python programming
4.
Test-driven development
5.
Building and monitoring data pipelines
6.
Proficient with the following Python packages: SQLAlchemy, pandas, sphinx, and pytest
7.
Git
8.
SQL
9.
Continuous Integration and Deployment (Gitlab-ci, Ansible, Jenkins, or equivalent)
10.
Experience developing in containerized environments (Docker, LXC)
11.
Experience with multiprocessing and multithreading in Python
12.
Experience with MongoDB and Redis is a plus
13.
Experience with a pipeline framework like Prefect, Airflow, Luigi is a plus
14.
Experience with AWS or other cloud service a plus
15.
Ability to design Python software from scratch
16.
Candidate's home phone #:
17.
Candidate's cell phone #:
18.
Candidate email address:
19.
Best time to contact the candidate:
20.
When is the candidate available to start?
","['mongodb', 'sql', 'linux', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker']","['commun', 'pipelin']",1,"['mongodb', 'sql', 'linux', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker', 'commun', 'pipelin']","['essenti', 'pipelin', 'aw', 'avail', 'python', 'packag', 'comput', 'texa', 'integr', 'engin']","['mongodb', 'sql', 'linux', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker', 'commun', 'pipelin', 'essenti', 'pipelin', 'aw', 'avail', 'python', 'packag', 'comput', 'texa', 'integr', 'engin']"
DE,"As a senior leader in Software Engineering, you'll combine your years of proven expertise with a never-ending quest to create innovative technology through solid engineering practices.
You'll lead a highly inspired and inquisitive team of technologists who are already developing and deploying applications to the highest standards.
With your deep knowledge of design, analytics, development, coding, testing, and application programming, your team will raise their game even more, meeting your standards, as well as satisfying both business and functional requirements.
Your passion and experience in one or more technology domains will help solve complex and mission critical problems, internally and externally.
As a constant learner and early adopter, you're already embracing leading-edge technologies and methodologies; your example encourages others to follow suit.
This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Mastery of application, data, and infrastructure architecture disciplines
Command of architecture, design, and business processes
Knowledge of industry-wide technology trends and best practices
Keen understanding of financial control and budget management
Expertise in working in in large, collaborative teams to achieve organizational goals
Passionate about building an innovative culture
Experienced in modern programming languages
This role requires a wide variety ofstrengths and capabilities, including:
BS/BA degree or equivalentexperience
Mastery of application, dataand infrastructure architecture disciplines
Command of architecture, designand business processes
Knowledge of industry widetechnology strategies and best practices
Keen understanding of financialcontrol and budget management
Expertise in working in inlarge, collaborative teams to achieve organizational goals, and passionateabout building an innovative culture
Experienced in modernprogramming languages such as Java, SQL, Scala (nice to have)Experience developing enterprise platforms using Big Data tools and technologies (e.g.
Hadoop, Spark, Hive, Impala, Zeppelin, Jupyter)
The hiring manager for this job openingwould welcome a conversation about flexibility.
This could range from ad hoc flexibility in a full time position, to amore formal Flexible Work Arrangemen
Equal Opportunity Employer/Disability/Veterans
","['sql', 'spark', 'scala', 'jupyt', 'hadoop', 'java', 'hive']",['big data'],1,"['sql', 'spark', 'scala', 'jupyt', 'hadoop', 'java', 'hive', 'big data']","['analyt', 'program', 'spark', 'hadoop', 'infrastructur', 'big', 'engin']","['sql', 'spark', 'scala', 'jupyt', 'hadoop', 'java', 'hive', 'big data', 'analyt', 'program', 'spark', 'hadoop', 'infrastructur', 'big', 'engin']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"Job Title: Python Data Engineer/Developer (9-15 yrs)
Posted on: 07-08-2020
Requirements:
Experience developing in containerized environments (Docker, LXC)
Experience with multiprocessing and multithreading in Python
Experience with MongoDB and Redis is a plus
Experience with a pipeline framework like Prefect, Airflow, Luigi is a plus
Experience with AWS or other cloud service a plus
Must have developed at the Application Level Strong Software Engineering level skills
Developed in Python –solid in Python – will be designing and developing data pipelines in Python.
Ability to design Python software from scratch
Desire to automate everything
Good communication skills
Independent and self-driven worker
Creative problem solver
Proficient with the following Python packages: SQLAlchemy, pandas, sphinx, and pytest
Git
SQL
Continuous Integration and Deployment (Gitlab-ci, Ansible, Jenkins, or equivalent)
Building and monitoring data pipelines
Good to have
Experience developing in containerized environments (Docker, LXC)
Experience with multiprocessing and multithreading in Python
Experience with MongoDB and Redis is a plus
Experience with a pipeline framework like Prefect, Airflow, Luigi is a plus
Experience with AWS or other cloud service a plus
Contact no: (281)957-5888 Ext: 384
Email: liza.k@wisemen.com
","['sql', 'mongodb', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker']","['commun', 'pipelin']",999,"['sql', 'mongodb', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker', 'commun', 'pipelin']","['pipelin', 'aw', 'python', 'packag', 'integr', 'engin']","['sql', 'mongodb', 'airflow', 'panda', 'git', 'aw', 'cloud', 'python', 'docker', 'commun', 'pipelin', 'pipelin', 'aw', 'python', 'packag', 'integr', 'engin']"
DE,"This nationwide portfolio of communications infrastructure connects cities and communities to essential data, technology and wireless service - bringing information, ideas and innovations to the people and businesses that need them.
S/he builds the automated data pipelines to ingest and prepare the data to meet the reporting and analytics needs of the organization.
This includes building and maintaining the data structures and architectures for data ingestion, processing and deployment for large-scale, data-intensive applications.
This individual must ensure that optimal ETL (Extract, Transformation, and Load) solutions are developed by applying best practices to the data modeling, code development and automation.Essential Job Functions* As part of an agile team, design, develop and maintain an optimal data pipeline architecture using both structured data sources and big data for both on-premise and cloud-based environments.
* Develop and automate ETL code using scripting languages, ETL tools and job scheduling software to support all reporting and analytical data needs.
* Design and build dimensional data models to support the data warehouse initiatives.
* Assemble large, complex data sets that meet the analytical needs of the data scientist teams.
* Assess new data sources to better understand availability and quality of data.
* Identify, design, and implement internal process improvements: automating manual processes, optimizing data pipeline performance, re-designing infrastructure for greater scalability and access to information.
* Participate in requirements gathering sessions to distill technical requirements from business requests.
* Collaborate with business partners to productionize, optimize, and scale enterprise analytics.
* Collaborate with data architects and modelers on data store designs and best practices* Provide off-hours support for all developed data pipelines in an on-call rotation.Education/Certifications* Bachelor's degree in Computer Science, Engineering, Information Science, Math or related discipline* Data engineering, data management or cloud certification is a plusExperience/Minimum Requirements* At least six (6) to eight (8) years of experience in in a data engineering role or related specialty with demonstrated ability in data modeling* At least two (2) years Data engineering experience on the Microsoft Azure, Amazon Web Services (AWS), or Snowflake* Experience using Extract, Transformation and Load (ETL) tools with Informatica (IICS) to build automated data pipelines* Experience with object-oriented/object function scripting languages: Python, Java, C++Other Skills/Abilities* Thorough understanding of relational, columnar and NoSQL database architectures and industry best practices for development* Understanding of dimensional data modeling for designing and building data warehouses* Excellent advanced SQL coding and performance tuning skills* Experience with big data tools: Hadoop, Spark, Kafka, etc.
* Experience with parsing data formats such as XML/JSON and leveraging external APIs* Understanding of agile development methodologies* Ability to work in a team-oriented, collaborative environment; good interpersonal skills* Strong analytical and problem-solving skills; ability to weigh various suggested technical solutions against the original business needs and choose the most cost-effective solution* Keen attention to detail and ability to access impact of design changes prior to implementation* Self-driven, highly motivated and ability to learn quick* Ability to effectively prioritize and execute tasks in a high-pressure environment* Strong customer service orientation* Ability to present and explain technical information to diverse types of audiences in a way that establishes rapport and gains understanding* Work experience with geospatial data and spatial analytics is preferredWorking Conditions: Works in a normal office setting with no exposure to adverse environmental conditions.
Provide on-call support for database application and layered products as needed.
","['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'excel', 'amazon web services']","['pipelin', 'normal', 'data modeling', 'optim', 'big data', 'etl', 'commun', 'math', 'geospati']",1,"['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'excel', 'pipelin', 'normal', 'data modeling', 'optim', 'big data', 'etl', 'commun', 'math', 'geospati']","['provid', 'python', 'etl', 'sourc', 'analyt', 'essenti', 'azur', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'particip', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'excel', 'pipelin', 'normal', 'data modeling', 'optim', 'big data', 'etl', 'commun', 'math', 'geospati', 'provid', 'python', 'etl', 'sourc', 'analyt', 'essenti', 'azur', 'hadoop', 'optim', 'avail', 'infrastructur', 'aw', 'warehous', 'set', 'big', 'engin', 'particip', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"CIC Montreal is an IBM Client Innovation Centre operated by LGS, a wholly owned subsidiary of IBM.
The Centre provides services in application development and support to public and private Canadian organizations.
It is distinguished by a capacity to simultaneously offer large-scale projects, highly stimulating professional challenges, an environment favorable to continuous learning, professional mentoring and, lastly, opportunities for career advancement.
What you will do
The Data Engineer role is part of the Cognitive Process Transformation (CPT) team which focuses on those activities that support the integration, design and modeling, storage, and organization of data.
This individual must have a solid understanding of SAP data services and process automation techniques.
This individual must have strong experience in building large complex data sets, be able to articulate best practices related to data quality and cleansing and be able to incorporate those practices into delivered solutions.
What you need to have
Ability to work with the client directly to understand and meet their requirements;
Excellent verbal and written communication abilities: must effectively communicate with technical and non-technical teams;
Participate in teams working in an Agile/Scrum or Waterfall process and ensure the stories/tasks are well defined and have all the information and tools to be successful
Ability to work independently on tasks and deliver with a high-level of quality;
Ability to work in teams and be open to comments and feedback;
Ability to learn quickly and to adapt to a fast-paced environment;
Data processing, data design and modeling, deploying the model;
Technical competencies
3+ years experience in SAP data services
3+ years Python experience (preferable using PySpark);
3+ year of in-depth database knowledge of SQL and NoSQL including the ability to write, tune, and interpret SQL queries;
Experience with Cloud services (Azure, AWS, IBM Cloud);
3+ year experience in data warehousing (Hadoop, MapReduce, HIVE, PIG, Apache Spark, Kafka)
Experience building large, complex big data sets and delivery mechanisms to support advanced analytics and insights analysis
Knowledge of tools to perform data quality, data cleansing, data wrangling and data standards;
Experience with other languages such as C #, Javascript, Matlab an asset
Experience working with container-orchestration system( Kubernetes) or any other containerized applications systems also an asset
Solid knowledge on different operating sytems such as Unix, Microsoft windows etc (an asset)
Basic Machine Learning Familiarity (an asset)
Previous experience industrializing machine learning projects will be highly considered.
1+years experience translating business needs to data requirements and designing data-driven solutions supporting analytics and insights.
Education
Must have a technical academic degree in the Comp engineering, Comp Science, Systems Engineering or Mathematics & Statistics field.
Job Type: Full-time
Work Remotely:
No
","['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab']","['cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun']",1,"['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab', 'cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun']","['techniqu', 'python', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'statist', 'asset', 'learn', 'public', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'relat']","['sql', 'mapreduc', 'python', 'c', 'nosql', 'kafka', 'azur', 'pig', 'unix', 'hadoop', 'javascript', 'hive', 'kubernet', 'pyspark', 'aw', 'microsoft', 'excel', 'spark', 'cloud', 'matlab', 'cleans', 'tune', 'machine learning', 'data warehousing', 'statist', 'big data', 'commun', 'techniqu', 'python', 'integr', 'analyt', 'challeng', 'azur', 'hadoop', 'statist', 'asset', 'learn', 'public', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'relat']"
DE,"a.dialogApplyBtn {
display: none;
}
Data Engineering Manager
#job-location.job-location-inline {
display: inline;
}
Req ID:
72986
Facility:
One Houston Center-130
Department:
Digital Transformation
Division:
Global Business Services
Basic Function
Through leading and working closely with other Data Engineers, Data Scientists, business domain experts, and IT, the Manager of Data Engineering will have the opportunity to provide significant tangible business value through leading, developing and deploying scalable Data Engineering capabilities on high value problems.
Roles & Responsibilities
Place emphasis on delivering value through leading data engineering pipelines and solutions, through excellent problem solving, development, and implementation of scalable solutions
Work collaboratively with Data Scientists and business SMEs (Process Engineers, Automation Engineers, Reliability Engineers, other business domain experts) in providing data pipelines and workflows which are critical to delivering data driven solutions on high impact problems
Work collaboratively with IT to develop and ensure proper architecture, security, exception handling, testing, and code development standards are adhered to
Architect and lead the development of data workflows and pipelines necessary for algorithm and ML solution deployment and maintenance for chemical manufacturing domains including predictive maintenance, reliability, preventing downtime, industrial automation and optimization, demand forecasting, and improving health and safety
Leverage deep data engineering, architecture, advanced technologies, software development expertise, and code development standards to lead the delivery of advanced analytics projects
Architect, define, and lead the development of technical platforms, frameworks, and applications, to provide data, business intelligence, and information
Network internally and externally to build relationships that foster technology transfer and collaboration
Assist with leading the promotion of digital transformation, training and development of other Data Engineers, Data Analysts, and other peers throughout the organization
Min.
Qualifications
BS in Computer Science, Software Engineering, Computer Engineering, or related technical field.
MS or PhD strongly preferred
9+ years professional hands on technical experience developing and implementing Data Engineering workflows, pipelines, and applications or 7+ years’ experience with PhD.
3+ years’ experience leading Data Engineering projects, other Data Engineers, and Software Developers
High level of enthusiasm and a love of data and software development
Strong quantitative and problem-solving skills, including strong data, technical, and mathematical knowledge and skills
Strong Python, SQL, and JavaScript programming and scripting skills
Strong experience with stream-processing systems, Big Data querying tools, MapReduce, MongoDB, Cassandra, integration of data from multiple data sources, ETL techniques and frameworks, and/or messaging systems
Proficiency and expertise in Real Time historian data, SQL, NoSQL, and NewSQL database technologies
Experience working with large structured and unstructured data and technologies
Experience with modern Data Engineering, ML and Data Science libraries such as Pandas, NumPy, Scikit-Learn, NLTK, Seaborn, Dplyr
Specialty skills in large scale computing, cluster computing, and cloud computing (Azure preferably)
Experience with Linux, Unix, Git, and software development testing frameworks
Specialized experience in providing data engineering solutions in various Data Science domains, such as Predictive Maintenance, Forecasting, Real Time streaming analytics, Image Analysis leveraging deep learning methodologies, and optimization
Preferred Qualifications
MS strongly preferred, with a PhD preferred
9+ years professional hands on technical experience developing and implementing Data Engineering workflows, pipelines, and applications
Strong programming and scripting skills in C#, C++, Java, Julia, or Scala
Experience with Azure DevOps, Jupyter Notebooks, or VS Code
Experienced working with Data Scientists on architecting and delivering scalable, end to end advanced analytics solutions
Knowledge of ML techniques & algorithms, such as ANNs, SVM, GBM, Decision Forests, Clustering algorithms
Experience with GPU technology, NVIDIA data science development stacks, and CUDA programming
Competencies
Builds effective teams
Collaborates
Cultivates innovation
Customer focus
Demonstrates courage
Drives results
Ensures accountability
Instills trust and exemplifies integrity
Nearest Major Market: Houston
","['sql', 'mapreduc', 'linux', 'python', 'java', 'c', 'nosql', 'scikit', 'dplyr', 'azur', 'unix', 'git', 'javascript', 'nltk', 'mongodb', 'cassandra', 'scala', 'panda', 'jupyt', 'excel', 'julia', 'numpi', 'cloud', 'seaborn']","['forecast', 'pipelin', 'predict', 'deep learning', 'optim', 'problem solving', 'gbm', 'big data', 'etl', 'svm', 'cluster', 'account']",1,"['sql', 'mapreduc', 'linux', 'python', 'java', 'c', 'nosql', 'scikit', 'dplyr', 'azur', 'unix', 'git', 'javascript', 'nltk', 'mongodb', 'cassandra', 'scala', 'panda', 'jupyt', 'excel', 'julia', 'numpi', 'cloud', 'seaborn', 'forecast', 'pipelin', 'predict', 'deep learning', 'optim', 'problem solving', 'gbm', 'big data', 'etl', 'svm', 'cluster', 'account']","['program', 'techniqu', 'stream', 'predict', 'python', 'etl', 'quantit', 'sourc', 'integr', 'algorithm', 'analyt', 'ml', 'azur', 'optim', 'learn', 'big', 'engin', 'digit', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'mapreduc', 'linux', 'python', 'java', 'c', 'nosql', 'scikit', 'dplyr', 'azur', 'unix', 'git', 'javascript', 'nltk', 'mongodb', 'cassandra', 'scala', 'panda', 'jupyt', 'excel', 'julia', 'numpi', 'cloud', 'seaborn', 'forecast', 'pipelin', 'predict', 'deep learning', 'optim', 'problem solving', 'gbm', 'big data', 'etl', 'svm', 'cluster', 'account', 'program', 'techniqu', 'stream', 'predict', 'python', 'etl', 'quantit', 'sourc', 'integr', 'algorithm', 'analyt', 'ml', 'azur', 'optim', 'learn', 'big', 'engin', 'digit', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"No need to be in Houston or relocate!
Here’s the deal:
The Data Engineer will be responsible for transforming data into a format that can be easily analyzed.
Principle Accountabilities:
Installation of clusters across multiple nodes
Security assessment and guidance.
Skills Needed:
MapR cluster administration (preferably with the latest version of MapR + MapR 5.2)
MapR security, monitoring, DB and Hbase administration
MapR ES and Kafka administration
MapR Data Governance story
Hive, Drill, Spark, Sqoop, Oozie installation, administration, and tuning
Automation/scripting using Ansible, Puppet, Chef or Salt
Experience with NFS or FUSE
Basic ETL experience
Cloud experience
Docker and Kubernetes experience a big plus
Qualifications and Requirements:
Strong work ethic, attitude and follow through ability.
Excellent communicative, presentation and interpersonal skills.
Makes compelling presentations to a variety of audiences using visual aids, PowerPoint presentations and software demos.
Is adept at getting the attention and involvement of the most sophisticated and difficult audiences.
Takes initiative and pursues opportunities.
Self-motivated and excellent multi-tasking skills.
Prioritizes and performs a variety of concurrent tasks with minimal direction.
Projects a professional and polished image that inspires confidence and trust.
","['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker']","['visual', 'analyz', 'etl', 'commun', 'cluster', 'account']",999,"['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker', 'visual', 'analyz', 'etl', 'commun', 'cluster', 'account']","['spark', 'visual', 'big', 'etl', 'engin']","['spark', 'node', 'kafka', 'db', 'cloud', 'hive', 'hbase', 'kubernet', 'powerpoint', 'excel', 'docker', 'visual', 'analyz', 'etl', 'commun', 'cluster', 'account', 'spark', 'visual', 'big', 'etl', 'engin']"
DE,"Position Title: Senior Data Engineer - C3.ai
Job Type: Full time Role with Benefits
Work Auth: ANY
Salary Method: W2
Salary: 250-350K
Data Engineers partner with data scientists, application developers, and business subject matter experts to understand business and technical requirements, build data ingestion pipelines and create time aligned object models for the algorithms to execute against.
As a member of the Artificial Intelligence team you will need to thrive in a fast pace and innovative environment.
Collaboration, creativity, and an intense focus on attaining positive business results will be necessary skills.
The successful candidate will possess the following:
Strong collaboration and communication skills and ability to work within cross functional teams
Strong economic thinking skills and business acumen
Self-motivation and a drive to create innovative solutions and see them applied
An entrepreneur's mindset and the persistence to create value through work process transformation
A Day In The Life Typically Includes:
Continuously improve data pipelines including solutions for data management, security and performance
Develop and implement solutions for data quality validation
Build APIs for data consumption
Work closely with Data Scientists to re-engineer model code with new features and deploy models to production
Monitor, resolve, and escalate data pipeline production issues as appropriate
Stay current on technologies and best practices, ensuring that best knowledge is leveraged to achieve success
What You Will Need:
Basic Qualifications:
Bachelor's Degree
10+ years proven professional experience with object-oriented programming in a technology focused role (including but not limited to IT Roles such as: Software Developer, Data Engineer, DevOps/Cloud Engineer, Data Scientist)
5+ years professional coding experience with Python language
5+ years' experience building Data Integrations and/or Machine Learning models for analytics
What Will Put You Ahead?
Preferred Qualifications:
Bachelor's Degree with an Engineering or Technology focus
Experience transforming datasets using Python Architecture skills including designing data pipelines, ETL workflows and integrations
Experience working with SQL and database programming
Hackathon Championships
Experience working or leading Agile Product Development Teams
Experience with cloud platforms like AWS/GCP/Azure
Salary and benefits commensurate with experience.
Equal Opportunity Employer.
Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test.
Salary 250-350K
","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python']","['pipelin', 'machine learning', 'etl', 'commun', 'econom']",1,"['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom']","['day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom', 'day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']"
DE,"Position Title: Senior Data Engineer - C3.ai
Job Type: Full time Role with Benefits
Work Auth: ANY
Salary Method: W2
Salary: 250-350K
Data Engineers partner with data scientists, application developers, and business subject matter experts to understand business and technical requirements, build data ingestion pipelines and create time aligned object models for the algorithms to execute against.
As a member of the Artificial Intelligence team you will need to thrive in a fast pace and innovative environment.
Collaboration, creativity, and an intense focus on attaining positive business results will be necessary skills.
The successful candidate will possess the following:
Strong collaboration and communication skills and ability to work within cross functional teams
Strong economic thinking skills and business acumen
Self-motivation and a drive to create innovative solutions and see them applied
An entrepreneur's mindset and the persistence to create value through work process transformation
A Day In The Life Typically Includes:
Continuously improve data pipelines including solutions for data management, security and performance
Develop and implement solutions for data quality validation
Build APIs for data consumption
Work closely with Data Scientists to re-engineer model code with new features and deploy models to production
Monitor, resolve, and escalate data pipeline production issues as appropriate
Stay current on technologies and best practices, ensuring that best knowledge is leveraged to achieve success
What You Will Need:
Basic Qualifications:
Bachelor's Degree
10+ years proven professional experience with object-oriented programming in a technology focused role (including but not limited to IT Roles such as: Software Developer, Data Engineer, DevOps/Cloud Engineer, Data Scientist)
5+ years professional coding experience with Python language
5+ years' experience building Data Integrations and/or Machine Learning models for analytics
What Will Put You Ahead?
Preferred Qualifications:
Bachelor's Degree with an Engineering or Technology focus
Experience transforming datasets using Python Architecture skills including designing data pipelines, ETL workflows and integrations
Experience working with SQL and database programming
Hackathon Championships
Experience working or leading Agile Product Development Teams
Experience with cloud platforms like AWS/GCP/Azure
Salary and benefits commensurate with experience.
Equal Opportunity Employer.
Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test.
Salary 250-350K
","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python']","['pipelin', 'machine learning', 'etl', 'commun', 'econom']",1,"['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom']","['day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom', 'day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']"
DE,"Do you want your voice heard and your actions to count?
Were a team that accepts responsibility for the future by asking the tough questions and owning the solutions.
Job Summary
As the Data Engineer, you need to be collaborative and passionate about solving complex data engineering problems.
You will develop, build, and operate the platform using DevSecOps and System Reliability Engineering (SRE) methods.
Major Responsibilities:
Gather and process large, complex, raw data sets at scale.
Build processes to support data transformation, data structures, metadata, dependency, and workload management.
Build the infrastructure required for optimal extraction, transformation, and loading of data.
Partner with risk management and security teams to identify the standards and lead the design, build, and rollout of secured and compliant data services.
Embrace Infrastructure-as-Code, and use Continuous Integration / Continuous Delivery Pipelines to handle the full data service lifecycle.
Write infrastructure, application, and data automated test cases and participate in code review sessions.
Provide Level 3 support for troubleshooting and services restoration in Production.
The right candidate will have:
8+ years of technical experience with data services solution design and implementation in a cloud-native environment, possessing expert-level skills in four or more of the following areas:
Data field encryption, tokenization and metadata management
SQL and NoSQL databases, including Postgres, DynamoDB etc.
Experience with data pipeline and workflow tools: Wherescape Streaming, Wherescape RED, StreamSets Data Collector etc.
Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc.
History of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering
Know-how of SQL, Informatica PowerCenter or similar.
Experience with secure cloud services for data management and integration
Developing automation with python, bash, java, powershell or similar languages
Familiar with DevOps toolchain, i.e.
BitBucket, JIRA, Jenkins Pipeline, Artifactory or Nexus, and experienced in deploying n-tier application stacks in AWS
Excellent data and system analysis, data mapping, and data profiling skills
Good understanding of cloud-native application models and patterns
Able to work alternative coverage schedules when necessary
Ability to find a solution with limited guidance
Bachelor's degree in computer science or related field, or equivalent professional experience
Desired Knowledge, Skills, and Experience:
Experience with container orchestration technologies such as Docker, Kubernetes, Openshift
AWS professional level certifications is preferred but not required
The above statements are intended to describe the general nature and level of the work being performed.
They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified.
A conviction is not an absolute bar to employment.
Factors such as the age of the offense, evidence of rehabilitation, seriousness of violation, and job relatedness are considered in all employment decisions.
Additionally, its the banks policy to only inquire into a candidates criminal history after an offer has been made.
Federal law prohibits banks from employing individuals who have been convicted of, or received a pretrial diversion for, certain offenses.
","['sql', 'spark', 'jira', 'aw', 'cloud', 'python', 'java', 'postgr', 'nosql', 'kubernet', 'kafka', 'excel', 'docker']","['optim', 'etl', 'risk', 'pipelin']",1,"['sql', 'spark', 'jira', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'excel', 'docker', 'optim', 'etl', 'risk', 'pipelin']","['stream', 'spark', 'pipelin', 'set', 'relat', 'divers', 'infrastructur', 'provid', 'optim', 'python', 'aw', 'comput', 'action', 'etl', 'engin', 'integr']","['sql', 'spark', 'jira', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'excel', 'docker', 'optim', 'etl', 'risk', 'pipelin', 'stream', 'spark', 'pipelin', 'set', 'relat', 'divers', 'infrastructur', 'provid', 'optim', 'python', 'aw', 'comput', 'action', 'etl', 'engin', 'integr']"
DE,"Essential Duties and Responsibilities:
Develops high performance distributed data warehouse, distributed analytic systems and cloud architectures.
Participates in the development of relational and non-relational data models focusing on design for optimal storage and retrieval
Develops, tests and debugs batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources; provides recommendations to improve data reliability, efficiency and quality.
Design and develop scalable solutions leveraging using technologies including Docker and Kubernetes
Working along side data scientists, supports the development and promotion of high-performance algorithms, models and prototypes.
Implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information.
Ensures that Data Warehousing and Big Data systems meet business requirements and industry practices; including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring requirements.
Troubleshoots data and performance related issues; implements adjustments, documents root cause and corrective measure; provides recommendations to stakeholders.
Documents technical specifications and participates with peers in design and code review sessions.
Employ a variety of scripting languages and tools to integrate data from multiple disparate data sources.
Use basic statistical and visualization techniques to analyze the resulting data sets of your processes
Learn and stay abreast of new technologies that can improve the efficacy of the analytics and data science teams
Stays current on the latest industry technologies, trends and strategies.
Other duties as assigned.
Job Requirements
Qualifications:
This position requires a minimum of five years of progressive database development and integration experience.
Strong knowledge of logical and physical data modeling is necessary, including but not limited to entity design, relationships, indexing and star schemas.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
SQL experience is required.
Scripting knowledge with SQL and Python.
Experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
Proven analytical problem solving and decision making skills is critical.
Ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task, and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Work Days:
Normal work days are Monday through Friday.
Occasional Saturdays and Sundays may be necessary.
Work Hours:
Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.
Not ready to
","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker']","['recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']",1,"['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']","['day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl', 'day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Responsible for the design, build and implementation of cloud-based analytics platform which includes an MPP Enterprise Data Warehouse and other Big Data technologies.
Essential Duties and Responsibilities:
Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture
Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources
Performs complex data calculations through data integration tools and scripting languages
Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information
Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers
Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team
Documents technical specifications and participates with peers in design and code review sessions
Develops complex cross application architectures in collaboration with cross functional teams
Stays current on the latest industry technologies, trends and strategies
Completes work in a timely and accurate manner while providing exceptional customer service
Other duties as assigned
Job Requirements
Qualifications:
This position requires a minimum of eight years of progressive database development and integration experience.
Proven understanding of logical and physical data modeling is imperative.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
Expert level SQL experience is required.
Scripting knowledge with SQL, Python, Java or R is necessary.
Proven experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Proven experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
Proven analytical problem solving and decision-making skills is critical.
Demonstrated ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Work Days:
Normal work days are Monday through Friday.
Occasional Saturdays and Sundays may be necessary.
Work Hours:
Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.
Not ready to
","['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel']","['pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']",1,"['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel', 'pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']","['day', 'analyt', 'essenti', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin', 'particip', 'integr']","['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel', 'pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl', 'day', 'analyt', 'essenti', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin', 'particip', 'integr']"
DE,"Purpose of Job
Data Engineers deliver quality reporting and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions.
Data Engineers (DEs) are engaged in all phases of the data management lifecycle; gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data.
Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored and controlled.
Job Requirements
This singular mission requires a dedication to innovative thinking at every level.
https://www.youtube.com/watch?v=kVCnnaJUH_c
Data Engineer A Realistic Preview
Identifies and manages existing and emerging risks that stem from business activities and the job role.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled.
Follows written risk and compliance policies and procedures for business activities.
Design and implement technical solutions.
Identify and solve significant technical problems and architecture deficiencies.
Participate in daily standups and design reviews.
Breakdown business features and into technical stories and approaches.
Analyze data and enable machine learning.
Create proof of concepts and prototypes.\
Help on-board entry level engineers.
Collaborate with the team and other engineers to plan and execute assignments and tasks.
May begin mentoring junior engineers.
Minimum Experience:
Bachelor's degree in related field of study,
OR
Certification from an approved technical field of study,
OR
4 additional years of related experience beyond the minimum required.
4 years of of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)
*Qualifications may warrant placement in a different job level*
This will take approximately 5 minutes.
Once you begin the questions you will not be able to finish them at a later time and you will not be able to change your responses.
Preferred Experience:
IBM DataStage Experience
4+ years of Data Warehousing experience
Unix bash Experience
Python Experience
Java Experience
Hadoop Experience
Control-m Experience
For Internal Candidates:
Must complete 12 months in current position (from date of hire or date of placement), or must have managers approval prior to posting.
","['java', 'python', 'unix', 'hadoop']","['machine learning', 'data warehousing', 'risk', 'analyz']",1,"['java', 'python', 'unix', 'hadoop', 'machine learning', 'data warehousing', 'risk', 'analyz']","['machin', 'collect', 'hadoop', 'python', 'relat', 'engin', 'particip']","['java', 'python', 'unix', 'hadoop', 'machine learning', 'data warehousing', 'risk', 'analyz', 'machin', 'collect', 'hadoop', 'python', 'relat', 'engin', 'particip']"
DE,"Your Impact
As one of the founding engineers on the team, you will make key design decisions that will shape a series of data products and services.
You will be responsible for creating scalable ETL and streaming processes, efficient data pipelines, and a data warehouse that delivers value to technical users in public safety agencies.
Your Day-to-Day
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Partner with internal teams and agencies to make public safety data accessible and actionable.
Influence peers, advise senior leaders, coach and mentor junior team members.
Facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.
Basic Qualifications
Bachelor’s degree in CS, engineering, or other quantitative field
5+ years of industry experience in data warehousing and modeling on highly available SQL and non-relational (NoSQL and distributed database management systems)
Fluent in writing and optimizing SQL with demonstrated strength in writing complex, high-optimized queries across large data sets
Proficiency in at least one scripting language, Python, R, or similar
Demonstrated strength in design, development, and optimization of low latency pipelines for both stream and batch data in Apache Spark or similar
Ability to make tough technical decisions based on requirements, constraints, and trade-offs
Preferred Qualifications
Experience with big data technologies like Hadoop, Spark, Cascading, Hive, PrestoDB, Zookeeper, etc.
Knowledge of distributed systems and resource optimization for data storage and processing
Backend engineering experience (Java, Scala, C++, or similar)
Experience with BI tools like Tableau, PowerBI, etc.
Compensation and Benefits
Competitive salary and 401K with employer match
Discretionary paid time off
An encouraging parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...
","['sql', 'spark', 'scala', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi']","['pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun']","['day', 'stream', 'spark', 'pipelin', 'set', 'relat', 'public', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'warehous', 'action', 'big', 'quantit', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun', 'day', 'stream', 'spark', 'pipelin', 'set', 'relat', 'public', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'warehous', 'action', 'big', 'quantit', 'engin']"
DE,"Data Engineer
9920
Scottsdale,
3/27/2019 1:46:38 PM
Application Development
Contractor - W2
Data Engineer / Lead
4-5 years of experience in ETL, SQL, Python, Data Management and Spark and strong fundamentals in distributed environments
Real project implementations with Big Data technologies based on Spark
Experience working on Serverless technologies
Experience implementing NoSQL technologies – Mongo or Cassandra
Experience with AWS cloud services: Lambda, S3, Glue, Redshift, and Athena, or their open source equivalent (Zeppelin, Presto, etc)
Data storage formats – Parquet, JSON, AVRO etc.
Experience with real-time data sources and message ingestion for processing by filtering, aggregating, and preparing the data for analysis using technologies such as Spark Streaming and Kafka, AWS Kinesis, Firehose etc.
Experience with data pipelining
Understanding of best practices within the development process
Build processes supporting data transformation, data structures, metadata, dependency and workload management
CI/CD and DevOps tools such as BitBucket/Git, Bamboo, and Maven
AWS technologies – Cloudwatch, CloudFormation, Security (IAM)
AWS certification
Job Requirements
","['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka']","['etl', 'big data']",999,"['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'etl', 'big data']","['stream', 'spark', 'aw', 'python', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'etl', 'big data', 'stream', 'spark', 'aw', 'python', 'big', 'etl', 'sourc', 'engin']"
DE,"This role is based in Pittsburgh or Phoenix.
Responsibilities
Partner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growth
Build custom integrations between cloud-based systems using APIs
Write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL
Architect, build, and launch new data models that provide intuitive analytics to the team
Build data expertise and own data quality for the pipelines you create
Requirements
Three or more years of relevant software engineering experience (Python, Scala and Java) in a data-focused role
Passion for creating data infrastructure technologies from scratch using the right tools for the job
A knack for writing, clean, readable, maintainable code
Comfort with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
Expertise in building out data pipelines, efficient ETL design, implementation, and maintenance
Experience with AWS tools
Proven track-record of solving complex data processing and storage challenges through scalable, fault-tolerant architecture
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'kafka']","['machine learning', 'clean', 'pipelin', 'etl']",999,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'kafka', 'machine learning', 'clean', 'pipelin', 'etl']","['analyt', 'machin', 'spark', 'challeng', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'scientist', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'kafka', 'machine learning', 'clean', 'pipelin', 'etl', 'analyt', 'machin', 'spark', 'challeng', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'scientist', 'etl', 'sourc', 'engin', 'integr']"
DE,"Job Summary:
The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills.
You will be using core cloud data warehouse tools, Python, spark, events streaming platforms and other data management related technologies.
You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.
Responsibilities:
· Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration.
Develop overall design and determine division of labor across various architectural components
Mentor client personnel.
Assist in development of task plans including schedule and effort estimation
Skills and Qualifications:
· Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
· Experience building high-performance, and scalable distributed systems
· Good experience in migration from Netezza/ DB2 to Snowflake.
· AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)
· Experience in ETL and ELT workflow management
· Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline
· Experience building internal cloud to cloud integrations is ideal
Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus
· 3+ years of Data Management Experience
· 3+ years of batch ETL tool experience (DataStage / Informatica / Talend)
· 3+ years’ experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
· 2+ years’ experience with Hadoop Ecosystem (HDFS/S3, Hive, Spark)
· 2+ years’ experience in a software engineering, leveraging Java, Python, Scala, etc.
· 2+ years’ advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns
· 2+ years’ experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases)
Experience in the financial services, banking and/ or Insurance industries is a nice to have
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
401(k)
Dental Insurance
Health Insurance
Paid Time Off
Vision Insurance
Additional Compensation:
Bonuses
Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19
","['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'lambda', 'kafka']","['graph', 'pipelin', 'optim', 'etl', 'commun']",1,"['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'lambda', 'kafka', 'graph', 'pipelin', 'optim', 'etl', 'commun']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'optim', 'aw', 'python', 'comput', 'packag', 'warehous', 'etl', 'engin', 'integr']","['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'nosql', 'hive', 'java', 'lambda', 'kafka', 'graph', 'pipelin', 'optim', 'etl', 'commun', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'optim', 'aw', 'python', 'comput', 'packag', 'warehous', 'etl', 'engin', 'integr']"
DE,"Publishers then have more time to do what they do best: create content.
Data Engineer Job Responsibilities:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Build dashboards that concisely and succinctly convey business metrics.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Data Engineer Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Education, Experience
Ample relevant knowledge and experience.
You either have a BS or MS degree in Computer Science or a related technical field, OR certification from a data science bootcamp + 2 years of experience in a role as a data engineer
Proficiency in Python and Java, Scala, or Go development experience
4+ years of SQL experience (Strong SQL required)
Familiarity with BI reporting tools like Tableau, Looker.
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience/knowledge of cloud computing platforms like AWS/GCP would be a plus
Excellent interpersonal and problem solving skills with the ability to communicate with team members to deliver actionable results
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
What you can expect in return:
Full-Time, Salaried Position
Medical, Dental, and Vision benefits
The opportunity to be part of something BIG
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
This role is not eligible for visa sponsorship
","['sql', 'gcp', 'looker', 'scala', 'bi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel']","['pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun']",1,"['sql', 'gcp', 'looker', 'scala', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel', 'pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun']","['analyt', 'digit', 'engin', 'pipelin', 'relat', 'bi', 'aw', 'avail', 'python', 'comput', 'action', 'big', 'integr', 'sourc', 'linear']","['sql', 'gcp', 'looker', 'scala', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'excel', 'pipelin', 'dashboard', 'data modeling', 'problem solving', 'commun', 'analyt', 'digit', 'engin', 'pipelin', 'relat', 'bi', 'aw', 'avail', 'python', 'comput', 'action', 'big', 'integr', 'sourc', 'linear']"
DE,"Senior Data Engineer
Locations: Phoenix AZ
Long term
Data model development and Model scoring
Work with Data Scientists and build scripts to meet their data needs
Required Qualifications
· 7+ years of overall experience
· 3+ years’ experience with Big Data ( HADOOP platforms) –Hive, Spark ( needs to be currently hands-on on Hadoop cluster)
· 5+ years of ETL (Extract, Transform, Load) - Scoop, INFA RDBMS Teradata, Oracle
· Experience in Python
· Reproduce issues faced by Data Scientists
· Knowledge of Agile is a must
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience:
HADOOP: 5 years (Required)
Big Data: 5 years (Required)
ETL: 8 years (Required)
Education:
Bachelor's (Preferred)
","['python', 'spark', 'oracl', 'hadoop']","['etl', 'cluster', 'big data']",1,"['python', 'spark', 'oracl', 'hadoop', 'etl', 'cluster', 'big data']","['spark', 'hadoop', 'python', 'scientist', 'big', 'etl', 'engin']","['python', 'spark', 'oracl', 'hadoop', 'etl', 'cluster', 'big data', 'spark', 'hadoop', 'python', 'scientist', 'big', 'etl', 'engin']"
DE,"Full Time Data Engineer Position
***Local Phoenix Candidates Only***
Create and maintain ETL pipelines
Build complex data sets to meet business requirements
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL technologies
","['sql', 'nosql']","['optim', 'etl', 'pipelin']",999,"['sql', 'nosql', 'optim', 'etl', 'pipelin']","['pipelin', 'set', 'infrastructur', 'optim', 'etl', 'sourc', 'engin']","['sql', 'nosql', 'optim', 'etl', 'pipelin', 'pipelin', 'set', 'infrastructur', 'optim', 'etl', 'sourc', 'engin']"
DE,"Do you consider yourself a superstar in Data Engineer?
Then you have reached the right job opportunity posting.
Role: Data Engineer
Duration: 9 Months with possibility of extension
Post COVID-19 crisis, work would need to be onsite at Fort Mill, SC or Tempe, AZ
Experience Requirements:
Proven expertise in MemSql, Mysql, and Oracle/PostgreSQL.
Prior experience with MemSQL is highly preferred.
Advanced experience in working with relational databases and ANSI-SQL
Demonstrated flexibility in working with large, complex, and ambiguous datasets
Proficient in one or more programming languages such as Python or Java
Good familiarity with AWS ecosystem including AWS EMR (preferred)
Familiar with one or more machine learning or statistical modeling tools such as R, scikit learn, and Spark MLlib (preferred)
Work successfully in a highly cross-functional environment
Strong analytical and quantitative problem-solving ability.
Experience on Data visualization tools such as Tableau and Power BI
Excellent communication, relationship skills, and a strong teammate.
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Schedule:
Monday to Friday
COVID-19 considerations:
Experience:
Memsql: 3 years (Preferred)
SQL: 5 years (Required)
Python: 3 years (Required)
Contract Length:
7 - 11 months
Contract Renewal:
Possible
Fully Remote
This Job Is Ideal for Someone Who Is:
Adaptable/flexible -- enjoys doing work that requires frequent shifts in direction
Detail-oriented -- would rather focus on the details of work than the bigger picture
Autonomous/Independent -- enjoys working with little direction
Innovative -- prefers working in unconventional ways or on tasks that require creativity
Innovative -- innovative and risk-taking
Outcome-oriented -- results-focused with strong performance culture
Team-oriented -- cooperative and collaborative
Benefit Conditions:
Work Remotely:
Temporarily due to COVID-19
","['sql', 'spark', 'mllib', 'bi', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'mysql', 'scikit', 'excel', 'oracl', 'power bi']","['visual', 'risk', 'machine learning', 'statist', 'commun']",999,"['sql', 'spark', 'mllib', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'scikit', 'excel', 'oracl', 'visual', 'risk', 'machine learning', 'statist', 'commun']","['analyt', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'power', 'bi', 'aw', 'python', 'statist', 'quantit', 'engin']","['sql', 'spark', 'mllib', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'scikit', 'excel', 'oracl', 'visual', 'risk', 'machine learning', 'statist', 'commun', 'analyt', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'power', 'bi', 'aw', 'python', 'statist', 'quantit', 'engin']"
DE,"All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
",[None],[None],999,[],[None],[None]
DE,"Overview
The Data Engineer will work on all phases of the development lifecycle.
This will be considered a leader position, with responsibilities that include design, development, implementation, documentation, and optimization of an assortment of solutions.
Candidates for this position will have prior experience delivering efficient, secure, high performance, and easy-to-support solutions primarily within the Microsoft suite of technologies.
They will also have prior experience in providing technical leadership and direction to junior team members.
Essential Functions
Design, document, develop, test, debug, deploy, and support a variety of custom SSIS packages, SQL databases, indexing, and performance tuning.
Provides technical leadership, creates standards for technical approaches and/or software development practices for highly performant ETL processes and database schema.
Maintains awareness of development trends, new tools, efficiencies - - shares with entire team on routine basis.
Proactive solution detection, analysis, and guidance
Gather, analyze, and document user reporting and/or data feed requirements
Code reviews
Provide escalated support for junior team members
Perform any other assigned tasks deemed necessary by management
SM123
Knowledge, Skills, Abilities and Physical Requirements
Bachelor’s Degree in Computer Science or related field preferred
6+ years of experience with Microsoft technologies, application design, development, and administration.
3+ years of experience with cloud-based solutions and/or hybrid connectivity
Strong understanding of secure application development with the ability to perform application security reviews
Prior experience with data visualization tools, dashboard and web based reporting tools
Strong requirements gathering and documentation skills
Strong analytical and problem solving skills
Retail environment experience is preferred but not required
Technical Skills:
Databases: MS SQL Server
Languages/Libraries: T-SQL, Powershell
Operating Systems: Windows 2012 R2, Windows 10, Mac OS X
Tools: Visual Studio, VSCode, GitHub, SQL Server Management Studio, SQL Server Reporting Services, Business Intelligence Design Studio, Team Foundation Server
Other: SharePoint, Microsoft Office, Visio
Personality Traits:
Maintain high levels of integrity and dependability
Maintain a focus on results, quality and customer satisfaction
Works well in a team environment and effectively manage work activities
Project a professional demeanor and appearance
Be extremely flexible and adaptable
Demonstrates the ability to function and stay focused in a constant pressure, fast growing and ever-changing organization
Communication Skills:
Ability to competently understand, speak, read and write English.
Ability to effectively present information and respond to questions from groups of managers, project steering committees and customers.
Requires excellent interpersonal communications skills.
SM123
Benefits
These programs include:
Competitive pay
Opportunities for career growth
Flexible schedules
Employee Assistance Program (EAP)
Affordable benefit coverage, including medical, dental vision
Pre-tax Flexible Spending Accounts for healthcare and dependent care
The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work.
This is not an exhaustive list of all duties, responsibilities, and requirements.
","['sql', 'cloud', 'microsoft', 'github', 'excel']","['healthcar', 'visual', 'optim', 'problem solving', 'analyz', 'etl', 'commun', 'account']",1,"['sql', 'cloud', 'microsoft', 'github', 'excel', 'healthcar', 'visual', 'optim', 'problem solving', 'analyz', 'etl', 'commun', 'account']","['basi', 'analyt', 'essenti', 'program', 'relat', 'visual', 'provid', 'optim', 'employe', 'comput', 'packag', 'etl', 'engin', 'integr']","['sql', 'cloud', 'microsoft', 'github', 'excel', 'healthcar', 'visual', 'optim', 'problem solving', 'analyz', 'etl', 'commun', 'account', 'basi', 'analyt', 'essenti', 'program', 'relat', 'visual', 'provid', 'optim', 'employe', 'comput', 'packag', 'etl', 'engin', 'integr']"
DE,"Big Data Engineer
9321
Phoenix, AZ
10/9/2018
Application Development
Contractor - W2
You won’t just shape the world of software.
You’ll shape the world of life, work and play.
This position requires a mix of strategic engineering and design along with hands-on, technical work.
Exceptional communication skills, for collaborating across many teams.
You will lead teams and deliver best-in-class products in an exciting fast-paced environment with the focus on reliability and automation.
Dynamic, smart people and inspiring, innovative technologies are the norm here.
The successful candidate will be highly self-motivated with a passion for quality and automation coupled with an ability to understand complex systems and a desire to constantly make things better.
You won’t just keep up, you’ll break new ground.
There are hundreds of opportunities to make your mark on technology and life at American Express.
Here’s just some of what you’ll be doing:
Building software and systems to manage/support applications through automation, support and monitoring
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
Are you up for the challenge?
Overall 5+yrs experience in a large scale *nix environment
5+ yrs experience handling BigData Environment
Software development experience with on or more of: Python, Java or Scala
Experience with relational databases such as PostgreSQL, MySQL
Experience with core Hadoop: HDFS, MapReduce, Yarn
Experience with Streaming Data Platform (Kafka, Storm)
Experience with distributed/NoSQL databases: HBase, MySQL
Solid understanding of *nix systems and networking fundamentals
Experience as a software developer with MapReduce or Spark
Experience with extended Hadoop ecosystem: Hive, Pig
Solid Scripting Skills in languages like Python, Shell
Oncall Experience
Excellent communication skills, written and spoken; troubleshooting skills
Evidence of self-learning and commitment to personal development
Bachelor's degree or higher in Computer Science, or equivalent experience\
Continues integration, testing and deployment using Git, Jenkins
At the core of Software Engineering
Agile Practices
Porting/Software Configuration
Programming Languages and Frameworks
Business Analysis
Analytical Thinking
Business Product Knowledge
Job Requirements
","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'mysql', 'hbase', 'kafka', 'excel']","['optim', 'commun', 'big data']",1,"['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data']","['analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data', 'analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Essential Duties and Responsibilities:
Develops high performance distributed data warehouse, distributed analytic systems and cloud architectures.
Participates in the development of relational and non-relational data models focusing on design for optimal storage and retrieval
Develops, tests and debugs batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources; provides recommendations to improve data reliability, efficiency and quality.
Design and develop scalable solutions leveraging using technologies including Docker and Kubernetes
Working along side data scientists, supports the development and promotion of high-performance algorithms, models and prototypes.
Implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information.
Ensures that Data Warehousing and Big Data systems meet business requirements and industry practices; including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring requirements.
Troubleshoots data and performance related issues; implements adjustments, documents root cause and corrective measure; provides recommendations to stakeholders.
Documents technical specifications and participates with peers in design and code review sessions.
Employ a variety of scripting languages and tools to integrate data from multiple disparate data sources.
Use basic statistical and visualization techniques to analyze the resulting data sets of your processes
Learn and stay abreast of new technologies that can improve the efficacy of the analytics and data science teams
Stays current on the latest industry technologies, trends and strategies.
Other duties as assigned.
Job Requirements
Qualifications:
This position requires a minimum of five years of progressive database development and integration experience.
Strong knowledge of logical and physical data modeling is necessary, including but not limited to entity design, relationships, indexing and star schemas.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
SQL experience is required.
Scripting knowledge with SQL and Python.
Experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
Proven analytical problem solving and decision making skills is critical.
Ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task, and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Work Days:
Normal work days are Monday through Friday.
Occasional Saturdays and Sundays may be necessary.
Work Hours:
Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.
Not ready to
","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker']","['recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']",1,"['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']","['day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl', 'day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Candidates should have strong familiarity working in an AWS cloud environment, as well as, working with other data engineers, product managers, and product delivery teams when required.
Qualifications Experience Candidates with 6+ yearsrsquo experience in data engineering, who have either obtained a Graduate degree in the field of Computer Science or related field, or Bachelor's degree with 8+ years of relevant experience in the above fields.
Key Responsibilities Provide technical solution leadership in data engineering team, driving technology decisions, mentoring others, and contributing significantly on an individual level Build frameworks to handle data at high scale using Apache Spark and data cataloging tools like Apache Hive, AWS Glue on top of a multi-tiered data lake storage Use exploration and analytic tools like AWS AthenaPresto to probe and validate data Build robust data processing pipelines using AWS Services and integrate with multiple data sources Collaborate with product owners and stakeholders to plan and define requirements Desired Skills AWS Services RDS, AWS Lambda, AWS Glue, Apache Spark, Kafka, Hive, etc.
SQL and NoSQL databases like MySQL, Postgres, and Elasticsearch AWS EMR Familiarity with Spark programming paradigms (batch and stream-processing) Strong programming skills in at least one of the following languages Java, Scala.
Familiarity with a scripting language like Python, as well as, UnixLinux shells AWS Athena Strong analytical skills and advanced SQL knowledge, indexing, query optimization techniques.
","['sql', 'spark', 'elasticsearch', 'scala', 'kafka', 'aw', 'lambda', 'python', 'hive', 'java', 'nosql', 'postgr', 'mysql', 'cloud', 'unixlinux']","['optim', 'pipelin']",1,"['sql', 'spark', 'elasticsearch', 'scala', 'kafka', 'aw', 'lambda', 'python', 'hive', 'java', 'nosql', 'cloud', 'unixlinux', 'optim', 'pipelin']","['analyt', 'program', 'techniqu', 'stream', 'spark', 'pipelin', 'provid', 'optim', 'python', 'aw', 'comput', 'relat', 'sourc', 'engin']","['sql', 'spark', 'elasticsearch', 'scala', 'kafka', 'aw', 'lambda', 'python', 'hive', 'java', 'nosql', 'cloud', 'unixlinux', 'optim', 'pipelin', 'analyt', 'program', 'techniqu', 'stream', 'spark', 'pipelin', 'provid', 'optim', 'python', 'aw', 'comput', 'relat', 'sourc', 'engin']"
DE,"Data Engineer– Emphasis on Elasticsearch / Kafka / JSON / XML
The Opportunity:
You'll be developing and deploying tools for the processing and import/export of data into and out of large scale Elasticsearch and Kafka environments.
The Day to Day:
Work to customer requirements for the import and export of data into various formats
Develop tools to automate this processing on a regular basis
Build back-end frameworks that are maintainable, flexible and scaleable
Requirements:
2-3 years of programming experience in Javascript (Node.js), Python, Ruby or Go
Experience working with any of Elasticsearch, Kafka, Hadoop (HDFS, Hive, Spark), MongoDB, MySQL or PostgreSQL
Strong preference for Elasticsearch experience
Experience working with data in JSON, XML and CSV data formats
Comfort doing development work on the Linux platform
Exposure to compute clusters and working with many terabytes of data
Bonus Points:
Operational experience with Hadoop, MongoDB, Redis, Cassandra, or other distributed big data systems
Mac OS X familiarity
BS or MS in a technology or scientific field of study
High energy level and pleasant, positive attitude!
Evidence of working well within a diverse team
Compensation:
Salary commensurate with experience, generally higher than competitive industries
Comprehensive benefits package
Opportunities for advancement and a clear career path
Powered by JazzHR
PzsSBHiFSF
","['mongodb', 'linux', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'javascript', 'python', 'hive', 'postgresql', 'mysql', 'kafka', 'rubi']","['cluster', 'big data']",1,"['mongodb', 'linux', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'javascript', 'python', 'hive', 'postgresql', 'sql', 'kafka', 'rubi', 'cluster', 'big data']","['day', 'basi', 'spark', 'divers', 'power', 'hadoop', 'python', 'packag', 'big']","['mongodb', 'linux', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'javascript', 'python', 'hive', 'postgresql', 'sql', 'kafka', 'rubi', 'cluster', 'big data', 'day', 'basi', 'spark', 'divers', 'power', 'hadoop', 'python', 'packag', 'big']"
DE,"Data Engineer ||Phoenix,AZ or San Antonio,TX, or Plano, TX
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
",[None],[None],999,[],"['basi', 'engin', 'evalu']","['basi', 'engin', 'evalu']"
DE,"A leading and a big data analytics software products and services organization in Deer Valley, who specializing in helping fortune 100 companies in their digital transformation journey is seeking to add to a Data Engineer their talented team of developers.
This is a great opportunity for whoever wants to work on the cutting- edge technologies in the global IT industry today.
This is a 12-month contracting position ending in July 2021, paying up to $55.00 per hour with benefits.In this position, you will join a team that will be enhancing and modernizing legacy technology.
This team develops Big Data and batch/real-time analytical solutions, creating insights about different Members across the full spectrum of digital channels, including search, mobile, email, social, and web.
If you have the desire to tell a story with data, and the talent to seamlessly integrate customer information across physical, digital, mobile, and social media, this is the team for you!Required Skills & Experience* 4+ years of IT experience* Very good experience in Hadoop, Hive, Spark Batch and streaming.
* Good to have experience with 1 NoSQL - HBase/ Cassandra.
","['spark', 'cassandra', 'hadoop', 'hive', 'hbase', 'nosql']",['big data'],999,"['spark', 'cassandra', 'hadoop', 'hive', 'hbase', 'nosql', 'big data']","['analyt', 'digit', 'spark', 'hadoop', 'big', 'engin']","['spark', 'cassandra', 'hadoop', 'hive', 'hbase', 'nosql', 'big data', 'analyt', 'digit', 'spark', 'hadoop', 'big', 'engin']"
DE,"* Collaborate with Data Science and Business Intelligence teams to identify, design, develop, and implement data applications such as truck arrival intelligence, network balance recommendations, service failure mitigation, and driver hours optimization.
* Extensive elastic search development.
* Provide troubleshooting, coding, and data pipeline expertise to Data Science and Business Intelligence teams.
* Identify and implement outside data sources and new technologies to enhance analysis and reporting impact on business problems.
* Collaborate with Data Warehouse/ ETL team to transition data sets into production/core environment and implement new data technologies for analysis and reporting use.
* Proactively work to assist others in achieving the organization's objectives.
* Skills:Must possess excellent interpersonal skills.Must be able to collaborate with others on team and across the organization.Must be able to present recommendations and/or findings to others including senior leadership.
* Education: Bachelors in computer science or related field or equivalent combination of education and experience required.
* Experience Required: 3+years related hands on experience required.
Previous experience with Java and SQL required.
Proven problem solver, creative thinker capabilities required.
Experience with Alteryx, Elastic Search, Hive/Impala, Scala, Spark, Python, HTML, Groovy preferred.
","['sql', 'spark', 'scala', 'python', 'hive', 'java', 'excel']","['recommend', 'etl', 'pipelin']",1,"['sql', 'spark', 'scala', 'python', 'hive', 'java', 'excel', 'recommend', 'etl', 'pipelin']","['spark', 'pipelin', 'set', 'relat', 'provid', 'python', 'comput', 'warehous', 'etl', 'sourc']","['sql', 'spark', 'scala', 'python', 'hive', 'java', 'excel', 'recommend', 'etl', 'pipelin', 'spark', 'pipelin', 'set', 'relat', 'provid', 'python', 'comput', 'warehous', 'etl', 'sourc']"
DE,"Title: Data Engineer
Duration: Contract
The data engineer will be primarily attached to the data management team and provide much needed support to migrate data (and reports) from an existing legacy system to a new enterprise application.
In addition, this resource will be deeply involved in the designing, development and documentation of a new enterprise data warehouse.
Roles & Responsibilities:
Reverse engineering of existing reports to retrieve and document technical specifications (data source, column definitions, etc.).
Data migration and mapping from legacy systems to a new transactional system.
Building of a new data warehouse.
Exposure to cross-functional teams to facilitate solutions to meet business needs.
Assisting in the preparation of strategic analysis and findings for department leadership and key stakeholders.
Developing metrics and creating ad-hoc reporting as needed.
Assist internal customers with acceptance and adoption of implemented solutions.
Adhering to best practices for problem analysis, concept evaluation, systems design, database development, modification, testing, and evaluation to ensure quality and consistency.
Conducting training to various audiences for data related issues and technical applications as needed.
Recommending solution options for data and reporting needs, and provide technical advice where needed to support companywide objectives and goals and resolve problems.
Interact with the business to understand change in processes, data implications and potential reporting modifications.
Documenting the types and structure of the business data (logical modeling).
Analyzing and mining business data to identify correlations among the various data points.
Mapping and tracing data from source to target systems in order to solve a given business or system problem.
Designing and creating data reports and dashboards to help the business in their decision making.
Perform statistical analysis of business data.
Ability to perform root cause analysis.
Required Skills:
SQL Development
Microsoft SQL Server, Microsoft Access, Microsoft Excel
SQL Server Integration Services (SSIS)
SQL Server Reporting Services (SSRS), Tableau
Preferred Skills:
OLTP and OLAP modelling and design
Data warehousing concepts
Power BI
C#, PowerShell, DAX, MDX
","['sql', 'ssr', 'bi', 'tableau', 'c', 'microsoft', 'excel', 'power bi']","['dashboard', 'correl', 'data warehousing', 'statist', 'analyz']",999,"['sql', 'ssr', 'powerbi', 'tableau', 'c', 'microsoft', 'excel', 'dashboard', 'correl', 'data warehousing', 'statist', 'analyz']","['relat', 'power', 'bi', 'statist', 'warehous', 'integr', 'sourc', 'engin', 'evalu']","['sql', 'ssr', 'powerbi', 'tableau', 'c', 'microsoft', 'excel', 'dashboard', 'correl', 'data warehousing', 'statist', 'analyz', 'relat', 'power', 'bi', 'statist', 'warehous', 'integr', 'sourc', 'engin', 'evalu']"
DE,"The successful candidate will design, implement, and maintain data storage and data flow solutions for structured and non-structured multi-model data in support of data science and machine learning pipelines.
Additionally, the ideal candidate will be an experienced data pipeline builder and data wrangler, who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Job Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create data tools for analytics and data science team members that assist them in building and optimizing data science products.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Job Requirements:
Masters in Computer Science, Engineering or a related field (Specifically, with exposure to cancer biology studies/data/research/etc.
being highly desired)
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience with relational SQL and NoSQL databases, including MongoDB, Cassandra, etc.
Strong analytic skills related to working with unstructured datasets
Experience with microservices architecture
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Proficiency in Python, Pandas, PySpark, Dask, Ray, etc.
Experience writing RESTful APIs
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team
Demonstrate a focus on listening to and understanding user needs and then delighting the customer by exceeding service and quality expectations
","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'kafka']","['research', 'pipelin', 'machine learning', 'optim', 'big data']",2,"['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'kafka', 'research', 'pipelin', 'machine learning', 'optim', 'big data']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'big', 'set', 'engin']","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'kafka', 'research', 'pipelin', 'machine learning', 'optim', 'big data', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'big', 'set', 'engin']"
DE,"• Experience building data pipelines to automate batch and real-time data delivery to the AWS data lake, warehouses, analytical and machine learning applications
• Experience integrating and shipping code into AWS cloud Production environments
• Strong experience with all phases of the software development life cycle (SDLC) using Agile methods
• Strong experience with using Amazon Cloud services
• Experience with programming languages: including Java, Node.js, Python
Job Requirements:
","['aw', 'java', 'python', 'cloud']","['machine learning', 'pipelin']",999,"['aw', 'java', 'python', 'cloud', 'machine learning', 'pipelin']","['analyt', 'machin', 'learn', 'pipelin', 'aw', 'python', 'integr']","['aw', 'java', 'python', 'cloud', 'machine learning', 'pipelin', 'analyt', 'machin', 'learn', 'pipelin', 'aw', 'python', 'integr']"
DE,"Make Next Happen Now.
This includes providing data integration services for all batch, near real-time, real-time and streaming data movement; managing and enhancing the Data Lake, Data Warehouse and dependent data marts; and providing support for analytics and business intelligence consumers.
Do you get excited when you see data?
Constantly looking for value in Data?
You will get an opportunity to closely work with internal vertical and business teams, understand the core functionality of banking applications and associated data.
You will build data pipelines, tools, and reports that enable analysts, product managers, and business executives.
Key Responsibilities:
Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs to calculate business metrics
Provision data for regulatory reporting needs
Design data schema and operate internal data warehouses and SQL/NoSQL database systems
Monitor and troubleshoot operational or data issues in the data pipelines
Drive architectural plans and implementation for future data storage, reporting, and analytic solutions
Bachelor's degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience
5 years of relevant work experience in analytics, data engineering, business intelligence or related field, and 5 years professional experience
2 years of experience in cloud technology: Amazon Web Services
2 years of experience in implementing big data processing technology: Glue, Kafka, Apache Spark, Python etc.
2 years of experience in implementing ETL technologies: Informatica, BODS, SSIS etc.
Experience using SQL queries, experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasets
Detailed knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environments
Preferred Qualifications
Graduate degree in Computer Science, Mathematics, Statistics, Finance, related technical field
Strong ability to effectively communicate with both business and technical teams
Demonstrated experience delivering actionable insights for a consumer business
Coding proficiency in at least one modern programming language (Ruby, Java, etc.)
Basic Experience with Cloud technologies
Experience in banking domain is a plus
","['sql', 'spark', 'amazon web services', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka', 'rubi']","['pipelin', 'financ', 'statist', 'big data', 'etl']",1,"['sql', 'spark', 'aw', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka', 'rubi', 'pipelin', 'financ', 'statist', 'big data', 'etl']","['analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'python', 'warehous', 'statist', 'comput', 'action', 'big', 'etl', 'engin', 'integr']","['sql', 'spark', 'aw', 'hadoop', 'cloud', 'python', 'java', 'nosql', 'kafka', 'rubi', 'pipelin', 'financ', 'statist', 'big data', 'etl', 'analyt', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'python', 'warehous', 'statist', 'comput', 'action', 'big', 'etl', 'engin', 'integr']"
DE,"RESPONSIBILITIES:
Summary:
Key Tasks:
Design, build and optimize data applications and data pipelines to extract, transform and load data from various data sources to internal and cloud targets
Participate in proactively identifying issues and resolving issues concerning data pipeline jobs
Identify and implement process improvements: automate data processing, build frameworks, design and implement near real-time data ingestion capabilities
Evaluate tools and emerging technologies and carry out POCs to identify tools that would optimize data processing and data pipelines
Design and implement data sharing tools and solutions including batch sharing and API capabilities
Design and implement data analytic tools and solutions that provides actionable insights to stakeholders
Work with product, application and business teams to assist with ad hoc data requests, data exploration requests and building business knowledge models
Work with data science teams to assist with assist with data and data processing requests
Contribute by the way of your past experiences and industry knowledge to the Data Engineering function that is driving significant changes to the tool stack to drive critical initiatives for the organization
REQUIREMENTS:
A minimum of 5 years of software development experience, covering the stages of the SDLC
A minimum of 3 years of experience working with data technologies and relational (oracle, MySQL, SQL server, PostgreSQL) as well as NoSQL databases
3 years of experience developing solutions on Java/J2EE platform
3 years of experience developing solutions with Amazon AWS data tools and technologies
2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting
2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting
2+ years of experience working with Hadoop and spark eco system
Advanced working knowledge of current and emerging data technologies
Vast knowledge of open source tools & technologies
Experience working with agile software development methodologies
Experience with spring framework using spring boot specifically is preferable
Experience working on cloud migration projects is preferable
Experience in developing and consuming REST based web services and working in and integrating with microservices based environment is preferable
Experience working with event driven architectures and distributed messaging broker such as Kafka
Experience working with one or more data streaming technologies is preferable
Experience working with Mongo DB or Elasticsearch and/or Redis technologies is preferable
Experience working with Informatica, HVR is preferable
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
","['sql', 'db', 'elasticsearch', 'hadoop', 'javascript', 'aw', 'python', 'java', 'r', 'nosql', 'postgresql', 'mysql', 'cloud', 'kafka', 'oracl']",['pipelin'],999,"['sql', 'db', 'elasticsearch', 'hadoop', 'javascript', 'aw', 'python', 'java', 'r', 'nosql', 'postgresql', 'cloud', 'kafka', 'oracl', 'pipelin']","['analyt', 'pipelin', 'hadoop', 'aw', 'python', 'action', 'relat', 'sourc', 'engin', 'particip', 'evalu']","['sql', 'db', 'elasticsearch', 'hadoop', 'javascript', 'aw', 'python', 'java', 'r', 'nosql', 'postgresql', 'cloud', 'kafka', 'oracl', 'pipelin', 'analyt', 'pipelin', 'hadoop', 'aw', 'python', 'action', 'relat', 'sourc', 'engin', 'particip', 'evalu']"
DE,"Position: Jr. Data Engineer
Type: Contract
Skills:
Strong communication skills.
3+ years' experience building data pipelines.
3+ years' experience programming in Python
Knowledge in fine tuning SQL, understanding optimizers, and execution plans.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.
Understanding of software development life cycle and release management
Past experience integrating with Oracle, Microsoft SQL Server
Self-motivated, independent, team-player
Equal Employment Opportunity Commission
The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.
Inception in 2007, privately held, Debt free
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
Awards and Accolades:
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 Fastest 50 by NJBiz
2018 Techserve Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1
","['sql', 'spark', 'python', 'microsoft', 'kafka', 'excel', 'oracl']","['optim', 'commun', 'pipelin', 'account']",999,"['sql', 'spark', 'python', 'microsoft', 'kafka', 'excel', 'oracl', 'optim', 'commun', 'pipelin', 'account']","['basi', 'spark', 'pipelin', 'infrastructur', 'optim', 'python', 'releas', 'employe', 'sourc', 'engin']","['sql', 'spark', 'python', 'microsoft', 'kafka', 'excel', 'oracl', 'optim', 'commun', 'pipelin', 'account', 'basi', 'spark', 'pipelin', 'infrastructur', 'optim', 'python', 'releas', 'employe', 'sourc', 'engin']"
DE,"About the Role
Responsibilities
Design, implement and maintain data warehouse/ business intelligence solutions which will handle growing business needs
Translate business questions and concerns into specific quantitative questions that can be answered with available data using sound methodologies.
In cases where questions cannot be answered with available data, partner with engineers to produce the required data
Automate, optimize, and maintain ETL flows in a repeatable, scalable manner
Handle and respond to ad-hoc data support/analytical requests
Explore/analyze data and work with Supple Chain Operations, Data Scientists and Product Managers
Requirements
5+ years of experience in data modeling, ETL, data warehousing, and transformation of large scale data sources using SQL, Redshift or other Big Data technologies
Proficiency with analytical SQL and knowledge of how to optimize and troubleshoot
BI Reporting experience with tools such as Looker or Tableau
Bachelor's or Master’s degree in computer science, information systems, or a related technical discipline
Experience with AWS Data technologies (such as Redshift, S3,Glue)
Familiarity with Python or any other scripting language
Excellent communication and problem solving skills
(Bonus) Experience with Spark, Hadoop or other Big Data technologies.
(Bonus) Experience working in an e-commerce business
(Bonus) Experience working with Supply Chain data
And in return, you'll have...
Autonomy.
The ability to make, own, and carry out decisions
Competitive salary, equity and full benefits (health/dental/vision insurance & 401k)
Flexible PTO
A unique perspective is critical to solving complex problems and inspiring a new generation to think secondhand first.
Everyone is welcome - be you.
","['sql', 'spark', 'looker', 'hadoop', 'bi', 'aw', 'redshift', 's3', 'python', 'tableau', 'excel']","['big data', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'analyz', 'etl', 'commun']",1,"['sql', 'spark', 'looker', 'hadoop', 'powerbi', 'aw', 'redshift', 's3', 'python', 'tableau', 'excel', 'big data', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'analyz', 'etl', 'commun']","['analyt', 'spark', 'handl', 'relat', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'aw', 'warehous', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']","['sql', 'spark', 'looker', 'hadoop', 'powerbi', 'aw', 'redshift', 's3', 'python', 'tableau', 'excel', 'big data', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'analyz', 'etl', 'commun', 'analyt', 'spark', 'handl', 'relat', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'aw', 'warehous', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']"
DE,"Position: Data Engineer
Duration: Long Term
• The ideal candidate must have a strong J2ee background (UI DEVELOPMENT USING J2EE) and currently performing a Hadoop Data Engineer role.
• Data model development and Model scoring
• Work with Data Scientists and build scripts to meet their data needs
Required Qualifications
• 10+ years of overall experience
• 3+ years' experience with Big Data ( HADOOP platforms) –Hive, Spark ( needs to be currently hands-on on Hadoop cluster)
• 4+ years of overall experience in UI development using J2ee
","['spark', 'hadoop']","['cluster', 'big data']",999,"['spark', 'hadoop', 'cluster', 'big data']","['spark', 'hadoop', 'scientist', 'big', 'engin']","['spark', 'hadoop', 'cluster', 'big data', 'spark', 'hadoop', 'scientist', 'big', 'engin']"
DE,"Data Engineer
Scottsdale, AZ, USA
USC, GC only
6+ months
MINIMUM QUALIFICATIONS: •
• AWS: 1 year experience
• DevOps Practices: 1 year experience
• 2+ years experience working with data warehousing, ETL development and ETL architecture.
• 2+ years experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
• 2 years experience working on large data initiatives (?5 terabytes).
• 1 years experience as a JavaScript
","['aw', 'javascript', 'snowflak', 'nosql', 'mysql', 'oracl']","['data warehousing', 'etl']",999,"['aw', 'javascript', 'snowflak', 'nosql', 'sql', 'oracl', 'data warehousing', 'etl']","['aw', 'etl', 'engin']","['aw', 'javascript', 'snowflak', 'nosql', 'sql', 'oracl', 'data warehousing', 'etl', 'aw', 'etl', 'engin']"
DE,"• Build, deploy and manage data engineering pipelines.
• Contribute to design and creation of high-quality solutions.
• Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems.
• Work with languages such as python and SQL.
• Handle batch and real-time data processing utilizing different tools and technologies.
Qualifications & Requirements:
• Degree in computer science or related.
• 3+ years of relevant professional experience.
• Extensive experience in Python and SQL.
• Experience with batch and real-time data processing tools and technologies (Databricks, Spark, Kafka)
• Knowledge of distributed data solutions, storage systems and columnar databases.
• Knowledge of Cloud Computing on Microsoft Azure or any other public cloud offering
• Familiarity with Continuous Integration/Continuous Deployment, Git.
• Knowledge about Agile development methods like Scrum and Kanban.
• Knowledge of key machine learning concepts & ML frameworks (like scikit-learn, H2O.ai, Keras, etc.)
is a plus.
Contact: sdonovan@judge.com
Job Requirements:
","['sql', 'kera', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'scikit', 'kafka']","['machine learning', 'pipelin']",1,"['sql', 'kera', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'scikit', 'kafka', 'machine learning', 'pipelin']","['machin', 'learn', 'spark', 'pipelin', 'azur', 'handl', 'ml', 'relat', 'public', 'python', 'comput', 'integr', 'engin']","['sql', 'kera', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'scikit', 'kafka', 'machine learning', 'pipelin', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'handl', 'ml', 'relat', 'public', 'python', 'comput', 'integr', 'engin']"
DE,"Role: Data Engineer
Contract : 12+ months
Mandatory Skills: AWS Lambda, Hive, Spark and Python
Role & Responsibilities:
Equal Employment Opportunity Commission
The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.
Inception in 2007, privately held, Debt free
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
Awards and Accolades:
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 Fastest 50 by NJBiz
2018 Techserve Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1
Stay safe and healthy!
Regards,
Divyansh Srivastava (Dave)
Client Delivery Manager Enterprise Business
Global HQ Address 7250 Dallas Pkwy, Suite 825 Plano, Texas 75024
Office: (201) 340-8700 x 477|Cell: (201) 479-3334|Fax: (201) 221-8131|Email: divyansh@net2source.com
https://www.linkedin.com/in/divyansh-srivastava-11563041/
Web: www.net2source.com | Social: Facebook | Twitter | LinkedIn
","['spark', 'aw', 'lambda', 'python', 'hive', 'excel']",['account'],999,"['spark', 'aw', 'lambda', 'python', 'hive', 'excel', 'account']","['basi', 'spark', 'aw', 'python', 'texa', 'employe', 'engin']","['spark', 'aw', 'lambda', 'python', 'hive', 'excel', 'account', 'basi', 'spark', 'aw', 'python', 'texa', 'employe', 'engin']"
DE,"Responsible for the design, build and implementation of cloud-based analytics platform which includes an MPP Enterprise Data Warehouse and other Big Data technologies.
Essential Duties and Responsibilities:
Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture
Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources
Performs complex data calculations through data integration tools and scripting languages
Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information
Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers
Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team
Documents technical specifications and participates with peers in design and code review sessions
Develops complex cross application architectures in collaboration with cross functional teams
Stays current on the latest industry technologies, trends and strategies
Completes work in a timely and accurate manner while providing exceptional customer service
Other duties as assigned
Job Requirements
Qualifications:
This position requires a minimum of eight years of progressive database development and integration experience.
Proven understanding of logical and physical data modeling is imperative.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
Expert level SQL experience is required.
Scripting knowledge with SQL, Python, Java or R is necessary.
Proven experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Proven experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
Proven analytical problem solving and decision-making skills is critical.
Demonstrated ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Work Days:
Normal work days are Monday through Friday.
Occasional Saturdays and Sundays may be necessary.
Work Hours:
Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.
Not ready to
","['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel']","['pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']",1,"['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel', 'pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl']","['day', 'analyt', 'essenti', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin', 'particip', 'integr']","['sql', 'mapreduc', 'microsoft', 'aw', 'cloud', 'redshift', 'python', 's3', 'nosql', 'java', 'r', 'excel', 'pipelin', 'normal', 'data modeling', 'data warehousing', 'problem solving', 'big data', 'etl', 'day', 'analyt', 'essenti', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'warehous', 'comput', 'big', 'etl', 'sourc', 'engin', 'particip', 'integr']"
DE,"Purpose of Job
Data Engineers deliver quality reporting and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions.
Data Engineers (DEs) are engaged in all phases of the data management lifecycle; gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data.
Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored and controlled.
Job Requirements
This singular mission requires a dedication to innovative thinking at every level.
https://www.youtube.com/watch?v=kVCnnaJUH_c
Data Engineer A Realistic Preview
Identifies and manages existing and emerging risks that stem from business activities and the job role.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled.
Follows written risk and compliance policies and procedures for business activities.
Design and implement technical solutions.
Identify and solve significant technical problems and architecture deficiencies.
Participate in daily standups and design reviews.
Breakdown business features and into technical stories and approaches.
Analyze data and enable machine learning.
Create proof of concepts and prototypes.\
Help on-board entry level engineers.
Collaborate with the team and other engineers to plan and execute assignments and tasks.
May begin mentoring junior engineers.
Minimum Experience:
Bachelor's degree in related field of study,
OR
Certification from an approved technical field of study,
OR
4 additional years of related experience beyond the minimum required.
4 years of of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)
*Qualifications may warrant placement in a different job level*
This will take approximately 5 minutes.
Once you begin the questions you will not be able to finish them at a later time and you will not be able to change your responses.
Preferred Experience:
IBM DataStage Experience
4+ years of Data Warehousing experience
Unix bash Experience
Python Experience
Java Experience
Hadoop Experience
Control-m Experience
For Internal Candidates:
Must complete 12 months in current position (from date of hire or date of placement), or must have managers approval prior to posting.
","['java', 'python', 'unix', 'hadoop']","['machine learning', 'data warehousing', 'risk', 'analyz']",1,"['java', 'python', 'unix', 'hadoop', 'machine learning', 'data warehousing', 'risk', 'analyz']","['machin', 'collect', 'hadoop', 'python', 'relat', 'engin', 'particip']","['java', 'python', 'unix', 'hadoop', 'machine learning', 'data warehousing', 'risk', 'analyz', 'machin', 'collect', 'hadoop', 'python', 'relat', 'engin', 'particip']"
DE,"Essential Duties and Responsibilities:
• Develops high performance distributed data warehouse, distributed analytic systems and cloud architectures.
• Participates in the development of relational and non-relational data models focusing on design for optimal storage and retrieval
• Develops, tests and debugs batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources; provides recommendations to improve data reliability, efficiency and quality.
• Design and develop scalable solutions leveraging using technologies including Docker and Kubernetes
• Working along side data scientists, supports the development and promotion of high-performance algorithms, models and prototypes.
• Implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information.
• Ensures that Data Warehousing and Big Data systems meet business requirements and industry practices; including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring requirements.
• Troubleshoots data and performance related issues; implements adjustments, documents root cause and corrective measure; provides recommendations to stakeholders.
• Documents technical specifications and participates with peers in design and code review sessions.
• Employ a variety of scripting languages and tools to integrate data from multiple disparate data sources.
• Use basic statistical and visualization techniques to analyze the resulting data sets of your processes
• Learn and stay abreast of new technologies that can improve the efficacy of the analytics and data science teams
• Stays current on the latest industry technologies, trends and strategies.
• Other duties as assigned.
Qualifications:
• This position requires a minimum of five years of progressive database development and integration experience.
• Strong knowledge of logical and physical data modeling is necessary, including but not limited to entity design, relationships, indexing and star schemas.
• Ability to translate a logical data model into a relational or non-relational solution is necessary.
• Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
• SQL experience is required.
• Scripting knowledge with SQL and Python.
• Experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
• Experience with data integrations and data processing for business intelligence and analytics workloads is required.
• Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
• Hands-on experience in database development using views, SQL scripts and transformations is needed.
• Proficient with Microsoft office, including skills with Word and Excel, is necessary.
• Experience working with large complex data sets.
• Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
• Proven analytical problem solving and decision making skills is critical.
• Ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
• Ability to work independently and in a team is vital.
• Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
• The ability to multi-task, and manage multiple projects to meet various deadlines simultaneously is required.
• The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
• In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Work Days:
Normal work days are Monday through Friday.
Occasional Saturdays and Sundays may be necessary.
Work Hours:
Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.
Job Requirements:
","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker']","['recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']",1,"['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl']","['day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'mapreduc', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'microsoft', 'kubernet', 'excel', 'docker', 'recommend', 'pipelin', 'normal', 'visual', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'etl', 'day', 'techniqu', 'visual', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'essenti', 'optim', 'statist', 'learn', 'aw', 'warehous', 'set', 'big', 'particip', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"You will also perform complex data analysis and present the harvested information.
* Troubleshoots data load failures or reporting inconsistencies to ensure that data presented is reliable, correct, highly available, and straightforward.
* Collaborates with developers, database administrators, and data architects on reporting and data management projects as needed.
* Provides elevated strategic assistance to the BI Director and the BI team by making recommendations in the areas of report design, data management, and best practices.
* Works with other BI Engineers and BI Analysts to create dashboards, scorecards, reports, and other executive-facing products.
* Builds, maintains, and communicates detailed reporting models to assist corporate leadership in evaluating potential ventures, as well as identifying current operational weaknesses.
* Critically evaluates information gathered from multiple sources, reconciles conflicts, decomposes high-level information into details, abstracts up from low-level information to a general understanding, and distinguishes user requests from the underlying true needs.
* Performs advanced data analysis, data modeling, and cube development.
* The BI team operates in a scrum environment; therefore, this position may fulfill the role of team member or scrum master depending upon the needs of the group.
Bachelor's degree in science, technology, engineering, or mathematics preferred.
* Proven demonstration of exemplary ability to complete the functions of a BI Engineer I or similar position.
* This position requires up to 10% travel.
* Ability to acquire and maintain BI continuing education credentials according to the BI training curriculum.
* Knowledge of BI best practices.
* Proven skills in data management and report development.
* Advanced Excel/data management skills required.
* Strong organizational and time management skills with the ability to successfully engage in multiple initiatives simultaneously.
* Strong knowledge of data warehouse design, data modeling, SQL, Excel, SQL/Relational database management, Power BI and other related tools.
* Experience with Azure required.
","['sql', 'azur', 'bi', 'excel', 'power bi']","['recommend', 'dashboard', 'commun', 'data modeling']",1,"['sql', 'azur', 'powerbi', 'excel', 'recommend', 'dashboard', 'commun', 'data modeling']","['azur', 'corpor', 'power', 'provid', 'bi', 'avail', 'relat', 'sourc', 'engin']","['sql', 'azur', 'powerbi', 'excel', 'recommend', 'dashboard', 'commun', 'data modeling', 'azur', 'corpor', 'power', 'provid', 'bi', 'avail', 'relat', 'sourc', 'engin']"
DE,"Need minimum 10 yrs of exp.
Job Requirements Data Engineer with expertise in Python and Big data technologies like Spark, Hive, Presto etc.
Experience with AWS services ndash S3, EC2, EMR, Lambda Functions and Step Functions.
Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.)
Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT.
Exposure to Scrum Agile framework Preferred experience in core Java technologies ( Java 1.8, Spring Boot, Spring MVC, Java EE fundamentals, Hibernateany Object relation mappers, OracleMySQL etc ) Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Strong analytical and problem-solving skills Regards Chandrakala Doddi 732-898-6795
","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db']","['commun', 'big data']",999,"['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data']","['analyt', 'spark', 'aw', 'python', 'big', 'relat', 'engin']","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data', 'analyt', 'spark', 'aw', 'python', 'big', 'relat', 'engin']"
DE,"3+ year of Industry experience.
Minimum 2+ years of Big Data/Hadoop experience Must have extensive Hadoop working knowledge and hands on experience in HDFS Experience in developing real time streaming applications using Flume and Kafka Good knowledge spark configurations and performance tuning Develop highly scalable and extensible Big Data platform.
","['kafka', 'spark', 'hadoop']",['big data'],999,"['kafka', 'spark', 'hadoop', 'big data']","['big', 'spark', 'hadoop']","['kafka', 'spark', 'hadoop', 'big data', 'big', 'spark', 'hadoop']"
DE,"Data Engineer role(SQL/Python/AWS) -
Contract - 6-9 months
Skills:
Python, Mem SQL, Data Eng, Hands-on AWS Required
Good to have R, AWS EMR, LOOKRProficient in one or more programming languages such as Python, Java, Scala, and R
Familiar with one or more machine learning or statistical modeling tools such as R, sci-kit learn, and Spark MLlib
Practical experience with distributed data platforms: Map/Reduce, Hadoop, SPARK
Knowledge and experience working with relational databases and SQL; Demonstrated flexibility in working with large, complex, and ambiguous datasets
Work successfully in a highly cross-functional environment
Strong analytical and quantitative problem-solving ability.
Excellent communication, relationship skills, and a strong teammate.
Engineer to put your research into practice
Experience on Data visualization tools such as Tableau and Power BISupply Chain, Retail - preferred
Thanks & Regards,
Satish Shinde
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience:
memSQL: 1 year (Preferred)
Big Data/Hadoop: 4 years (Required)
AWS: 5 years (Required)
SQL: 8 years (Required)
Python: 7 years (Required)
Tempe, AZ (Preferred)
Work authorization:
United States (Required)
Work Remotely:
Yes
","['sql', 'spark', 'scala', 'mllib', 'hadoop', 'aw', 'tableau', 'java', 'python', 'r', 'excel', 'power bi']","['research', 'visual', 'machine learning', 'statist', 'big data', 'commun']",999,"['sql', 'spark', 'scala', 'mllib', 'hadoop', 'aw', 'tableau', 'java', 'python', 'r', 'excel', 'powerbi', 'research', 'visual', 'machine learning', 'statist', 'big data', 'commun']","['analyt', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'power', 'hadoop', 'aw', 'python', 'statist', 'big', 'quantit', 'engin']","['sql', 'spark', 'scala', 'mllib', 'hadoop', 'aw', 'tableau', 'java', 'python', 'r', 'excel', 'powerbi', 'research', 'visual', 'machine learning', 'statist', 'big data', 'commun', 'analyt', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'power', 'hadoop', 'aw', 'python', 'statist', 'big', 'quantit', 'engin']"
DE,"Here’s what to expect:
Ability to pitch the right solution when it comes to Data be it a relational databases or non-relational databases
Build magical product experiences.
Make the technical& complex, simple and effortless.
Participate in all aspects of the software life-cycle, including ideation, development, and production support.
Mentor and be mentored, educate and learn, lead, and be led by other engineers in problem solving and solutioning.
Drive improvements to enable your team to deliver quality outcomes that lead to customer success.
Collaborate deeply with a diverse, cross functional team of Engineers, UX, Product, and Operations.
Leverage engineering best practices such as CI/CD, Pairing, and Test-Driven Development to deliver early and often.
Actively engage with the education community to understand product needs.
Work in an open space environment (no cube walls)
The Successful Candidate possesses:
BS in a related field, bootcamp, self-taught or equivalent experience
Experience working with both SQL and NoSQL databases, knowing which paradigm works best for different use cases
Demonstrated experience working with various components of Big Data ecosystem: Hadoop, Spark/Spark Streaming, Hive, Kafka
Ability to use industry standard dimensional modeling techniques to organize data from disparate systems into Data Marts and Warehouses
Experience with both OLTP and OLAP environments
Familiarity with ETL and ELT data transformation best practices
Mastery of Business Intelligence is a must
Experience to a variety of data technologies (Relational, Non-Relational, NoSQL, Data Warehouses, Big Data)
Ability to deliver iterative vertical slices of business value consisting of data visualization, transformation, and persistence.
Data security, e.g., HIPPA or FERPA compliance, experience helpful, but not required
Experience developing in AWS, Azure, or other cloud services
Preference for working within an Agile methodology (i.e.
Scrum, Kanban, XP)
Prior knowledge of Snowflake is a bonus
Familiarity with software engineering best practices
Passion for self-driven, continuous learning, both in and out of the office
Leadership qualities and capabilities
Opinionated on technology, in theory, but flexible in practice
Passion for Education is a must; experience in Ed-Tech helpful, but not required
","['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'cloud', 'hive', 'nosql', 'kafka']","['visual', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'cloud', 'hive', 'nosql', 'kafka', 'visual', 'problem solving', 'big data', 'etl', 'commun']","['techniqu', 'stream', 'learn', 'spark', 'azur', 'relat', 'visual', 'divers', 'hadoop', 'aw', 'warehous', 'big', 'etl', 'engin', 'particip']","['sql', 'spark', 'azur', 'hadoop', 'aw', 'snowflak', 'cloud', 'hive', 'nosql', 'kafka', 'visual', 'problem solving', 'big data', 'etl', 'commun', 'techniqu', 'stream', 'learn', 'spark', 'azur', 'relat', 'visual', 'divers', 'hadoop', 'aw', 'warehous', 'big', 'etl', 'engin', 'particip']"
DE,"Maybe youve stopped by for a coffee, fueled up your car or grabbed something to eat on the go.
Making everyday life easier for people all over the world.
RESPONSIBILITIES
• Prototype, build, deploy and manage data engineering pipelines.
• Contribute to design and creation of high-quality solutions.
• Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems.
REQUIRED QUALIFICATIONS
• Degree in computer science or related.
• 3+ years of relevant professional experience.
• Extensive experience in Python and SQL
• Experience with batch and real-time data processing tools and technologies (Databricks, Spark, Kafka)
• Knowledge of distributed data solutions, storage systems and columnar databases.
• Knowledge of Cloud Computing on Microsoft Azure or any other public cloud offering
• Familiarity with Continuous Integration/Continuous Deployment, Git.
• Fluent in spoken and written English.
• Knowledge about Agile development methods like Scrum and Kanban.
We´re looking forward to reviewing your application.
Please inform the Companys Human Resources Representative if you need assistance completing any forms or to otherwise participate in the application process.
In English
In Spanish
Job Requirements:
","['sql', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'kafka']","['machine learning', 'pipelin']",2,"['sql', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'kafka', 'machine learning', 'pipelin']","['machin', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'human', 'public', 'python', 'comput', 'integr', 'engin', 'particip']","['sql', 'spark', 'azur', 'git', 'cloud', 'python', 'microsoft', 'kafka', 'machine learning', 'pipelin', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'human', 'public', 'python', 'comput', 'integr', 'engin', 'particip']"
DE,"You can grow and learn in diverse areas across many disciplines such as Advanced Analytics, Investments, Actuarial, Accounting, Risk Management, Critical Business Advisor and so much more.
The hiring manager will create job expectations for each project position.Job DescriptionRELATIONSHIP: Reporting relationships will be determined based on the needs of the assigned project.AN EXPLANATION OF PROJECT STAFFING:STAFFING: A ""project basis staffing"" is a job temporarily assigned in order to perform a specific activity in support of an approved corporate project.
The assignment is typically two years or less.
The limited life of this assignment distinguishes a ""project"" position from most other positions.
Project staffing allows management a greater level of flexibility in selecting an incumbent with the needed level of expertise.
Because of the truly temporary status of the project job, the need for specific skills, and a fair administration of compensation for project personnel, the project jobs are to be staffed on a lateral basis (i.e.
the incumbent remains on the same pay band he/she held before project participation).LENGTH OF ASSIGNMENT: The assignment is typically two years or less.
However there are two types of project staffing assignments.
A short-term assignment would normally be four months or less and would not involve a permanent transfer.
Salary would be administered as normal, and the previous position would typically be held open.
Qualified associates will typically be provided 60 calendar days to search for a new position.
For clarification, contact your local HR representative.SALARY ADMINISTRATION: Individuals hired would typically receive a pro-rated merit increase.
During the project, merit salary adjustments pursuant to the salary administration guidelines will be based on the employee's pay band at the time of placement on the project job.
Performance evaluations will be conducted as they would on any job as appropriate.END OF PROJECT: Prior to the end of the project, Human Resources will be notified by the Project Executive regarding the project completion date.
Human Resources will assist the project job holder for a period of sixty calendar days, if qualified, in his/her search for a position .
The success of the associate in securing a position is contingent upon prior related work experience, satisfactory performance on the project and the availability of open positions matching the associates?
skills and experience.
There is no guarantee that the associate leaving the project will secure another position.JOB REQUIREMENTS:Vary based on project needs.
Specific circumstances may allow or require some people assigned to the job to perform a somewhat different combination of duties.Credit Check: AS APPLICABLE TO THE SPECIFIC PROJECT - Due to the fiduciary accountabilities within this job, a valid credit check and/or background check will be required as part of the selection process.
",[None],"['normal', 'risk', 'account']",999,"['normal', 'risk', 'account']","['day', 'basi', 'analyt', 'corpor', 'relat', 'credit', 'divers', 'human', 'provid', 'avail', 'employe', 'particip', 'evalu']","['normal', 'risk', 'account', 'day', 'basi', 'analyt', 'corpor', 'relat', 'credit', 'divers', 'human', 'provid', 'avail', 'employe', 'particip', 'evalu']"
DE,"Are you looking for a unique, truly innovative role?
What if it could be with one of the most impactful IT companies in the world?
In this amazing role, you will provide technology consulting to external customers and internal project teams.
You will be responsible for providing technical support and/or leadership in the creation and delivery of technology solutions designed to meet customers business needs and, consequently, for understanding customers businesses.
As a trusted advisor create and maintain effective customer relationships so as to insure customer satisfaction.
Maintain knowledge of leading edge technologies and industry/market domain knowledge.
Actively contribute to the companys solutions portfolio by providing information ranging from technical knowledge to methodologies based on experience gained from customer projects.
Shape technical direction and technical strategies within the organization and for external customers.
Accountable for consistent and significant chargeability levels (or expense relief for internal project teams) and for assisting in meeting or exceeding revenue and customer satisfaction goals.
Contribute to the organizations profitability by generating and cultivating new business opportunities and by providing technical support for deal proposal development.
How youll make your mark:
• You will be responsible for verifying and implementing the detailed technical design solution to the problem as identified by the Project/Technical Manager.
• You'll regularly leads in the technical assessment and delivery of specific technical solutions to the customer.
Provides a team structure conducive to high performance, and manages the team lifecycle stages.
• You coordinate implementation of new installations, designs, and migrations for technology solutions in one of the following work domains: networks, applications or platforms.
• You provide advanced technical consulting and advice to others on proposal efforts, solution design, system management, tuning and modification of solutions.
• You'll collect and determine data from appropriate sources to assist in determining customer needs and requirements.
About you:
• You have a Bachelors degree in Computer Science or a related area of study and relevant experience.
• You have 8+ years of relevant experience.
• You have sufficient depth and breadth of technical knowledge in Linux and to design and scope multiple deliverables across a number of technologies.
• You have led team in the delivery of multiple deliverables across multiple technologies.
• You have frequently used product and application knowledge along with internals or architectural knowledge to develop solutions.
A recognized expert in one or more technologies within own technical community and also at regional level.
Holds a vendor or industry certification in at least one discipline area.
https://www.facebook.com/HPECareers
https://twitter.com/HPE_Careers
1062768
",['linux'],"['commun', 'tune', 'account']",1,"['linux', 'commun', 'tune', 'account']","['provid', 'relat', 'comput', 'sourc']","['linux', 'commun', 'tune', 'account', 'provid', 'relat', 'comput', 'sourc']"
DE,"Data Engineer Phoenix, AZ Note Required only 10+ years candidates with strong exp in python coding.
Job Requirements Data Engineer with expertise in Python and Big data technologies like Spark, Hive, Presto etc.
Experience with AWS services ndash S3, EC2, EMR, Lambda Functions and Step Functions.
Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.)
Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT.
Exposure to Scrum Agile framework Preferred experience in core Java technologies ( Java 1.8, Spring Boot, Spring MVC, Java EE fundamentals, Hibernateany Object relation mappers, OracleMySQL etc ) Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Onshore position ndash Person should be able to work from Marsh Phoenix office Strong analytical and problem-solving skills Thanks, Sandeep Pedhi 732-898-6796
","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db']","['commun', 'big data']",999,"['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data']","['analyt', 'spark', 'aw', 'python', 'big', 'relat', 'engin']","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data', 'analyt', 'spark', 'aw', 'python', 'big', 'relat', 'engin']"
DE,"Experience with AWS services ndash S3, EC2, EMR, Lambda Functions and Step Functions.
Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.)
Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT.
Exposure to Scrum Agile framework Soft Skills Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Onshore position ndash Person should be able to work from March Phoenix office bullStrong analytical and problem-solving skills Arunkumar, 408-484-3155
","['sql', 'mongodb', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hbase', 'nosql', 'hive', 'db']","['commun', 'big data']",999,"['sql', 'mongodb', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hbase', 'nosql', 'hive', 'db', 'commun', 'big data']","['analyt', 'spark', 'aw', 'python', 'big']","['sql', 'mongodb', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hbase', 'nosql', 'hive', 'db', 'commun', 'big data', 'analyt', 'spark', 'aw', 'python', 'big']"
DE,"Big Data Engineer (5+ years)
Tech Stack : Java 1.6+, Spring Boot, Spring MVC, Hadoop , Hive, HDFS, Map Reduce, Spark Batch & Spark Streaming, Scala, Kafka
Proven hands-on Software Development experience
Proven working experience in Java development
Hands on experience in designing and developing applications using Java EE platforms
Hands on experience working on Hadoop ecosystem (Hadoop , Hive , HBase , PIg)
Hands on experience in the Spring Boot
Hands on experience in build and deploying web applications in tomcat, jboss web servers.
Object Oriented analysis and design using common design patterns.
Profound insight of Java and JEE internals (Classloading, Memory Management, Transaction management etc)
Excellent knowledge of map reduce and relevant big data programming paradigms
Experience with test-driven development
Cloud Experience is a plus.
Preferable : Spark programming experience
","['spark', 'scala', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'hbase', 'kafka', 'excel']",['big data'],999,"['spark', 'scala', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'hbase', 'kafka', 'excel', 'big data']","['program', 'stream', 'spark', 'hadoop', 'big', 'common', 'engin']","['spark', 'scala', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'hbase', 'kafka', 'excel', 'big data', 'program', 'stream', 'spark', 'hadoop', 'big', 'common', 'engin']"
DE,"Passionate about writing Quality Code/Best Practices
Knowledge of Agile principles
Experience with contemporary tools and frameworks commonly utilized by agile developers such as- Java web components, REST, Web services, Struts, Spring boot and Angular JS
Good understanding of
Microservices architecture
Solid principles
Refactoring and unit test practices
","['java', 'angular']",[None],999,"['java', 'angular']",[None],"['java', 'angular', None]"
DE,"Skills:
8-10+ years of experience
Big Data Sr.
Engineer
Java Spark, Elastic Search, Hbase, HIVE , REST API and SQL
Solids hands on
Great communicator
Leadership skills
","['sql', 'spark', 'java', 'hive', 'hbase']","['commun', 'big data']",999,"['sql', 'spark', 'java', 'hive', 'hbase', 'commun', 'big data']","['big', 'spark', 'engin']","['sql', 'spark', 'java', 'hive', 'hbase', 'commun', 'big data', 'big', 'spark', 'engin']"
DE,"Data Engineer
9920
Scottsdale,
3/27/2019 1:46:38 PM
Application Development
Contractor - W2
Data Engineer / Lead
4-5 years of experience in ETL, SQL, Python, Data Management and Spark and strong fundamentals in distributed environments
Real project implementations with Big Data technologies based on Spark
Experience working on Serverless technologies
Experience implementing NoSQL technologies – Mongo or Cassandra
Experience with AWS cloud services: Lambda, S3, Glue, Redshift, and Athena, or their open source equivalent (Zeppelin, Presto, etc)
Data storage formats – Parquet, JSON, AVRO etc.
Experience with real-time data sources and message ingestion for processing by filtering, aggregating, and preparing the data for analysis using technologies such as Spark Streaming and Kafka, AWS Kinesis, Firehose etc.
Experience with data pipelining
Understanding of best practices within the development process
Build processes supporting data transformation, data structures, metadata, dependency and workload management
CI/CD and DevOps tools such as BitBucket/Git, Bamboo, and Maven
AWS technologies – Cloudwatch, CloudFormation, Security (IAM)
AWS certification
Job Requirements
","['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka']","['etl', 'big data']",999,"['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'etl', 'big data']","['stream', 'spark', 'aw', 'python', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'cassandra', 'git', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'etl', 'big data', 'stream', 'spark', 'aw', 'python', 'big', 'etl', 'sourc', 'engin']"
DE,"Your Impact
As one of the founding engineers on the team, you will make key design decisions that will shape a series of data products and services.
You will be responsible for creating scalable ETL and streaming processes, efficient data pipelines, and a data warehouse that delivers value to technical users in public safety agencies.
Your Day-to-Day
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Partner with internal teams and agencies to make public safety data accessible and actionable.
Influence peers, advise senior leaders, coach and mentor junior team members.
Facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.
Basic Qualifications
Bachelor’s degree in CS, engineering, or other quantitative field
5+ years of industry experience in data warehousing and modeling on highly available SQL and non-relational (NoSQL and distributed database management systems)
Fluent in writing and optimizing SQL with demonstrated strength in writing complex, high-optimized queries across large data sets
Proficiency in at least one scripting language, Python, R, or similar
Demonstrated strength in design, development, and optimization of low latency pipelines for both stream and batch data in Apache Spark or similar
Ability to make tough technical decisions based on requirements, constraints, and trade-offs
Preferred Qualifications
Experience with big data technologies like Hadoop, Spark, Cascading, Hive, PrestoDB, Zookeeper, etc.
Knowledge of distributed systems and resource optimization for data storage and processing
Backend engineering experience (Java, Scala, C++, or similar)
Experience with BI tools like Tableau, PowerBI, etc.
Compensation and Benefits
Competitive salary and 401K with employer match
Discretionary paid time off
An encouraging parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...
","['sql', 'spark', 'scala', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi']","['pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun']","['day', 'stream', 'spark', 'pipelin', 'set', 'relat', 'public', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'warehous', 'action', 'big', 'quantit', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'r', 'powerbi', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'commun', 'day', 'stream', 'spark', 'pipelin', 'set', 'relat', 'public', 'etl', 'hadoop', 'optim', 'bi', 'python', 'avail', 'warehous', 'action', 'big', 'quantit', 'engin']"
DE,"Role: Big Data Engineer
Job Type: Contract
Need Big Data, Spark.
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
",['spark'],['big data'],999,"['spark', 'big data']","['basi', 'spark', 'big', 'engin', 'evalu']","['spark', 'big data', 'basi', 'spark', 'big', 'engin', 'evalu']"
DE,"Return to Job SearchSenior Data EngineerU-Haul Intl.
is seeking a Senior Data Engineer in the Phoenix area.
Responsible for developing, debugging & supporting applications for big data analytics platforms.
Bachelor's degree or foreign equivalent in Computer Science, Data Science, Statistics, Mathematics, Engineering, Bioinformatics, Physics, or related field.
+ 5yrs wrk exp.
w/ Java or Scala, Hadoop, Kafka, Data Lake or Databricks environments, SQL, Apache Spark, Linux, Unix, HDFS or DBFS, Data science, data analytics, or machine learning, scalability analysis & performance monitoring & measuring techniques, Hive & HBase, Zookeeper.
U-Haul Intl.
is a drug free environment & an EEO employer.
","['sql', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'kafka']","['machine learning', 'bioinformat', 'statist', 'big data']",1,"['sql', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'kafka', 'machine learning', 'bioinformat', 'statist', 'big data']","['analyt', 'machin', 'techniqu', 'learn', 'spark', 'hadoop', 'comput', 'statist', 'big', 'relat', 'engin']","['sql', 'linux', 'spark', 'scala', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'kafka', 'machine learning', 'bioinformat', 'statist', 'big data', 'analyt', 'machin', 'techniqu', 'learn', 'spark', 'hadoop', 'comput', 'statist', 'big', 'relat', 'engin']"
DE,"This position requires a dynamic data engineer who is entrepreneurial, has the ability to work with the ambiguity involved with launching completely new experiences, can develop and articulate strategic plans and drive their execution.
You will share in the ownership of the technical vision and direction for advanced analytics and insight products.
You will be a part of a team of top notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence.
Members of this team will be challenged to innovate using big data technologies.
If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you!
Basic Qualifications
· Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
· 3~6 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
· Demonstrated strength in data modeling, ETL development, and data warehousing
· Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, Elastic Search, etc.)
· Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
· Knowledge of data management fundamentals and data storage principles
· Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
· Experience working with AWS big data technologies (Redshift, S3)
· Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
· Experience providing technical leadership and mentoring other engineers for best practices on data engineering
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
","['spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'excel']","['data warehousing', 'etl', 'data modeling', 'big data']",1,"['spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'excel', 'data warehousing', 'etl', 'data modeling', 'big data']","['analyt', 'spark', 'challeng', 'relat', 'hadoop', 'aw', 'comput', 'big', 'etl', 'sourc', 'engin']","['spark', 'cogno', 'hadoop', 'aw', 'tableau', 'redshift', 's3', 'hive', 'hbase', 'excel', 'data warehousing', 'etl', 'data modeling', 'big data', 'analyt', 'spark', 'challeng', 'relat', 'hadoop', 'aw', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"Job Title: Big Data Engineer
Duration: Full Time
7-8 years of experience working on Spark, Hadoop, relational databases.
Good understanding of Data warehousing concepts.
Strong in concurrent programming.
Experience with frameworks like Spark, MapReduce, Hive, Pig, HBase.
Job Type: Full-time
","['mapreduc', 'spark', 'pig', 'hadoop', 'hive', 'hbase']","['data warehousing', 'big data']",999,"['mapreduc', 'spark', 'pig', 'hadoop', 'hive', 'hbase', 'data warehousing', 'big data']","['spark', 'hadoop', 'big', 'relat', 'engin']","['mapreduc', 'spark', 'pig', 'hadoop', 'hive', 'hbase', 'data warehousing', 'big data', 'spark', 'hadoop', 'big', 'relat', 'engin']"
DE,"Experience with AWS services S3, EC2, EMR, Lambda Functions and Step Functions.
Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.)
Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT.
Exposure to Scrum Agile framework Preferred experience in core Java technologies ( Java 1.8, Spring Boot, Spring MVC, Java EE fundamentals, Hibernateany Object relation mappers, OracleMySQL etc ) Soft Skills Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Onshore position Person should be able to work from March Phoenix office Strong analytical and problem-solving skills
","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db']","['commun', 'big data']",999,"['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data']","['analyt', 'spark', 'aw', 'python', 'big', 'relat']","['mongodb', 'sql', 'spark', 'elasticsearch', 'scala', 'ec2', 'git', 'aw', 'lambda', 'python', 's3', 'hive', 'nosql', 'java', 'hbase', 'db', 'commun', 'big data', 'analyt', 'spark', 'aw', 'python', 'big', 'relat']"
DE,"Role: Senior Data Engineer
Job Type: Contract
Key Responsibilities:
End to end ownership of ETL data pipelines, from ingestion of data to consumption by business intelligence and advanced analytics teams.
Design and build an automated, self-service data platform, freeing teams to focus on customer features and analysis.
Evolve existing tools and framework to support new scalability requirements as well new functionality as needed.
Identify and drive new solutions to enhance the development cycle to increase development productivity.
Work with product owners to identify and mature upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Work with team to identify and resolve technical debt to improve the team's throughput.
Skills:
Strong communication skills.
Deep experience designing and implementing highly scalable, distributed application systems.
5+ years' experience building data pipelines.
5+ years' experience programming in Python
Extensive knowledge in fine tuning SQL, understanding optimizers, and execution plans.
Extensive experience architecting complex data models to handle millions of transactions.
Experience in application design and Implementation using agile practices & TDD.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.
Strong understanding of software development life cycle and release management
Past experience integrating with Oracle, Microsoft SQL Server
Self-motivated, independent, team-player
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'spark', 'python', 'microsoft', 'kafka', 'oracl']","['optim', 'etl', 'commun', 'pipelin']",999,"['sql', 'spark', 'python', 'microsoft', 'kafka', 'oracl', 'optim', 'etl', 'commun', 'pipelin']","['basi', 'analyt', 'spark', 'pipelin', 'infrastructur', 'optim', 'python', 'releas', 'etl', 'sourc', 'engin', 'evalu']","['sql', 'spark', 'python', 'microsoft', 'kafka', 'oracl', 'optim', 'etl', 'commun', 'pipelin', 'basi', 'analyt', 'spark', 'pipelin', 'infrastructur', 'optim', 'python', 'releas', 'etl', 'sourc', 'engin', 'evalu']"
DE,"NOTE: ONLY GREEN CARD HOLDERS and U.S. CITIZENS WILL BE CONSIDERED.
Think again.
And youll have fun doing it.
The Role:
The Big Data Engineer position works in an agile environment interacting with multiple technology and business areas designing and developing next generation analytics platforms and applications.
The Engineer is responsible for the strategy and design of complex projects as well as coding, and also supports project planning and mentoring.
Effective communication is important as this individual will be interfacing with different areas including business clients, technology, architecture and infrastructure services.Job Qualifications
Primary skills:
Proven track record with Hadoop administration.
Administration experience on Hadoop, HDFS, YARN, Spark, Sentry/Ranger, HBase and Zookeeper.
Design, install, and maintain big data analytics platforms (on-prem/cloud) including design, security, capacity planning, cluster setup and performance tuning.
Deep understanding of distributed Hadoop ecosystem, networking connectivity and IO throughput along with other factors that affect distributed system performance.
Expert in configuring & troubleshooting of all the components in the Hadoop ecosystem like MapReduce, YARN, Pig, Hive, HBase, Sqoop, Flume, Zookeeper, Oozie (understanding of all these).
Experience in installing/configuring cluster monitoring tools like Cloudera Manager/Ambari, Ganglia, or Nagios.
(one of these).
Hands-on experience with scripting with bash, perl, ruby, or python (one of these).
Working knowledge of hardening Hadoop with Kerberos, TLS,SSL and HDFS encryption.
Working knowledge on Jenkins, git, AWS.
Good understanding on automation tools (e.g., Puppet, Ansible).
Secondary:
Manage public and private cloud infrastructure.
Expert in configuring & troubleshooting of all the components in the Hadoop ecosystem Spark, Solr, Scala, Kafka etc.
Working closely with the various teams - data science, database, network, BI and application teams to make sure that all the big data applications are highly available and performing as expected
Working knowledge on Jenkins, git, AWS.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
","['mapreduc', 'spark', 'perl', 'scala', 'pig', 'solr', 'git', 'hadoop', 'bi', 'aw', 'cloud', 'hive', 'hbase', 'kafka', 'rubi']","['commun', 'cluster', 'big data']",999,"['mapreduc', 'spark', 'perl', 'scala', 'pig', 'solr', 'git', 'hadoop', 'powerbi', 'aw', 'cloud', 'hive', 'hbase', 'kafka', 'rubi', 'commun', 'cluster', 'big data']","['analyt', 'program', 'spark', 'public', 'hadoop', 'primari', 'infrastructur', 'bi', 'avail', 'aw', 'big', 'engin']","['mapreduc', 'spark', 'perl', 'scala', 'pig', 'solr', 'git', 'hadoop', 'powerbi', 'aw', 'cloud', 'hive', 'hbase', 'kafka', 'rubi', 'commun', 'cluster', 'big data', 'analyt', 'program', 'spark', 'public', 'hadoop', 'primari', 'infrastructur', 'bi', 'avail', 'aw', 'big', 'engin']"
DE,"DATA ENGINEER
Data Engineer Responsibilities
• Design, construct, install, test and maintain data management systems.
• Build high-performance algorithms, models, and prototypes.
• Ensure that all systems meet the business/company requirements as well as industry practices.
• Integrate up-and-coming data management and software engineering technologies into existing data structures.
• Develop set processes for data mining, data modeling, and data production.
• Create custom software components and analytics applications.
• Research new uses for existing data.
• Employ an array of technological languages and tools to connect systems together.
• Collaborate with members of your team (eg, data architects, the IT team, data scientists) on the project’s goals.
• Install/update disaster recovery procedures.
• Recommend different ways to constantly improve data reliability and quality.
Qualifications
Data Engineer Requirements:
• Bachelor’s degree in computer science, software/computer engineering, applied mathematics, or physics statistics.
• Possible work experience and proof of technical expertise.
• Intellectual curiosity to find new and unusual ways of how to solve data management issues.
• Ability to approach data organization challenges while keeping an eye on what’s important.
Additional Information
All your information will be kept confidential according to EEO guidelines.
",[None],"['recommend', 'research', 'data mining', 'data modeling', 'statist']",1,"['recommend', 'research', 'data mining', 'data modeling', 'statist']","['analyt', 'challeng', 'set', 'comput', 'scientist', 'appli', 'statist', 'integr', 'engin', 'algorithm']","['recommend', 'research', 'data mining', 'data modeling', 'statist', 'analyt', 'challeng', 'set', 'comput', 'scientist', 'appli', 'statist', 'integr', 'engin', 'algorithm']"
DE,"2 to 3 years' experience designing and developing in Python, Scala, Spark, HDFS.
1 to 2 years' experience with Unix shell scripting
3 to 5 years' experience with SQL
Experience with version control tools and processes.
Good to have understanding of regulatory requirement, Basel-III, CCAR, CECL etc
","['sql', 'spark', 'scala', 'unix', 'python']",[None],999,"['sql', 'spark', 'scala', 'unix', 'python']","['python', 'spark']","['sql', 'spark', 'scala', 'unix', 'python', 'python', 'spark']"
DE,"Experience in Spark Good experience in kafka, streaming data.
Hands on experience on Cassandra and elastic search.
Should have experience in handling huge data in production Worked on at least 1 or 2 production usecase 2+ years of experience working in enterprise using Big Data Tool stack.
Hive concepts and writing queries experience is must.
Can work independently (without much support due to total remote work these days) on creating, running and debugging jobs hive queries on Hadoop platform ( preferably HDP )
","['spark', 'cassandra', 'hadoop', 'hive', 'kafka']",['big data'],999,"['spark', 'cassandra', 'hadoop', 'hive', 'kafka', 'big data']","['day', 'big', 'spark', 'hadoop']","['spark', 'cassandra', 'hadoop', 'hive', 'kafka', 'big data', 'day', 'big', 'spark', 'hadoop']"
DE,"Of Yrs Domain Experience No.
Of Yrs Relevant Exp No.
Of Yrs Skills Years of Exp Self-Rating (1-10) JavaScript No.
Of Yrs SPAs No.
Of Yrs Algorithms run-time No.
Of Yrs Candidates Availability On Confirmation can start in how many weeks?
Reason for change Best time - Phone Interview Best time - In-person Interview Any Other Interviews
",['javascript'],[None],999,['javascript'],"['avail', 'algorithm']","['javascript', 'avail', 'algorithm']"
DE,"Hi Role Data Engineer With Strong Python Experience looking for strong profiles and should have 12+ years' experience.
Design and build an automated, self-service data platform, freeing teams to focus on customer features and analysis.
Evolve existing tools and framework to support new scalability requirements as well new functionality as needed.
Identify and drive new solutions to enhance the development cycle to increase development productivity.
Work with product owners to identify and mature upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Work with team to identify and resolve technical debt to improve the team's throughput.
Skills Strong communication skills.
Deep experience designing and implementing highly scalable, distributed application systems.
5+ years' experience building data pipelines.
5+ years' experience programming in Python Extensive knowledge in fine tuning SQL, understanding optimizers, and execution plans.
Extensive experience architecting complex data models to handle millions of transactions.
Experience in application design and Implementation using agile practices TDD.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.
","['sql', 'kafka', 'python', 'spark']","['optim', 'commun', 'pipelin']",999,"['sql', 'kafka', 'python', 'spark', 'optim', 'commun', 'pipelin']","['spark', 'pipelin', 'infrastructur', 'optim', 'python', 'sourc', 'engin']","['sql', 'kafka', 'python', 'spark', 'optim', 'commun', 'pipelin', 'spark', 'pipelin', 'infrastructur', 'optim', 'python', 'sourc', 'engin']"
DE,"RESPONSIBILITIES:
Summary:
As Azure Data Factory (ADF) Developer, this role will be responsible for creating Data orchestration with Azure Data Factory Pipelines & Dataflows.
The key role is to understand the business requirements and implement the requirements using Azure Data Factory.
Roles & Responsibilities:
Understand business requirement and actively provide inputs from Data perspective
Understand the underlying data and flow of data
Build simple to complex pipelines & dataflows
Work with other Azure stack modules like Azure Data Lakes, SQL DW, etc.
Should be able to implement modules that has security and authorization frameworks
Recognize and adapt to the changes in processes as the project evolves in size and function
REQUIREMENTS:
Bachelor's or Master's degree in Computer Science or Data engineering
At least 7-8 years of software development experience, and 1 year of experience on Azure Data factory
Experience working with at least 1 project on Azure Data Factory
Expert level knowledge on Azure Data Factory
Expert level knowledge of SQL DB & Datawarehouse
Should know at least one programming language
Should be able to analyze and understand complex data
Knowledge of Azure data lake is required
Knowledge of other Azure Services like Analysis Service, SQL Databases will be an added advantage
Excellent interpersonal/communication skills (both oral/written) with the ability to communicate at various levels with clarity & precision
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
","['sql', 'db', 'excel', 'azur']","['commun', 'pipelin']",1,"['sql', 'db', 'excel', 'azur', 'commun', 'pipelin']","['program', 'pipelin', 'azur', 'input', 'comput', 'engin']","['sql', 'db', 'excel', 'azur', 'commun', 'pipelin', 'program', 'pipelin', 'azur', 'input', 'comput', 'engin']"
DE,"Big Data Engineer
Long term contract
JD:
Big Data Engineer:
Essential Duties and Responsibilities:
Day to Day support of existing data ingest jobs
Creation, support, and scheduling of ingestions from new sources
Ongoing data engineering and enhancements to data sets within the environment
Qualifications
Proficient with big data technologies
Hive, Pig, HBase, MapReduce
Experience with open source ingestion tools
Sqoop, Flume, Spark streaming, Kafka, Nifi
Proven experience building real-time steaming data sets
Experience working with and performing analysis using large data sets
10-1000 TB
Familiarity with common data science toolkits, such as R, Jupyter, & Python
Previous experience with traditional databases such as: Netezza, MySQL, Teradata, Oracle, etc
Preferred Experience with Azure products: Azure Data Lake Store, Azure HD Insights, Cosmos DB, PowerBI
","['mapreduc', 'powerbi', 'spark', 'azur', 'pig', 'jupyt', 'db', 'python', 'hive', 'hbase', 'r', 'mysql', 'kafka', 'oracl']",['big data'],999,"['mapreduc', 'powerbi', 'spark', 'azur', 'pig', 'jupyt', 'db', 'python', 'hive', 'hbase', 'r', 'sql', 'kafka', 'oracl', 'big data']","['day', 'essenti', 'spark', 'azur', 'python', 'big', 'set', 'common', 'sourc', 'engin']","['mapreduc', 'powerbi', 'spark', 'azur', 'pig', 'jupyt', 'db', 'python', 'hive', 'hbase', 'r', 'sql', 'kafka', 'oracl', 'big data', 'day', 'essenti', 'spark', 'azur', 'python', 'big', 'set', 'common', 'sourc', 'engin']"
DE,"Phoenix, AZ
Qualification:
BE/ B.Tech
Experience:
7 Years
Technology:
Big Data
Role/Skills:
Very strong server-side Java experience, especially in an open source, data-intensive, distributed environments.
Strong previous professional experience building Distributed Solutions dealing with high volumes of data.
Hands on experience on HDFS, Hive, Pig, Sqoop and NOSQL.
Experience in Apache Spark Batch and/or Spark Streaming (at least 6 months)
Good understanding of algorithms, data structure, performance optimization techniques and exposure to complete SDLC and PDLC
Well aware of architectural concepts (Multi-tenancy, SOA, SCA etc.)
and NFR’s (performance, scalability, monitoring etc.)
Responsibilities:
Implementing various solutions arising out of large data processing (GB’s/ PB’s) over various NoSQL, Hadoop and MPP based products—both on-premise and in the cloud
Actively participating in various architecture and design calls with Big Data customers
Developing Hive scripts and being involved in writing MapReduce jobs
Implementing complex projects dealing with considerable data size (GB/PB) and with high complexity
Working with Sr. Architects and providing implementation details to the Offshore team
Conducting sessions/ writing whitepapers/ Case Studies pertaining to Big Data
Being responsible for timely and quality deliveries
","['mapreduc', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'nosql']","['optim', 'big data']",999,"['mapreduc', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'nosql', 'optim', 'big data']","['techniqu', 'stream', 'spark', 'hadoop', 'optim', 'big', 'sourc', 'algorithm']","['mapreduc', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'nosql', 'optim', 'big data', 'techniqu', 'stream', 'spark', 'hadoop', 'optim', 'big', 'sourc', 'algorithm']"
DE,"Big Data Engineer
9321
Phoenix, AZ
10/9/2018
Application Development
Contractor - W2
You won’t just shape the world of software.
You’ll shape the world of life, work and play.
This position requires a mix of strategic engineering and design along with hands-on, technical work.
Exceptional communication skills, for collaborating across many teams.
You will lead teams and deliver best-in-class products in an exciting fast-paced environment with the focus on reliability and automation.
Dynamic, smart people and inspiring, innovative technologies are the norm here.
The successful candidate will be highly self-motivated with a passion for quality and automation coupled with an ability to understand complex systems and a desire to constantly make things better.
You won’t just keep up, you’ll break new ground.
There are hundreds of opportunities to make your mark on technology and life at American Express.
Here’s just some of what you’ll be doing:
Building software and systems to manage/support applications through automation, support and monitoring
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
Are you up for the challenge?
Overall 5+yrs experience in a large scale *nix environment
5+ yrs experience handling BigData Environment
Software development experience with on or more of: Python, Java or Scala
Experience with relational databases such as PostgreSQL, MySQL
Experience with core Hadoop: HDFS, MapReduce, Yarn
Experience with Streaming Data Platform (Kafka, Storm)
Experience with distributed/NoSQL databases: HBase, MySQL
Solid understanding of *nix systems and networking fundamentals
Experience as a software developer with MapReduce or Spark
Experience with extended Hadoop ecosystem: Hive, Pig
Solid Scripting Skills in languages like Python, Shell
Oncall Experience
Excellent communication skills, written and spoken; troubleshooting skills
Evidence of self-learning and commitment to personal development
Bachelor's degree or higher in Computer Science, or equivalent experience\
Continues integration, testing and deployment using Git, Jenkins
At the core of Software Engineering
Agile Practices
Porting/Software Configuration
Programming Languages and Frameworks
Business Analysis
Analytical Thinking
Business Product Knowledge
Job Requirements
","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'mysql', 'hbase', 'kafka', 'excel']","['optim', 'commun', 'big data']",1,"['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data']","['analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data', 'analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Hi,
Hope you're doing great!!
I would appreciate if you refer someone for this
position
Role: Data Engineer
Duration: Long term
Data model development and Model scoring
Work with Data Scientists and build scripts to meet their data needs
Required Qualifications:
10+ years of overall experience
3+ years experience with Big Data ( HADOOP platforms) Hive, Sp
( needs to be currently hands-on on Hadoop cluster)
5+ years of ETL (Extract, Transform, Load) - Scoop, INFA RDBMS Teradata,
Oracle
Experience in Python
Reproduce issues faced by Data Scientists
Knowledge of Agile is a must
My Best,
Manvitha
Ph:
E:
Show moreShow less
","['python', 'hive', 'oracl', 'hadoop']","['etl', 'cluster', 'big data']",999,"['python', 'hive', 'oracl', 'hadoop', 'etl', 'cluster', 'big data']","['hadoop', 'python', 'scientist', 'big', 'etl', 'engin']","['python', 'hive', 'oracl', 'hadoop', 'etl', 'cluster', 'big data', 'hadoop', 'python', 'scientist', 'big', 'etl', 'engin']"
DE,"Make Next Happen Now.
This includes providing data integration services for all batch data movement; managing and enhancing the data warehouse, Data Lake and dependent data marts; and providing support for analytics and business intelligence consumers.
The incumbent will participate in design and lead development of technology solutions to address Enterprise Data quality business needs.
The successful candidate will help develop the next generation solutions using Informatica IDQ products.
The position requires in-depth experience and understanding of Enterprise data quality, Informatica Data quality, Enterprise Data quality tools, data warehousing and data integration skillsets.
Responsibilities include, but are not limited to:
• Architect and design secure, robust, fault-tolerant and highly scalable Informatica Data quality frameworks for Cloud and on premise environments.
• Develop and deliver MDM/IDQ components as specified in the design, functional and non-functional requirements, within established budget, time and quality standards.
• Quality and completeness of detailed technical specifications, solution designs, including architecture, design, code development and code reviews as well as adherence to the non-functional requirements.
• Performing hands-on development work with Informatica IDQ for Data quality and Data transformations for MDM on the cloud and on premise environments.
• Recommend data governance processes, organizational models, and Informatica IDQ technology platform.
• Responsible for estimating the effort for work activities and assisting peers and matrix team for successful execution of work activities.
• Identify, document and communicate technical risks, issues and alternative solutions discovered during project.
• Develop and unit test data solutions, data integrations, data services.
• Use agile engineering practices and various data development technologies to rapidly develop creative and efficient data products.
• Align and integrate well with architects, data analysts, data modelers and other stakeholders.
• Communicate with other developers across teams, both as ad hoc problem solving, and check-ins and discussions with other initiatives.
• Mentor and provide direction to data engineers and quality engineers, both on-site and offshore.
Core Technical Requirements of the role
• Minimum 8 years of data engineering experience, with hands-on experience implementing Informatica IDQ on cloud and on premise environments.
• Minimum of 5 years is must with implementing Informatica IDQ products suite.
Completed at least two Informatica IDQ implementations from the scratch.
• Strong understanding of IDQ Process & Procedures such as Define, Discovery, Profiling, Remediation, and Monitoring.
• Experience in using IDQ tool for source data profiling, creating and applying rules.
• Expertise in implementing data quality processes including transliteration, parsing, analysis, standardization and data enrichment using IDQ transformations.
• Experience in Design and execute a Data Quality Audit/Assessment and data quality mappings that will cleanse, de-duplicate.
• Expertise on data profiling to identify data anomalies.
• Experience in developing human task workflow scenarios and IDQ scorecards to support data remediation.
• Related work experience in the areas of Master Data Management, Data Governance, deep experience in data warehousing and analysis.
• Hands-on implementation experience in Big Data technologies is preferred
• Expertise to ensure data quality and reliability and provide feedback to businesses and IT team on how to improve the quality of the data.
• Experience in financial domain is a plus
",['cloud'],"['recommend', 'anomali', 'risk', 'data warehousing', 'problem solving', 'big data']",2,"['cloud', 'recommend', 'anomali', 'risk', 'data warehousing', 'problem solving', 'big data']","['analyt', 'relat', 'human', 'warehous', 'big', 'integr', 'sourc', 'engin']","['cloud', 'recommend', 'anomali', 'risk', 'data warehousing', 'problem solving', 'big data', 'analyt', 'relat', 'human', 'warehous', 'big', 'integr', 'sourc', 'engin']"
DE,"Big Data Engineer II
9273
85050, AZ
10/1/2018 9:46:00 PM
IT
Contractor - W2
Big Data Developer (7+ years)
Tech Stack : Java 1.8, Spring Boot, Spring MVC, Hadoop , Hive, Map Reduce, Spark
· Proven hands-on Software Development experience
· Proven working experience in Java development
· Hands on experience in designing and developing applications using Java EE platforms
· Hands on experience working on Hadoop ecosystem (Hadoop , Hive , HBase , PIg)
· Hands on experience in the Spring Boot
· Hands on experience in build and deploying web applications in tomcat, jboss web servers.
· Object Oriented analysis and design using common design patterns.
· Profound insight of Java and JEE internals (Classloading, Memory Management, Transaction management etc)
· Excellent knowledge of map reduce and relevant big data programming paradigms
· Experience with test-driven development
· Preferable : Spark programming experience
Job Requirements
","['spark', 'pig', 'hadoop', 'java', 'hive', 'hbase', 'excel']",['big data'],999,"['spark', 'pig', 'hadoop', 'java', 'hive', 'hbase', 'excel', 'big data']","['spark', 'hadoop', 'big', 'common', 'engin']","['spark', 'pig', 'hadoop', 'java', 'hive', 'hbase', 'excel', 'big data', 'spark', 'hadoop', 'big', 'common', 'engin']"
DE,"Semiconductor
You will architect and implement a Greenplum Massively Parallel Processing Big Data cluster that is scalable for ingesting, transforming , persistence and query of peta bytes of data from heterogenous structured data sources where storage is separate from compute and capable of performant and multi-tenant data analytics including machine learning.
The platform should provide ability to manage big data with attributes of Volume, Velocity, Variety and Veracity
Responsibilities:
You will be responsible for defining the Greenplum big data architectural blueprint for the cluster including hardware, software, storage and networking needs under the supervision of senior leaders of the organization
You will be responsible for setting up the infrastructure, defining big data pipelines for data ingestion, data transformation and data analytics
You will be responsible for building reference implementations on the platform based on real world use cases
Qualifications
Essential Skills:
Detailed knowledge of key ingredients of a Greenplum architecture with hands on experience in setting up such platforms in large corporations following industry standard patterns and practices using both on premise and public cloud infrastructures
Desirable Skills:
Hands on experience using a Greenplum big data platform for predictive modeling, statistics, Machine Learning, Data Mining, and other data analysis techniques to collect, explore, and extract insights from structured data
Hands on experience integrating Hadoop HDFS with a Greenplum infrastructure.
Additional Information
All your information will be kept confidential according to EEO guidelines.
","['cloud', 'hadoop']","['pipelin', 'data mining', 'hardwar', 'predict', 'machine learning', 'statist', 'big data', 'supervis', 'cluster']",999,"['cloud', 'hadoop', 'pipelin', 'data mining', 'hardwar', 'predict', 'machine learning', 'statist', 'big data', 'supervis', 'cluster']","['analyt', 'essenti', 'techniqu', 'learn', 'machin', 'pipelin', 'corpor', 'public', 'predict', 'hadoop', 'infrastructur', 'comput', 'statist', 'big', 'sourc']","['cloud', 'hadoop', 'pipelin', 'data mining', 'hardwar', 'predict', 'machine learning', 'statist', 'big data', 'supervis', 'cluster', 'analyt', 'essenti', 'techniqu', 'learn', 'machin', 'pipelin', 'corpor', 'public', 'predict', 'hadoop', 'infrastructur', 'comput', 'statist', 'big', 'sourc']"
DE,"Must have java experience.
Must have Spark streaming experience.
Skills Required Good knowledge of ScalaJavaJ2EE Web Services.
Designed architected and implemented complex projects dealing with the considerable data size (GB PB) and with high complexity.
Should have experience on working with batch processing real-time systems using various Open Source technologies like Hadoop, NoSQL DBrsquos, Spark, Scala, Kafka, etc.
Capable of providing the design and Architecture for the typical business problems, exposure on Hadoop distribution used in big data solution.
Excellent ability to grasp business processes and translate them into what is needed to be done technically to implement them.
Good communication, problem solving interpersonal skills.
","['spark', 'scala', 'hadoop', 'java', 'nosql', 'kafka', 'excel']","['problem solving', 'commun', 'big data']",999,"['spark', 'scala', 'hadoop', 'java', 'nosql', 'kafka', 'excel', 'problem solving', 'commun', 'big data']","['spark', 'big', 'sourc', 'hadoop']","['spark', 'scala', 'hadoop', 'java', 'nosql', 'kafka', 'excel', 'problem solving', 'commun', 'big data', 'spark', 'big', 'sourc', 'hadoop']"
DE,"Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction.
Defines scope, plans, and deliverables for assigned components.
Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems.
Applies metrics to monitor performance and measure key project parameters.
Prepares system documentation.
Conforms to security and quality standards.
Stays current on emerging tools, techniques, and technologies.Responsibilities:+ Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives+ Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams+ Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.+ Develop visualization, user experience and configuration elements of solution design.+ Execute and coordinate requirements management and change management processes.
Participates as a member of and leads development teams.+ Designs units for others.+ Completes development to implement complex components.+ Designs solutions for others to develop.+ Participates in cross-functional teams.+ Leads design activities and provides mentoring and guidance to developers.+ Designs, prepares and executes unit tests.+ Demonstrates technical leadership and exerts influence outside of immediate team.+ Develops innovative team solutions to complex problems.+ Contributes to strategic direction for teams.+ Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g.
Determination on requests for reasonable accommodation are considered on a case-by-case basis.
This contact information (email and phone) is intended for application assistance and accommodation requests only.
",[None],['visual'],999,['visual'],"['basi', 'analyt', 'techniqu', 'corpor', 'visual', 'appli', 'engin', 'particip']","['visual', 'basi', 'analyt', 'techniqu', 'corpor', 'visual', 'appli', 'engin', 'particip']"
DE,"Sr Big Data Engineer
9322
Phoenix, AZ
10/9/2018
Application Development
Contractor - W2
You won’t just shape the world of software.
You’ll shape the world of life, work and play.
This position requires a mix of strategic engineering and design along with hands-on, technical work.
Exceptional communication skills, for collaborating across many teams.
You will lead teams and deliver best-in-class products in an exciting fast-paced environment with the focus on reliability and automation.
Dynamic, smart people and inspiring, innovative technologies are the norm here.
The successful candidate will be highly self-motivated with a passion for quality and automation coupled with an ability to understand complex systems and a desire to constantly make things better.
You won’t just keep up, you’ll break new ground.
Here’s just some of what you’ll be doing:
Building software and systems to manage/support applications through automation, support and monitoring
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
.
Are you up for the challenge?
Overall 5+yrs experience in a large scale *nix environment
5+ yrs experience handling BigData Environment
Software development experience with on or more of: Python, Java or Scala
Experience with relational databases such as PostgreSQL, MySQL
Experience with core Hadoop: HDFS, MapReduce, Yarn
Experience with Streaming Data Platform (Kafka, Storm)
Experience with distributed/NoSQL databases: HBase, MySQL
Solid understanding of *nix systems and networking fundamentals
Experience as a software developer with MapReduce or Spark
Experience with extended Hadoop ecosystem: Hive, Pig
Solid Scripting Skills in languages like Python, Shell
Oncall Experience
Excellent communication skills, written and spoken; troubleshooting skills
Evidence of self-learning and commitment to personal development
Bachelor's degree or higher in Computer Science, or equivalent experience\
Continues integration, testing and deployment using Git, Jenkins
At the core of Software Engineering
Agile Practices
Porting/Software Configuration
Programming Languages and Frameworks
Business Analysis
Analytical Thinking
Business Product Knowledge
Job Requirements
Here’s just some of what you’ll be doing:
Building software and systems to manage/support applications through automation, support and monitoring
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'mysql', 'hbase', 'kafka', 'excel']","['optim', 'commun', 'big data']",1,"['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data']","['analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['mapreduc', 'spark', 'scala', 'pig', 'git', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgresql', 'sql', 'hbase', 'kafka', 'excel', 'optim', 'commun', 'big data', 'analyt', 'program', 'stream', 'learn', 'challeng', 'spark', 'relat', 'hadoop', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Primary Responsibilities:
Develop quality assurance systems in support of high integrity data sets
Assist quantitative analysts in crafting custom, bespoke data sets
Support timely and fault tolerant data systems in support of production trading algorithms
Meet tight deadlines in an efficient manner
Requirements of the Candidate include:
Bachelor’s degree in Computer Science or applicable degree and very strong exposure to programming and computer systems
Experience with Python and C++
Experience with data storage and manipulation using approaches such as HDF5, Pandas, and RDBMS/SQL
The desire to work in a fast-paced, hardworking and committed environment with a talented team
Strong problem solving skills, critical thinking and clear written and verbal communications
Financial industry, market data and cloud data environment experience are beneficial
","['sql', 'panda', 'python', 'cloud']","['problem solving', 'commun']",1,"['sql', 'panda', 'python', 'cloud', 'problem solving', 'commun']","['program', 'set', 'primari', 'python', 'algorithm', 'comput', 'quantit', 'integr']","['sql', 'panda', 'python', 'cloud', 'problem solving', 'commun', 'program', 'set', 'primari', 'python', 'algorithm', 'comput', 'quantit', 'integr']"
DE,"Overview
Required Skills
• Experience with MPP databases.
Ability to troubleshoot issues and develop functions in an MPP environment is highly desired
• Very good knowledge of the software development life cycle, agile methodologies, and test-driven development
• Experience utilizing and extending ETL solutions in a complex, high-volume data environment is highly desired
• 2+ years of SQL experience and ETL development is required
• 2+ year of programming experience in either Spark, C, Java, Python, R and/or other functional programming skills
• Sound understanding of continuous integration & continuous deployment environments
• Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards
• Strong analytical skills with a passion for testing
• Excellent problem solving and debugging skills
• Exposure in Data Management, Governance and Controls functions
• 4+ years of certified learning in Information Management, Computer Science, Engineering or Application/Platform Development
Preferred / Complementary Skills
• Technical and quantitative reasoning skills with regard to marketing data
• Experience in Informatica, Talend, Pentaho, Ab Initio
• Experience in Hadoop, Hive, Impala, Pig or Kafka
• Experience in Greenplum or Netezza
Ability to troubleshoot issues and develop functions in an MPP environment is highly desired • Very good knowledge of the software development life cycle, agile methodologies, and test-driven development • Experience utilizing and extending ETL solutions in a complex, high-volume data environment is highly desired • 2+ years of SQL experience and ETL development is required • 2+ year of programming experience in either Spark, C, Java, Python, R and/or other functional programming skills • Sound understanding of continuous integration & continuous deployment environments • Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards • Strong analytical skills with a passion for testing • Excellent problem solving and debugging skills • Exposure in Data Management, Governance and Controls functions • 4+ years of certified learning in Information Management, Computer Science, Engineering or Application/Platform Development Preferred / Complementary Skills • Technical and quantitative reasoning skills with regard to marketing data • Experience in Informatica, Talend, Pentaho, Ab Initio • Experience in Hadoop, Hive, Impala, Pig or Kafka • Experience in Greenplum or Netezza
?
","['sql', 'spark', 'pig', 'excel', 'hadoop', 'c', 'hive', 'python', 'java', 'r', 'kafka', 'pentaho']","['etl', 'problem solving']",999,"['sql', 'spark', 'pig', 'excel', 'hadoop', 'c', 'hive', 'python', 'java', 'r', 'kafka', 'pentaho', 'etl', 'problem solving']","['analyt', 'program', 'techniqu', 'learn', 'spark', 'etl', 'hadoop', 'python', 'comput', 'quantit', 'engin', 'integr']","['sql', 'spark', 'pig', 'excel', 'hadoop', 'c', 'hive', 'python', 'java', 'r', 'kafka', 'pentaho', 'etl', 'problem solving', 'analyt', 'program', 'techniqu', 'learn', 'spark', 'etl', 'hadoop', 'python', 'comput', 'quantit', 'engin', 'integr']"
DE,"Data Engineer, Ad Technology
Data Engineer, Ad Tech
Level
Experienced
Philadelphia - Philadelphia, PA
Position Type
Full Time
Job Category
Information Technology
Job Title: Data Engineer, Ad Technology
Overview:
You will champion infrastructure-as-code and test driven development, building out the workings of modern data transformation & data science pipelines.
Your Python chops are impressive, your SQL skills even more so.
You prefer a bash terminal to a spreadsheet and cannot fathom life without source control.
You take pride in your uncanny ability to productionize anything, be it a Python script, a Jupyter Notebook or a SQL model.
You live for those two little words: “ship it”.
What’s Special About this Role:
Spend your time writing software and solving business problems, not sitting in endless meetings.
Work to improve products that make real people’s lives better.
Use the best tools out there, and sometimes write even better ones.
Engage with a team that wants to do big things fast, and values a healthy work-life balance while doing them.
What You Do:
Build and automate infrastructure using deployable code
What You Bring:
Develop in Java/Python engineering critical, large scale distributed services
Experience with Kafka, Spark, Hadoop
Experience in the AdTech / advertising / online identity space.
Prior experience with integrations with the adtech ecosystem including DMPs, third party measurement firms like ComScore, Crossix, Experian.
Prior experience creating and working with device and user identity graphs
Nice to Haves:
Experience with display ads: Facebook Ads, DFP/DFA
Experience with marketing/remarketing, etc.
AWS microservices suite
Snowflake
Alooma
Fivetran
DBT
Airflow
SnowShu
All team members - from executives to managers to people in entry-level positions - sit together in an open office and enjoy a fully integrated, collaborative and supportive work environment.
All team members enjoy free personal training classes, a full supply of La Colombe coffee, frequent on-site social events, Summer Fridays, and regular complimentary family-style lunches.
Learn more at www.health-union.com.
All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, disability status, age, protected veteran status or any other characteristic protected by law.
Successful candidates must pass the E-Verify process after hire.
Applicants must be legally eligible to work in the U.S for any employer.
*Candidates only.
No direct calls or emails.
No recruiters/agencies, please.
","['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka']","['information technology', 'graph', 'pipelin']",999,"['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka', 'information technology', 'graph', 'pipelin']","['learn', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'parti', 'big', 'integr', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka', 'information technology', 'graph', 'pipelin', 'learn', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'parti', 'big', 'integr', 'sourc', 'engin']"
DE,"71635
Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.
Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience:
Strong problem solving skills and critical thinking ability.
Strong collaboration and communication skills within and across teams.
5 or more years of progressively complex related experience.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Ability to understand complex systems and solve challenging analytical problems.
Experience with bash shell scripts, UNIX utilities & UNIX Commands.
Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Experience building data transformation and processing solutions.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
from you via e-mail.
Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter.
","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",1,"['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"Country:
United States
Cities:
Boston, Hartford, New York City, Philadelphia, Providence
Area of expertise:
Analytics
14-time winner of Microsoft Partner of the Year
24,000+ certifications in Microsoft technology
90+ Microsoft partner awards
17 Gold Competencies
3,500 analytics professionals worldwide
1,000 data engineers
Implemented analytics systems for more than 550 clients
400 AI practitioners
300 cognitive service experts
About you:
You design data solutions that enable clients to see the whole picture and provide insightful and accurate analysis that helps to build successful businesses.
About the job:
As a Manager, Data Engineering, you use modern data engineering techniques and Advanced Analytics methods to give your clients the information they need.
You collect, aggregate, store and reconcile data from various sources, helping to design and build data pipelines, streams, reporting tools, data generators and a whole range of tools to provide information and insight.
You know how to read the patterns and trends that influence business outcomes.
Day-to-day, you will:
Give colleagues and clients the tools to find and use data for routine and non-routine analysis
Use your sound eye for business to translate business requirements into technical solutions
Analyze current business practices, processes and procedures to spot future opportunities
Assess client needs to build bespoke data design services
Build the building blocks for transforming enterprise data solutions
Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)
Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools
Implement effective metrics and monitoring
Be comfortable to make your own decisions and guide your colleagues
Travel as needed to various client locations
Your skills and business experience include:
Transforming business needs into technical solutions
Mapping data and analytics
Data profiling, cataloguing and mapping to enable the design and build of technical data flows
Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration
Knowledge of multiple Azure data applications including Azure Databricks
Experience in preparing data for and building pipelines and architecture
You probably have a Bachelors or Master’s degree in a quantitative field such as computer science, applied mathematics, statistics or machine learning – or an equivalent combination of education and experience.
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['microsoft', 'azur']","['pipelin', 'visual', 'machine learning', 'statist', 'analyz']",1,"['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz']","['day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']","['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz', 'day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']"
DE,"LOC_1300_MKT-Wanamaker Building Req ID: 42481
Shift: Days
Employment Status: Regular - Full Time
Job Summary
The ideal candidate has knowledge of and is excited to learn about all aspects of data from multiple complex sources who enjoys optimizing data systems and building them from the ground up.
They will also support non-technical colleagues in the collection and appropriate use of clinical and non-clinical data.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Job Responsibilities
Data Modeling – evaluate structured and unstructured data, determine the most appropriate schema for new fact tables, data marts, etc.
Validate data to ensure quality.
Reporting – collaborate with colleagues across the enterprise to scope requests.
Extract data from various data sources, validate results, create relevant data visualizations, and share with requester.
Develop dashboards and automate refreshes as appropriate.
Governance / Best Practices – adhere and contribute to enterprise data governance standards.
Also educates and supports colleagues in best practices to ensure that data is used appropriately.
Product Ownership – collaborate and act as the voice of the customer to offer concrete feedback and project requests as well as an advocate for analytics from within the business units themselves.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including ground, hybrid cloud, and cloud) using SQL and various programming technologies.
Develop analytics tools that utilize data resources to provide actionable insights, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Clinical, and Analyst teams to assist with data-related technical issues and support their data infrastructure needs.
Develop optimized tools for analytics and data scientist team members that assist them in building and optimizing projects into an innovative industry leader.
Proficient at integrating predictive and prescriptive models into applications and processes.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Participate in a shared production on-call support model.
Be a critical part of an Agile Scrum software development team, ensuring the team successfully meets its deliverables each sprint.
Required Education and Experience
Required Education:
Bachelor’s Degree in Computer Science, Computer/Software Engineering, Information Technology or related fields.
Required Experience:
Minimum of one (1) year of Data Engineering/Business Intelligence/Data Warehousing experience, preferably in a healthcare environment.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience supporting and working with cross-functional teams in a dynamic environment.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Previous experience manipulating, processing and extracting value from large disconnected datasets.
Preferred Education, Experience & Cert/Lic
Preferred Education:
Advanced Degree in Computer Science, Informatics, Information Systems or another quantitative field.
Preferred Experience:
Minimum of three (3) years of experience in a Data Engineer role
Additional Technical Requirements
Strong analytic skills related to working with structured and unstructured datasets.
Must possess critical thinking and creative problem solving skills along with the ability to communicate well with stakeholders throughout the organization.
Strong communication, project management and organizational skills.
Proficient in SQL
Exposure to big data tools: Hadoop, Spark, Kafka, BigSQL, Hive, etc.
Experience with relational SQL and NoSQL databases, including IBM PDA (Netezza), MS SQL Server and HBase.
Exposure to data integration tools: Informatica, MS Integration Services, Sqoop, etc.
Exposure to stream-processing systems: IBM Streams, Flume, Storm, Spark-Streaming, etc.
Exposure consuming and building APIs
Exposure to object-oriented/object function programming languages: Python, Java, C++, Scala, etc.
Exposure to statistical data analysis tools: R, SAS, SPSS, etc.
Exposure to visual analytics tools: QlikView, Tableau, Power BI etc.
Familiarity to Agile methodology for development
Familiarity with electronic health record and financial systems.
i.e.
Epic Systems, Cerner, WorkDay, Infor, Strata etc.
In an effort to achieve this goal, employment at Children's Hospital of Philadelphia, other than for positions with regularly scheduled hours in New Jersey, is contingent upon an attestation that the job applicant does not use tobacco products or nicotine in any form and a negative nicotine screen (the latter occurs after a job offer).
Children's Hospital of Philadelphia is an equal opportunity employer.
VEVRAA Federal Contractor/Seeking priority referrals for protected veterans.
Talent Acquisition
2716 South Street, 6th Floor
Philadelphia, PA 19146
Phone: 866-820-9288
Email:TalentAcquisition@email.chop.edu
","['sql', 'spark', 'scala', 'sa', 'spss', 'hadoop', 'bi', 'tableau', 'python', 'hive', 'java', 'nosql', 'hbase', 'r', 'cloud', 'kafka', 'power bi']","['healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'information technology', 'commun']",1,"['sql', 'spark', 'scala', 'sa', 'spss', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'hbase', 'r', 'cloud', 'kafka', 'healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'information technology', 'commun']","['day', 'stream', 'visual', 'predict', 'python', 'integr', 'quantit', 'sourc', 'collect', 'evalu', 'analyt', 'sa', 'hadoop', 'optim', 'action', 'clinic', 'infrastructur', 'bi', 'big', 'set', 'engin', 'particip', 'spark', 'power', 'comput', 'scientist', 'relat']","['sql', 'spark', 'scala', 'sa', 'spss', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'java', 'nosql', 'hbase', 'r', 'cloud', 'kafka', 'healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'big data', 'information technology', 'commun', 'day', 'stream', 'visual', 'predict', 'python', 'integr', 'quantit', 'sourc', 'collect', 'evalu', 'analyt', 'sa', 'hadoop', 'optim', 'action', 'clinic', 'infrastructur', 'bi', 'big', 'set', 'engin', 'particip', 'spark', 'power', 'comput', 'scientist', 'relat']"
DE,"Name: Data Engineer I
Type: Staffing
Status: Pending
Client: Childrens Hospital of Philadelphia
Start: 07/27/2020
End: 01/27/2021
Reason: New project
Department: IS * Clinical Applications & EMR*Philadelphia Campus Job Category: NC-Non Patient Care-IS Job Title: Data Engineer I
Candidate should be passionate about Python coding.
Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) 2.
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills: Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.2.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.2.
Foundational knowledge in Change Control Mgt.
Processes3.
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs1.
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design1.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one's priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
Keywords:
Education: • Bachelor's degree in computer related field required.• 2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience
5+ years of experience with Python
3+ years of experience working with BigData platform, large and complex
3+ file types such as XML, JSON, AVRO, PARQUET, ORC etc.,
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
Skills and Experience:
Required Skills:
PYTHON
Languages:
English
Read
Write
Speak
Patents: No
Publications: No
Veteran Status: No
# of Positions: 1
Philadelphia, PA
Wanamaker Building, 1300 Market Street
Schedule:
Start Date: 07/27/2020
Estimated End Date: 01/27/2021
Hours Per Week: 40.00
Hours Per Day: 8.00
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['day', 'stream', 'pipelin', 'relat', 'visual', 'power', 'public', 'optim', 'python', 'warehous', 'comput', 'clinic', 'integr', 'engin']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'day', 'stream', 'pipelin', 'relat', 'visual', 'power', 'public', 'optim', 'python', 'warehous', 'comput', 'clinic', 'integr', 'engin']"
DE,"Interested candidates will forward current resume to Dan Kuphal at dkuphal@myvoca.com and call 952.303.8110.
Thanks!
Assignment Details
Candidate should be passionate about Python coding.
Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary "" streaming"" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
Qualifications:
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one' s priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
Bachelor’ s degree in computer related field required.
2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience
5+ years of experience with Python
3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
Shift:
Standard, 40 hours/week
IND-IT
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"Candidate should be passionate about Python coding.
Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one's priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
","['sql', 'tableau', 'python', 'powerpoint', 'excel']","['healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",999,"['sql', 'tableau', 'python', 'powerpoint', 'excel', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']","['sql', 'tableau', 'python', 'powerpoint', 'excel', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']"
DE,"Job Summary As part of Damanrsquos Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms.
The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills.
You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies.
You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration.
","['cloud', 'spark']",['commun'],999,"['cloud', 'spark', 'commun']","['analyt', 'spark', 'relat', 'packag', 'warehous', 'integr', 'engin']","['cloud', 'spark', 'commun', 'analyt', 'spark', 'relat', 'packag', 'warehous', 'integr', 'engin']"
DE,"Title: Data Engineer
Type: 6+ Month Contract
Compensation: DOE
3 Must haves:
Python
SQL
Big Data framework
Candidate should be passionate about Python coding.
Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills:
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power BI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
• Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Education:
• Bachelor's degree in computer related field required.
• 2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience
5+ years of experience with Python
3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
","['sql', 'git', 'jira', 'bi', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'power bi']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'big data', 'commun']",1,"['sql', 'git', 'jira', 'powerbi', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'big data', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'bi', 'python', 'warehous', 'comput', 'big', 'integr', 'engin']","['sql', 'git', 'jira', 'powerbi', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'big data', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'bi', 'python', 'warehous', 'comput', 'big', 'integr', 'engin']"
DE,"Candidate should be passionate about Python coding.
Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming "" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
SQL and Database Knowledge Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one's priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
","['sql', 'tableau', 'python', 'powerpoint', 'excel']","['pipelin', 'visual', 'optim', 'data warehousing', 'commun']",999,"['sql', 'tableau', 'python', 'powerpoint', 'excel', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']","['sql', 'tableau', 'python', 'powerpoint', 'excel', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']"
DE,"Title: Data Engineer I
Type: Contract
Candidate should be passionate about Python coding.
Key Accountabilities:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming "" processes.
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Basic knowledge on structured operational processes, conformance with SLAs, and metrics-based reporting.2.
Foundational knowledge in Change Control Mgt.
processes3.
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
SQL and Database Knowledge Understanding SQL, Relational and Multidemensional Databases and Designs.
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design.
Creating/Maintaining Tables, views, & indexes &bull.
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Required Skills:
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one's priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
Preferred Experience:
5+ years of experience with Python.
3+ years of experience working with BigData platform, large and complex.
3+ file types such as XML, JSON, AVRO, PARQUET, ORC etc.
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
10490677
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'account']",999,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'account']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr', 'engin']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'account', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr', 'engin']"
DE,"The Data Science and Engineering team is acquiring, studying, simulating, and modeling to enable data as a key driver and core functional component toward better understanding, predicting, and dynamically optimizing the access network to improve overall user experience.
Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.
What you will will be doing as a Data Engineer:
Heavy Python programming involved in the day-to-day, must be able to hit the ground running with strong Python experience.
Utilizing AWS lambda and API Gateway.
Developing Spark streaming and batch jobs to clean and transform data.
Writing unit and integration tests.
What you will bring to the table as a Data Engineer:
Strong development experience coding in Python - a Python coding challenge will be given in the interview.
Experience transferring data using queries and working inside an Oracle database
Hands-on experience with AWS, specifically Lambda, and Neptune
Experience with Cloud Computing
Nice to have: experience with Kinesis, Kafka, Spark, and Redis
NextGeners are Connectors.
NextGeners are Collaborators.
NextGeners give 24/7 Support.
","['spark', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'oracl']",['predict'],999,"['spark', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'oracl', 'predict']","['day', 'stream', 'spark', 'challeng', 'predict', 'aw', 'python', 'comput', 'integr', 'engin']","['spark', 'aw', 'lambda', 'python', 'cloud', 'kafka', 'oracl', 'predict', 'day', 'stream', 'spark', 'challeng', 'predict', 'aw', 'python', 'comput', 'integr', 'engin']"
DE,"Site Name: UK - London - Brentford, USA - North Carolina - Research Triangle Park, USA - Pennsylvania - Philadelphia
Posted Date: Mar 17 2020
Data Engineer is accountable for developing and delivering cloud-based data ingestion solutions across the Pharma Commercial.
You will be working on fully up to date technologies in the Data & Analytics environment and in a team, which is fully committed to remain at the leading edge of this skill set.
Therefore, the impetus to keep improving skills and acquiring skill sets in new technologies will be very strong.
If you are a top-flight developer who wants to continue to keep learning and remain at the cutting edge of the very fast-moving Data Analytics technology environment, then this role is for you.
This is very hands-on development role which includes developing & delivering code through from origin to production, plus working in partnership with 3rd party development service providers to help ensure that code comes in on time, to quality and in line with the overall ecosystem being established.
The Data engineer will directly contribute to the extensive and varied build and deployment activities involved in establishing the new platform then continue to work on the already significant and growing pipeline of future build-outs on the platform.
This role will provide YOU the opportunity to lead key activities to progress YOUR career, these responsibilities include some of the following
Development: Hands on, sleeves up development and delivery expected as a matter of course.
Delivery: Ensure project goals are achieved on time in alignment with the stakeholders expectation.
Ability to work on complex projects and in a distributed environment.
Escalate to other Data & Analytics leadership team when needing support.
Work in close collaboration with other team members in the CH BI Tech team, to ensure Development/Delivery aspects are well represented in the projects requirements and deliverables.
Methodology: .
Incorporate agile ways of working into the delivery process thru use of DABL (Discovery, Alpha, Beta, Launch) framework to show value periodically.
Individuals will work as part of product-centric delivery team(s) that will focus on delivering value independently while fully embracing integrated DevOps approaches.
Ownership: Take ownership for the delivery/development projects and help steer until completion.
Governance: Follow governance that allows projects and stakeholders to manage overall project performance and manage programme risks within the global nature of some of the programmes.
Forward looking: Remain flexible towards technology approaches to ensure that the best advantage is being taken of new technologies.
Keep abreast of industry developments in analytics and be able to interpret how these would impact services and present new opportunities.
Quality, Risk & Compliance: Ensure all risk and issues associated with owned projects are recorded and managed in the appropriate Risk & Issue logs in a timely manner.
Ensure all Risks and Issues have clear action/mitigation/contingency plans defined, with named action owners and timelines for completion.
Technical Architecture: Be conversant with technical architecture to contribute to design discussions in partnership with the Delivery/Development Director and dedicated Analytics & Data Architect.
Why you?
Basic Qualifications:
MS/BS degree in Computer Science, Engineering, Design or equivalent experience.
6-8 years as a Developer in the Data & Analytics arena with demonstrated expertise in emerging technologies and data technology platforms and management.
Fully conversant with agile and DevOps development methodology and concepts.
Must have worked in CI/CD ways of working using tools like Azure DevOps.
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Ideal candidate would have built an impressive hands-on career to date in an advanced, recognized and innovative environment around Data & Analytics.
Fully conversant with big-data processing approaches and schema-on-read methodologies is a must and knowledge of Azure Data Factory/DataBricks/Azure Data Lake/Azure DW/Analysis Services is highly preferred.
Understanding AWS and GCP cloud platforms is a plus.
Experience of the Azure analytics components, Power BI, Power Apps & Microsoft Visual Studio is desirable.
Good to have an excellent development skills & extensive hand-on development & coding experience in a variety of languages, e.g.
C#, Python, SQL, DAX, etc.
Ability to work in close partnership with other IT functions such as IT security, compliance, infrastructure, etc.
as well as partner closely with business stakeholders in the commercial and digital organizations.
Experience in executing Data Analytics projects in an Agile manner, articulation of Value depending on the project life cycle stage, Creating MVPs, developing plans for scale up are all very important experience to be successful in this role.
Great communication skills and ability to communicate inherently complicated technical concepts to non-technical stakeholders.
These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork.
Agile and distributed decision-making using evidence and applying judgement to balance pace, rigour and risk
Managing individual and team performance.
Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.
Implementing change initiatives and leading change.
Sustaining energy and well-being, building resilience in teams.
Continuously looking for opportunities to learn, build skills and share learning both internally and externally.
Developing people and building a talent pipeline.
Translating strategy into action - a compelling narrative, motivating others, setting objectives and delegation.
Building strong relationships and collaboration, managing trusted stakeholder relationships internally and externally.
Budgeting and forecasting, commercial and financial acumen.
This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Important notice to Employment businesses/ Agencies
For more information, please visit GSKs Transparency Reporting For the Record site.
","['sql', 'gcp', 'azur', 'bi', 'aw', 'python', 'c', 'cloud', 'microsoft', 'excel', 'power bi']","['research', 'pipelin', 'visual', 'risk', 'commun', 'account']",1,"['sql', 'gcp', 'azur', 'powerbi', 'aw', 'python', 'c', 'cloud', 'microsoft', 'excel', 'research', 'pipelin', 'visual', 'risk', 'commun', 'account']","['visual', 'provid', 'python', 'integr', 'analyt', 'challeng', 'azur', 'line', 'action', 'learn', 'infrastructur', 'bi', 'aw', 'parti', 'big', 'set', 'engin', 'digit', 'pipelin', 'power', '3rd', 'comput']","['sql', 'gcp', 'azur', 'powerbi', 'aw', 'python', 'c', 'cloud', 'microsoft', 'excel', 'research', 'pipelin', 'visual', 'risk', 'commun', 'account', 'visual', 'provid', 'python', 'integr', 'analyt', 'challeng', 'azur', 'line', 'action', 'learn', 'infrastructur', 'bi', 'aw', 'parti', 'big', 'set', 'engin', 'digit', 'pipelin', 'power', '3rd', 'comput']"
DE,"Candidate should be passionate about Python coding.
Job Responsibilities 1.
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) 2.
3.
Build automated test pipelines to ensure data integrity and completion.
4.
Identify and improve current data pipelines through automation and optimization.
Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
",['python'],"['optim', 'healthcar', 'pipelin']",999,"['python', 'optim', 'healthcar', 'pipelin']","['stream', 'pipelin', 'relat', 'optim', 'python', 'integr']","['python', 'optim', 'healthcar', 'pipelin', 'stream', 'pipelin', 'relat', 'optim', 'python', 'integr']"
DE,"About the role
On a typical day, you may work on a new feature, develop a new algorithm, help construct a dataset, discover and fix a software bug, message a customer to understand the problem they're trying to solve, or improve your team's tools.
PS: There is no typical day.
You will perform data and concrete analysis with human intuition, build data pipelines and develop tools to enable machine learning models to learn from data.
You will regularly deploy solutions to enable the team to release small, frequent iterations to customers via mobile apps, the web, and embedded systems.
Effective communication is extremely important.
About You
MS in Computer Engineering, Computer Science, Electrical Engineering or equivalent practical experience.
Proficient in Python, C/C++ or Rust
Deep understanding of software systems fundamentals
You are actively interested in developing and deploying AI on a fleet of devices to improve people's shopping experience.
Experience with or exposure to creating highly available, scalable, low-latency, global systems.
A track record of pursuing self directed side projects, research, or open source projects.
Innate curiosity and a desire to explore solutions in a small, highly focused team.
You take pride in your work and ownership of the solutions you build.
Python, C++, Rust
Javascript, React, Angular-JS
TensorFlow, TensorRT
MySQL, MongoDB, Kafka, Kubernetes
AWS, GCP
","['mongodb', 'gcp', 'rust', 'angular', 'javascript', 'aw', 'python', 'c', 'tensorflow', 'mysql', 'kubernet', 'kafka', 'react']","['research', 'commun', 'machine learning', 'pipelin']",999,"['mongodb', 'gcp', 'rust', 'angular', 'javascript', 'aw', 'python', 'c', 'tensorflow', 'sql', 'kubernet', 'kafka', 'react', 'research', 'commun', 'machine learning', 'pipelin']","['day', 'machin', 'pipelin', 'human', 'aw', 'python', 'avail', 'comput', 'sourc', 'engin', 'algorithm']","['mongodb', 'gcp', 'rust', 'angular', 'javascript', 'aw', 'python', 'c', 'tensorflow', 'sql', 'kubernet', 'kafka', 'react', 'research', 'commun', 'machine learning', 'pipelin', 'day', 'machin', 'pipelin', 'human', 'aw', 'python', 'avail', 'comput', 'sourc', 'engin', 'algorithm']"
DE,"Nearly half of U.S. households struggle to handle an unexpected expense of $400 or more.
When it comes to managing household expenses, things like a broken appliance or a growing family can be financially burdensome.
Applicants should be highly motivated and comfortable with taking on and adapting to a diverse array of subject matter.
This opportunity is both unique and pivotal, as it provides the chance to contribute greatly to a rapidly-growing team.Initial Responsibilities* Analyze and resolve complex challenges around data and tools.
* Strong working knowledge of SQL/NoSQL, relational databases and Python is required (2+ years experience)Preferred Qualifications* Knowledge and practical experience in machine learning and AI fundamentals* Experience implementing machine learning solutions at scale* Experience working with both Batch and Real Time data processing systems* Ability to work and communicate effectively with stakeholders.
","['sql', 'python', 'nosql']","['machine learning', 'analyz']",999,"['sql', 'python', 'nosql', 'machine learning', 'analyz']","['machin', 'learn', 'challeng', 'divers', 'python', 'relat']","['sql', 'python', 'nosql', 'machine learning', 'analyz', 'machin', 'learn', 'challeng', 'divers', 'python', 'relat']"
DE,"Overview
Emphasis of this position will be in developing and deploying a robust data processing pipeline and streams.
Responsibilities
Contributes design, configuration, deployment, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, enrichment, and loading of data into a variety of cloud data platforms, including AWS and Microsoft Azure
Identifies gaps and improves the existing platform to improve quality, robustness, maintainability, and speed
Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions
Qualifications
4-6 years of experience in a Data Engineering role
Experience building, maintaining, and improving Data Processing Pipeline / Data routing in large scale environments
Fluency in common query languages, API development, data transformation, and integration of data streams
Strong experience with large dataset platforms such as (e.g.
Amazon EMR, Amazon Redshift, Elasticsearch, Amazon Athena, Azure SQL Database, Azure Database for PostgreSQL, Azure Cosmos DB)
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data flow monitoring, batching, and streaming
Experience working with variety of databases and expert using SQL
Experience data storage performance analysis and enhancements such as data replication, distribution, and compression
Experience with acquiring data from varied sources such as: API, data queues, flat-file, remote databases
Creativity to go beyond current tools to deliver the best solution to the problem
Data QA experience is a plus
Experience with event-based and transaction-based system is a plus
The only predictable thing about life is that it’s wildly unpredictable.
(And people who love snacks.)
","['sql', 'elasticsearch', 'azur', 'postgresql', 'aw', 'cloud', 'redshift', 'db', 'microsoft', 'kafka']","['pipelin', 'predict']",999,"['sql', 'elasticsearch', 'azur', 'postgresql', 'aw', 'cloud', 'redshift', 'db', 'microsoft', 'kafka', 'pipelin', 'predict']","['stream', 'pipelin', 'azur', 'predict', 'aw', 'integr', 'common', 'sourc', 'engin']","['sql', 'elasticsearch', 'azur', 'postgresql', 'aw', 'cloud', 'redshift', 'db', 'microsoft', 'kafka', 'pipelin', 'predict', 'stream', 'pipelin', 'azur', 'predict', 'aw', 'integr', 'common', 'sourc', 'engin']"
DE,"Come see what all the excitement is about.
Your Function:
In addition, the Lead Data Engineer defines and promotes adaptation of best practices, provides consultative guidance and recommendations on pragmatic solutions to solve complex business requirements within the context of one or more digital transformation initiatives.
The various initiatives for the team affects multiple business units, large end-user populations, corporate client delivery, applications, and other areas of solution delivery across the organization.
As such, this position is critical to ensuring the appropriate stakeholders are engaged in evaluating the optimal solution design to meet the complex business needs.
The position requires someone with excellent communications skills (verbal and written), works well under pressure, and effectively work with cross-functional teams throughout a diverse business community.
This position requires extensive knowledge of batch and real-time data pipelines, modern data warehousing, and experience working in an agile organization.
Knowledge of IT processes, supporting technologies, and how these technologies integrate across initiatives and existing environments is a must.
Day to Day:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and enabling data-driven decisions across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate, secure, and available for key stakeholders and business processes that depend on it.
Contributes to engineering communities of practice, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Your Profile:
BS or MS degree in Computer Information Science or related technical field
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus)
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Two working days per year volunteering for a local charity
Health and Wellness program including healthy food, fresh Coffee Plaza, free health checks, fun health & vitality activities
Flexible hours with possibility to work from home (within job scope)
Career development opportunities: online learning, member development programs
Highly competitive benefits
Additional Information:
Hiring subject to successful completion of a background check and drug test.
EOE
","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel']","['recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']",1,"['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']","['day', 'analyt', 'digit', 'program', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun', 'day', 'analyt', 'digit', 'program', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"AIVA works alongside operators, loan officers and lenders to make their jobs more efficient.
Each quarter, she reads millions of documents and then summarizes significant data points for her colleagues to review.
Although she's early career, she already has cut down certain manual tasks from 30 minutes to just 8 minutes!
Her make-up is well beyond mainstream workflow automationRPA.
There is room for a range of skills.
Some you already have and some you will quickly gain when you are here.
Agile (Scaled Agile Framework) Machine Learning ( Natural Language Processing ,Vision , Classification , Search) DevOps ( Infrastructure as Code , Continuous Integration and Continuous Delivery) Behavioral Driven Development Design and Architecture Cloud (AWS) Languages (Java , AngularJS , Python) GENERAL DUTIES RESPONSIBILITIES Participates in project meetings with other technical staff, business owners and subject matter experts.
Designs scalable, highly available, fault tolerant and resilient data processing infrastructure, assembled from microservices in a Continuous Integration Continuous Delivery environment.
Builds streaming and batch data extraction transformation and load processes using AWS Serverless technologies.
Interacts with product managers andor users to define system requirements andor necessary modifications.
Assesses and develops design requirements for the project and communicates in writing or in meetings with development team while assessing detailed specifications against design requirements.
Develops andor reviews development of test protocols for testing application before user acceptance.
Reviews application in progress of development to ensure compliance with overall design parameters and corporate development standards.
Verify stability, interoperability, portability, security, or scalability of system architecture.
Monitor system operation to detect potential problems.
Document design specifications, installation instructions, and other system-related information.
Performs additional related duties as assigned.
EDUCATIONAL GUIDELINES A Bachelor's degree in Computer Engineering, Computer Science or other related discipline or equivalent combination of education and experience that is required for the specific job level.
GENERAL KNOWLEDGE, SKILLS ABILITIES Experience building and optimizing 'big data' data pipelines, architectures and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Experience with build processes supporting data transformation, data structures, metadata, dependency and workload management Experience with object-orientedobject function scripting languages Python, Java, C++, Scala, etc.
Experience with big data tools such as Hadoop, Spark, Kafka, etc.
strongly preferred Experience with relational SQL and NoSQL databases, including Postgres and Cassandra preferred Experience with data pipeline and workflow management tools such as Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services S3, Glue, Athena, Kinesis, DynamoDB, Sagemaker, EMR, RDS, Redshift a plus Experience with stream-processing systems such as Storm, Spark-Streaming, etc.
a plus Strong analytic skills related to working with unstructured datasets A successful history of manipulating, processing and extracting value from large disconnected datasets Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores Experience building production quality cloud products preferred Experience with AWS , DevOPs , CICD preferred Knowledge of financial services industry a plus Knowledge of banking practices, regulations and operations within the assigned line(s) of business a plus Outstanding verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors) Excellent analytical, decision-making, problem-solving, team, and time management skills Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed Positive outlook, strong work ethic, and responsive to internal and external clients and contacts Willingly and successfully fulfills the role of teacher, mentor and coach JOB FAMILY LEVEL Intermediate professional role.
Proficient in at least two higher-level programming languages and knowledge of at least one systems development life cycle model.
Understands the products, services, practices, regulations and operations associated with the assigned line of business.
Conducts detailed analyses of all defined systems specifications for changes in systems requirements, business requirements or equipment configurations, and develops all levels of logic flow charts.
Develops and prepares moderately complex computer programs, prepares program test data, tests and debugs programs.
Documents all procedures used throughout the computer program when it is formally established.
Receives general supervision and is competent in most phases of programming to work on own, and requires only some general direction for the balance of the activities.
May assist and help train Entry-level software engineers.
Typically requires five (5) or more years of software engineering work experience or an equivalent combination of education and experience.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status.
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'postgr', 'kafka', 'excel']","['classif', 'natural language processing', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun']",1,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'kafka', 'excel', 'classif', 'nlp', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun']","['program', 'stream', 'python', 'integr', 'analyt', 'line', 'hadoop', 'avail', 'learn', 'corpor', 'infrastructur', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'kafka', 'excel', 'classif', 'nlp', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun', 'program', 'stream', 'python', 'integr', 'analyt', 'line', 'hadoop', 'avail', 'learn', 'corpor', 'infrastructur', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"71635
Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.
Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience:
Strong problem solving skills and critical thinking ability.
Strong collaboration and communication skills within and across teams.
5 or more years of progressively complex related experience.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Ability to understand complex systems and solve challenging analytical problems.
Experience with bash shell scripts, UNIX utilities & UNIX Commands.
Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Experience building data transformation and processing solutions.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
from you via e-mail.
Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter.
","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",1,"['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'provid', 'optim', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"Data Engineer, Ad Technology
Data Engineer, Ad Tech
Level
Experienced
Philadelphia - Philadelphia, PA
Position Type
Full Time
Job Category
Information Technology
Job Title: Data Engineer, Ad Technology
Overview:
You will champion infrastructure-as-code and test driven development, building out the workings of modern data transformation & data science pipelines.
Your Python chops are impressive, your SQL skills even more so.
You prefer a bash terminal to a spreadsheet and cannot fathom life without source control.
You take pride in your uncanny ability to productionize anything, be it a Python script, a Jupyter Notebook or a SQL model.
You live for those two little words: “ship it”.
What’s Special About this Role:
Spend your time writing software and solving business problems, not sitting in endless meetings.
Work to improve products that make real people’s lives better.
Use the best tools out there, and sometimes write even better ones.
Engage with a team that wants to do big things fast, and values a healthy work-life balance while doing them.
What You Do:
Build and automate infrastructure using deployable code
What You Bring:
Develop in Java/Python engineering critical, large scale distributed services
Experience with Kafka, Spark, Hadoop
Experience in the AdTech / advertising / online identity space.
Prior experience with integrations with the adtech ecosystem including DMPs, third party measurement firms like ComScore, Crossix, Experian.
Prior experience creating and working with device and user identity graphs
Nice to Haves:
Experience with display ads: Facebook Ads, DFP/DFA
Experience with marketing/remarketing, etc.
AWS microservices suite
Snowflake
Alooma
Fivetran
DBT
Airflow
SnowShu
All team members - from executives to managers to people in entry-level positions - sit together in an open office and enjoy a fully integrated, collaborative and supportive work environment.
All team members enjoy free personal training classes, a full supply of La Colombe coffee, frequent on-site social events, Summer Fridays, and regular complimentary family-style lunches.
Learn more at www.health-union.com.
All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, disability status, age, protected veteran status or any other characteristic protected by law.
Successful candidates must pass the E-Verify process after hire.
Applicants must be legally eligible to work in the U.S for any employer.
*Candidates only.
No direct calls or emails.
No recruiters/agencies, please.
","['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka']","['information technology', 'graph', 'pipelin']",999,"['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka', 'information technology', 'graph', 'pipelin']","['learn', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'parti', 'big', 'integr', 'sourc', 'engin']","['sql', 'spark', 'airflow', 'jupyt', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'kafka', 'information technology', 'graph', 'pipelin', 'learn', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'aw', 'python', 'parti', 'big', 'integr', 'sourc', 'engin']"
DE,"You should have a deep passion for understanding and solving data issues and bring logic, enthusiasm and an analytical approach to work issues.
RESPONSIBILITIES
Drive engineering efforts to design, develop, and test data-driven solutions
Drive the optimization, testing and tooling that will improve data quality
Partner with engineering, product and operations teams to design tech solutions
Utilize CI/CD throughout the development, automation and testing lifecycle
Deliver updates in different backend components and
Contribute strategy and tactics to the long-term roadmap for enterprise data strategy
REQUIREMENTS
3+ years(TM) experience building large scale big data applications in a business setting
Excellent SQL and database knowledge
Ability to pull data from disparate sources, (RDBMS, Excel), and deliver to Cloud data lake
Hands-on experience with Python, Ruby, Scala, and/or Java.
Knowledge/experience using AWS, Snowflake or other Cloud technology
Proficient with Linux commands and environments
Demonstrated ability to learn new languages.
Bachelor(TM)s degree in an analytic or technical field, (Math, Stat, Computer Science, etc.)
PREFERENCES
Hands-on experiences with data processing, reporting or analysis tools
Experiences with Hadoop/Spark/Kafka/Presto/etc.
","['sql', 'linux', 'spark', 'scala', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'kafka', 'excel', 'rubi']","['optim', 'math', 'big data']",1,"['sql', 'linux', 'spark', 'scala', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'kafka', 'excel', 'rubi', 'optim', 'math', 'big data']","['analyt', 'spark', 'hadoop', 'optim', 'aw', 'python', 'comput', 'big', 'sourc', 'engin']","['sql', 'linux', 'spark', 'scala', 'hadoop', 'aw', 'snowflak', 'python', 'java', 'cloud', 'kafka', 'excel', 'rubi', 'optim', 'math', 'big data', 'analyt', 'spark', 'hadoop', 'optim', 'aw', 'python', 'comput', 'big', 'sourc', 'engin']"
DE,"Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.2.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store client information.3.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.2.
Foundational knowledge in Change Control Mgt.
processes3.
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidemensional Databases and Designs1.
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design1.
CreatingMaintaining Tables, views, indexes bull Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain.
Soft Skills Comfortable working in a collaborative environment.
Ability to self-organize one's priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
Keywords Education bull Bachelor's degree in computer related field required.bull 2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex 3+ file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment.
Skills and Experience Required Skills PYTHON
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"Data EngineerData Engineer
You will support the engineering team's data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once.
You'll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.
• Work with internal stakeholders to load data into the data warehouse
• Troubleshoot and resolve issues relating to data integrity
• Help establish procedures and best practices for transforming and storing data
• Lead requirements gathering around data pipeline automation improvements
• Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
• Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
• Enjoy the peace that comes with working in a mature software development environment
• Marvel at the speed with which your creation makes it into production
• Research and implement new technologies with a team of developers to execute strategies and implement solutions
• Produce peer reviewed quality software
• Solve complex problems related to the real-time discovery of large data
• Experienced in writing scalable applications on distributed architectures
• Data driven, testing and measuring as much as you can
• Eager to both review peer code and have your code reviewed
• Comfortable on the command line and consider it an essential tool
• Confident in SQL, you know it, write smart queries
Required skills and experience
• 5+ years of work experience
• 3+ years of experience with Python
• 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
• 1+ years of experience with AWS EMR, AWS S3 service.
Comfortable using AWS CLI and boto3
• Comfortable working in remote environments
• Comfortable using *nix command line (shell scripting, AWK, SED)
• Experience with MySQL and Postgres
Desired experience
• Experience with Apache Airflow
• Experience with Apache Zeppelin
• Experience with healthcare data
Interested candidates please send resume in Word format Please reference job code 528434 when responding to this ad.
Job Requirements:
","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'postgr', 'mysql', 'docker']","['research', 'healthcar', 'pipelin']",999,"['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin']","['essenti', 'spark', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin', 'essenti', 'spark', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Data Engineer in Philadelphia, PA 19103
Interview Logistics:
WebEx Interview
Required Skills Set:
Years of Experience:4+
Education Required:Bachelors Degree
BS/MS degree in Computer Science, Mathematics, or other relevant science and engineering discipline.
4+ years working as a software engineer.
2+ years working within an enterprise data lake/warehouse environment or big data architecture.
Excellent programming skills with experience in at least one of Python, Scala, Java, Node.js.
Great communication skills.
Proficiency in testing frameworks and writing unit/integration tests
Proficiency in Unix-based operating systems and bash scripts.
Additional Preferred Skills:
Experience with working in Spark
Experience with AWS
Experience with monitoring and visualization tools such as Grafana, Prometheus, Data Dog, and Cloudwatch.
Experience with NoSQL databases, such as DynamoDB, MongoDB, Redis,Cassandra, or HBase
Join a multi-disciplinary team of devops engineers, software engineers, data analysts, and data scientists working together to improve the user experience.
Do you likebigchallenges and working within ahighly motivatedteam environment?As a data software engineer on the Data Science and Engineering team within the organization, you will be part of a team that thrives onbigchallenges, results, quality, and agility.
Who does the Data Engineer work with?
What are some interesting problems you'll be working on?
Expose services over REST APIs.
Work closely with various engineering teams to solve key optimization, insight and access network data challenges.
Where can you make an impact?
The Data Science and Engineering teamis acquiring, studying, simulating, and modeling to enable data as a key driver and core functional component toward better understanding, predicting, and dynamically optimizing the access network to improve overall user experience.
Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.
Responsibilities:
Developing REST APIs utilizing AWS lambda and API Gateway.
Developing Spark streaming and batch jobs to clean and transform data.
Writing build automation to deploy and manage cloud resources.
Writing unit and integration tests.
Programming Languages (Python, Scala, Golang, Node.js)
Build Environment: GitHub Enterprise, Concourse CI, Jira, Serverless, SAM
Cloud Computing (AWS Lambda, EC2, ECS)
Spark(AWS EMR, Databricks)
Stream Data Platforms: Kinesis, Kafka
Databases: S3, MySQL, Oracle,MongoDB, DynamoDB
Caching Frameworks (ElasticCache/Redis)
Physical Environment and Working Conditions:
Must be able to work on site in Philadelphia after coronavirus restrictions are lifted.
","['jira', 'python', 's3', 'java', 'nosql', 'hbase', 'kafka', 'oracl', 'unix', 'lambda', 'mongodb', 'cassandra', 'scala', 'ec2', 'aw', 'mysql', 'excel', 'spark', 'cloud', 'github']","['visual', 'predict', 'optim', 'big data', 'logist', 'commun']",1,"['jira', 'python', 's3', 'java', 'nosql', 'hbase', 'kafka', 'oracl', 'unix', 'lambda', 'mongodb', 'cassandra', 'scala', 'ec2', 'aw', 'sql', 'excel', 'spark', 'cloud', 'github', 'visual', 'predict', 'optim', 'big data', 'logist', 'commun']","['stream', 'spark', 'challeng', 'set', 'visual', 'predict', 'optim', 'aw', 'python', 'warehous', 'scientist', 'comput', 'big', 'integr', 'engin']","['jira', 'python', 's3', 'java', 'nosql', 'hbase', 'kafka', 'oracl', 'unix', 'lambda', 'mongodb', 'cassandra', 'scala', 'ec2', 'aw', 'sql', 'excel', 'spark', 'cloud', 'github', 'visual', 'predict', 'optim', 'big data', 'logist', 'commun', 'stream', 'spark', 'challeng', 'set', 'visual', 'predict', 'optim', 'aw', 'python', 'warehous', 'scientist', 'comput', 'big', 'integr', 'engin']"
DE,"Candidate are to be passionate about Python coding.
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store client information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidimensional Databases and Designs Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design CreatingMaintaining Tables, views, indexes Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain.
Education Bachelorrsquos degree in computer related field required.
2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment.
USTECH was founded in 2000 by Manoj Agarwal.
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"Hands on experience in developing big data pipelines end to end
· 5-8 years of Java and Python experience
· Experience in software development of largescale distributed systems including proven track record of delivering backend systems that participate in a complex ecosystem
· Experience working with imperfect data sets that, at times, will require improvements to process, definition and collection
· Experience with real-time data pipelines and components including Kafka, Spark Streaming
· Proficient in Unix/Linux environments
· AWS experience developing data streaming pipelines
· Deep Spark understanding
· Must Have Skills : Spark, SQL,Kinesis/Kafka, python for scripting on AWS and Java for APIs
","['sql', 'linux', 'spark', 'unix', 'aw', 'java', 'python', 'kafka']","['pipelin', 'big data']",999,"['sql', 'linux', 'spark', 'unix', 'aw', 'java', 'python', 'kafka', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'aw', 'python', 'big', 'set', 'collect']","['sql', 'linux', 'spark', 'unix', 'aw', 'java', 'python', 'kafka', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'aw', 'python', 'big', 'set', 'collect']"
DE,"DA Krasner believes that justice is best achieved through policies grounded in research, data, and science.
Holding true to these commitments, the DAO has improved its capabilities to process data, craft policy, and conduct research.
The Data Engineer will join an innovative team of researchers, lawyers, analysts, and software engineers in the development and deployment of applications to support prosecution, analytics, data-driven policy, and public transparency.
They will support DA Krasners targeted areas for informed reform, including charging, diversion, sentencing, focusing on the most serious crimes, preventing violence, enhancing harm reduction, and ameliorating disparities in the criminal justice process.
The Data Engineer will be expected to:
Build and maintain the DAO data warehouse to house both internally created data, administrative data, and other data from external partners about the criminal justice system and the welfare of individuals associated with the criminal justice system.
Maintain a cloud-based development environment for a team of analysts to work in
Evangelize and educate teams across the DAO on standard methodologies around data services development, data privacy and security, as well as improving efficiency
Qualifications
A Bachelors or Masters degree in Computer Science, Software Engineering, Mathematics or related technical discipline is preferred, but not required.
Additional years of experience exceeding the minimum requirement as well as a proven track record of work may stand in for a degree.
The preferred candidate will also have or demonstrate:
Ability to professionally deal with changing environments, be self-directed; and given broad direction, be able to prioritize work and allocate time and resources effectively;
Experience with or knowledge of: web application security concerns and solutions to mitigate those concerns;
Familiarity with Bitbucket, Git, or similar version control systems;
Experience with developing and maintaining custom and non-proprietary software based data platform infrastructure services
Experience with a systems language such as C, C++, C#, Go, Java or Scala
Experience with a scripting language such as Python, PHP, or Ruby
Experience developing data solutions using storage platforms like S3, Cassandra, Hbase, and using data warehouse systems like Snowflake, Redshift, Cosmos DB, or BigQuery
Experience developing applications and solutions using data ecosystem components such as Spark, Flink, Solr, Redis, InfluxDB, Elasticsearch
Experience working with cloud services such as AWS, Google Cloud, Azure
Experience with building and integrating logging, alerting, and monitoring as an integral part of data platform services
Enthusiasm to collaborate, innovate, and solve problems;
A proven ability to prioritize work in a team and independently, and handle multiple complex tasks simultaneously;
Experience working in the legal field/with attorneys and a knowledge of the criminal justice system are both a plus.
","['python', 's3', 'redshift', 'java', 'c', 'php', 'hbase', 'rubi', 'azur', 'bigqueri', 'git', 'snowflak', 'google cloud', 'elasticsearch', 'cassandra', 'scala', 'aw', 'spark', 'solr', 'cloud', 'db']",['research'],1,"['python', 's3', 'redshift', 'java', 'c', 'php', 'hbase', 'rubi', 'azur', 'bigqueri', 'git', 'snowflak', 'google cloud', 'elasticsearch', 'cassandra', 'scala', 'aw', 'spark', 'solr', 'cloud', 'db', 'research']","['analyt', 'spark', 'azur', 'handl', 'relat', 'divers', 'public', 'infrastructur', 'aw', 'python', 'comput', 'warehous', 'integr', 'engin']","['python', 's3', 'redshift', 'java', 'c', 'php', 'hbase', 'rubi', 'azur', 'bigqueri', 'git', 'snowflak', 'google cloud', 'elasticsearch', 'cassandra', 'scala', 'aw', 'spark', 'solr', 'cloud', 'db', 'research', 'analyt', 'spark', 'azur', 'handl', 'relat', 'divers', 'public', 'infrastructur', 'aw', 'python', 'comput', 'warehous', 'integr', 'engin']"
DE,"Expose services over REST APIs and work closely with various engineering teams to solve key optimization, insight and access network data challenges.
Develop large scale data pipelines exposing data sources Develop REST APIs utilizing AWS lambda and API Gateway.
Develop Spark streaming and batch jobs to clean and transform data.
Write build automation to deploy and manage cloud resources and unit and integration tests.
Qualifications Requirements Strong python development experience required Experiences with Oracle database, especially in AWS RDS, Cloud Computing (AWS Lambda, EC2, ECS), and Spark (AWS EMR, Databricks).
BSMS degree in Computer Science, Mathematics, or other relevant science and engineering discipline.
4+ years working as a software engineer and 2+ years working within an enterprise data lakewarehouse environment or big data architecture.
Proficiency in testing frameworks and writing unitintegration tests and in Unix-based operating systems and bash scripts.
Preferred Additional Skills Experience with working in Spark Experience with AWS Experience with monitoring and visualization tools such as Grafana, Prometheus, Data Dog, and Cloudwatch.
Experience with NoSQL databases, such as DynamoDB, MongoDB, Redis, Cassandra, or HBase If interested, please email LEltringhamjudge.com with an updated resume!
Thanks!
","['mongodb', 'spark', 'cassandra', 'unix', 'ec2', 'aw', 'lambda', 'python', 'cloud', 'hbase', 'nosql', 'oracl']","['optim', 'visual', 'pipelin', 'big data']",2,"['mongodb', 'spark', 'cassandra', 'unix', 'ec2', 'aw', 'lambda', 'python', 'cloud', 'hbase', 'nosql', 'oracl', 'optim', 'visual', 'pipelin', 'big data']","['stream', 'spark', 'challeng', 'pipelin', 'visual', 'optim', 'aw', 'python', 'comput', 'big', 'integr', 'sourc', 'engin']","['mongodb', 'spark', 'cassandra', 'unix', 'ec2', 'aw', 'lambda', 'python', 'cloud', 'hbase', 'nosql', 'oracl', 'optim', 'visual', 'pipelin', 'big data', 'stream', 'spark', 'challeng', 'pipelin', 'visual', 'optim', 'aw', 'python', 'comput', 'big', 'integr', 'sourc', 'engin']"
DE,"How you will help
You will support the engineering team’s data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once.
You’ll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.
What you will do
Troubleshoot and resolve issues relating to data integrity
Help establish procedures and best practices for transforming and storing data
Lead requirements gathering around data pipeline automation improvements
Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
Enjoy the peace that comes with working in a mature software development environment
Marvel at the speed with which your creation makes it into production
Research and implement new technologies with a team of developers to execute strategies and implement solutions
Produce peer reviewed quality software
Solve complex problems related to the real-time discovery of large data
About you
You are...
Experienced in writing scalable applications on distributed architectures
Data driven, testing and measuring as much as you can
Eager to both review peer code and have your code reviewed
Comfortable on the command line and consider it an essential tool
Confident in SQL, you know it, write smart queries, it’s no big deal
Required skills and experience
5+ years of work experience
3+ years of experience with Python
3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
1+ years of experience with AWS EMR, AWS S3 service.
Comfortable using AWS CLI and boto3
Comfortable working in remote environments
Comfortable using *nix command line (shell scripting, AWK, SED)
Experience with MySQL and Postgres
Desired experience
Experience with Apache Airflow
Experience with Apache Zeppelin
Experience with healthcare data
Empowering clients with highly rewarding data discovery and licensing tools
Ingesting and managing billions of healthcare records from a wide variety of partners
Standardizing on common data models across data types
Orchestrating an industry-leading HIPAA privacy layer
Building a culture that supports rapid iteration and new possibilities
You must be able to think big while still delivering on near-term requirements.
","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'postgr', 'mysql', 'docker']","['research', 'healthcar', 'pipelin']",999,"['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin']","['essenti', 'spark', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'big', 'integr', 'common', 'sourc', 'engin']","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin', 'essenti', 'spark', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'big', 'integr', 'common', 'sourc', 'engin']"
DE,"suggest, prototype and deploy machine learning solutions.
expose your models as service interfaces for the application integration.
build data pipelines for sourcing data from disparate sources which later can be analyzed, cleaned and transformed for building intelligent systems.
Primary Requirements:
• Strong and proven software development skills
• Proficient in Python programming language
• Should be able to build interfaces for collecting data, data extraction, transformations through Python
• Should be able to work with data through SQL - Preparing data, cleanup, Aggregating, Slicing/Dicing Data
• Should have worked in Python Stats, Machine learning packages - Pandas, Data frames, Scipy, NumPy, StatsModel, Scikit packages.
Familiarity with frameworks such as MLlib, H2O, TensorFlow or similar.
• Should have experience in working with Kafka using Python or Java API
• Should have worked with a NoSQL database.
Should be able to build interfaces for serving data-in/data-out of the database through API.
• Preferred Telecom and Billing
• Experience working in an Agile organization and understanding of Agile fundamentals, JIRA with CI/CD
• Excellence at formulating, understanding, and solving complex, non-routine problems
• Exposure in working with Big Data; experience with Hadoop, Kafka, and Spark preferred
• Familiarity with managed cloud-based options for building machine learning models
","['sql', 'spark', 'numpi', 'panda', 'mllib', 'h2o', 'jira', 'hadoop', 'cloud', 'python', 'java', 'scipi', 'nosql', 'tensorflow', 'scikit', 'kafka', 'excel']","['pipelin', 'analyz', 'machine learning', 'clean', 'big data']",999,"['sql', 'spark', 'numpi', 'panda', 'mllib', 'h2o', 'jira', 'hadoop', 'cloud', 'python', 'java', 'scipi', 'nosql', 'tensorflow', 'scikit', 'kafka', 'excel', 'pipelin', 'analyz', 'machine learning', 'clean', 'big data']","['machin', 'learn', 'spark', 'pipelin', 'hadoop', 'primari', 'python', 'packag', 'big', 'integr', 'sourc']","['sql', 'spark', 'numpi', 'panda', 'mllib', 'h2o', 'jira', 'hadoop', 'cloud', 'python', 'java', 'scipi', 'nosql', 'tensorflow', 'scikit', 'kafka', 'excel', 'pipelin', 'analyz', 'machine learning', 'clean', 'big data', 'machin', 'learn', 'spark', 'pipelin', 'hadoop', 'primari', 'python', 'packag', 'big', 'integr', 'sourc']"
DE,"Although the role says SAS to python conversion, but look for more of big data hive, python AWS, Machine Learning, Artificial intelligence kind of profiles on this
Role: Data Engineer
Duration: 07 Months +
VISA: No H1B & OPT & CPT
SAS to Python conversion,
HIVE, Python,
AWS,
Big Data Technologies,
Machine Learning, Artificial intelligence
","['aw', 'sa', 'python', 'hive']","['machine learning', 'big data']",999,"['aw', 'sa', 'python', 'hive', 'machine learning', 'big data']","['machin', 'learn', 'sa', 'look', 'aw', 'python', 'big', 'engin']","['aw', 'sa', 'python', 'hive', 'machine learning', 'big data', 'machin', 'learn', 'sa', 'look', 'aw', 'python', 'big', 'engin']"
DE,"Country:
United States
Cities:
Boston, Hartford, New York City, Philadelphia
Area of expertise:
Analytics
Love living in the cloud?
As an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions.
You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools.
You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.
Day-to-day, you will:
• Translate business requirements to technical solutions using strong business insight.
• Analyzes current business practices, processes and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
• Support the planning and implementation of data design services, providing sizing and configuration assistance, and performing needs assessments.
• Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
• Design and Build Modern Data Pipelines and Data Streams.
• Design and Build Data Service APIs.
• Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
• Expose data to end users using Power BI, Azure API Apps or other modern visualization platform or experience.
• Implement effective metrics and monitoring processes.
• Able to travel approximately 80%
Your technical/non-technical skills include:
• Demonstrable experience of turning business use cases and requirements to technical solutions.
• Experience in business processing mapping of data and analytics solutions.
• Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
• T-SQL is required.
• Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.
• Experience preparing data for Data Science and Machine Learning.
• Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
• Strong collaboration ethic and experience working with remote teams.
• Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
(nice to have)
• Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
(nice to have)
• Knowledge of Lambda and Kappa architecture patterns.
(nice to have)
Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field.
An equivalent combination of education and experience will also suffice.
Preferred Years of Work Experience:
You likely have about 3-5+ years of relevant professional experience.
• 14-time winner of Microsoft Partner of the Year
• 24,000+ certifications in Microsoft technology
• 90+ Microsoft partner awards
• 17 Gold Competencies
• 3,500 analytics professionals worldwide
• 1,000 data engineers
• Implemented analytics systems for more than 550 clients
• 400 AI practitioners
• 300 cognitive service experts
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['sql', 'azur', 'bi', 'lambda', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']",1,"['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']","['day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']","['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology', 'day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']"
DE,"This is a contract opportunity.
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
PYTHON DATA ENGINEER JOB RESPONSIBILITIES Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
PYTHON DATA ENGINEER JOB REQUIREMENTS Bachelorrsquos degree in computer related field required.
2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment.
Basic knowledge on structured operational processes, conformance with SLAs, and metrics-based reporting.
Foundational knowledge in Change Control Mgt.
processes Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidimensional Databases and Designs Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design CreatingMaintaining Tables, views, indexes Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain.
PYTHON DATA ENGINEER PREFERRED JOB REQUIREMENTS 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment.
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr', 'engin']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr', 'engin']"
DE,"Position:
Data Engineer II
AIVA works alongside operators, loan officers and lenders to make their jobs more efficient.
Each quarter, she reads millions of documents and then summarizes significant data points for her colleagues to review.
Although she's early career, she already has cut down certain manual tasks from 30 minutes to just 8 minutes!
Her make-up is well beyond mainstream workflow automation/RPA.
There is room for a range of skills.
Some you already have and some you will quickly gain when you are here.
Agile (Scaled Agile Framework)
Machine Learning ( Natural Language Processing ,Vision , Classification , Search)
DevOps ( Infrastructure as Code , Continuous Integration and Continuous Delivery)
Behavioral Driven Development
Design and Architecture
Cloud (AWS)
Languages (Java , AngularJS , Python)
GENERAL DUTIES & RESPONSIBILITIES
Participates in project meetings with other technical staff, business owners and subject matter experts.
Designs scalable, highly available, fault tolerant and resilient data processing infrastructure, assembled from microservices in a Continuous Integration Continuous Delivery environment.
Builds streaming and batch data extraction transformation and load processes using AWS Serverless technologies.
Interacts with product managers and/or users to define system requirements and/or necessary modifications.
Assesses and develops design requirements for the project and communicates in writing or in meetings with development team while assessing detailed specifications against design requirements.
Develops and/or reviews development of test protocols for testing application before user acceptance.
Reviews application in progress of development to ensure compliance with overall design parameters and corporate development standards.
Verify stability, interoperability, portability, security, or scalability of system architecture.
Monitor system operation to detect potential problems.
Document design specifications, installation instructions, and other system-related information.
Performs additional related duties as assigned.
EDUCATIONAL GUIDELINES
A Bachelor's degree in Computer Engineering, Computer Science or other related discipline; or equivalent combination of education and experience that is required for the specific job level.
GENERAL KNOWLEDGE, SKILLS & ABILITIES
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience with build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Experience with big data tools such as Hadoop, Spark, Kafka, etc.
strongly preferred
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra preferred
Experience with data pipeline and workflow management tools such as Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: S3, Glue, Athena, Kinesis, DynamoDB, Sagemaker, EMR, RDS, Redshift a plus
Experience with stream-processing systems such as Storm, Spark-Streaming, etc.
a plus
Strong analytic skills related to working with unstructured datasets
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
Experience building production quality cloud products preferred
Experience with AWS , DevOPs , CI/CD preferred
Knowledge of banking practices, regulations and operations within the assigned line(s) of business a plus
Outstanding verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors)
Excellent analytical, decision-making, problem-solving, team, and time management skills
Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed
Positive outlook, strong work ethic, and responsive to internal and external clients and contacts
Willingly and successfully fulfills the role of teacher, mentor and coach
JOB FAMILY LEVEL
Intermediate professional role.
Proficient in at least two higher-level programming languages and knowledge of at least one systems development life cycle model.
Understands the products, services, practices, regulations and operations associated with the assigned line of business.
Conducts detailed analyses of all defined systems specifications for changes in systems requirements, business requirements or equipment configurations, and develops all levels of logic flow charts.
Develops and prepares moderately complex computer programs, prepares program test data, tests and debugs programs.
Documents all procedures used throughout the computer program when it is formally established.
Receives general supervision and is competent in most phases of programming to work on own, and requires only some general direction for the balance of the activities.
May assist and help train Entry-level software engineers.
Typically requires five (5) or more years of software engineering work experience or an equivalent combination of education and experience.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status.
Philadelphia, PA
Time Type:
Full time
","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'postgr', 'kafka', 'excel']","['classif', 'natural language processing', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun']",1,"['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'kafka', 'excel', 'classif', 'nlp', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun']","['program', 'stream', 'python', 'integr', 'analyt', 'line', 'hadoop', 'avail', 'learn', 'corpor', 'infrastructur', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'spark', 'airflow', 'cassandra', 'scala', 'sagemak', 'hadoop', 'aw', 'cloud', 'python', 's3', 'redshift', 'nosql', 'java', 'kafka', 'excel', 'classif', 'nlp', 'pipelin', 'machine learning', 'big data', 'supervis', 'commun', 'program', 'stream', 'python', 'integr', 'analyt', 'line', 'hadoop', 'avail', 'learn', 'corpor', 'infrastructur', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Data EngineerLocation: Philadelphia, PA Full-time/Permanent position Summary: You will support the engineering team’s data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once.
Comfortable using AWS CLI and boto3 -Comfortable working in remote environments -Comfortable using *nix command line (shell scripting, AWK, SED) -Experience with MySQL and Postgres Desired experience -Experience with Apache Airflow -Experience with Apache Zeppelin -Experience with healthcare data
","['mysql', 'aw', 'postgr', 'airflow']",['healthcar'],999,"['sql', 'aw', 'airflow', 'healthcar']","['aw', 'line', 'engin']","['sql', 'aw', 'airflow', 'healthcar', 'aw', 'line', 'engin']"
DE,"Your career starts now.
Were looking for the next generation of health care leaders.
If you want to make a difference, wed like to hear from you.
Under the direction of the VP, Chief Analytics Officer this position will oversee the Business Analysis, Data Engineering and Automation and Business Intelligence functions within Enterprise Analytics.
This role will be responsible for oversight of building, deploying and testing of Business Intelligence (BI) systems that integrate with databases, data warehouses, data marts and data lake.
This position will be responsible for dashboards and/or reports according to the identified technical requirements in addition to being responsible for performance tuning, version controls and documentation of those dashboards/reports.
Position will collaborate closely with internal and external stakeholders across the organization to develop sustainable solutions that support data-driven decision making while acting as a champion for data and analytics services.
Will lead a team of Tableau developers, Business Analysts to support a common data and analytics platform integrating new and existing data sources to eliminate silos.
Responsibilities:
Oversee solutions/framework for provisioning data for self-service consumption by business users who create BI applications, as well as supporting any type of adhoc reporting/analytic requests.
Define, build and execute the Data Services and technology roadmap ensuring that it is in alignment with the Application, Infrastructure and Business roadmaps.
Drive build and automation of tableau reports; ensure requirements are successfully executed and issues are resolved timely.
Very strong tableau and data warehousing expertise required
Proactively identify opportunities and technologies regarding the access, processing, visual display and analysis of data; maintain a pulse on data visualization and engineering trends.
Collaborates with internal departments to understanding their evolving data needs and develop and execute on analytics to meet those needs while driving towards improved outcomes and overall performance metrics.
Independently complete analysis, development and enhancement of BI solutions to fulfill business needs.
Maintain in-depth knowledge of project planning methodologies and tools as well as IT standards and guidelines.
Serve as a thought leader for technical business processes, developing forward-thinking prototypes that promote increased efficiency and productivity on multiple levels.
Education/ Experience:
Bachelors Degree (Computer Science or related field preferred required; Masters' degree preferred
Need strong Business Intelligence and Analytical skills; Tableau is preferred.
Power BI and Qlikview is a plus.
8-10 years experience in analytics, preferably in the health care industry.
Minimum of 5 years people management experience.
Other Skills:
Analytical mindset focused on process and data analysis; problem solving aptitude.
Some experience of relevant data visualization, including but not limited to experience with visualizations and web-based interactives, geospatial, and/or visual story telling preferred.
Superior organizational, close attention to detail and written/oral communication skills.
Superior technical writing skills in business requirements, queries, reports, and presentations.
Superior technical skills in Tableua Excel and PowerPoint with the ability to learn other analytic tools.
Advanced analytical and quantitative skills with experience collecting, organizing, mining, analyzing, visualizing and disseminating abundant information with the utmost accuracy and presentation.
Experience in conducting and documenting QA testing using problem solving techniques for analytic and reporting solutions.
Efficiently manages time-based on the continual evaluation of priorities, meets deadlines with high-quality deliverable reflecting complete understanding of expectations, able to multi-task.
Back
Share
","['bi', 'tableau', 'powerpoint', 'excel', 'power bi']","['dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']",1,"['powerbi', 'tableau', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']","['analyt', 'techniqu', 'relat', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']","['powerbi', 'tableau', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati', 'analyt', 'techniqu', 'relat', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']"
DE,"Interested candidates will forward current resume to Dan Kuphal at dkuphal@myvoca.com and call 952.303.8110.
Thanks!
Assignment Details
Exhibits the ability to assist in client requirements assessment, solution set designs, coding, testing and implementation.
Demonstrates advanced SQL knowledge, relational and multidimensional models, and business intelligence delivery tools.
Demonstrates the ability to modify existing architecture to solve complex problems.
Recommends and establishes conventions and standards for all technical areas related to data storage, transformation and aggregations.
Participates in selection of new technologies, and consults on requirements.
Acts as a technical escalation point or SME for a particular aspect of the environment, assisting with complex problems and solutions.
Has an advanced understanding of technical environment and integration points, including all software, hardware, and supporting environments.
Demonstrates basic knowledge of project management concepts and may act as an implementation lead for department initiatives.
Demonstrates the ability in creating detailed documentation, including project plans, requirements status reports and operations documentation.
Exhibits ability to clearly articulate problems, issues, requirements and potential solutions to team members and clients.
Works with analysts to identify and understand source data systems
Demonstrates the ability to serve as a resource to cross- functional work teams
Exhibits the ability to guide associate Information Architects and provide teaching support to clients, Operations, and Help Desk as needed.
Knowledgeable in Research Administration (Finance, Pre/Post Award, Compliance) workflow including Project Accounting, Time & Effort & supported technologies
Qualifications:
Information Security Requirements
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Experience in the development or implementation of structured operational processes, conformance with SLAs, and metrics based reporting.
Intermediate knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Moderate to advanced experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Analytics
Cube dimensions (creating/maintaining translations, attribute relations, hierarchies)
Dimension usage: reference dimensions, many to many relationships, fact relationships, role-playing relationships granularity
Creating and maintaining data source views and reporting models
Hypothesis development, design test/experiments, & developing actionable recommendations
Statistical modeling techniques (logistic regression, log linear regression, etc...)
Advanced experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Education:
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Bachelor’ s degree in computer/Analytics/Data Science related field.
3-6 years of Business Intelligence/Data Warehousing experience, preferably in a healthcare & research environment.
Shift:
Standard, 40 hours/week
IND-IT
","['sql', 'powerpoint', 'excel', 'tableau']","['recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account']",1,"['sql', 'powerpoint', 'excel', 'tableau', 'recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account']","['analyt', 'techniqu', 'set', 'relat', 'visual', 'power', 'optim', 'comput', 'statist', 'action', 'warehous', 'integr', 'sourc', 'linear', 'particip']","['sql', 'powerpoint', 'excel', 'tableau', 'recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account', 'analyt', 'techniqu', 'set', 'relat', 'visual', 'power', 'optim', 'comput', 'statist', 'action', 'warehous', 'integr', 'sourc', 'linear', 'particip']"
DE,"Candidate should be passionate about Python coding.
Job Responsibilities:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming "" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Requirements:
Bachelor's degree in computer related field required.
2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.
-MUST HAVE PYTHON experience
Preferred experience
5+ years of experience with Python
3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"The client and the team yoursquoll be working in are great and offer positive challenges in a dynamic environment.
Specific Responsibilities Design and develop Extraction, Transformation and Loading (ETL) processes using standard ETL tools to streamline and automate the collection of data from source systems using SQL, Snowflake and AWS technologies Assemble large, complex data sets that meet functional non-functional business requirements Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Marketing, CRM, Finance, Operations, Product, and Development teams to assist with data-related technical issues and support their data infrastructure needs.
Develop repeatable and scalable data quality audits Requirements Design and develop Extraction, Transformation and Loading (ETL) processes using standard ETL tools to streamline and automate the collection of data from source systems using SQL, Snowflake and AWS technologies Assemble large, complex data sets that meet functional non-functional business requirements Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Marketing, CRM, Finance, Operations, Product, and Development teams to assist with data-related technical issues and support their data infrastructure needs.
Develop repeatable and scalable data quality audits Requirements Bachelorrsquos Degree in Computer Science, MIS or related field 5+ years Advanced SQL knowledge and experience working with relational databases, query authoring (T-SQL) as well as working familiarity with a variety of databases.
Able to write advanced SQL queries with multi-table joins, group functions, subqueries, set operations, functions, Stored Procedures and basic Java Scripting.
2+ years of Experience with Snowflake Cloud Datawarehouse Platform 2+ Years Experience with object-orientedobject function in one of scripting languages (Python, PySpark) Experience with AWS DatabasesTools a plus (S3, Glue, Lambda) Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong understanding of data modeling and data warehousing principles Excellent documentation and general organization of ongoing tasks, including the ability to evaluate and question business rules.
This is an urgent opportunity and the client committed to moving quick when they identify the right person.
","['sql', 'pyspark', 'aw', 'snowflak', 'java', 'python', 's3', 'lambda', 'cloud', 'excel']","['data warehousing', 'etl', 'data modeling', 'financ']",1,"['sql', 'pyspark', 'aw', 'snowflak', 'java', 'python', 's3', 'lambda', 'cloud', 'excel', 'data warehousing', 'etl', 'data modeling', 'financ']","['challeng', 'set', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'etl', 'sourc', 'collect']","['sql', 'pyspark', 'aw', 'snowflak', 'java', 'python', 's3', 'lambda', 'cloud', 'excel', 'data warehousing', 'etl', 'data modeling', 'financ', 'challeng', 'set', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'etl', 'sourc', 'collect']"
DE,"Candidate should be passionate about Python coding.
bullBuild automated test pipelines to ensure data integrity and completion.
bullIdentify and improve current data pipelines through automation and optimization.
Skills bullBasic experience and proven use of one or more of the subject areas listed below bullSQL and Database Knowledge - Understanding SQL, Relational and Multidemensional Databases and Designs bullKnowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization bullModerate experience and proven use of one or more of the subject areas listed below bullTableau, Qliksense, Power PI, or any other data visualization application.
bullData Warehouse Support and Design bullCreatingMaintaining Tables, views, indexes bullProficiency in appropriate Business IntelligenceData Warehousing technology or subject domain Soft Skills bullComfortable working in a collaborative environment.
bullAbility to self-organize one's priorities and schedule.
bullHave mindset to perform necessary documentation.
bullShould be a self-starter to work independently or in a team.
All applicants applying for U.S. job openings must be authorized to work in the United States.
All applicants applying for Canadian job openings must be authorized to work in Canada.
An Equal Opportunity Employer MFDisabilityVeterans.
","['sql', 'python']","['optim', 'data warehousing', 'visual', 'pipelin']",999,"['sql', 'python', 'optim', 'data warehousing', 'visual', 'pipelin']","['pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']","['sql', 'python', 'optim', 'data warehousing', 'visual', 'pipelin', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'warehous', 'integr']"
DE,"Country:
United States
Cities:
Boston, Hartford, New York City, Philadelphia, Providence
Area of expertise:
Analytics
14-time winner of Microsoft Partner of the Year
24,000+ certifications in Microsoft technology
90+ Microsoft partner awards
17 Gold Competencies
3,500 analytics professionals worldwide
1,000 data engineers
Implemented analytics systems for more than 550 clients
400 AI practitioners
300 cognitive service experts
About you:
You design data solutions that enable clients to see the whole picture and provide insightful and accurate analysis that helps to build successful businesses.
About the job:
As a Manager, Data Engineering, you use modern data engineering techniques and Advanced Analytics methods to give your clients the information they need.
You collect, aggregate, store and reconcile data from various sources, helping to design and build data pipelines, streams, reporting tools, data generators and a whole range of tools to provide information and insight.
You know how to read the patterns and trends that influence business outcomes.
Day-to-day, you will:
Give colleagues and clients the tools to find and use data for routine and non-routine analysis
Use your sound eye for business to translate business requirements into technical solutions
Analyze current business practices, processes and procedures to spot future opportunities
Assess client needs to build bespoke data design services
Build the building blocks for transforming enterprise data solutions
Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs)
Craft the architectures, data warehouses and databases that support access and Advanced Analytics, and bring them to life through modern visualization tools
Implement effective metrics and monitoring
Be comfortable to make your own decisions and guide your colleagues
Travel as needed to various client locations
Your skills and business experience include:
Transforming business needs into technical solutions
Mapping data and analytics
Data profiling, cataloguing and mapping to enable the design and build of technical data flows
Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration
Knowledge of multiple Azure data applications including Azure Databricks
Experience in preparing data for and building pipelines and architecture
You probably have a Bachelors or Master’s degree in a quantitative field such as computer science, applied mathematics, statistics or machine learning – or an equivalent combination of education and experience.
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['microsoft', 'azur']","['pipelin', 'visual', 'machine learning', 'statist', 'analyz']",1,"['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz']","['day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']","['microsoft', 'azur', 'pipelin', 'visual', 'machine learning', 'statist', 'analyz', 'day', 'program', 'techniqu', 'stream', 'visual', 'provid', 'quantit', 'integr', 'sourc', 'analyt', 'azur', 'appli', 'statist', 'learn', 'warehous', 'engin', 'machin', 'pipelin', 'comput']"
DE,"Come see what all the excitement is about.
Your Function:
In addition, the Lead Data Engineer defines and promotes adaptation of best practices, provides consultative guidance and recommendations on pragmatic solutions to solve complex business requirements within the context of one or more digital transformation initiatives.
The various initiatives for the team affects multiple business units, large end-user populations, corporate client delivery, applications, and other areas of solution delivery across the organization.
As such, this position is critical to ensuring the appropriate stakeholders are engaged in evaluating the optimal solution design to meet the complex business needs.
The position requires someone with excellent communications skills (verbal and written), works well under pressure, and effectively work with cross-functional teams throughout a diverse business community.
This position requires extensive knowledge of batch and real-time data pipelines, modern data warehousing, and experience working in an agile organization.
Knowledge of IT processes, supporting technologies, and how these technologies integrate across initiatives and existing environments is a must.
Day to Day:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and enabling data-driven decisions across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate, secure, and available for key stakeholders and business processes that depend on it.
Contributes to engineering communities of practice, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Your Profile:
BS or MS degree in Computer Information Science or related technical field
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus)
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Two working days per year volunteering for a local charity
Health and Wellness program including healthy food, fresh Coffee Plaza, free health checks, fun health & vitality activities
Flexible hours with possibility to work from home (within job scope)
Career development opportunities: online learning, member development programs
Highly competitive benefits
Additional Information:
Hiring subject to successful completion of a background check and drug test.
EOE
","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel']","['recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']",1,"['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']","['day', 'analyt', 'digit', 'program', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun', 'day', 'analyt', 'digit', 'program', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Position: SQL Data Engineer
Job Type: Contract to Hire
Duration: 4 Months
Exhibits the ability to assist in client requirements assessment, solution set designs, coding, testing and implementation.
Demonstrates advanced SQL knowledge, relational and multidimensional models, and business intelligence delivery tools.
Demonstrates the ability to modify existing architecture to solve complex problems.
Recommends and establishes conventions and standards for all technical areas related to data storage, transformation and aggregations.
Participates in selection of new technologies, and consults on requirements.
Acts as a technical escalation point or SME for a particular aspect of the environment, assisting with complex problems and solutions.
Has an advanced understanding of technical environment and integration points, including all software, hardware, and supporting environments.
Demonstrates basic knowledge of project management concepts and may act as an implementation lead for department initiatives.
Demonstrates the ability in creating detailed documentation, including project plans, requirements status reports and operations documentation.
Exhibits ability to clearly articulate problems, issues, requirements and potential solutions to team members and clients.
Works with analysts to identify and understand source data systems
Demonstrates the ability to serve as a resource to cross- functional work teams
Exhibits the ability to guide associate Information Architects and provide teaching support to clients, Operations, and Help Desk as needed.
Knowledgeable in Research Administration (Finance, Pre/Post Award, Compliance) workflow including Project Accounting, Time & Effort & supported technologies
Skills
Information Security Requirements
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Experience in the development or implementation of structured operational processes, conformance with SLAs, and metrics based reporting.
Intermediate knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Moderate to advanced experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Analytics
Cube dimensions (creating/maintaining translations, attribute relations, hierarchies)
Dimension usage: reference dimensions, many to many relationships, fact relationships, role-playing relationships granularity
Creating and maintaining data source views and reporting models
Hypothesis development, design test/experiments, & developing actionable recommendations
Statistical modeling techniques (logistic regression, log linear regression, etc...)
Advanced experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Education
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Bachelor’ s degree in computer/Analytics/Data Science related field.
3-6 years of Business Intelligence/Data Warehousing experience, preferably in a healthcare & research environment.
","['sql', 'powerpoint', 'excel', 'tableau']","['recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account']",1,"['sql', 'powerpoint', 'excel', 'tableau', 'recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account']","['analyt', 'techniqu', 'engin', 'set', 'relat', 'visual', 'power', 'optim', 'comput', 'statist', 'action', 'warehous', 'integr', 'sourc', 'linear', 'particip']","['sql', 'powerpoint', 'excel', 'tableau', 'recommend', 'research', 'healthcar', 'hypothesi', 'hardwar', 'visual', 'regress', 'optim', 'data warehousing', 'statist', 'financ', 'logist', 'commun', 'account', 'analyt', 'techniqu', 'engin', 'set', 'relat', 'visual', 'power', 'optim', 'comput', 'statist', 'action', 'warehous', 'integr', 'sourc', 'linear', 'particip']"
DE,"Proficient in Big Data application development skills as well as multiple design
techniques
Working proficiency in Big Data development toolset to
design, develop, test, deploy, maintain and improve
software
Strong understanding of Agile methodologies with
ability to work in at least one of the common
frameworks
Strong understanding of techniques such as
Continuous Integration, Continuous Delivery, Test
Driven Development, Cloud Development,
application resiliency and security
Proficiency in one or more general purpose
programming languages
Working proficiency in a portion of software
engineering disciplines and demonstrates
understanding of overall software skills including
business analysis, development, testing,
deployment, maintenance and improvement of
software
Dress Codes:
Casual
Roles and Responsibilities: Qualifications
"" Bachelor's or Advanced Degree in Information Management, Computer Science, Mathematics, Statistics, or related fields desired
"" Financial Services background or experience preferred across different LOBs.
Proven proficiency with data analysis and ability to troubleshoot data issues.
""
Proficiency across the full range of big data technologies and/or database and business intelligence tools; publishing and presenting information in an engaging way
"" Intensive, recent experience in assessing and sourcing data needs
"" Detail oriented with a commitment to innovation
Knowledge/Technical Skills
"" Strong experience in Big Data Hadoop stack Hive, Impala, Spark, Spark/SQL
"" Experience in Python, Java, Spark development is highly desired.
""
Experience with MPP databases like Teradata or Oracle is preferred.
""
Experience in Big Data technologies like Pig, Kafka is preferred
"" Exposure to machine learning is preferred
"" Exposure to ETL tools ( ABINTIO, INFORMATICA, DataStage or Others) a plus.
""
Good communication skills and ability to interact with users, make presentations and guide conversations.
""
Experience with scheduling & data integration tools like Control-M and Ni-Fi is highly desired.
""
Strong exposure in Data Management, Governance and Controls functions
"" Experience in data analysis, validation and reporting is a plus.
""
Ability to present complex information in an understandable and compelling mannr
Either skills or additional skills are required
Skills:
Category
Name
Required
Importance
Level
Last Used
Experience
No items to display.
Additional Skills: Role
the Data Engineer will be part of the core group responsible for the end to end data management on the Big Data platform.
The Data Engineer will be responsible for data ingestion, data validation, code development on various big data technologies, data analysis troubleshooting and user interactions working in an agile team.
Responsibilities
"" Effectively partner with the Data and Analytics team to understand their data requirements, work on data ingestion, analysis, validation, transformation etc.
""
Partner closely with various analytics projects to provide value from a data engineering perspective.
""
Provide support for development work supporting the ingestion framework, SQL and anything else needed for generating validations.
""
Support & monitor various data feeds as required for the project.
""
Experience in working in Agile/Scrum framework and participate in all ceremonies and deliver
Remarks:
","['sql', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'python', 'kafka', 'oracl']","['machine learning', 'statist', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'python', 'kafka', 'oracl', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['analyt', 'machin', 'program', 'techniqu', 'learn', 'spark', 'relat', 'hadoop', 'provid', 'python', 'comput', 'statist', 'big', 'etl', 'common', 'engin', 'integr']","['sql', 'spark', 'pig', 'hadoop', 'cloud', 'java', 'hive', 'python', 'kafka', 'oracl', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'analyt', 'machin', 'program', 'techniqu', 'learn', 'spark', 'relat', 'hadoop', 'provid', 'python', 'comput', 'statist', 'big', 'etl', 'common', 'engin', 'integr']"
DE,"Site Name: USA - North Carolina - Research Triangle Park, USA - Pennsylvania - Philadelphia
Posted Date: Jun 3 2020
Are you looking to expand your expertise as a Data Engineer within a global environment that allows you to keep pace with the speed of change?
As a Data Engineer - Enterprise Data Analytics Platform you will be in a fast-paced and entrepreneurial environment where you will be handling multiple concurrent projects while working independently and in teams.
This role will provide YOU the opportunity to lead key activities to progress YOUR career, these responsibilities include some of the following
Build robust end-to-end systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Handle multiple functions / roles for the projects / Agile teams
Work with established standards across the team and organization
Understand complex multi-tier, multi-platform systems
Contribute to building a framework of a significant complexity
Work with internal team of data engineers (both full-time associates and/or third party resources)
Why you?
Basic Qualifications:
Bachelors Degree
3 years coding, or at least 3 years experience in data warehousing or at least 3 years in unstructured data environments
1 years experience in Azure cloud technologies Azure
1 years experience in big data technologies (Cassandra, , HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, Zookeeper, or similar)
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Master's Degree in fields such as Computer Science, Computer Engineering, Data Science, related discipline or military experience
Agile experience is preferred
1+ years experience in Azure cloud technologies and other cloud data platform like AWS, MapR, Cloudera, Google Cloud
Comprehensive understanding of data ingestion and preparation processes.
Experience in semantic data store (Azure SQL DW/Snowflake, Cosmos DB), PowerBI, Azure Databricks, Data Ingestion/Acquisition/Prep (Azure ADF, StreamSets, Talend) are desirable.
Experience building and operating in a regulated environment (GxP, Sarbanes-Oxley)
Experience with API and Microservices technologies/architecture
Understanding of IAM and authorization (RBAC and ABAC)
Understanding of container technologies (Docker/Kubernetes), streaming technologies (Kafka), and automated testing patterns and tools a plus
5+ years experience with NoSQL implementation (Mongo, Cassandra, etc.
a plus)
5+ years experience developing Java based software solutions
5+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)
5+ years experience developing software solutions to solve complex business problems
5+ years experience with Relational Database Systems and SQL
5+ years experience designing, developing, and implementing ETL
5+ years experience with UNIX/Linux including basic commands and shell scripting
Familiarity with data science tools and concepts
Experience in industry, i.e.
code committer, published, white paper, etc.
These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork.
Operating at pace and agile decision-making using evidence and applying judgement to balance pace, rigour and risk.
Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.
Continuously looking for opportunities to learn, build skills and share learning.
Sustaining energy and well-being.
Building strong relationships and collaboration, honest and open conversations.
Budgeting and cost-consciousness.
This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Important notice to Employment businesses/ Agencies
For more information, please visit GSKs Transparency Reporting For the Record site.
","['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'azur', 'unix', 'hadoop', 'javascript', 'snowflak', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'aw', 'spark', 'cloud', 'db', 'powerbi']","['research', 'risk', 'data warehousing', 'big data', 'etl', 'account']",1,"['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'azur', 'unix', 'hadoop', 'javascript', 'snowflak', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'aw', 'spark', 'cloud', 'db', 'powerbi', 'research', 'risk', 'data warehousing', 'big data', 'etl', 'account']","['analyt', 'spark', 'challeng', 'azur', 'handl', 'relat', 'hadoop', 'aw', 'python', 'parti', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'linux', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'azur', 'unix', 'hadoop', 'javascript', 'snowflak', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'aw', 'spark', 'cloud', 'db', 'powerbi', 'research', 'risk', 'data warehousing', 'big data', 'etl', 'account', 'analyt', 'spark', 'challeng', 'azur', 'handl', 'relat', 'hadoop', 'aw', 'python', 'parti', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"The client and the team yoursquoll be working in are great and offer positive challenges in a dynamic environment.
In addition, the Lead Data Engineer defines and promotes adaptation of best practices, provides consultative guidance and recommendations on pragmatic solutions to solve complex business requirements within the context of one or more digital transformation initiatives.
The various initiatives for the team affects multiple business units, large end-user populations, corporate client delivery, applications, and other areas of solution delivery across the organization.
As such, this position is critical to ensuring the appropriate stakeholders are engaged in evaluating the optimal solution design to meet the complex business needs.
The position requires someone with excellent communications skills (verbal and written), works well under pressure, and effectively work with cross-functional teams throughout a diverse business community.
This position requires extensive knowledge of batch and real-time data pipelines, modern data warehousing, and experience working in an agile organization.
Knowledge of IT processes, supporting technologies, and how these technologies integrate across initiatives and existing environments is a must.
Day to Day Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and enabling data-driven decisions across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate, secure, and available for key stakeholders and business processes that depend on it.
Contributes to engineering communities of practice, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
This is an urgent opportunity and the client committed to moving quick when they identify the right person.
position is able to work on a salary + benefit basis.
",['excel'],"['recommend', 'pipelin', 'optim', 'data warehousing', 'commun']",999,"['excel', 'recommend', 'pipelin', 'optim', 'data warehousing', 'commun']","['day', 'basi', 'analyt', 'digit', 'challeng', 'pipelin', 'corpor', 'relat', 'divers', 'optim', 'avail', 'integr', 'sourc', 'engin']","['excel', 'recommend', 'pipelin', 'optim', 'data warehousing', 'commun', 'day', 'basi', 'analyt', 'digit', 'challeng', 'pipelin', 'corpor', 'relat', 'divers', 'optim', 'avail', 'integr', 'sourc', 'engin']"
DE,"Overview
The Senior Data Engineer works closely with the Director of Engineering in a small, cross-functional team to develop a one-of-a-kind, native sports betting experience.
Successful candidates for this role will leverage data from the largest casino chain in the United States to assist in tasks such as affinity, personalization, bonusing, promotions, etc.
Your daily responsibilities include
Design a big data stack and data processing infrastructure and platform
Improve data validation and data quality monitoring
Work with client and backend teams to ensure appropriate logging
Optimize and tune the databases to improve performance and reduce cost
To be successful in this position it will require the following skill set
Minimum 5+ years of experience building large scale, cost effective and robust data processing platforms
Strong experience in database schema design and data modeling for analytics purposes
Experience with Hadoop, Redshift, Pentaho, MySQL, etc.
Experience with AWS and/or Azure cloud
Good understanding of data security and encryption
Ability to work effectively as part of a small team, with strong interpersonal and communication skills
BONUS POINTS
BSc or MSc in Computer Science or another STEM field
A passion for sports or betting
Familiarity with Kotlin
Something to leave you with
","['azur', 'hadoop', 'aw', 'redshift', 'mysql', 'pentaho']","['optim', 'data modeling', 'commun', 'big data']",999,"['azur', 'hadoop', 'aw', 'redshift', 'sql', 'pentaho', 'optim', 'data modeling', 'commun', 'big data']","['analyt', 'azur', 'hadoop', 'infrastructur', 'optim', 'aw', 'comput', 'big', 'set', 'engin']","['azur', 'hadoop', 'aw', 'redshift', 'sql', 'pentaho', 'optim', 'data modeling', 'commun', 'big data', 'analyt', 'azur', 'hadoop', 'infrastructur', 'optim', 'aw', 'comput', 'big', 'set', 'engin']"
DE,"Duration: 6+ months
Role: Data Governance Expert
Requirements:
• 10+ years of work experience with at least 3+ years of experience in Data engineering/system migration (Amazon Web Services)
• Experience designing business processes to enable enterprise data management, building metadata artifacts like business glossaries, data catalogs, data dictionaries, etc
• Data Systems Integration Knowledge and Experience is must to have
• Scripting knowledge with Python or Spark is essential
• Should have Solution Design Experience
• Data Analysis / SQL skills are critical
• Working knowledge of data management concepts including modeling, ETL, quality, governance, master data management, metadata management, etc.
• Working knowledge of the technologies that are required to support Data Governance Practices
• Working knowledge of Data Quality Tools, Data Discovery Tools, Sensitive Data Management, Collibra Catalog, Lineage, CollibraConnect is a plus
• AWS, Collibra certifications preferred
The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.
The Group reported 2018 global revenues of EUR 13.2 billion.
","['sql', 'spark', 'aw', 'python', 'amazon web services']","['machine learning', 'etl']",2,"['sql', 'spark', 'aw', 'python', 'machine learning', 'etl']","['essenti', 'machin', 'learn', 'spark', 'etl', 'aw', 'python', 'integr', 'engin']","['sql', 'spark', 'aw', 'python', 'machine learning', 'etl', 'essenti', 'machin', 'learn', 'spark', 'etl', 'aw', 'python', 'integr', 'engin']"
DE,"Candidate should be passionate about Python coding.
Job Responsibilites 1.
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) 2.
3.
Build automated test pipelines to ensure data integrity and completion.
4.
Identify and improve current data pipelines through automation and optimization.
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
2.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information.
3.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General 1.
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
2.
Foundational knowledge in Change Control Mgt.
processes 3.
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidemensional Databases and Designs 1.
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design 1.
CreatingMaintaining Tables, views, indexes bull Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain.
Education bull Bachelorrsquos degree in computer related field required.
bull 2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment.
Thanks for your support.
P Go Green!
Print this email only when necessary.
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"Data Engineer II
The Opportunity
Were building a data pipeline that merges the tested design of data warehousing and the latest big data technologies.
You will be designing and building data projects while securing core data elements.
Responsibilities
· Research new technology and share knowledge with team
· Implement and release data applications in AWS using big data technologies
· Assist development efforts across the organization
· Understand and adhere to development standards and processes
· Other duties as assigned
What someone will need to be successful in this role
· 2 years of hands on Apache Spark experience (PySpark preferred)
· 5-7 years of hands on Business Intelligence / Data Warehousing experience
· 2 years of experience building high-performance algorithms in scalable languages such as Scala, Python and R
· Excellent interpersonal, verbal and written communication skills
· Strong logical, analytical, problem solving and reporting skills
· Familiarity with Agile and Scrum methodologies
· AWS Elastic Map Reduce (EMR) experience
· Able to use version control (git) and other build, packaging & release management tools
· Passionate about developing quality products
Employee Benefits
· 18 PTO days + 2 floating holidays & 10 paid holidays per year
· Generous tuition reimbursement towards a Masters or Bachelors degree
· 401K match up to 6%
· 12 weeks of 100% paid paternity/maternity leave
· Mentorship with industry professionals
","['pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel']","['research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun']",1,"['pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel', 'research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun']","['day', 'analyt', 'spark', 'pipelin', 'aw', 'python', 'releas', 'big', 'employe', 'engin', 'algorithm']","['pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel', 'research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun', 'day', 'analyt', 'spark', 'pipelin', 'aw', 'python', 'releas', 'big', 'employe', 'engin', 'algorithm']"
DE,"This drive helps each organization use technology, management, and insight to turn ideas into action.
Specific responsibilities for the Senior Data Engineer position include:
Develop data solutions using data storage, integration, and pipeline platforms such as HDFS, Spark, Hive, Impala, Cassandra, Informatica, Ab Initio, SQL Server, Oracle, Python, and Kafka
Design/develop data models and consolidate data into them of various types/formats from various sources
Optimize data solutions for the right mix of performance, reliability, and maintainability
Collaborate with Quality Assurance resources and systems administrators to debug code and ensure the timely delivery of products
Prepare documentation conveying design and support information
Convey complex technical information in a clear, concise manner to all levels of stakeholders, including executives, business users, IT, application developers, and customers
Participate in requirements gathering sessions with business and technical staff
Fully understand clients’ business philosophies and IT Strategies.
Recommend process improvements to increase efficiency and reliability of data solutions
Qualifications
Specific qualifications for the Senior Data Engineer, Analytics position include:
Bachelor's Degree in Computer Science, MIS, or equivalent combination of education and experience preferred.
Strong SQL and Python skills
Experience optimizing data solutions for strong performance
Development experience with Hadoop and related platforms
Development experience with database platforms (Redshift, Snowflake, MS or Azure SQL, Teradata, Oracle, etc.)
Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system
Preferred - development experience with programming languages
Preferred - development experience with Unix tools and shell scripts preferred
Minimum of 5 years’ experience designing, developing, and testing software aligned with defined requirements
Experience with version control (Git, TFS, JIRA, etc.)
and test driven development
Exposure to Business Intelligence tools such as Tableau, Power BI, Qlik, Domo, Birst, Business Objects, SSRS, Cognos, MicroStrategy, etc.
Additional Information
All CapTechers can keep their hands-on technology no matter what position they hold.
Competitive salary with performance-based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career
Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
All positions include the possibility of travel.
","['sql', 'cogno', 'jira', 'ssr', 'python', 'redshift', 'kafka', 'oracl', 'azur', 'tf', 'unix', 'git', 'hadoop', 'snowflak', 'hive', 'qlik', 'cassandra', 'bi', 'tableau', 'power bi', 'spark']","['recommend', 'graph', 'pipelin', 'optim', 'etl']",1,"['sql', 'cogno', 'jira', 'ssr', 'python', 'redshift', 'kafka', 'oracl', 'azur', 'tf', 'unix', 'git', 'hadoop', 'snowflak', 'hive', 'qlik', 'cassandra', 'powerbi', 'tableau', 'spark', 'recommend', 'graph', 'pipelin', 'optim', 'etl']","['program', 'python', 'etl', 'integr', 'sourc', 'analyt', 'azur', 'hadoop', 'optim', 'action', 'bi', 'concis', 'engin', 'particip', 'spark', 'pipelin', 'power', 'comput', 'relat']","['sql', 'cogno', 'jira', 'ssr', 'python', 'redshift', 'kafka', 'oracl', 'azur', 'tf', 'unix', 'git', 'hadoop', 'snowflak', 'hive', 'qlik', 'cassandra', 'powerbi', 'tableau', 'spark', 'recommend', 'graph', 'pipelin', 'optim', 'etl', 'program', 'python', 'etl', 'integr', 'sourc', 'analyt', 'azur', 'hadoop', 'optim', 'action', 'bi', 'concis', 'engin', 'particip', 'spark', 'pipelin', 'power', 'comput', 'relat']"
DE,"The combined worldwide revenue of independent member firms is $3.6 billion.
* Create and maintain optimal data pipeline architecture* Assemble large, complex data sets that meet functional / non-functional business requirements* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
* Experience with big data tools: Hadoop, Spark, Kafka, etc.
* Query authoring (SQL) as well as working familiarity with a variety of databases* Experience building and optimizing 'big data' data pipelines, architectures and data sets* Deep experience with AWS cloud services: EC2, EMR, RDS, Redshift* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement* Strong analytic skills related to working with unstructured datasets* Build processes supporting data transformation, data structures, metadata, dependency and workload management* Experience with relational SQL and NoSQL databases, including PostgreSQL and Cassandra* A successful history of manipulating, processing and extracting value from large disconnected datasets* Working knowledge of message queuing, stream processing and highly scalable 'big data' data stores* Experience with stream-processing systems: Storm, Spark-Streaming, etc.
* Strong project management and organizational skills required* Experience supporting and working with cross-functional teams in a dynamic environment* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow preferred
","['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'big', 'set']","['sql', 'spark', 'airflow', 'cassandra', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'nosql', 'postgresql', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'aw', 'big', 'set']"
DE,"This drive helps each organization use technology, management, and insight to turn ideas into action.
Recommend process improvements to increase efficiency and reliability of data solutionsQualificationsSpecific qualifications for the Senior Data Engineer, Analytics position include:* Bachelor's Degree in Computer Science, MIS, or equivalent combination of education and experience preferred.
* Strong SQL and Python skills* Experience optimizing data solutions for strong performance* Development experience with Hadoop and related platforms* Development experience with database platforms (Redshift, Snowflake, MS or Azure SQL, Teradata, Oracle, etc.
)* Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system* Preferred - development experience with programming languages* Preferred - development experience with Unix tools and shell scripts preferred* Minimum of 5 years' experience designing, developing, and testing software aligned with defined requirements* Experience with version control (Git, TFS, JIRA, etc.)
and test driven development* Exposure to Business Intelligence tools such as Tableau, Power BI, Qlik, Domo, Birst, Business Objects, SSRS, Cognos, MicroStrategy, etc.Additional InformationWe offer challenging and impactful jobs with professional career paths.
All CapTechers can keep their hands-on technology no matter what position they hold.
","['sql', 'qlik', 'azur', 'cogno', 'tf', 'unix', 'jira', 'git', 'bi', 'snowflak', 'python', 'redshift', 'ssr', 'tableau', 'hadoop', 'oracl', 'power bi']","['recommend', 'etl', 'graph']",1,"['sql', 'qlik', 'azur', 'cogno', 'tf', 'unix', 'jira', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'ssr', 'tableau', 'hadoop', 'oracl', 'recommend', 'etl', 'graph']","['analyt', 'program', 'challeng', 'azur', 'relat', 'power', 'hadoop', 'bi', 'python', 'comput', 'action', 'etl', 'engin']","['sql', 'qlik', 'azur', 'cogno', 'tf', 'unix', 'jira', 'git', 'powerbi', 'snowflak', 'python', 'redshift', 'ssr', 'tableau', 'hadoop', 'oracl', 'recommend', 'etl', 'graph', 'analyt', 'program', 'challeng', 'azur', 'relat', 'power', 'hadoop', 'bi', 'python', 'comput', 'action', 'etl', 'engin']"
DE,"LOC_1300_MKT-Wanamaker Building Req ID: 57161
Shift: Days
Employment Status: Regular - Full Time
Job Summary
The ideal candidate has been exposed to all aspects of data from multiple complex sources who enjoys optimizing data systems and building them from the ground up.
They will also support non-technical colleagues in the collection and appropriate use of clinical and non-clinical data.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Job Responsibilities
Data Modeling – evaluate structured and unstructured data, determine the most appropriate schema for new fact tables, data marts, etc.
Validate data to ensure quality.
Reporting – collaborate with colleagues across the enterprise to scope requests.
Extract data from various data sources, validate results, create relevant data visualizations, and share with requester.
Develop dashboards and automate refreshes as appropriate.
Governance / Best Practices – adhere and contribute to enterprise data governance standards.
Also educates and supports colleagues in best practices to ensure that data is used appropriately.
Product Ownership – collaborate and act as the voice of the customer to offer concrete feedback and project requests as well as an advocate for analytics from within the business units themselves.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including ground, hybrid cloud, and cloud) using SQL and various programming technologies.
Develop analytics tools that utilize data resources to provide actionable insights, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Clinical, and Analyst teams to assist with data-related technical issues and support their data infrastructure needs.
Develop optimized tools for analytics and data scientist team members that assist them in building and optimizing projects into an innovative industry leader.
Proficient at integrating predictive and prescriptive models into applications and processes.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Make recommendations about platform adoption, including technology integrations, application servers, libraries, and frameworks.
Participate in a shared production on-call support model.
Be a critical part of an Agile Scrum software development team, ensuring the team successfully meets its deliverables each sprint.
Required Licenses, Certifications, Registrations
One (1) Certifications or proficiency in appropriate Data Science/Data Integration/Data Warehousing technology or subject domain.
Required Education and Experience
Required Education:
Bachelor’s Degree in Computer Science, Computer/Software Engineering, Information Technology or related fields.
Required Experience:
Minimum of three (3) years of Data Engineering/Business Intelligence/Data Warehousing experience, preferably in a healthcare environment.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience supporting and working with cross-functional teams in a dynamic environment.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Previous experience manipulating, processing and extracting value from large disconnected datasets.
Preferred Education, Experience & Cert/Lic
Preferred Education:
Advanced Degree in Computer Science, Informatics, Information Systems or another quantitative field.
Preferred Experience:
Minimum of six (6) years of experience in a Data Engineer role
Additional Technical Requirements
Strong analytic skills related to working with structured and unstructured datasets.
Must possess critical thinking and creative problem solving skills along with the ability to communicate well with stakeholders throughout the organization.
Strong communication, project management and organizational skills.
Proficient in SQL
Exposure to big data tools: Hadoop, Spark, Kafka, BigSQL, Hive, etc.
Experience with relational SQL and NoSQL databases, including IBM PDA (Netezza), MS SQL Server and HBase.
Experience with data integration tools: Informatica, MS Integration Services, Sqoop, etc.
Exposure to cloud vendors and services: AWS, Google, Microsoft, IBM
Exposure to stream-processing systems: IBM Streams, Flume, Storm, Spark-Streaming, etc.
Experience consuming and building APIs
Experience with object-oriented/object function programming languages: Python, Java, C++, Scala, etc.
Experience with statistical data analysis tools: R, SAS, SPSS, etc.
Experience with visual analytics tools: QlikView, Tableau, Power BI etc.
Familiarity to Agile methodology for development
Familiarity with electronic health record and financial systems.
i.e.
Epic Systems, Cerner, WorkDay, Infor, Strata etc.
In an effort to achieve this goal, employment at Children's Hospital of Philadelphia, other than for positions with regularly scheduled hours in New Jersey, is contingent upon an attestation that the job applicant does not use tobacco products or nicotine in any form and a negative nicotine screen (the latter occurs after a job offer).
Children's Hospital of Philadelphia is an equal opportunity employer.
VEVRAA Federal Contractor/Seeking priority referrals for protected veterans.
Talent Acquisition
2716 South Street, 6th Floor
Philadelphia, PA 19146
Phone: 866-820-9288
Email:TalentAcquisition@email.chop.edu
","['sql', 'spss', 'python', 'java', 'hbase', 'nosql', 'kafka', 'sa', 'hadoop', 'hive', 'scala', 'bi', 'aw', 'tableau', 'r', 'microsoft', 'power bi', 'spark', 'cloud']","['recommend', 'healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'information technology', 'commun']",1,"['sql', 'spss', 'python', 'java', 'hbase', 'nosql', 'kafka', 'sa', 'hadoop', 'hive', 'scala', 'powerbi', 'aw', 'tableau', 'r', 'microsoft', 'spark', 'cloud', 'recommend', 'healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'information technology', 'commun']","['day', 'stream', 'visual', 'predict', 'python', 'integr', 'quantit', 'sourc', 'collect', 'evalu', 'analyt', 'sa', 'hadoop', 'optim', 'statist', 'action', 'clinic', 'infrastructur', 'bi', 'aw', 'big', 'set', 'engin', 'particip', 'spark', 'power', 'comput', 'scientist', 'relat']","['sql', 'spss', 'python', 'java', 'hbase', 'nosql', 'kafka', 'sa', 'hadoop', 'hive', 'scala', 'powerbi', 'aw', 'tableau', 'r', 'microsoft', 'spark', 'cloud', 'recommend', 'healthcar', 'dashboard', 'visual', 'predict', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'statist', 'big data', 'information technology', 'commun', 'day', 'stream', 'visual', 'predict', 'python', 'integr', 'quantit', 'sourc', 'collect', 'evalu', 'analyt', 'sa', 'hadoop', 'optim', 'statist', 'action', 'clinic', 'infrastructur', 'bi', 'aw', 'big', 'set', 'engin', 'particip', 'spark', 'power', 'comput', 'scientist', 'relat']"
DE,"Title : Software Engineer (Data)
Looking for a talented and dedicated Data Analaytics Software engineer to develop and maintain its Analytics data store and tools/applications in cloud infrastructure.
These tools are used across software development organization, and are critical to the deployment and release of features and devices for the Platform, including but not limited to set top and broadband premise equipment.
As a member of the team, you will be responsible for developing new tools and platforms, managing existing tools and infrastructure, and setting policy for usage and future direction.
You will be a member of a fast-paced team that comprises subject matter experts, and will be a primary point of contact when issues need urgent attention.
You will be responsible for interfacing with many peripheral teams, including partners outside of Client, to ensure that products and services are deployed in a timely manner, and at a level of quality that maximizes service integrity and minimizes the risk of down-time.
Good communication skills are essential.
Responsibilities:
· Development and maintenance of ETL applications written using Java, SQL and Python.
· Performance tuning and management of SQL based Databases like MySQL, Redshift etc.
· Backup/Restore/Archiving of Database
· Exploring new data store technologies for various business scenarios like Dashboards/Reporting/Visualization, Business Intelligence software, Data Warehousing, Real time stream analytics etc.
Required Skills:
· Java experience is a must.
· Hands on ETL and SQL experience with relational DB like Teradata, Redshift, Snowflake etc.
· Hands on experience in either one of the leading BI tools like Tableau, Looker etc or Visualization/Front End/Web Development experience.
· Experience with Open stack and/or AWS Cloud services
· Experience with Linux (Ubuntu preferred) system
Minimum bachelor's degree required
Job Type: Contract
Experience:
AWS: 1 year (Required)
ETL/SQL: 5 years (Required)
Java: 5 years (Required)
Linux/Ubuntu: 1 year (Required)
Tableau: 5 years (Required)
Data Analysis: 5 years (Required)
Teradata, Redshift, Snowflake : 5 years (Required)
Education:
Bachelor's (Required)
Contract Length:
More than 1 year
Varies
Contract Renewal:
Likely
Schedule:
Monday to Friday
","['sql', 'linux', 'looker', 'bi', 'snowflak', 'python', 'redshift', 'java', 'tableau', 'aw', 'mysql', 'cloud', 'db']","['dashboard', 'visual', 'risk', 'tune', 'data warehousing', 'etl', 'commun']",1,"['sql', 'linux', 'looker', 'powerbi', 'snowflak', 'python', 'redshift', 'java', 'tableau', 'aw', 'cloud', 'db', 'dashboard', 'visual', 'risk', 'tune', 'data warehousing', 'etl', 'commun']","['analyt', 'essenti', 'stream', 'set', 'relat', 'visual', 'primari', 'look', 'infrastructur', 'bi', 'python', 'aw', 'releas', 'etl', 'engin', 'integr']","['sql', 'linux', 'looker', 'powerbi', 'snowflak', 'python', 'redshift', 'java', 'tableau', 'aw', 'cloud', 'db', 'dashboard', 'visual', 'risk', 'tune', 'data warehousing', 'etl', 'commun', 'analyt', 'essenti', 'stream', 'set', 'relat', 'visual', 'primari', 'look', 'infrastructur', 'bi', 'python', 'aw', 'releas', 'etl', 'engin', 'integr']"
DE,"Identify and improve current data pipelines through automation and optimization.
SkillsUnderstand and comply with all enterprise and IS departmental information security policies, procedures and standards.Support the integration of information security in the development, design, and implementation of Technology Resources that process, transmit, or store information.Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
GeneralBasic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.Foundational knowledge in Change Control Mgt.
processes3.Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge.Understanding SQL, Relational and Multidemensional Databases and DesignsKnowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design1.CreatingMaintaining Tables, views, indexes.Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain.
Soft SkillsComfortable working in a collaborative environment.Ability to self-organize one's priorities and schedule.Have mindset to perform necessary documentation.Should be a self-starter to work independently or in a team.
EducationBachelors degree in computer related field required.2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment.Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment.
","['sql', 'git', 'jira', 'tableau', 'python', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']","['sql', 'git', 'jira', 'tableau', 'python', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr']"
DE,"§ Analysis of Users, Systems Entitlement, Data Classifications, PII data elements identification
§ Create IAM Roles and Policies relevant to newly added sources and tables based on entitlements
§ Update S3 bucket policies relevant to newly added sources/tables
",['s3'],['classif'],999,"['s3', 'classif']",['sourc'],"['s3', 'classif', 'sourc']"
DE,"Duration & Type: 12 months Contract with a media & communications industry client
No.
of Positions: Multiple
Responsibilities:
Develop solutions to big data problems utilizing common tools found in the ecosystem.
Develop solutions to real-time and offline event collecting from various systems.
Develop, maintain, and perform analysis within a real-time architecture supporting large amounts of data from various sources.
Analyze massive amounts of data and help drive prototype ideas for new tools and products.
Design, build and support APIs and services that are exposed to other internal teams
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Required:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in one or more languages: Python, Scala/Java, Spark, Batch, Streaming, ML
Experience with Python unit testing and code coverage frameworks
Experienced in NoSQL / SQL, Microservice, RESTful API development
Strong Experience with AWS Core such as Kinesis, Lambda, API Gateway, CloudFormation, CloudWatch
Experienced with one of the Analytics tools – Presto / Athena, QuickSight, Tableau
Strong Experience with Container technologies and Real-time Streaming (such as Kafka, Kinesis)
Preferred:
Test-driven development/test automation, continuous integration, and deployment automation experience
Experience with Performance tuning at scale
Experience working on big data platforms in the cloud or on traditional Hadoop platforms
Experience working in agile/iterative development and delivery environments
Enjoy working with data – data analysis, data quality, reporting, and visualization
Great design and problem solving skills, with a strong bias for architecting at scale
Excellent communication skills
Experience in software development of large-scale distributed systems
For consideration, please send resume to career@infoquestgroup.com
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'java', 'tableau', 'nosql', 'cloud', 'kafka', 'excel']","['visual', 'problem solving', 'statist', 'big data', 'commun']",1,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'java', 'tableau', 'nosql', 'cloud', 'kafka', 'excel', 'visual', 'problem solving', 'statist', 'big data', 'commun']","['analyt', 'stream', 'spark', 'ml', 'relat', 'visual', 'hadoop', 'amount', 'aw', 'python', 'comput', 'statist', 'big', 'integr', 'common', 'sourc']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'lambda', 'python', 'java', 'tableau', 'nosql', 'cloud', 'kafka', 'excel', 'visual', 'problem solving', 'statist', 'big data', 'commun', 'analyt', 'stream', 'spark', 'ml', 'relat', 'visual', 'hadoop', 'amount', 'aw', 'python', 'comput', 'statist', 'big', 'integr', 'common', 'sourc']"
DE,"Req ID: 71635BR
Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.
Fundamental Components included but are not limited to:
+ Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
+ Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
+ Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.
+ Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.
+ Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
+ Builds data marts and data models to support internal customers.
+ Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
+ Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
+ Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Qualifications Requirements and Preferences:
+ Strong problem solving skills and critical thinking ability.
+ Strong collaboration and communication skills within and across teams.
+ 5 or more years of progressively complex related experience.
+ Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
+ Ability to understand complex systems and solve challenging analytical problems.
+ Experience with bash shell scripts, UNIX utilities & UNIX Commands.
Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
+ Experience building data transformation and processing solutions.
+ Has strong knowledge of large scale search applications and building high volume data pipelines.
+ Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
Benefit Eligibility
Benefit eligibility may vary by position.
Job Function: Data & Analytics
Aetna is an Equal Opportunity/Affirmative Action employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected Veterans status.
","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'mysql']","['pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']",1,"['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'optim', 'python', 'avail', 'comput', 'action', 'etl', 'sourc', 'collect', 'particip', 'algorithm']","['cassandra', 'pig', 'sa', 'unix', 'python', 'hive', 'java', 'nosql', 'sql', 'pipelin', 'machine learning', 'optim', 'problem solving', 'analyz', 'information technology', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'pipelin', 'set', 'relat', 'sa', 'optim', 'python', 'avail', 'comput', 'action', 'etl', 'sourc', 'collect', 'particip', 'algorithm']"
DE,"The old way of buying cars is outdated.
This is the future of car buying.
Structure, manage, and ensure accuracy of data in a SQL data warehouse environment to support various levels of analytics products using the latest ETL methods and technologies.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, marketing, sales, operational efficiency and other key business performance metrics
Perform data analysis required to troubleshoot and resolve data related issues
Skills & Requirements:
5+ years of data engineering experience
5+ years of experience using SQL
Experience working with segment.io
Experience with R or Python a plus
Bachelor’s or Master’s degree in Computer Science, Information Systems or related field
Ability to develop/structure data to support analytical work, BI reporting, and data visualization
Ability to assess the best engineering approach and implement ETL, ELT, Transformations, Triggers, Stored Procedures, or other methods to support optimal SQL data warehouse structuring.
Experience managing Schemas, Tables, Views
Demonstrated experience ensuring accuracy and reliability of core data through sound data governance practices
Auto industry experience not required
","['sql', 'bi', 'python', 'r']","['optim', 'etl', 'visual', 'pipelin']",1,"['sql', 'powerbi', 'python', 'r', 'optim', 'etl', 'visual', 'pipelin']","['analyt', 'pipelin', 'relat', 'visual', 'infrastructur', 'optim', 'bi', 'python', 'avail', 'comput', 'action', 'warehous', 'etl', 'engin']","['sql', 'powerbi', 'python', 'r', 'optim', 'etl', 'visual', 'pipelin', 'analyt', 'pipelin', 'relat', 'visual', 'infrastructur', 'optim', 'bi', 'python', 'avail', 'comput', 'action', 'warehous', 'etl', 'engin']"
DE,"Country:
United States
Cities:
Boston, Hartford, New York City, Philadelphia
Area of expertise:
Analytics
Love living in the cloud?
As an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions.
You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools.
You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.
Day-to-day, you will:
• Translate business requirements to technical solutions using strong business insight.
• Analyzes current business practices, processes and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
• Support the planning and implementation of data design services, providing sizing and configuration assistance, and performing needs assessments.
• Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
• Design and Build Modern Data Pipelines and Data Streams.
• Design and Build Data Service APIs.
• Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
• Expose data to end users using Power BI, Azure API Apps or other modern visualization platform or experience.
• Implement effective metrics and monitoring processes.
• Able to travel approximately 80%
Your technical/non-technical skills include:
• Demonstrable experience of turning business use cases and requirements to technical solutions.
• Experience in business processing mapping of data and analytics solutions.
• Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
• T-SQL is required.
• Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.
• Experience preparing data for Data Science and Machine Learning.
• Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
• Strong collaboration ethic and experience working with remote teams.
• Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
(nice to have)
• Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
(nice to have)
• Knowledge of Lambda and Kappa architecture patterns.
(nice to have)
Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field.
An equivalent combination of education and experience will also suffice.
Preferred Years of Work Experience:
You likely have about 3-5+ years of relevant professional experience.
• 14-time winner of Microsoft Partner of the Year
• 24,000+ certifications in Microsoft technology
• 90+ Microsoft partner awards
• 17 Gold Competencies
• 3,500 analytics professionals worldwide
• 1,000 data engineers
• Implemented analytics systems for more than 550 clients
• 400 AI practitioners
• 300 cognitive service experts
Important Note about this Future Opportunity:
What does that mean for you?
It means that you will have the chance to connect with leaders and hiring managers at your own pace.
","['sql', 'azur', 'bi', 'lambda', 'cloud', 'microsoft', 'power bi']","['pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']",1,"['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology']","['day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']","['sql', 'azur', 'powerbi', 'lambda', 'cloud', 'microsoft', 'pipelin', 'dashboard', 'end user', 'visual', 'machine learning', 'analyz', 'information technology', 'day', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'azur', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'relat', 'engin']"
DE,"Your career starts now.
Were looking for the next generation of health care leaders.
If you want to make a difference, wed like to hear from you.
Under the direction of the VP, Chief Analytics Officer this position will oversee the Business Analysis, Data Engineering and Automation and Business Intelligence functions within Enterprise Analytics.
This role will be responsible for oversight of building, deploying and testing of Business Intelligence (BI) systems that integrate with databases, data warehouses, data marts and data lake.
This position will be responsible for dashboards and/or reports according to the identified technical requirements in addition to being responsible for performance tuning, version controls and documentation of those dashboards/reports.
Position will collaborate closely with internal and external stakeholders across the organization to develop sustainable solutions that support data-driven decision making while acting as a champion for data and analytics services.
Will lead a team of Tableau developers, Business Analysts to support a common data and analytics platform integrating new and existing data sources to eliminate silos.
Responsibilities:
Oversee solutions/framework for provisioning data for self-service consumption by business users who create BI applications, as well as supporting any type of adhoc reporting/analytic requests.
Define, build and execute the Data Services and technology roadmap ensuring that it is in alignment with the Application, Infrastructure and Business roadmaps.
Drive build and automation of tableau reports; ensure requirements are successfully executed and issues are resolved timely.
Very strong tableau and data warehousing expertise required
Proactively identify opportunities and technologies regarding the access, processing, visual display and analysis of data; maintain a pulse on data visualization and engineering trends.
Collaborates with internal departments to understanding their evolving data needs and develop and execute on analytics to meet those needs while driving towards improved outcomes and overall performance metrics.
Independently complete analysis, development and enhancement of BI solutions to fulfill business needs.
Maintain in-depth knowledge of project planning methodologies and tools as well as IT standards and guidelines.
Serve as a thought leader for technical business processes, developing forward-thinking prototypes that promote increased efficiency and productivity on multiple levels.
Education/ Experience:
Bachelors Degree (Computer Science or related field preferred required; Masters' degree preferred
Need strong Business Intelligence and Analytical skills; Tableau is preferred.
Power BI and Qlikview is a plus.
8-10 years experience in analytics, preferably in the health care industry.
Minimum of 5 years people management experience.
Other Skills:
Analytical mindset focused on process and data analysis; problem solving aptitude.
Some experience of relevant data visualization, including but not limited to experience with visualizations and web-based interactives, geospatial, and/or visual story telling preferred.
Superior organizational, close attention to detail and written/oral communication skills.
Superior technical writing skills in business requirements, queries, reports, and presentations.
Superior technical skills in Tableua Excel and PowerPoint with the ability to learn other analytic tools.
Advanced analytical and quantitative skills with experience collecting, organizing, mining, analyzing, visualizing and disseminating abundant information with the utmost accuracy and presentation.
Experience in conducting and documenting QA testing using problem solving techniques for analytic and reporting solutions.
Efficiently manages time-based on the continual evaluation of priorities, meets deadlines with high-quality deliverable reflecting complete understanding of expectations, able to multi-task.
Back
Share
","['bi', 'tableau', 'powerpoint', 'excel', 'power bi']","['dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']",1,"['powerbi', 'tableau', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']","['analyt', 'techniqu', 'relat', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']","['powerbi', 'tableau', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati', 'analyt', 'techniqu', 'relat', 'visual', 'power', 'infrastructur', 'bi', 'comput', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']"
DE,"This position can be 100% remote work from home.
There will be both hands on work building data models as well as interacting with the business.
Responsible for translating requirements from the business and being involved in the hands on work to make this happen.
This candidate will also be responsible for coaching more junior members of the team.
You will be relied upon to bring new ideas about big data, data analytics and insights and tools.
Required:
8+ years of experience in a data engineering/ architect role.
Strong experience with event streaming platforms.
Strong experience in Big Data and data analytics.
(Hadoop or Kafka)
Must have experience with Cloud Data.
Must have experience with Python, Spark or Azure.
Employing the Future
","['spark', 'azur', 'hadoop', 'cloud', 'python', 'kafka']",['big data'],999,"['spark', 'azur', 'hadoop', 'cloud', 'python', 'kafka', 'big data']","['analyt', 'spark', 'azur', 'hadoop', 'python', 'big', 'engin']","['spark', 'azur', 'hadoop', 'cloud', 'python', 'kafka', 'big data', 'analyt', 'spark', 'azur', 'hadoop', 'python', 'big', 'engin']"
DE,"C2C will also be considered with proper verification and video call Interview: Phone/Video (Candidate should be able come to nearby Collabera office for phone/video interview.
GiveÂfeedback at ourÂCenter of Business ExcellenceÂor call 1-866-398-6484 Privacy/Confidentiality From: Vishal Choudhary Sent: Thursday, January 16, 2020 9:39 AM To: avinash@tekpyramids.com Subject: Capital One Bulk Recs Software Engineers: LOB- Financial Services Manager Name- Sriram Srinivasan Security Standard and Applications.
Will work on Legacy System & Modification work.
Java Spring based application, API, Library APIÃââs Most Application are Built on Spring Boot, Standalone Teams & Tech Lead they are looking.
Have to Rewrite Services , AngularJS bit, Kafka Backend-PostgreSQL.
200 not confirmed Yet.
AWS is preferred but Strong AWS is mandate.
First Week of Jan budget will get Approve.
They have Deadline till Q1.
Project will go to End of Next year Will hire Juniors, Mid-Level & C2H-Mention eligibility on top before submitting.
C2H is preferred.
Top 3: Java Basics, Cloud, CI/CD Most are going to be Backend, few with Full Stack Developers.
Data Engineers: Spark Streaming, Snowflake Infra ETL & Metadata.
SQL knowledge is good to have.
Cloud is must Regards Vishal Choudhary Associate Delivery Manager T: (973) 841-2279 How am I doing?
GiveÂfeedback at ourÂCenter of Business ExcellenceÂor call 1-866-398-6484 Privacy/Confidentiality
","['sql', 'spark', 'aw', 'snowflak', 'java', 'cloud', 'postgresql', 'kafka']",['etl'],999,"['sql', 'spark', 'aw', 'snowflak', 'java', 'cloud', 'postgresql', 'kafka', 'etl']","['spark', 'stream', 'aw', 'etl', 'engin']","['sql', 'spark', 'aw', 'snowflak', 'java', 'cloud', 'postgresql', 'kafka', 'etl', 'spark', 'stream', 'aw', 'etl', 'engin']"
DE,"What if you could
Architect and build a cloud-based data lake for the application of machine learning and analytics, creating the pipeline architecture to optimize the data flow from numerous data sources
Design and build data pipelines, data streams, and data service APIs in Azure, which drive data analytics and insight tools
Partner closely with DBAs, Infrastructure & Operations, and Data Scientists to prioritize data sources for ingestion into the data lake that enable a robust environment for AI, ML, and BI solutions
Your journey up to this point will likely have included success in:
5+ years of experience in a Data Engineer role or IT platform implementation role in a technical and analytical capacity
Experience leading large-scale global data warehousing and analytics projects, working with structured and unstructured data
Hands on experience architecting and building a Microsoft Azure based Data Lake, including establishing timelines, resources, and cost estimates for data lake development and creation
Proven track record with full Azure product suite, including cloud provider hosting
Design, integrate, and document technical components for data flows or applications that feed the data lake and use the lake to make decisions across the enterprise
Identify and implement best practices in maintaining a data rich environment for ML, AI, and BI solutions.
Experience with big data tools and ETL Data Pipelines
Understanding data best practices in ETL processes as well as data governance and monitoring
Advanced SQL knowledge and experience with relational databases
Experience working with development operations on databases that power APIs for front-end applications
Strong experience with cloud data tools and platforms
Ability to think strategically about business, product, and technical challenges in an enterprise environment
Bachelor's or MA degree, in Computer Science, Engineering, Mathematics or a related field
Business analytics and visualization experience is a plus
Want to join forces?
Medical/Prescription/Vision
Dental/Orthodontia
Tuition reimbursement
Health & Wellness reimbursement
","['sql', 'azur', 'bi', 'cloud', 'microsoft']","['pipelin', 'visual', 'machine learning', 'data warehousing', 'big data', 'etl']",1,"['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'visual', 'machine learning', 'data warehousing', 'big data', 'etl']","['stream', 'visual', 'provid', 'etl', 'integr', 'sourc', 'analyt', 'challeng', 'ml', 'azur', 'learn', 'infrastructur', 'bi', 'big', 'engin', 'machin', 'pipelin', 'power', 'comput', 'scientist', 'relat']","['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'pipelin', 'visual', 'machine learning', 'data warehousing', 'big data', 'etl', 'stream', 'visual', 'provid', 'etl', 'integr', 'sourc', 'analyt', 'challeng', 'ml', 'azur', 'learn', 'infrastructur', 'bi', 'big', 'engin', 'machin', 'pipelin', 'power', 'comput', 'scientist', 'relat']"
DE,"Manage file systems such as Hadoop, Amazon Web Services or Azure to manage Big Data processes.Analyze output from DBs to categorize consumer behavior & predict response rates to products & services.
Use tools including R, Python, Hbase, Pig, Hive to access data.
Build machine learning algorithms & ensemble learning techniques for predictive models.
Data mining (use R, SQL, Time Series, etc.)
to ID targets.
Analyze research & survey data for behavior patterns.
Create datasets from online & offline data, using ETL, SQL & PL/SQL.
Develop customer segments in Data Warehouse environ, & data infrastructure & access tools to support real time analytics.
Implement UNIX /Linux scripting for backup of DBs.
Integrate advanced analytic tools & Hadoop File Sys for large scale distributed processing.
Extract data from Cloud & deliver visualization using Tableau.
Write Unix shell command for data processing in Hadoop.
Find best solution for IP over Cable Data Network cost & bit rate.
May undergo background checks & drug screen.
Must have MS in IT, Comp Sci, Stats, or related field, and 3 yrs relevant IT experience in IT Services or retail/eCommerce.
Also requires the following skills:
(3 yrs exp) in Java programming and Unix/Linux Op Sys;
(2 yrs exp) in Big Data platform, HDFS (Hadoop), scripting tools for automation and manipulation of DBs, and data analysis/engr incl.
analytics;
and (1 yr exp) data modeling and ETL processes for manipulating large data sets.
Send Resume to: recruitment@numericjobs.com
","['sql', 'linux', 'azur', 'pig', 'unix', 'hadoop', 'db', 'tableau', 'python', 'hive', 'java', 'hbase', 'r', 'cloud', 'amazon web services']","['research', 'big data', 'data mining', 'visual', 'segment', 'predict', 'machine learning', 'data modeling', 'analyz', 'etl', 'time series']",999,"['sql', 'linux', 'azur', 'pig', 'unix', 'hadoop', 'db', 'tableau', 'python', 'hive', 'java', 'hbase', 'r', 'cloud', 'aw', 'research', 'big data', 'data mining', 'visual', 'segment', 'predict', 'machine learning', 'data modeling', 'analyz', 'etl', 'time series']","['analyt', 'machin', 'program', 'techniqu', 'azur', 'set', 'relat', 'visual', 'predict', 'hadoop', 'infrastructur', 'python', 'algorithm', 'warehous', 'big', 'etl', 'integr']","['sql', 'linux', 'azur', 'pig', 'unix', 'hadoop', 'db', 'tableau', 'python', 'hive', 'java', 'hbase', 'r', 'cloud', 'aw', 'research', 'big data', 'data mining', 'visual', 'segment', 'predict', 'machine learning', 'data modeling', 'analyz', 'etl', 'time series', 'analyt', 'machin', 'program', 'techniqu', 'azur', 'set', 'relat', 'visual', 'predict', 'hadoop', 'infrastructur', 'python', 'algorithm', 'warehous', 'big', 'etl', 'integr']"
DE,"Designs and develops updated infrastructure in support of one or more business processes.
Helps to ensure a balance between tactical and strategic technology solutions.
Considers business problems ""end-to-end"": including people, process, and technology, both within and outside the enterprise, as part of any design solution.
Mentors, reviews codes and verifies that the object oriented design best practices and that coding and architectural guidelines are adhered to.
Identifies and drives issues through closure.This role brings to bear significant cloud experience in the private and public cloud space as well as streaming data, big data and software engineering.
This role will be key in the delivery of a productized real-time event processing product for customer experience.
This person will engage and lead software delivery teams and contribute to several strategic efforts that drive personalized customer experiences across product usage, support interactions and customer journeys.
This role leads the building of real-time big data platforms, machine learning algorithms and data services that enable proactive responses for customers at every critical touch point.Core Responsibilities* Enterprise-Level architect for ""Big Data"" real-time event processing, analytics, data store, and cloud platforms.
* Enterprise-Level architect for cloud applications and ""Platform as a Service"" capabilities* Detailed current-state product and requirement analysis.
* Security Architecture for ""Big Data"" applications and infrastructure* 3rd party vendor analysis and management.
* Ensures programs are envisioned, designed, developed, and implemented across the enterprise to meet business needs.
Interfaces with the enterprise architecture team and other functional areas to ensure that most efficient solution is designed to meet business needs.
* Ensures solutions are well engineered, operable, maintainable, and delivered on schedule.
Develops, documents, and ensures compliance with best practices including but not limited to the following coding standards, object oriented design, platform and framework specific design concerns and human interface guidelines.
* Tracks and documents requirements for enterprise development projects and enhancements.
* Monitors current and future trends, technology and information that will positively affect organizational projects; applies and integrates emerging technological trends to new and existing systems architecture.
Mentors team members in relevant technologies and implementation architecture.
* Contributes to the overall system implementation strategy for the enterprise and participates in appropriate forums, meetings, presentations etc.
to meet goals.
* Gathers and understands client needs, finding key areas where technology leverage is possible to improve business processes, defines architectural approaches and develops technology proofs.
Communicates technology direction.
* Monitors the project lifecycle from intake through delivery.
Ensures the entire solution design is complete and consistent from the start and seeks to remove as much re-work as possible.
* Works with product marketing to define requirements.
Develops and communicates system/subsystem architecture.
Develops clear system requirements for component subsystems.
* Acts as architectural lead on project in partnership with domain/program architects.
* Applies new and innovative ideas to old or new problems.
Fosters environments that encourage innovation.
Contributes to and supports effort to further build intellectual property via patents.
* Consistent exercise of independent judgment and discretion in matters of significance.
* Regular, consistent and punctual attendance.
Must be able to work nights and weekends, variable schedule(s) as necessary.
* Other duties and responsibilities as assigned.Job Specification:* Demonstrated experience with ""Platform as a Service"" (PaaS) architectures including strategy, architectural patterns and standards, approaches to multi-tenancy, scalability, and security.
* Demonstrated experience with schema and data governance and message metadata stores* Demonstrated experience with 3rd party software vendor relationships, vendor evaluation, vendor quality audits, and contract negotiation.
* Demonstrated experience with private cloud infrastructure both OpenStack and VMWare.
* Demonstrated experience with public cloud resources such as AWS.
* Demonstrated experience with cloud automation technologies including Ansible, Terraform, Chef, Puppet, etc* Hands-on experience with Data Flow processing engines, such as Apache NiFi* Working knowledge / experience with Big Data platforms (Kafka, Hadoop, Storm/Spark, NoSQL, In-memory data grid)* Working knowledge / experience with Linux, Java, Python.
* Product Management experience highly desired* Staff/People Management experience highly desiredJob Specification:* Bachelors Degree or Equivalent in Engineering, Computer Science* 15+ years in Software Engineering Experience* 10+ years in Technical Leadership roles* 5+ years in Cloud InfrastructureComcast is an EOE/Veterans/Disabled/LGBT employer
","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka']","['machine learning', 'commun', 'big data']",1,"['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'machine learning', 'commun', 'big data']","['program', 'python', 'integr', 'algorithm', 'analyt', 'evalu', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'machin', 'spark', '3rd', 'comput']","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'machine learning', 'commun', 'big data', 'program', 'python', 'integr', 'algorithm', 'analyt', 'evalu', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'machin', 'spark', '3rd', 'comput']"
DE,"Immediate need for a talented SparkBig Data Engineer.
This is a 12+ Months Contract opportunity with long-term potential and is located in Philadelphia PA 19103.
Job ID 70750-170751-1 REMOTE OPTION IS ALSO AVAILABLE FOR THIS POSITION MUST HAVE SKILLS 3+ yrs.
Spark exp Will prefer Python over Scala Hadoop (Hive, General HDFS knowledge) specifically Cloudera Bash scripting would be helpful Must be independent and coach junior developers.
","['spark', 'scala', 'hadoop', 'python', 'hive']",['big data'],999,"['spark', 'scala', 'hadoop', 'python', 'hive', 'big data']","['spark', 'hadoop', 'avail', 'python', 'engin']","['spark', 'scala', 'hadoop', 'python', 'hive', 'big data', 'spark', 'hadoop', 'avail', 'python', 'engin']"
DE,"Position: Python Data Engineer
Job Type: Contract
Duration: 6 months to start
Candidate should be passionate about Python coding.
Job Responsibilities:
Develop and enhance data pipelines, mostly batch (if necessary "" streaming"" processes)
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills:
Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt.
processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:
SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:
Tableau, Qliksense, Power PI, or any other data visualization application.
Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Soft Skills:
Comfortable working in a collaborative environment.
Ability to self-organize one' s priorities and schedule.
Have mindset to perform necessary documentation.
Should be a self-starter to work independently or in a team.
Education:
Bachelor’ s degree in computer related field required.
2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.
Preferred experience
5+ years of experience with Python
3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,
Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment.
","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker']","['research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']",1,"['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun']","['stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr', 'engin']","['sql', 'git', 'jira', 'tableau', 'python', 'powerpoint', 'excel', 'docker', 'research', 'healthcar', 'pipelin', 'visual', 'optim', 'data warehousing', 'commun', 'stream', 'pipelin', 'relat', 'visual', 'power', 'optim', 'python', 'comput', 'warehous', 'integr', 'engin']"
DE,"Job Title: Data Engineer
Duration: 12 Month Contract
W2 ONLY
C2C/1099 NOT ALLOWED
This position requires that the selected candidate be located in Wilmington, DE
Ecosystem.
optimizing data flow and collection for cross functional teams.
The ideal candidate is an
experienced data pipeline builder and data wrangler who enjoys optimizing data systems and
building them from the ground up.
database architects, data analysts and data scientists on data initiatives and will ensure optimal
data delivery architecture is consistent throughout ongoing projects.
They must be self-directed
and comfortable supporting the data needs of multiple teams, systems and products.
The right
Qualifications:
attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems
or another quantitative field.
• Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
• Experience with relational SQL, Snowflake and NoSQL databases, including
Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: NiFi, Kylo, Luigi,
Airflow, Azkaban etc.
• Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python,
Java, C++, PySpark, Scala, etc.
As an S3 employee, youre eligible for a full benefits package which may include: Medical Insurance, Dental Insurance, Vision Insurance, 401(k) Plan, Vacation Package, Life & Disability Insurance Plans, Flexible Spending Accounts, and Tuition Reimbursement.
D1-JK*
Job Requirements:
","['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'postgr', 'cloud', 'kafka']","['pipelin', 'optim', 'statist', 'big data', 'account']",2,"['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'optim', 'statist', 'big data', 'account']","['stream', 'engin', 'spark', 'pipelin', 'relat', 'hadoop', 'optim', 'aw', 'python', 'employe', 'packag', 'statist', 'scientist', 'comput', 'big', 'quantit', 'collect']","['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'optim', 'statist', 'big data', 'account', 'stream', 'engin', 'spark', 'pipelin', 'relat', 'hadoop', 'optim', 'aw', 'python', 'employe', 'packag', 'statist', 'scientist', 'comput', 'big', 'quantit', 'collect']"
DE,"The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills.
You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies.
You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration.
Develop overall design and determine division of labor across various architectural components Deploy and customize Client Standard Architecture components Mentor client personnel.
Train clients on the Client Integration Methodology and related supplemental solutions Provide feedback and enhance Client intellectual property related to data management technology deployments Assist in development of task plans including schedule and effort estimation Skills and Qualifications 1+ year experience working with Azure analytical stack Experience with building analytical solutions on Azure Synapse Experience with building Delta Lake is required (Databricks) Strong experience building and deploying data management solutions on DataBricks is required Experience building high-performance, and scalable distributed systems Experience in ETL and ELT workflow management Continuous Data Movement Streaming Messaging Experience with related technologies ex Spark streaming or other message brokers like Kafka is a PLUS 3+ yearsrsquo experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing) 3+ yearsrsquo experience in a software engineering, leveraging Java, Python, Scala, etc.
","['spark', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka']","['etl', 'commun', 'pipelin']",999,"['spark', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'etl', 'commun', 'pipelin']","['analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'provid', 'python', 'packag', 'warehous', 'etl', 'engin', 'integr']","['spark', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'etl', 'commun', 'pipelin', 'analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'provid', 'python', 'packag', 'warehous', 'etl', 'engin', 'integr']"
DE,"Bachelor's degree (B.A or B.S) from four-year College or University required
Degrees in IT, Computer Science, Information Science, MIS, Finance or similar preferred
Excellent written and verbal communication skills, including technical writing skills
Basic understanding of systems
Excellent creative problem solving skills
Must be detail oriented and a critical thinker
Motivated, hardworking and self-starter
Ability to handle multiple projects and meet deadlines
Excellent problem resolution and consultative skills
Ability to prioritize and be organized
Ability to work independently, self-driven, highly motivated and results-oriented
#CB
The First Flyer Data Engineer IT Program is offering a 2-year rotational training program that will allow you to experience hands on training among 4 different teams in the technology side of the business.
Essential Job Functions:
Analyze the business and IT resources to take lead and create a project solution during each rotation
Working with a mentor to create a personal certification/training plan within the first 6 months
Work with the Business Solutions Group to research business areas for inefficient and informal processes
Develop strategic workflow designs for process improvement to enhance loan processing
Develop Business Solutions with the Corporate Support Solutions team to enhance the business through IT solutions
Act as a liaison on the Service Desk between business analyst, application support and the business
Perform quality assurance testing of system changes and enhancements
Create Visual Reporting Structures on Freedoms Enterprise Performance Management team
Use systems such as Tableau, Business Object, Sharepoint and IBM Blueworks Live
","['tableau', 'excel']","['research', 'visual', 'financ', 'problem solving', 'analyz', 'commun']",1,"['tableau', 'excel', 'research', 'visual', 'financ', 'problem solving', 'analyz', 'commun']","['essenti', 'program', 'corpor', 'visual', 'comput', 'engin']","['tableau', 'excel', 'research', 'visual', 'financ', 'problem solving', 'analyz', 'commun', 'essenti', 'program', 'corpor', 'visual', 'comput', 'engin']"
DE,"Startup in Center City focused on tackling a newly supported industry with few competitors.
Legislation has opened up a large sector of the market doing billions a year in transactions.
This platform has to be very high traffic, consumer friendly and run like a trading site.
The team is hiring on their 2nd Data Engineer and is looking for a senior member.
The ideal candidate has strong data pipeline skills, data warehousing and backend logic skills, AWS experience with any data specific services and strong Python skills.
The team is very collaborative and culture driven.
If you've ever said ""I don't want to be the smartest person in the room"" this is a great place.
No ego's here and lots of exciting problems to solve.Desired Skills & Experience* 5+ years in general engineering* 2+ years in a data engineering role specifically (or where that was a large portion of your time)* Experience with Spark/PySpark* Python* AWSWhat You Will Be DoingDaily Responsibilities* 60% Hands On* 20% Architecture* 20% Product/Company workThe Offer* Competitive Salary: Up to $140K/year, DOEYou will receive the following benefits:* Medical Insurance plans offered with low deductible and premium* 401(k) with 3% match* 18 days PTO, 5 sick days* Fast growth track at this stageApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['aw', 'pyspark', 'python', 'spark']","['data warehousing', 'pipelin']",999,"['aw', 'pyspark', 'python', 'spark', 'data warehousing', 'pipelin']","['day', 'basi', 'spark', 'pipelin', 'aw', 'python', 'engin']","['aw', 'pyspark', 'python', 'spark', 'data warehousing', 'pipelin', 'day', 'basi', 'spark', 'pipelin', 'aw', 'python', 'engin']"
DE,"Senior Data Engineer
Senior Spark Engineer - Senior Data Engineer
The Opportunity
Were building a data pipeline that merges the tested design of data warehousing and the latest big data technologies.
You will be designing and building data projects while securing core data elements.
Responsibilities
· Research new technology and share knowledge with team and peers
· Design, implement and release data applications in AWS using big data technologies
· Coordinate development efforts across the organization
· Influence team in development standards and processes
· Teach and mentor peers
· Other duties as assigned
Qualifications
· 2-4 years of hands on Apache Spark experience (PySpark preferred)
· 7-10 years of hands on Business Intelligence / Data Warehousing experience
· 1-2 years of experience building high-performance algorithms in scalable languages such as Scala, Python and R
Essential Duties
Will hold critical role in creating data pipelines using Spark and other technologies
Work closely with Platform Architects to achieve going from an Event Source data model, into Spark and out to SQL.
Other tasks as assigned.
What someone will need to be successful in this role
Excellent interpersonal, verbal and written communication skills
Experience mentoring junior engineers and performing code reviews
Strong logical, analytical, problem solving and reporting skills
Familiarity with Agile principles and methodologies
AWS Elastic Map Reduce (EMR) experience
Able to use version control (git) and other build, packaging & release management tools
Passionate about developing quality products
Employee Benefits
18 PTO days + 2 floating holidays & 10 paid holidays per year
Generous tuition reimbursement towards a Masters or Bachelors degree
401K match up to 6%
12 weeks of 100% paid paternity/maternity leave
Mentorship with industry professionals
","['sql', 'pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel']","['research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun']",1,"['sql', 'pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel', 'research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun']","['day', 'analyt', 'essenti', 'spark', 'pipelin', 'aw', 'python', 'releas', 'big', 'employe', 'sourc', 'engin', 'algorithm']","['sql', 'pyspark', 'spark', 'scala', 'git', 'aw', 'python', 'r', 'excel', 'research', 'pipelin', 'data warehousing', 'problem solving', 'big data', 'commun', 'day', 'analyt', 'essenti', 'spark', 'pipelin', 'aw', 'python', 'releas', 'big', 'employe', 'sourc', 'engin', 'algorithm']"
DE,"Role : Data Engineer
Client :IQVIA
Good knowledge and Hands on Scala Programming
•Good skills on modular program development in Scala
•Data pipeline using Spark Scala
•Load disparate data sets by leveraging Kafka consumers
•Ability to utilize Hive, Spark, Cassandra, Mesos and Kafka
•Experience with AWS components and services, particularly EMR, S3, and Lambda
•Good understanding of file formats including JSON, Parquet, Avro, and others
•Experience with open source NoSQL technologies such as Cassandra
•Experience with messaging and complex event-processing systems such as Kafka and Storm
•Extensive hands on Data frame and Dataset operations of Spark
•Basic Understanding of Bigdata Technologies and Hadoop
•Good knowledge on Hive and HiveQL
•Knowledge on Cassandra Architecture and CQL is desirable
•Build reusable code, with the ability to scale with very large data volumes
•Knowledge on Java Programming is desirable
•Data Migration/ETL knowledge is desirable
•Expert knowledge of software development tools, practices, and lifecycle methodologies
•Extensive experience with legacy and modern data warehouse concepts and implementations
•Extensive experience with data integration concepts and implementations including ETL, ELT, CDC, streaming, pub/sub
•Able to produce high quality design documentation and provide thorough reviews of design documentation produced by others
Ability to establish and maintain effective working relationships with co-workers, managers and clients
•Flexibility, ability to simultaneously juggle multiple complex projects and adapt to rapidly changing priorities
","['spark', 'cassandra', 'scala', 'hadoop', 'aw', 'lambda', 'java', 's3', 'hive', 'nosql', 'kafka']","['etl', 'pipelin']",999,"['spark', 'cassandra', 'scala', 'hadoop', 'aw', 'lambda', 'java', 's3', 'hive', 'nosql', 'kafka', 'etl', 'pipelin']","['program', 'spark', 'pipelin', 'set', 'hadoop', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['spark', 'cassandra', 'scala', 'hadoop', 'aw', 'lambda', 'java', 's3', 'hive', 'nosql', 'kafka', 'etl', 'pipelin', 'program', 'spark', 'pipelin', 'set', 'hadoop', 'aw', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"Quil, the joint venture between Independence Health Group (Independence) and Comcast, is the digital health platform that offers personalized and interactive health journeys to consumers and their caregivers.
Quil is committed to educating and engaging consumers, leading to better health experiences and better outcomes, at a lower cost.
Quil serves patients, members, and their caregivers and partners with healthcare providers and health plans nationally.
This position is open to both local and remote candidates who are comfortable working in the us-east time zone.
You have the skills to be successful in this role if you have:
5+ years of software development and engineering,
3+ years experience as data engineer
Define and implement best practices for data management and governance
Experience working with Looker, Tableau, or some other BI tool
Experience working with data stores
Experience working with Python and Scala
You love to collaborate and are comfortable leading a team
You are comfortable working across the organization to assemble and refine requirements
You love getting your hands dirty and working through tough problems
You are comfortable working with a product team to tease out requirements
You have excellent communications and problem solving skills
It is a plus if you have:
Experience with Apache Spark and AWS Glue
Experience working with AWS and relevant components including IAM, S3, etc
Experience with Snowflake
Experience with GCP data products including Firebase and BigQuery
Experience working with Terraform
Experience specifically with Looker for building reports
","['gcp', 'spark', 'firebas', 'looker', 'scala', 'bigqueri', 'bi', 'snowflak', 'python', 's3', 'tableau', 'aw', 'excel']","['healthcar', 'problem solving', 'commun']",999,"['gcp', 'spark', 'firebas', 'looker', 'scala', 'bigqueri', 'powerbi', 'snowflak', 'python', 's3', 'tableau', 'aw', 'excel', 'healthcar', 'problem solving', 'commun']","['digit', 'spark', 'bi', 'provid', 'python', 'aw', 'engin']","['gcp', 'spark', 'firebas', 'looker', 'scala', 'bigqueri', 'powerbi', 'snowflak', 'python', 's3', 'tableau', 'aw', 'excel', 'healthcar', 'problem solving', 'commun', 'digit', 'spark', 'bi', 'provid', 'python', 'aw', 'engin']"
DE,"WANTED: a Big Data Engineer with the ability to obtain a Secret Clearance.As a Big Data Engineer you will:* $10,000 sign on bonus!!
* Work with a team of driven, supportive and highly skilled professionals.
* Receive a robust benefits package that includes Employee Stock Ownership Plan!
* Enjoy flexibility managing your work hours and personal needs with a single accrual leave plan.RP: 1388A week in the life of a Big Data Engineer:* Provides engineering advisory and expert level technical and operational support relating to system engineering and integration.
* Analyzes business requirements, goals and objectives, and business process architecture to identify information requirements, process/application points of interaction and usage patterns.
* Plans and coordinate engineering activities associated with the missions of Defense Security Cooperation Agency and SC Partner Enterprise/Systems Integration.
* Develops and implement the transition and future state of Architecture, standards and interoperability for the Business Mission Area.
* Conducts extensive investigation and analyzing largely undefined factors and conditions to determine the nature and scope of problems and to devise solutions.
* Establishes priorities and allocate resources to establish and maintain Business Processes and Data Architectures for use by system developers and process engineers.
* Defines critical problems by identifying the need for areas of analysis, formulating and justifying proposals.
* Participates in the planning, evaluation, and continuing oversight of IT system development, test, and production and throughput activities.
* Assists in the development and implementation of metrics programs for measuring and analysis of production system performance.
* Develops program logic charts and creates documentation for one-of-a-kind projects.
* Reviews regulatory or procedural changes for potential impact to assigned projects.
* Determines nature and extent of changes required, time required to program the changes and makes appropriate recommendations.
* Maintains configuration management of the requirements set, along with corresponding requirement attributes and traceability to source documentation.
* Attends meetings, seminars, etc., and reviews technical literature to enhance knowledge of new or emerging state-of-the-art advances in software engineering techniques and practices, and of equipment being considered for production use.
* Highly proficient in Microsoft Word, Excel, PowerPoint, and VisioHighly Desirable Knowledge, Skills and Abilities:* Specific and detailed understanding of DSCA data, and its flow, and database equipment and software capabilities/limitations in order to recognize and resolve systemic database problems, ensure feasibility and efficiency of proposed database structures and coordinate efforts with other affected organizational elements.
* Amazon Web Service (AWS) experience preferred* AWS Certified: DevOps Engineer or Solutions Architect* AWS Certified: Big Data or Advanced Networking or SecurityTraining and Experience:* Qualified candidates must have a minimum of 5 years' experience working in a similar role* Security+ CE* Compliance with DoD 8570.01-M Requirements* Ability to obtain a Secret Clearance (SSBI) or Top Secret ClearanceEOE M/F/Disability/Veterans
","['aw', 'powerpoint', 'excel', 'microsoft']","['recommend', 'big data', 'analyz']",999,"['aw', 'powerpoint', 'excel', 'microsoft', 'recommend', 'big data', 'analyz']","['program', 'techniqu', 'set', 'provid', 'aw', 'employe', 'packag', 'big', 'integr', 'sourc', 'engin', 'evalu']","['aw', 'powerpoint', 'excel', 'microsoft', 'recommend', 'big data', 'analyz', 'program', 'techniqu', 'set', 'provid', 'aw', 'employe', 'packag', 'big', 'integr', 'sourc', 'engin', 'evalu']"
DE,"* Prepares detailed system documentation including requirements, specifications, test plans and user manuals.
* Performs unit and system tests and, as needed, validation testing.
* Assists DBA with database design.
* Strong SQL and Analytical skills to pull data from Snowflake/Oracle/SQL Server and perform usage analysis and/or data profiling.
* Understanding of data warehousing and data modelling principles* Experience in understanding both logical and physical data models* Coordinates with Operations staff on deployment of applications.
* Works with business partners to analyze and address support requests including training.
* Provides support for existing systems and technologies, including second line application support during and after deployment.
* Ensures all activities are performed with quality and compliance.
* Design and implementation of ETL batches that meet the SLAs for the business' Business Intelligence reporting.
* Development of data collection, data staging, data movement, data quality and archiving strategies.
* Plan and conduct ETL Unit and development tests, monitoring results and taking corrective action when necessary.
* Design automation processes to control data access, transformation and movement and ensures source system data availabilityQualificationsEducation and Experience:* Bachelor's degree or equivalent and relevant formal academic / vocational qualification* Previous experience that provides the knowledge, skills, and abilities to perform the job (comparable to 3 years') or equivalent combination of education, training, and experienceKnowledge, Skills and Abilities* Experienced in the use of the ETL tools (Talend/Informatica) and Cloud based database tools (Snowflake/Redshift/Google Cloud)* Client focused approach with strong interpersonal skills* Must be able to multitask and pay close attention to detail* Solid knowledge of basic relational database platforms (Snowflake, Oracle, SQL Server) and languages (PL/SQL, SQL)* Build processes supporting data transformation, data structures, metadata, dependency and workload management* Strong logic, analysis, and problem-solving skills* A good understanding of the concepts and best practices of data warehouse ETL design.
* Ability to follow functional ETL specifications and challenge business logic and schema design where appropriate, as well as manage their time effectively* Have designed and developed ETL work packages on at least one data warehouse project.
* Expertise with big data tools: Hadoop, Spark, Kafka, etc.
is a plus* Expertise on any of the programming languages: Python or Scala* Experience in building and optimizing 'big data' data pipelines, architectures and data sets.
* Experience with AWS cloud services: Snowflake, Redshift, S3, EC2, EMR
","['sql', 'google cloud', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka', 'oracl']","['data warehousing', 'etl', 'pipelin', 'big data']",1,"['sql', 'google cloud', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka', 'oracl', 'data warehousing', 'etl', 'pipelin', 'big data']","['program', 'provid', 'python', 'packag', 'etl', 'sourc', 'collect', 'analyt', 'challeng', 'line', 'hadoop', 'action', 'aw', 'warehous', 'set', 'big', 'spark', 'pipelin', 'relat']","['sql', 'google cloud', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'redshift', 'cloud', 'kafka', 'oracl', 'data warehousing', 'etl', 'pipelin', 'big data', 'program', 'provid', 'python', 'packag', 'etl', 'sourc', 'collect', 'analyt', 'challeng', 'line', 'hadoop', 'action', 'aw', 'warehous', 'set', 'big', 'spark', 'pipelin', 'relat']"
DE,"The Data Engineer will support the engineering teams data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once.
Additional Responsibilities:
Work with internal stakeholders to load data into the data warehouse
Troubleshoot and resolve issues relating to data integrity
Help establish procedures and best practices for transforming and storing data
Lead requirements gathering around data pipeline automation improvements
Work with open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
Research and implement new technologies with a team of developers to execute strategies and implement solutions
Solve complex problems related to the real-time discovery of large data
Qualifications
Successful Data Engineers will have 5+ years of experience writing scalable applications on distributed architectures.
Additional Qualifications:
3+ years of experience with Python
3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
1+ years of experience with AWS EMR, AWS S3 service.
Comfortable using AWS CLI and boto3
Comfortable using *nix command line (shell scripting, AWK, SED)
Experience with MySQL and Postgres
Experience with Apache Airflow preferred
Experience with Apache Zeppelin preferred
Experience with healthcare data preferred
Additional Information
Your application will be reviewed within 24-hours.
You must submit your application to be considered - please no phone calls or third parties.
INTERVIEW PROCESS
Round 2 = Phone Interview with HR (30 minutes)
Round 3 = Online Tech Assessment (done at home)
Round 4 = Video Interview with Engineers + Interactive Coding Challenge
Round 5 = Decision
","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'postgr', 'mysql', 'docker']","['research', 'healthcar', 'pipelin']",999,"['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin']","['spark', 'challeng', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'parti', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'pyspark', 'spark', 'airflow', 'hadoop', 'aw', 'lambda', 'python', 's3', 'docker', 'research', 'healthcar', 'pipelin', 'spark', 'challeng', 'pipelin', 'relat', 'line', 'hadoop', 'aw', 'python', 'parti', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Successful candidates for this position must have experience designing and building production-level data pipelines.
The candidate should be able to :
Create / enhance data pipelines, including the creation, enhancement and automation of advanced ETL flows and processes.
Collaborate in the design, development, test and maintenance of scalable data management solutions.
Assist other teams with data analysis (i.e.
feature extraction) where appropriate and applicable.
Provide commercial quality software processes in a professional and timely manner.
Author clear technical documentation.
Accurately scope work and perform to agreed times lines.
Requirements
3+ years of experience with Python Programming Language.
Bachelors Degree in Computer Science or related field.
Hands-on experience in data modeling.
Develop applications and services that run on a cloud infrastructure (Private and Public).
Experience working in a Linux/Unix environment.
Note that the majority of the work will be within the Azure infrastructure.
Knowledge and experience in ETL tools and automating data processing workflows.
Good level of understanding of data warehousing, business intelligence, and application data integration solutions.
Working knowledge of Apache Spark.
Knowledge of Azure Databricks.
Ability to thrive in a fast-paced, ever changing environment.
Excellent problem-solving skills and troubleshooting issues in a large, complex environment.
Experience with container management and deployment, such as Docker and Kubernetes.
Experience in data wrangling software as an analyst.
Excellent communication skills, both verbal and written.
This includes the ability to write technical documentation, user guides, etc.
as appropriate
Benefits
The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
","['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker']","['pipelin', 'data modeling', 'data warehousing', 'etl', 'commun']",1,"['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun']","['program', 'spark', 'challeng', 'azur', 'pipelin', 'relat', 'line', 'public', 'infrastructur', 'provid', 'python', 'comput', 'etl', 'integr']","['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun', 'program', 'spark', 'challeng', 'azur', 'pipelin', 'relat', 'line', 'public', 'infrastructur', 'provid', 'python', 'comput', 'etl', 'integr']"
DE,"Big Data Engineer
Length: 1+ yearÂ
Â
Interview mode: Video/Skype
Â
Would be Remote until the Current Pandemic (Covid â 19) is settled
Â
Details
- Scala development
- Python scripting
- Experience with Hive
- AWS experience (DynamoDB, S3, Lambda, Kinesis, EMR, etc.)
","['scala', 'aw', 'lambda', 'python', 'hive', 's3']",['big data'],999,"['scala', 'aw', 'lambda', 'python', 'hive', 's3', 'big data']","['aw', 'python', 'â', 'big', 'engin']","['scala', 'aw', 'lambda', 'python', 'hive', 's3', 'big data', 'aw', 'python', 'â', 'big', 'engin']"
DE,"Kubernetes - MUST 2.
Docker ndash MUST (Kubernetes comes hand in hand with Docker) 3.
Python ndash MUST (makes most sense) 4.
Cloud (if they know the above items they should know the cloud piece) 3-5 years experience is fine.
Years does not really matter.
2-3 if they know the skills is fine too.
The DE will be responsible for supporting the existing capabilities, enhancing and building new capabilities.
The candidate should be able to quickly learn and adapt to new technologies as the work demands.
They should also have working experience using the following Strong Python is a must.
Should have at least mid-level experience with buildingdeploying applications to Kubernetes.
Able to troubleshoot and debug using logsevents in Kubernetes.
TechnologySkills ProgrammingScripting Python, Shell scripting, Groovy(good to have) Golang(good to have) Cloud AWS (EC2, EMR, Lamda, IAM, EFS, EBS etc.
), Data StreamingProcessing KinesisKafkaSQS, Spark CICD Jenkins, GitHubBitbucket, Container Technologies Docker, Kubernetes Configuration ManagementOrchestration Ansible, Chef, AWS CloudFormation Any experience with Airflow, Restful API is preferred.
","['spark', 'airflow', 'ec2', 'aw', 'cloud', 'python', 'kubernet', 'docker']",[None],999,"['spark', 'airflow', 'ec2', 'aw', 'cloud', 'python', 'kubernet', 'docker']","['aw', 'python', 'spark']","['spark', 'airflow', 'ec2', 'aw', 'cloud', 'python', 'kubernet', 'docker', 'aw', 'python', 'spark']"
DE,"Understanding of AWS and at least hands on experience working in projects with AWS.
Knowledge of the software development life cycle, agile methodologies, and test-driven development.
Sound understanding of continuous integration continuous deployment environments.
Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards Strong analytical skills with a passion for testing.
Excellent problem solving and debugging.
Skills Exposure in Data Management, Governance and Controls functions a plus Technical and quantitative reasoning skills with regard to marketing data is a plus.
Thanks Regards Gaurav Dhuware Sr. Technical Recruiter (732) 200-1307 Ext 5089 Gauravdaptask.com mailtoGauravdaptask.com Address 120 Wood Ave South, Iselin, NJ 08830
","['aw', 'excel']",['problem solving'],999,"['aw', 'excel', 'problem solving']","['analyt', 'program', 'techniqu', 'aw', 'quantit', 'integr']","['aw', 'excel', 'problem solving', 'analyt', 'program', 'techniqu', 'aw', 'quantit', 'integr']"
DE,"middot Understanding of AWS and at least hands on experience working in projects with AWS.
Knowledge of the software development life cycle, agile methodologies, and test-driven development.
middot Sound understanding of continuous integration continuous deployment environments.
Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards middot Strong analytical skills with a passion for testing.
Excellent problem solving and debugging skills middot Exposure in Data Management, Governance and Controls functions a plus middot Technical and quantitative reasoning skills with regard to marketing data is a plus.
Share Resume Raviomegasolutioninc.com
","['aw', 'excel']",['problem solving'],999,"['aw', 'excel', 'problem solving']","['analyt', 'program', 'techniqu', 'aw', 'quantit', 'integr']","['aw', 'excel', 'problem solving', 'analyt', 'program', 'techniqu', 'aw', 'quantit', 'integr']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"Job Summary:
Designs and develops updated infrastructure in support of one or more business processes.
Helps to ensure a balance between tactical and strategic technology solutions.
Considers business problems “end-to-end”: including people, process, and technology, both within and outside the enterprise, as part of any design solution.
Mentors, reviews codes and verifies that the object oriented design best practices and that coding and architectural guidelines are adhered to.
Identifies and drives issues through closure.
This role brings to bear significant cloud experience in the private and public cloud space as well as streaming data, big data and software engineering.
This role will be key in the delivery of a productized real-time event processing product for customer experience.
This person will engage and lead software delivery teams and contribute to several strategic efforts that drive personalized customer experiences across product usage, support interactions and customer journeys.
Core Responsibilities
-Enterprise-Level architect for “Big Data” real-time event processing, analytics, data store, and cloud platforms.
-Enterprise-Level architect for cloud applications and “Platform as a Service” capabilities
-Detailed current-state product and requirement analysis.
-Security Architecture for “Big Data” applications and infrastructure
-3rd party vendor analysis and management.
-Ensures programs are envisioned, designed, developed, and implemented across the enterprise to meet business needs.
Interfaces with the enterprise architecture team and other functional areas to ensure that most efficient solution is designed to meet business needs.
-Ensures solutions are well engineered, operable, maintainable, and delivered on schedule.
Develops, documents, and ensures compliance with best practices including but not limited to the following coding standards, object oriented design, platform and framework specific design concerns and human interface guidelines.
-Tracks and documents requirements for enterprise development projects and enhancements.
-Monitors current and future trends, technology and information that will positively affect organizational projects; applies and integrates emerging technological trends to new and existing systems architecture.
Mentors team members in relevant technologies and implementation architecture.
-Contributes to the overall system implementation strategy for the enterprise and participates in appropriate forums, meetings, presentations etc.
to meet goals.
-Gathers and understands client needs, finding key areas where technology leverage is possible to improve business processes, defines architectural approaches and develops technology proofs.
Communicates technology direction.
-Monitors the project lifecycle from intake through delivery.
Ensures the entire solution design is complete and consistent from the start and seeks to remove as much re-work as possible.
-Works with product marketing to define requirements.
Develops and communicates system/subsystem architecture.
Develops clear system requirements for component subsystems.
-Acts as architectural lead on project in partnership with domain/program architects.
-Applies new and innovative ideas to old or new problems.
Fosters environments that encourage innovation.
Contributes to and supports effort to further build intellectual property via patents.
-Consistent exercise of independent judgment and discretion in matters of significance.
-Regular, consistent and punctual attendance.
Must be able to work nights and weekends, variable schedule(s) as necessary.
-Other duties and responsibilities as assigned.
Job Specification:
-Demonstrated experience with “Platform as a Service” (PaaS) architectures including strategy, architectural patterns and standards, approaches to multi-tenancy, scalability, and security.
-Demonstrated experience with schema and data governance and message metadata stores
-Demonstrated experience with 3rd party software vendor relationships, vendor evaluation, vendor quality audits, and contract negotiation.
-Demonstrated experience with private cloud infrastructure both OpenStack and VMWare.
-Demonstrated experience with public cloud resources such as AWS.
-Demonstrated experience with cloud automation technologies including Ansible, Terraform, Chef, Puppet, etc…
-Hands-on experience with Data Flow processing engines, such as Apache NiFi
-Working knowledge / experience with Big Data platforms (Kafka, Hadoop, Storm/Spark, NoSQL, In-memory data grid)
-Working knowledge / experience with Linux, Java, Python.
-Product Management experience highly desired
-Staff/People Management experience highly desired
Job Specification:
-Bachelors Degree or Equivalent in Engineering, Computer Science
-15+ years in Software Engineering Experience
-10+ years in Technical Leadership roles
-5+ years in Cloud Infrastructure
","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka']","['commun', 'big data']",1,"['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'commun', 'big data']","['program', 'python', 'integr', 'evalu', 'analyt', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'spark', '3rd', 'comput']","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'commun', 'big data', 'program', 'python', 'integr', 'evalu', 'analyt', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'spark', '3rd', 'comput']"
DE,"Lead Big Data Engineer in Wayne, PA 19087 Interview Logistics Phone screen followed by Multiple rounds of Video Conferences Required Skills Set Years of Experience 8-10 years of Information Technology experience with data management 5+ years of software engineering experience in Python, Scala, Java, Spark, Hadoop or .NET 5+ years of experience with schema design, dimensional data mo
","['spark', 'scala', 'hadoop', 'java', 'python']","['information technology', 'logist', 'big data']",999,"['spark', 'scala', 'hadoop', 'java', 'python', 'information technology', 'logist', 'big data']","['spark', 'hadoop', 'python', 'big', 'set', 'engin']","['spark', 'scala', 'hadoop', 'java', 'python', 'information technology', 'logist', 'big data', 'spark', 'hadoop', 'python', 'big', 'set', 'engin']"
DE,"This is a full time position with a competitive salary and benefits.
Qualified candidates should send their resume (Word Format) along with salary requirements to Harold Miller at hmillergenuent.com This is a direct hire role.
No contractors please.
In addition, the Lead Data Engineer defines and promotes adaptation of best practices, provides consultative guidance and recommendations on pragmatic solutions to solve complex business requirements within the context of one or more digital transformation initiatives.
The various initiatives for the team affects multiple business units, large end-user populations, corporate client delivery, applications, and other areas of solution delivery across the organization.
As such, this position is critical to ensuring the appropriate stakeholders are engaged in evaluating the optimal solution design to meet the complex business needs.
The position requires someone with excellent communications skills (verbal and written), works well under pressure, and effectively work with cross-functional teams throughout a diverse business community.
This position requires extensive knowledge of batch and real-time data pipelines, modern data warehousing, and experience working in an agile organization.
Knowledge of IT processes, supporting technologies, and how these technologies integrate across initiatives and existing environments is a must.
Day to Day Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and enabling data-driven decisions across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate, secure, and available for key stakeholders and business processes that depend on it.
Contributes to engineering communities of practice, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Your Profile BS or MS degree in Computer Information Science or related technical field 8-10 years of Information Technology experience with data management 5+ years of software engineering experience in Python, Scala, Java, or .NET 5+ years of experience with schema design, dimensional data modeling, and data storage technology 5+ years of multiple kinds of database experience (SQL and No-SQL) Proven ability in managing and communicating data warehouse plans to internal clients Experience designing, building, and maintaining secure, reliable batch and real time data pipelines Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus) Experience with event streaming platforms like Kafka or Azure Event Hubs Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management Knowledge of best practices and IT operations in an always-up, always-available service Experience with or knowledge of Agile Development methodologies (SAFe is a plus) Excellent analytical problem solving and troubleshooting skills Excellent oral and written communication skills with a keen sense of customer service Excellent team player with proven ability to influence Highly adaptable to a continuously changing environment Able to give and receive open, honest feedback and to foster a feedback environment Outstanding communication, interpersonal, relationship building skills for team development Possible Travel (10) Experience within financial services is a plus
","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel']","['recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']",1,"['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun']","['day', 'analyt', 'digit', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'recommend', 'pipelin', 'data modeling', 'optim', 'data warehousing', 'problem solving', 'information technology', 'commun', 'day', 'analyt', 'digit', 'pipelin', 'azur', 'corpor', 'relat', 'divers', 'optim', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Designs and develops updated infrastructure in support of one or more business processes.
Helps to ensure a balance between tactical and strategic technology solutions.
Considers business problems ""end-to-end"": including people, process, and technology, both within and outside the enterprise, as part of any design solution.
Mentors, reviews codes and verifies that the object oriented design best practices and that coding and architectural guidelines are adhered to.
Identifies and drives issues through closure.
This role brings to bear significant cloud experience in the private and public cloud space as well as streaming data, big data and software engineering.
This role will be key in the delivery of a productized real-time event processing product for customer experience.
This person will engage and lead software delivery teams and contribute to several strategic efforts that drive personalized customer experiences across product usage, support interactions and customer journeys.
This role leads the building of real-time big data platforms, machine learning algorithms and data services that enable proactive responses for customers at every critical touch point.
Core Responsibilities -Enterprise-Level architect for ""Big Data"" real-time event processing, analytics, data store, and cloud platforms.
-Enterprise-Level architect for cloud applications and ""Platform as a Service"" capabilities-Detailed current-state product and requirement analysis.-Security Architecture for ""Big Data"" applications and infrastructure-3rd party vendor analysis and management.-Ensures programs are envisioned, designed, developed, and implemented across the enterprise to meet business needs.
Interfaces with the enterprise architecture team and other functional areas to ensure that most efficient solution is designed to meet business needs.
-Ensures solutions are well engineered, operable, maintainable, and delivered on schedule.
Develops, documents, and ensures compliance with best practices including but not limited to the following coding standards, object oriented design, platform and framework specific design concerns and human interface guidelines.
-Tracks and documents requirements for enterprise development projects and enhancements.
-Monitors current and future trends, technology and information that will positively affect organizational projects; applies and integrates emerging technological trends to new and existing systems architecture.
Mentors team members in relevant technologies and implementation architecture.
-Contributes to the overall system implementation strategy for the enterprise and participates in appropriate forums, meetings, presentations etc.
to meet goals.
-Gathers and understands client needs, finding key areas where technology leverage is possible to improve business processes, defines architectural approaches and develops technology proofs.
Communicates technology direction.-Monitors the project lifecycle from intake through delivery.
Ensures the entire solution design is complete and consistent from the start and seeks to remove as much re-work as possible.
-Works with product marketing to define requirements.
Develops and communicates system/subsystem architecture.
Develops clear system requirements for component subsystems.
-Acts as architectural lead on project in partnership with domain/program architects.-Applies new and innovative ideas to old or new problems.
Fosters environments that encourage innovation.
Contributes to and supports effort to further build intellectual property via patents.
-Consistent exercise of independent judgment and discretion in matters of significance.
-Regular, consistent and punctual attendance.
Must be able to work nights and weekends, variable schedule(s) as necessary.
-Other duties and responsibilities as assigned.
Job Specification:-Demonstrated experience with ""Platform as a Service"" (PaaS) architectures including strategy, architectural patterns and standards, approaches to multi-tenancy, scalability, and security.-Demonstrated experience with schema and data governance and message metadata stores-Demonstrated experience with 3rd party software vendor relationships, vendor evaluation, vendor quality audits, and contract negotiation.-Demonstrated experience with private cloud infrastructure both OpenStack and VMWare.-Demonstrated experience with public cloud resources such as AWS.-Demonstrated experience with cloud automation technologies including Ansible, Terraform, Chef, Puppet, etc...-Hands-on experience with Data Flow processing engines, such as Apache NiFi-Working knowledge / experience with Big Data platforms (Kafka, Hadoop, Storm/Spark, NoSQL, In-memory data grid)-Working knowledge / experience with Linux, Java, Python.-Product Management experience highly desired-Staff/People Management experience highly desired Job Specification:-Bachelors Degree or Equivalent in Engineering, Computer Science-15+ years in Software Engineering Experience-10+ years in Technical Leadership roles-5+ years in Cloud InfrastructureComcast is an EOE/Veterans/Disabled/LGBT employerPDN-COM219139-en-us
","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka']","['machine learning', 'commun', 'big data']",1,"['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'machine learning', 'commun', 'big data']","['program', 'python', 'integr', 'algorithm', 'analyt', 'evalu', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'machin', 'spark', '3rd', 'comput']","['linux', 'spark', 'hadoop', 'aw', 'cloud', 'java', 'python', 'nosql', 'kafka', 'machine learning', 'commun', 'big data', 'program', 'python', 'integr', 'algorithm', 'analyt', 'evalu', 'judgment', 'human', 'hadoop', 'appli', 'public', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'particip', 'machin', 'spark', '3rd', 'comput']"
DE,"In this role you will be addressing real world challenges through the development of new features and integration of new devices on an ever-growing IoT platform.
You will also be developing a cross-platform software products for web, tablet, and mobile use while owning small-medium size features from technical design to completion.This data engineer will be an integral part that is forging a new path in IoT technology by working with internal and external stakeholders to understand how they are using the product's data, align on useful metrics, and produce data representations an interpretable way.
You will use creative analysis to find solutions across multiple databases and ensure data findings are accessible to users at all levels through dashboards, visualizations, and reports.
This resource features localized tech salaries for 12 different cities.
You'll find the data to guide your job search here.
Download Motion Recruitment's 2020 Salary Guide today!
https://hubs.ly/H0nmhDT0
",[None],"['dashboard', 'visual']",999,"['dashboard', 'visual']","['integr', 'interpret', 'visual', 'engin']","['dashboard', 'visual', 'integr', 'interpret', 'visual', 'engin']"
DE,"Bachelor's degree (B.A or B.S) from four-year College or University requiredDegrees in IT, Computer Science, Information Science, MIS, Finance or similar preferred* Excellent written and verbal communication skills, including technical writing skills* Basic understanding of systems* Excellent creative problem solving skills* Must be detail oriented and a critical thinker* Motivated, hardworking and self-starter* Ability to handle multiple projects and meet deadlines* Excellent problem resolution and consultative skills* Ability to prioritize and be organized* Ability to work independently, self-driven, highly motivated and results-oriented#CBThe First Flyer Data Engineer IT Program is offering a 2-year rotational training program that will allow you to experience hands on training among 4 different teams in the technology side of the business.
A Mentor will be assigned to each trainee to support the First Flyer's needs throughout his/her first 2 years at Freedom Mortgage.Essential Job Functions:* Analyze the business and IT resources to take lead and create a project solution during each rotation* Travel and shadow line of business under the context of learning the mortgage industry* Working with a mentor to create a personal certification/training plan within the first 6 months* Work with the Business Solutions Group to research business areas for inefficient and informal processes* Develop strategic workflow designs for process improvement to enhance loan processing* Develop Business Solutions with the Corporate Support Solutions team to enhance the business through IT solutions* Act as a liaison on the Service Desk between business analyst, application support and the business* Perform quality assurance testing of system changes and enhancements* Create Visual Reporting Structures on Freedom's Enterprise Performance Management team* Use systems such as Tableau, Business Object, Sharepoint and IBM Blueworks Live
","['tableau', 'excel']","['research', 'visual', 'financ', 'problem solving', 'analyz', 'commun']",1,"['tableau', 'excel', 'research', 'visual', 'financ', 'problem solving', 'analyz', 'commun']","['program', 'corpor', 'line', 'visual', 'comput', 'engin']","['tableau', 'excel', 'research', 'visual', 'financ', 'problem solving', 'analyz', 'commun', 'program', 'corpor', 'line', 'visual', 'comput', 'engin']"
DE,"Successful candidates for this position must have experience designing and building production-level data pipelines.
The candidate should be able to :
Create / enhance data pipelines, including the creation, enhancement and automation of advanced ETL flows and processes.
Collaborate in the design, development, test and maintenance of scalable data management solutions.
Assist other teams with data analysis (i.e.
feature extraction) where appropriate and applicable.
Provide commercial quality software processes in a professional and timely manner.
Author clear technical documentation.
Accurately scope work and perform to agreed times lines.
Requirements
3+ years of experience with Python Programming Language.
Bachelor’s Degree in Computer Science or related field.
Hands-on experience in data modeling.
Develop applications and services that run on a cloud infrastructure (Private and Public).
Experience working in a Linux/Unix environment.
Note that the majority of the work will be within the Azure infrastructure.
Knowledge and experience in ETL tools and automating data processing workflows.
Good level of understanding of data warehousing, business intelligence, and application data integration solutions.
Working knowledge of Apache Spark.
Knowledge of Azure Databricks.
Ability to thrive in a fast-paced, ever changing environment.
Excellent problem-solving skills and troubleshooting issues in a large, complex environment.
Experience with container management and deployment, such as Docker and Kubernetes.
Experience in data wrangling software as an analyst.
Excellent communication skills, both verbal and written.
This includes the ability to write technical documentation, user guides, etc.
as appropriate
Benefits
The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
","['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker']","['pipelin', 'data modeling', 'data warehousing', 'etl', 'commun']",1,"['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun']","['program', 'spark', 'challeng', 'azur', 'pipelin', 'relat', 'line', 'public', 'infrastructur', 'provid', 'python', 'comput', 'etl', 'integr']","['linux', 'spark', 'azur', 'unix', 'cloud', 'python', 'kubernet', 'excel', 'docker', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun', 'program', 'spark', 'challeng', 'azur', 'pipelin', 'relat', 'line', 'public', 'infrastructur', 'provid', 'python', 'comput', 'etl', 'integr']"
DE,"A local Center City is building out their Data Analytics team and is looking for a strong Data Engineer to add onto their team.
This is a very forward thinking, innovation group that predicts outcomes of customer's behaviors.
They're currently looking for a Data Engineer to build out data pipeline for them.
They really hire for fundamentals and culture fit over specific technologies but this person should be experienced building a pipeline from scratch.
Also, great to have a data science understanding as that will be the next evolution of this group.
Can pay well, great quality of life, lots of growth and movement opportunities.Required Skills & Experience* At least 3 years' experience working as a Data Engineer* 3+ years' experience with building out Data Pipelines* Strong Python or Java experience* Engineering skills they look for are: python, scala (pyspark is great too), Spark and big data databases.
* Software Engineering Background is a plus* Interest in Machine Learning as you would be working alongside the data scientists* AWS is a plusWhat You Will Be DoingTech Breakdown* 100% Building out Data PipelinesDaily Responsibilities* 100% Hands OnThe Offer* Competitive Salary: Up to $140K/year, DOEYou will receive the following benefits:* Educational assistance* Medical Insurance, Vision, and Dental* 401(k)* Catered lunches* WFH options/ Remote flex* SEPTA accessibleApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['pyspark', 'spark', 'scala', 'aw', 'java', 'python']","['machine learning', 'pipelin', 'big data']",999,"['pyspark', 'spark', 'scala', 'aw', 'java', 'python', 'machine learning', 'pipelin', 'big data']","['basi', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'aw', 'python', 'scientist', 'big', 'engin']","['pyspark', 'spark', 'scala', 'aw', 'java', 'python', 'machine learning', 'pipelin', 'big data', 'basi', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'aw', 'python', 'scientist', 'big', 'engin']"
DE,"Data Engineers
Duration: 10 months with possible extension/ with possible contract to hire
Glider:
Make sure to use the coding section
Team Info:
Retail Banking/ Data transformation team, Big Data shop
Primarily used Ab Initio and Teradata and have now migrated to cloud and modernizing using Python, Spark, etc.
Goal is to build complementary products, and consolidate into one place
5 plus years' experience (mid-senior level)
Migrate on-prem Ab Initio legacy platform into AWS
Skills: Python core programming skills is top, Scala, Spark,
Understanding of data tools and analytics (no particular tools needed)- how to join data
AWS (EC2, EMR, Lambda)
Build is on Automation pipeline so deployment experience is a plus
Will take PySpark in lieu of having Python
Glider: Big Data/ Machine Learning.
","['pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python']","['machine learning', 'pipelin', 'big data']",999,"['pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python', 'machine learning', 'pipelin', 'big data']","['analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'aw', 'python', 'big', 'engin']","['pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python', 'machine learning', 'pipelin', 'big data', 'analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'aw', 'python', 'big', 'engin']"
DE,"The world experiences constant change, but one industry that will always be prevalent and in need of continuous innovation is the healthcare industry.
They are now looking to bring on a skilled Data Engineer to their team of expert technologists.
You will be joining a team of experts in the field who will cultivate your growth and provide you with the tools needed to succeed.
This resource features localized tech salaries for 12 different cities.
You'll find the data to guide your job search here.
Download Motion Recruitment's 2020 Salary Guide today!
https://hubs.ly/H0nmhDT0
",[None],['healthcar'],999,['healthcar'],['engin'],"['healthcar', 'engin']"
DE,"One of the largest and most innovative digital media and entertainment companies in North America is looking to add a new member to their super welcoming and close knit team.
Their offices are in the heart of center city Philadelphia, open to candidates who need to work remotely but have great preference to those in the greater Philadelphia area.
Looking for someone with a strong background working as a data engineer pulling and extracting data.
Mostly working with Spark and PySpark from On-Prem and AWS databases.
Experience working building data pipelines is preferred.
","['aw', 'pyspark', 'spark']",['pipelin'],999,"['aw', 'pyspark', 'spark', 'pipelin']","['digit', 'spark', 'pipelin', 'aw', 'engin']","['aw', 'pyspark', 'spark', 'pipelin', 'digit', 'spark', 'pipelin', 'aw', 'engin']"
DE,"Senior Data Engineer
Senior or Lead Data Engineer - Python, data - Philadelphia!
Ideally, this person will be passionate about data and engineering - Python, SQL, and AWS - and will have 5+ years of engineering experience to date.
What You Need for this Position
Python
AWS
SQL
deep data engineering and platform build outs
mentoring / lead experience
What You Will Be Doing
Data processing pipeline development using Python and SQL.
New database solutions architecture.
Strategic planning and external client interaction.
What's In It for You
An opportunity to join a very smart, motivated team and to directly impact the product portfolio!
Excellent benefits and vacation policy.
Growth potential and a great working environment.
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'aw', 'python', 'excel']",['pipelin'],999,"['sql', 'aw', 'python', 'excel', 'pipelin']","['aw', 'python', 'engin', 'pipelin']","['sql', 'aw', 'python', 'excel', 'pipelin', 'aw', 'python', 'engin', 'pipelin']"
DE,"Immediate need for a talented Big Data Engineerwith experience in the IT Industry.
This is a 8+ Months contract opportunity with long-term potential and is located in Wilmington, DE.
Job ID: 20-09894
Note: ** This opportunity is REMOTE until COVID-19 crisis setttles down**
They should also have working experience using the following software/tools:
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, PySpark, Scala, etc.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
#san J2W:CB3
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'postgr', 'cloud', 'kafka']","['pipelin', 'big data']",999,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']"
DE,"Director Business Intelligence Data Engineering & Automation
Philadelphia, PA
Telecommuter?
Yes
ID**
19067
Your career starts now.
Under the direction of the VP, Chief Analytics Officer this position will oversee the Business Analysis, Data Engineering and Automation and Business Intelligence functions within Enterprise Analytics.
This role will be responsible for oversight of building, deploying and testing of Business Intelligence (BI) systems that integrate with databases, data warehouses, data marts and data lake.
This position will be responsible for dashboards and/or reports according to the identified technical requirements in addition to being responsible for performance tuning, version controls and documentation of those dashboards/reports.
Position will collaborate closely with internal and external stakeholders across the organization to develop sustainable solutions that support data-driven decision making while acting as a champion for data and analytics services.
Will lead a team of Tableau developers, Business Analysts to support a common data and analytics platform integrating new and existing data sources to eliminate silos.
Responsibilities:
Oversee solutions/framework for provisioning data for self-service consumption by business users who create BI applications, as well as supporting any type of adhoc reporting/analytic requests.
Define, build and execute the Data Services and technology roadmap ensuring that it is in alignment with the Application, Infrastructure and Business roadmaps.
Drive build and automation of tableau reports; ensure requirements are successfully executed and issues are resolved timely.
Very strong tableau and data warehousing expertise required
Proactively identify opportunities and technologies regarding the access, processing, visual display and analysis of data; maintain a pulse on data visualization and engineering trends.
Collaborates with internal departments to understanding their evolving data needs and develop and execute on analytics to meet those needs while driving towards improved outcomes and overall performance metrics.
Independently complete analysis, development and enhancement of BI solutions to fulfill business needs.
Maintain in-depth knowledge of project planning methodologies and tools as well as IT standards and guidelines.
Serve as a thought leader for technical business processes, developing forward-thinking prototypes that promote increased efficiency and productivity on multiple levels.
Education/ Expereince:
Master’s Degree.
Bachelor’s Degree (Computer Science or related field preferred).
Associate degree courses in Programming, Data Structures and Algorithms, Information Design or Mathematics a plus.
8-10 year’ experience in analytics, preferably in the health care industry.
Other Skills:
Analytical mindset focused on process and data analysis; problem solving aptitude.
Some experience of relevant data visualization, including but not limited to experience with visualizations and web-based interactives, geospatial, and/or visual story telling preferred.
Superior organizational, close attention to detail and written/oral communication skills.
Superior technical writing skills in business requirements, queries, reports, and presentations.
Superior technical skills in Excel, PowerPoint, Tableau, Visio, Microsoft Project with the ability to learn other analytic tools.
Advanced analytical and quantitative skills with experience collecting, organizing, mining, analyzing, visualizing and disseminating abundant information with the utmost accuracy and presentation.
Experience in conducting and documenting QA testing using problem solving techniques for analytic and reporting solutions.
Efficiently manages time-based on the continual evaluation of priorities, meets deadlines with high-quality deliverable reflecting complete understanding of expectations, able to multi-task.
","['bi', 'tableau', 'microsoft', 'powerpoint', 'excel']","['dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']",1,"['powerbi', 'tableau', 'microsoft', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati']","['analyt', 'program', 'techniqu', 'relat', 'visual', 'infrastructur', 'bi', 'comput', 'algorithm', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']","['powerbi', 'tableau', 'microsoft', 'powerpoint', 'excel', 'dashboard', 'visual', 'data warehousing', 'problem solving', 'commun', 'geospati', 'analyt', 'program', 'techniqu', 'relat', 'visual', 'infrastructur', 'bi', 'comput', 'algorithm', 'warehous', 'quantit', 'common', 'sourc', 'engin', 'evalu']"
DE,"One of the top IOT companies located in Philadelphia is looking for a Data Engineer.
As a Data Engineer you would be joining a team who is creating & molding the next generation of smart building technology.
Regardless of your background, they are looking adventurous, multifaceted individuals.
You will utilize your experience to assist in gaining user experience insights, product successes, key data opportunities, and overall scaling of their platform.Required Skills & Experience* 2+ years' experience in the field addressing real-world challenges.
* Ability to positively influence decision-making at all levels within an engineering organization with data insights;* Familiarity with Looker (LookML), AWS, Airflow, or similar technologies* Understanding of engineering principles, scrum practices* initiative, drive, organization, and task-management skillsWhat You Will Be DoingTech Breakdown* 80% Hands on* 20% Presenting findings of the businessThe Offer* Competitive Salary: Up to $120K/year, DOEYou will receive the following benefits:* Medical Insurance & Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Dog-friendly office for people-friendly dogs* Ability to see your contributions impact lives every day* Pre-tax Commuter BenefitApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets.
","['aw', 'airflow', 'looker']",['account'],999,"['aw', 'airflow', 'looker', 'account']","['day', 'aw', 'engin', 'basi']","['aw', 'airflow', 'looker', 'account', 'day', 'aw', 'engin', 'basi']"
DE,"Â
Â
Job Name: Lead Data Engineer III - Market A
Re-Location: Wayne, PA|| Remote Initially, but will require 100% onsite once Current Pandemic (Covid â 19) related restrictions are lifted.
Full time PositionÂ
Â
Day to Day:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and enabling data-driven decisions across the organization.
Contributes to engineering communities of practice, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Additional Information :
BS or MS degree in Computer Information Science or related technical field
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus)
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Â
","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel']","['pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']",1,"['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']","['day', 'analyt', 'engin', 'pipelin', 'azur', 'relat', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'â']","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun', 'day', 'analyt', 'engin', 'pipelin', 'azur', 'relat', 'python', 'avail', 'comput', 'warehous', 'integr', 'sourc', 'â']"
DE,"Site Name: UK - London - Brentford, USA - North Carolina - Research Triangle Park, USA - Pennsylvania - Upper Providence
Posted Date: Jul 13 2020
Are you looking for a challenging opportunity to work in an area where cutting edge science meets cutting edge technology with an aim of delivering drugs to patients in need across a broad range of pharmaceutical areas including genetics, functional genomics, clinical, biopharma and others ?
If so, this Data Engineer - Manager role could be an exciting opportunity to explore.
The Data & Compute Delivery (DCD) Data Engineering team is a crucial component of the environment and are responsible for delivery of data pipelines populating and maintaining data for scientific use in HPCs, Cloud and the R&D Information Platform (RDIP).
The successful candidate must be able to learn and work independently, lead pipeline development efforts and collaborate effectively with co-workers.
This role will provide YOU the opportunity to lead key activities to progress YOUR career, these responsibilities include some of the following:
Partner with data strategy leads to translate R&D strategy and conceptual data flows into pipelines
Partner with the metadata leads to translate conceptual data models into physical database/tables optimized for data analytics in RDIP using established environments and tools
Lead the design and build of data acquisition and processing pipelines including but not limited to the creation/maintenance of appropriate artifacts
Ensure the preservation of data integrity from source to target state including but not limited to the acquisition of appropriate metadata and the incorporation of appropriate QC checks into the pipelines
Surface opportunities to improve data quality and/or increase efficiency to DSD Leads
Support the use and growth of the Data Engineering DataOps environment, influence strategy and roadmap for the curation toolset, work with R&D and Tech to prioritize enhancements
Lead in the monitoring and tracking of data quality and data flow dynamic
Provide Tier 3 support for production pipelines
Support DCS and broader R&D in self-service/exploratory efforts
Influence vendor roadmaps, work with R&D and Tech to prioritize DataOps enhancements, and onboard these tools or enhancements
Ensure the quality consistency and availability of guidance documentation of end users of the tools to support high quality outputs
Extend current pipelines to support clinical biomarkers
Assess GxP readiness as it related to the upstream data pipelines and develop a plan for addressing any gaps
Why you?
Basic Qualifications:
Computer Science, Bioinformatics, or related degree; 10+ years experience in data movement, data wrangling and delivery of data or analytics pipelines
Experience architecting, implementing and maintaining, data or analytic pipelines.
Leadership experience.
Experience with Big Data technologies (ideally Cloudera stack including HDFS, Hive, Impala and Spark), Cloud-based offerings (Microsoft Azure, GCP, AWS, etc), and corresponding tools.
Experience with open source software, bioinformatics tools and languages such as SQL, R, Perl, Python, Java, and ETL tools.
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Experience with data movement and management in the Pharmaceutical industry or related scientific fields.
Strong background and experience in LIMS systems, Next Generation Sequencing (NGS) workflows, Cloud computing and HPC systems.
Understanding of diverse omic data types including RNA-Seq, DNA-Seq, Chip-Seq, WES, WGS, ATAC-seq, microbiome, proteomic, metabolomic data etc.
from different sources.
Familiarity with data mining, machine learning and artificial intelligence techniques
Strong interpersonal skills and effective communication of complex concepts to stake holders with wide range of expertise.
These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork.
Operating at pace and agile decision-making using evidence and applying judgement to balance pace, rigour and risk.
Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.
Continuously looking for opportunities to learn, build skills and share learning.
Sustaining energy and well-being.
Building strong relationships and collaboration, honest and open conversations.
Budgeting and cost-consciousness
*LI-GSK
This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Important notice to Employment businesses/ Agencies
For more information, please visit GSKs Transparency Reporting For the Record site.
","['sql', 'gcp', 'spark', 'perl', 'azur', 'aw', 'cloud', 'python', 'hive', 'java', 'r', 'microsoft']","['research', 'bioinformat', 'pipelin', 'data mining', 'end user', 'risk', 'machine learning', 'optim', 'sequenc', 'big data', 'exploratori', 'etl', 'commun', 'account']",1,"['sql', 'gcp', 'spark', 'perl', 'azur', 'aw', 'cloud', 'python', 'hive', 'java', 'r', 'microsoft', 'research', 'bioinformat', 'pipelin', 'data mining', 'end user', 'risk', 'machine learning', 'optim', 'sequenc', 'big data', 'exploratori', 'etl', 'commun', 'account']","['techniqu', 'provid', 'python', 'etl', 'integr', 'sourc', 'analyt', 'challeng', 'azur', 'optim', 'avail', 'clinic', 'learn', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']","['sql', 'gcp', 'spark', 'perl', 'azur', 'aw', 'cloud', 'python', 'hive', 'java', 'r', 'microsoft', 'research', 'bioinformat', 'pipelin', 'data mining', 'end user', 'risk', 'machine learning', 'optim', 'sequenc', 'big data', 'exploratori', 'etl', 'commun', 'account', 'techniqu', 'provid', 'python', 'etl', 'integr', 'sourc', 'analyt', 'challeng', 'azur', 'optim', 'avail', 'clinic', 'learn', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']"
DE,"Immediate need for a talented Big Data Engineerwith experience in the IT Industry.
This is a 8+ Months contract opportunity with long-term potential and is located in Wilmington, DE.
Job ID: 20-09894
Note: ** This opportunity is REMOTE until COVID-19 crisis setttles down**
They should also have working experience using the following software/tools:
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, PySpark, Scala, etc.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
#san
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'postgr', 'cloud', 'kafka']","['pipelin', 'big data']",999,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']"
DE,"IT Advisor, Performance Improvement - Technology Consulting (Digital Data and Analytics -Data Engineer) (Manager) (Multiple Positions), Ernst & Young U.S. LLP, Philadelphia, PA.
Help clients navigate the complex world of modern data analytics.
Provide clients with a unique business perspective on how Data Analytics can transform and improve their entire organization, applying a data driven approach (KPIs) in tying technology solutions to specific business outcomes.
Meet client goals by deliver the latest in data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyze data from multiple sources.
Analyze business needs and clarify problems, seeking opportunities for improving methods and outcomes.
Communicate findings, recommendations, and opportunities to improve data systems and solutions.
Unify, enrich and analzye customer data to derive insights and opportunities.
Seek information to learn about emerging methodologies and technologies.
Collaborate, influence and build consensus.
Full time employment, Monday – Friday, 40 hours per week, 8:30 am – 5:30 pm.
MINIMUM REQUIREMENTS:
Bachelor's degree in Mathematics, Information Systems, Statistics, Computer Science, or a related field of study and 5 years of post-baccalaureate, progressive related work experience; or a Master's degree Mathematics, Information Systems, Statistics, Computer Science, or a related field of study and 4 years of related work experience.
Must have 4 years of professional services or consulting experience
Must have 4 years of hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, Cassandra, MongoDB, Azure DB or Neo4J.
Must have 2 years of hands on experience with architecture and implementation of data warehouse and ETL solutions
Must have 3 years of experience with logical and physical data modeling
Must have 4 years of experience with SQL, C#, R and Typescript/Javascript.
Must have 2 years of experience in API development and 3-tier application development
Must have 2 years of experience with big data platforms, Microsoft Azure technologies and server-less environment
Must have 2 years of experience leading teams of data professionals
Requires travel up to 80%.
Employer will accept any suitable combination of education, training or experience.
This particular position at Ernst & Young in the United States requires the qualified candidate to be a ""United States worker"" as defined by the U.S. Department of Labor regulations at 20 CFR 656.3.
You can review this definition at https://www.gpo.gov/fdsys/pkg/CFR-2011-title20-vol3/pdf/CFR-2011-title20-vol3-sec656-3.pdf at the bottom of page 750.
","['mongodb', 'sql', 'cassandra', 'azur', 'javascript', 'db', 'c', 'r', 'nosql', 'microsoft', 'oracl']","['recommend', 'big data', 'data modeling', 'statist', 'analyz', 'etl', 'commun', 'kpi']",1,"['mongodb', 'sql', 'cassandra', 'azur', 'javascript', 'db', 'c', 'r', 'nosql', 'microsoft', 'oracl', 'recommend', 'big data', 'data modeling', 'statist', 'analyz', 'etl', 'commun', 'kpi']","['analyt', 'digit', 'azur', 'relat', 'provid', 'comput', 'warehous', 'statist', 'big', 'etl', 'sourc', 'engin']","['mongodb', 'sql', 'cassandra', 'azur', 'javascript', 'db', 'c', 'r', 'nosql', 'microsoft', 'oracl', 'recommend', 'big data', 'data modeling', 'statist', 'analyz', 'etl', 'commun', 'kpi', 'analyt', 'digit', 'azur', 'relat', 'provid', 'comput', 'warehous', 'statist', 'big', 'etl', 'sourc', 'engin']"
DE,"Position: Data Engineer
Duration: Contract/ Full time
DESIRED TECHNICAL SKILLS:
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: NiFi, Kylo, Luigi, Airflow, Azkaban, etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, PySpark, Scala, etc.
Qualifications for Data Engineer:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
","['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'postgr', 'cloud', 'kafka']","['pipelin', 'big data']",999,"['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']","['sql', 'pyspark', 'spark', 'airflow', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 's3', 'python', 'nosql', 'java', 'cloud', 'kafka', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set', 'engin']"
DE,"In this role, you will be a valuable in managing large amounts of data and implement loading disparate data sets while managing the technical communication between the team and client.
Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
Manage the technical communication between the team and client
Work with big data team to deliver cutting edge solutions
2-5 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
Ideally, this would include work on the following technologies:
Proficiency in at least one of the following: R, C++ or Python (preferred).
Scala knowledge a strong advantage.
Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc..
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks
In addition, the ideal candidate would have great problem-solving skills, and the ability & confidence to hack their way out of tight corners.
Education:
Bachelor's degree in Computer Science or related technical degree
","['sql', 'linux', 'spark', 'scala', 'pig', 'mllib', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'hive', 'r']","['commun', 'big data']",1,"['sql', 'linux', 'spark', 'scala', 'pig', 'mllib', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'hive', 'r', 'commun', 'big data']","['spark', 'relat', 'line', 'hadoop', 'amount', 'aw', 'python', 'comput', 'big', 'set', 'common']","['sql', 'linux', 'spark', 'scala', 'pig', 'mllib', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'hive', 'r', 'commun', 'big data', 'spark', 'relat', 'line', 'hadoop', 'amount', 'aw', 'python', 'comput', 'big', 'set', 'common']"
DE,"In this role, you will be a valuable expert in data coding and ETL and will lead the development of data models, structures, and data wrangling.
Responsibilities
Implementation including loading from disparate data sets, pre-processing using Hive and Pig
Scope and deliver various Big Data solutions
Design solutions based on high-level architecture
Manage technical communications between clients (technical and non-technical) and internal teams
Building a cloud-based platform that allows easy development of new applications
Collaborate with developers, subject matter experts, and business users in gathering and documenting requirements for data extraction and transformation
Perform code reviews, analyze execution plans, improve ETL jobs efficiencies, correct ETL defects and respond quickly to fixes and enhancement requests
Follow design standards for ETL and ELT processes
Analyze, assess and implement changes to existing solutions in response to business changes
Perform data research and analysis to find insights
Analyze data models to identify variances and discrepancies
Design, update, and maintain data sets
Work with geographically distributed team structure in a fast paced and challenging environment; multi-task and work under pressure
Communicate results with internal and external stakeholders
Qualifications:
An ideal candidate will have experience leading data quality assessment, will be a strong problem-solver and will have excellent interpersonal and communications skills.
Over 5 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc.
Strong background with ETL Design and coding for Data loading, Data cleansing, Data Validation
Proficiency in SQL, SAS, R and Python – with preference to SQL
Previous experience designing custom deliverables based on user requirements
Excellent written and verbal communication skills – must be able to communicate fluently in English both verbally and in writing
Strong analytical and statistical analysis skills to extract insights and recommendations
Strong problem-solving skills, and the ability & confidence to hack their way out of tight corners
Insurance domain experience is preferred
","['sql', 'pig', 'sa', 'hadoop', 'cloud', 'python', 'hive', 'r', 'excel']","['recommend', 'research', 'big data', 'cleans', 'statist', 'analyz', 'etl', 'commun']",999,"['sql', 'pig', 'sa', 'hadoop', 'cloud', 'python', 'hive', 'r', 'excel', 'recommend', 'research', 'big data', 'cleans', 'statist', 'analyz', 'etl', 'commun']","['analyt', 'set', 'sa', 'hadoop', 'python', 'statist', 'big', 'etl']","['sql', 'pig', 'sa', 'hadoop', 'cloud', 'python', 'hive', 'r', 'excel', 'recommend', 'research', 'big data', 'cleans', 'statist', 'analyz', 'etl', 'commun', 'analyt', 'set', 'sa', 'hadoop', 'python', 'statist', 'big', 'etl']"
DE,"Job Summary As part of Damanrsquos Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms.
The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills.
You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies.
You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration.
","['cloud', 'spark']",['commun'],999,"['cloud', 'spark', 'commun']","['analyt', 'spark', 'relat', 'packag', 'warehous', 'integr', 'engin']","['cloud', 'spark', 'commun', 'analyt', 'spark', 'relat', 'packag', 'warehous', 'integr', 'engin']"
DE,"The candidate should be a data engineer who has build pipelines to ingest data into data lakes, understandanalyze data, script (SQL, UNIX), understand business process.
A coder vs user of traditional ETL tools like Informatica.
Soft skills are critical with this role - the manager is looking for a resource who is a problem solver, not just take direction.
They need someone who can work independently, focus on the learning the data to relate it to the requirements.
Key technical skills Big Data tools experience with SparkHiveImpala Datawarehouse experience with TeradatanetezzaGreenplum MPP Databases OR Hadoop Data Lake Experience Cloud Experience - AWS, Azure, or Google is a nice-to-have, but not required Build Data PipeLineELTETL using JAVA or Python or Linux scripting Advanced SQL development Experience The resource should be willing to relocate to San Antonio Job Summary Data Engineer.
This particular role will focus on developing datamarts and datasets in conjunction with analysts and data scientists.
HiveImpala or Snowflake preferred Strong background in preparing data for analysis and reporting creating analytical datasets and working with others to define simple to use data models (i.e.
star schema) Experience with analytical tools for data discovery modeling, visualization, and analysis Success in a highly dynamic technology demand driven environment with ability to shift priorities with agility Ability to go from whiteboard discussion to code Willingness to explore and implement new ideas and technologies Ability to effectively communicate with technical and non-technical audiences Ability to work independently with minimal supervision Minimum Qualifications 8+ years experience with SQL 6+ years experience with data modeling design and implementation 4+ years experience working directly with subject matter experts in both business and technology domains 4+ years experience with BI and analytic tools such as Tableau, Datameer, R, or similar Nice-to-have Hands-on experience with Cloudera or Snowflake Familiarity with Agile methodologies Education Bachelor s in Computer Science, Information Systems, Engineering, science discipline, or similar
","['sql', 'linux', 'azur', 'unix', 'hadoop', 'bi', 'snowflak', 'python', 'java', 'aw', 'tableau', 'r', 'cloud']","['pipelin', 'visual', 'data modeling', 'big data', 'supervis', 'etl']",1,"['sql', 'linux', 'azur', 'unix', 'hadoop', 'powerbi', 'snowflak', 'python', 'java', 'aw', 'tableau', 'r', 'cloud', 'pipelin', 'visual', 'data modeling', 'big data', 'supervis', 'etl']","['analyt', 'pipelin', 'azur', 'visual', 'hadoop', 'bi', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'engin']","['sql', 'linux', 'azur', 'unix', 'hadoop', 'powerbi', 'snowflak', 'python', 'java', 'aw', 'tableau', 'r', 'cloud', 'pipelin', 'visual', 'data modeling', 'big data', 'supervis', 'etl', 'analyt', 'pipelin', 'azur', 'visual', 'hadoop', 'bi', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'engin']"
DE,"**
*Job Summary:**
The Data Engineer will be responsible for developing expanding, testing and/or optimizing the infrastructure and architecture of existing and future data pipelines, as well as optimizing data collection, flow, and delivery for cross-functional teams including software engineers, data scientists, and business partners.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products
*Responsibilities:**
+ Assemble large, complex data sets that meet functional and non-functional business requirements
+ Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
+ Develop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, error management, failure recovery, and output validation
+ Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
+ Collaborate with data scientists to build data products that ingest data from a variety of data sources, process it with sophisticated data science techniques, and produces results that are consumed by business partners for analysis or action
+ Contribute to the project planning process by estimating tasks and deliverables
+ Communicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable language
+ Utilize and stay current in programming languages and software technology
*Qualifications:**
+ Bachelor's Degree in Computer Science, Information Technology, Informatics, or Applied Math - Graduate degree desired
+ 5+ years of commercial experience in a data engineer role with a proven record of manipulating, processing and extracting value from large disconnected datasets.
+ Strong understanding of ETL processes
+ Expert-level knowledge of SQL and experience with NoSQL databases
+ Strong analytic skills related to working with unstructured datasets
+ Solid programming skills and expertise in Python
+ Experience working with REST and SOAP APIs
+ Strong project management, organizational, communication, and presentation skills
+ Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
+ Experience supporting and working with cross-functional teams in a dynamic environment
San Antonio, TX: 20880 Stone Oak Parkway, 78258
Position Type
Regular
Click here at http://iheartmediacareers.com/Pages/EEO.aspx to learn about E-Verify.
","['sql', 'python', 'nosql']","['pipelin', 'optim', 'information technology', 'etl', 'commun', 'math']",1,"['sql', 'python', 'nosql', 'pipelin', 'optim', 'information technology', 'etl', 'commun', 'math']","['analyt', 'program', 'techniqu', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'python', 'scientist', 'appli', 'comput', 'action', 'etl', 'sourc', 'collect']","['sql', 'python', 'nosql', 'pipelin', 'optim', 'information technology', 'etl', 'commun', 'math', 'analyt', 'program', 'techniqu', 'engin', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'python', 'scientist', 'appli', 'comput', 'action', 'etl', 'sourc', 'collect']"
DE,"Data Engineer
Chargify | San Antonio, TX, USA
The ideal candidate has a strong software engineering background, but gravitates towards building scalable data products.
Role Responsibilities
Ensure the reliability, efficiency, and scalability of the ETL system
Work closely with the development and data science teams to redesign data warehouse schemas
Perform and automate data preparation for internal consumption upon request
You'll have opportunities to work on high impact projects that improve data availability/quality and provide reliable access to data for the analytics team and the rest of the business
Requirements
Experience with solution building and architecting with AWS
Expertise in data pipeline tools such as Airflow or Luigi
Expertise in SQL, SQL Tuning, schema design
Expertise in Ruby and Python, particularly with ensuring data quality across multiple datasets used for analytic purposes
Experience with database systems including MySQL, Elasticsearch, Postgres, Redshift, etc.
Great communication skills
Working knowledge of Jupyter
Bonus points for:
Bachelor's degree in Computer Science, Computer Engineering, or equivalent field
Elasticsearch expertise
Worked with data infrastructure in a SaaS product
Knowledge of statistical sciences
Have worked with marketing organizations or advertising technologies
Have worked with Data Scientists
No visa sponsorship is available for this position*
","['sql', 'airflow', 'elasticsearch', 'jupyt', 'aw', 'python', 'redshift', 'postgr', 'mysql', 'rubi']","['pipelin', 'tune', 'statist', 'etl', 'commun']",1,"['sql', 'airflow', 'elasticsearch', 'jupyt', 'aw', 'python', 'redshift', 'rubi', 'pipelin', 'tune', 'statist', 'etl', 'commun']","['analyt', 'pipelin', 'infrastructur', 'aw', 'avail', 'python', 'statist', 'scientist', 'comput', 'etl', 'engin']","['sql', 'airflow', 'elasticsearch', 'jupyt', 'aw', 'python', 'redshift', 'rubi', 'pipelin', 'tune', 'statist', 'etl', 'commun', 'analyt', 'pipelin', 'infrastructur', 'aw', 'avail', 'python', 'statist', 'scientist', 'comput', 'etl', 'engin']"
DE,"Great sales are the result of strong purpose, conviction and pride - pride in your ability and your product.
UnitedHealth Group offers a portfolio of products that are greatly improving the life of others.
Bring along your passion and do your life's best work.
(sm)
Primary Responsibilities:
Review data loaded into the clinical systems for accuracy
Determine business impact level for data quality issues.
Develop and execute data cleanup measures
Resolve all internal data exceptions in a timely and accurate manner
Identify areas of improvement to achieve data quality
Provides research and analysis related to reporting functions as needed
Generate recurring reports, queries and other data related tasks
Utilizes departmental data information to analyze trends and identify opportunities for process improvement, to include policy and procedure updates
Completes all user acceptance testing (UAT) for all system and operational changes that affect the end user experience
Independently and proactively problem solves and identifies alternative solutions
Performs support duties including Clinical systems configuration and system maintenance as directed
Performs all other related duties as assigned
Excellent analytical and problem solving abilities with special attention to accuracy and detail
Youll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
Associate's Degree or higher level of education or equivalent experience
2 or more years of experience in data management and analysis
2 or more years of experience Healthcare IT environment
Careers with WellMed.
For you, that means one incredible team and a singular opportunity to do your life's best work..(sm)
OptumCares support services do not interfere with or control the practice of medicine by the medical practices or any of their physicians.
Candidates are required to pass a drug test before beginning employment
",['excel'],"['research', 'healthcar', 'problem solving', 'end user']",1,"['excel', 'research', 'healthcar', 'problem solving', 'end user']","['analyt', 'primari', 'provid', 'clinic', 'relat']","['excel', 'research', 'healthcar', 'problem solving', 'end user', 'analyt', 'primari', 'provid', 'clinic', 'relat']"
DE,"H-E-B Digital is seeking new team members (Partners)!
In the Data Engineer position, that means you have a…
HEAD FOR BUSINESS… you can follow technical guidance and understand why it’s important
PASSION FOR RESULTS… you’ll take the initiative to get familiar with technology / the software development process
What you’ll do
Contribute to existing data platforms and implement new technologies
Ensure data is distributed in a timely and accurate manner
Make data discoverable and accessible to business users
Who You Are
2 years of data engineering experience
Proficient with data technologies (e.g.
Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
Strong understanding of SQL and data modeling
Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
Bachelor's degree in computer science or comparable field or equivalent experience
A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling
What are the Perks?
A robust Benefits plan with coverage starting Day One
Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
Partner Care Team, for any time you have healthcare or coverage questions
Telehealth offers 24/7 access to board-certified doctors by phone
Partner Guidance allows free counselor visits
Funeral leave, jury duty, and military pay (subject to applicable law)
Maternal / paternal leave for new parents, including adoptions
10"" off H-E-B brand products in-store and online
Eligibility to participate in 401(k)
Opportunity to become a “Partner-Owner” after 12 months
H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
04-2019
DASO3232
","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'amazon web services']","['healthcar', 'pipelin', 'data modeling', 'etl', 'account']",1,"['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'etl', 'account']","['day', 'digit', 'spark', 'pipelin', 'python', 'comput', 'texa', 'etl', 'common', 'engin']","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'etl', 'account', 'day', 'digit', 'spark', 'pipelin', 'python', 'comput', 'texa', 'etl', 'common', 'engin']"
DE,"Master s in Computer Science or related degree Preferred.
All employment is decided on the basis of qualifications, merit, and business need.
",[None],[None],2,[],"['basi', 'relat', 'comput']","['basi', 'relat', 'comput']"
DE,"Great sales are the result of strong purpose, conviction and pride - pride in your ability and your product.
UnitedHealth Group offers a portfolio of products that are greatly improving the life of others.
Bring along your passion and do your life's best work.
(sm)
Primary Responsibilities:
Excellent analytical and problem solving capabilities with special attention to accuracy and detail
Self-starter with a proven ability to take ownership of job responsibilities and ensure successful completion of all projects and requests
Designs, develops, tests, documents and maintains database queries and reports related to-but not limited to - clinical systems data
Works with customers to define and document additional requirements to enrich reporting capabilities
Develops systematic reporting processes and procedures to ensure timely delivery of daily, weekly, monthly, annual and ad hoc reporting to management
Troubleshoots data integrity issues, analyzes data for completeness to meet business needs, and proposes documented solution recommendations
Transfers data into meaningful, professional and easy to understand formats for various audiences
Combines various data sources into a comprehensive understanding of customer behaviors and feedback for service improvement opportunities
Manages reporting and analysis elements on all business initiative projects
Recommends and implements new or modified reporting methods and procedures to improve report content and completeness of information
Troubleshoots and coordinates resolutions to all system issues affecting clinical applications
Youll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
Bachelor's degree in Business, Healthcare Administration, Information Technology or related field required.
(4 additional years of comparable work experience beyond the required years of experience may be substituted in lieu of a bachelor?s degree)
Four or more years related experience in a reporting and analytic role
Two or more years of experience in Healthcare or Clinical environment
Proficient in SQL, SQL Reporting Services, , MS Access, and MS Excel
Preferred Qualifications:
MS Visual Studio
SSIS
Ability to effectively prioritize and multi-task in high volume workload situations.
Careers with WellMed.
For you, that means one incredible team and a singular opportunity to do your life's best work.
(sm)
OptumCares support services do not interfere with or control the practice of medicine by the medical practices or any of their physicians.
Candidates are required to pass a drug test before beginning employment
","['sql', 'excel']","['recommend', 'healthcar', 'visual', 'problem solving', 'information technology']",1,"['sql', 'excel', 'recommend', 'healthcar', 'visual', 'problem solving', 'information technology']","['analyt', 'relat', 'visual', 'primari', 'provid', 'clinic', 'integr', 'sourc']","['sql', 'excel', 'recommend', 'healthcar', 'visual', 'problem solving', 'information technology', 'analyt', 'relat', 'visual', 'primari', 'provid', 'clinic', 'integr', 'sourc']"
DE,"Responsibilities The Data Engineer will need to demonstrate experience and knowledge of Activity Management, Requirement Identification, and Project Programming Programs or cross-Directorate programs which are still under development and need support to meet the end-state.
Develop life-cycle sustainment and risk mitigation strategy recommendations for two FYDP's for each portfolio.
Analyze built infrastructure data, aggregate requirements, assesses asset performance predictions from SMS, analyze statistics and trend across portfolios, validate data, perform data management, compile other summarizing reports for each AMP portfolio, and recommend enhancements to processes.
Support AMP Enterprise Managers with the development of portfolio-specific SEED's to support Installation Development Plans (IDPs) and the Enterprise Planning Process.
Maturation of Requirement Identification by analyzing built infrastructure data, aggregating requirements, assessing asset performance predictions from SMS, analyzing statistics and trends across portfolios, validating data, performing data management, compiling other summarizing reports for each AMP portfolio, and recommending enhancements to process.
Assisting in the AFCAMP process, establishment of document templates/tools, logistic support, training, and process refinement recommendations associated with the maturation of the AFCAMP process.
Qualifications + BA/BS or MA/MS degree in the field(s) of Data Engineering and Information Technology + 3 to 10 years of related experience in Web Based Application Design, Data Management, Document Management/Repository + Active Public Trust clearance Qualifications - Desired + Air Force Activity Management Experience Additional Requirements The successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described.
All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.
All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.
",[None],"['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist']",1,"['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist']","['basi', 'asset', 'program', 'public', 'predict', 'infrastructur', 'statist', 'relat', 'engin']","['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist', 'basi', 'asset', 'program', 'public', 'predict', 'infrastructur', 'statist', 'relat', 'engin']"
DE,"Long Term Need candidates who are strong in ETL, Python Spark Scala is mandatory.
Snowflake not mandatory for this role, good to have.
","['scala', 'snowflak', 'python', 'spark']",['etl'],999,"['scala', 'snowflak', 'python', 'spark', 'etl']","['etl', 'python', 'spark']","['scala', 'snowflak', 'python', 'spark', 'etl', 'etl', 'python', 'spark']"
DE,"OverviewInterested in working with talented people to help develop innovative solutions to some of society's most complex and challenging problems?
Maturation of Requirement Identification by analyzing built infrastructure data, aggregating requirements, assessing asset performance predictions from SMS, analyzing statistics and trends across portfolios, validating data, performing data management, compiling other summarizing reports for each AMP portfolio, and recommending enhancements to process.Assisting in the AFCAMP process, establishment of document templates/tools, logistic support, training, and process refinement recommendations associated with the maturation of the AFCAMP process.Qualifications* BA/BS or MA/MS degree in the field(s) of Data Engineering and Information Technology* 3 to 10 years of related experience in Web Based Application Design, Data Management, Document Management/Repository* Active Public Trust clearanceQualifications - Desired* Air Force Activity Management ExperienceAdditional RequirementsThe successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described.DisclaimerGuidehouse is an Equal Employment Opportunity / Affirmative Action employer.
All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies.
",[None],"['recommend', 'predict', 'statist', 'information technology', 'logist']",1,"['recommend', 'predict', 'statist', 'information technology', 'logist']","['asset', 'public', 'predict', 'infrastructur', 'statist', 'action', 'relat', 'engin']","['recommend', 'predict', 'statist', 'information technology', 'logist', 'asset', 'public', 'predict', 'infrastructur', 'statist', 'action', 'relat', 'engin']"
DE,"Ability to coordinate with people of many different types of skillsets including systems engineers, network defenders, network engineers, data scientists and analytics developers.
Must be able to identify, analyze, normalize, ingest, and parse structured and unstructured cybersecurity and intelligence data from a wide variety of sources, to include large datasets (over 1TB).
Data types include, but are not limited to, network appliance event logs, system logs, domain logs, firewall logs, Zeek logs, audit logs, vulnerability scans, packet capture, STIX formatted messages, PDF/text files, .csv files.
Statement of Work for Data Engineer
Job Responsibilities:
· Utilize various Big Data Platform technologies, to include but not limited to, Elastic/Lucene databasing, Hadoop Distributed File System (HDFS), Kafka, and Gem to prepare various datasets for use in data analytics.
· Work with 35 IS and external support engineers to develop, adapt, modify, and implement data parsers for analytic use in the Big Data Platform and other Air Force CS&D weapon systems.
· Utilize one or more of several coding languages to include Java, Python, and Scala.
· Develop documentation and comprehensive user manuals for all developed projects, to be understandable by the average analyst familiar with BDP.
· Assist in the development and implementation of a data analytics program within the 35 IS.
· Provide support to training development and instruction focusing on CS&D data analytics development.
Knowledge/Skills Ability:
Required: · Be Director of Central Intelligence Directives 6/4 eligible (Top Secret) with a current Single Scope Background Investigation (SSBI)
· Proficiency in one or more big data programming languages, such as R, Python, Scala, or Java.
· Experience working with a hybrid team of analyst, engineers, and developers to conduct research, and build and deploy complex, but easy-to-use analytical platforms.
· Previous experience performing research in Data Analytics or big data.
Minimum Experience/Education:
· Minimum 2 years of recent experience in data engineering
· Programming experience, ideally in Python, Spark, Kafka, or Java
· Experience in data cleaning, wrangling, visualization and reporting
· Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources
· Knowledge of data mining, machine learning, natural language processing, or information retrieval
Highly Desired:
· Education/ Certifications:
·Bachelor’s Degree or more in Computer Science or related field
·DoD 8570 Sec+
· 4+ years of experience in data analytics or quantitative intelligence analysis
· Four (4) years of experience in an intelligence field at a tactical or operational level
· Experience with a DoD Big Data platform is a plus
*BEAT LLC IS AN EQUAL OPPORTUNITY EMPLOYER - DISABILITY AND VETERANS*
","['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka']","['research', 'natural language processing', 'big data', 'data mining', 'normal', 'visual', 'machine learning', 'analyz']",1,"['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka', 'research', 'nlp', 'big data', 'data mining', 'normal', 'visual', 'machine learning', 'analyz']","['analyt', 'machin', 'program', 'spark', 'relat', 'visual', 'hadoop', 'provid', 'amount', 'python', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']","['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka', 'research', 'nlp', 'big data', 'data mining', 'normal', 'visual', 'machine learning', 'analyz', 'analyt', 'machin', 'program', 'spark', 'relat', 'visual', 'hadoop', 'provid', 'amount', 'python', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']"
DE,"The goal is to build out this product.
Requirements ndash MUST HAVE all of the bullets below 5+ years Data Engineering experience Big Data tools experience with SparkHiveImpala Must have 1 of these 2 Data Warehouse builds with TeradataNetezzaGreenplum OR lsquoHadoop Data Lakersquo experience Building Data PipelinesELTETL using Java or Python or Linux scripting Cloud experience AWS or Azure OR GCP Advanced SQL development Bachelorrsquos degree ndash (let me know if you find super strong talent without) Preferred ndash The strongest candidates will have Experience with 1 or more Talend, Ab Initio, Datastage, Informatica Oracle EBS What other ERP systems used to gather data?
Hadoop experience?
Agile methodology environments BI and Analystic tools Tableau, Datameer, R, or similar Educational Qualifications Required - Bachelorrsquos degree in Computer Science, Information Technology, Computer Engineering or closely related or equivalent Preferred - Masterrsquos degree in Management Information Systems (MIS), Computer Science, Big Data or Analytics or equivalent Travel Open to travel based up on the nature of the engagement Thanks Regards Srikanth Donkani Sr.
Talent Acquisition Specialist (w) 312-448-6138 (E) srikanth.drsrit.com www.rsrit.com 2260 Haggerty Road, Suite 285 Northville, MI 48167 Equal Employment Opportunity Reliable Software employment does not discriminate on the basis of race, religion, gender, sexual orientation, age or any other basis as covered by federal, state, or local law.
Employment decisions are based solely on qualifications, merit and business needs.
","['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'bi', 'aw', 'python', 'java', 'tableau', 'r', 'cloud', 'oracl']","['information technology', 'big data']",1,"['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'cloud', 'oracl', 'information technology', 'big data']","['basi', 'analyt', 'azur', 'hadoop', 'bi', 'aw', 'python', 'warehous', 'comput', 'big', 'relat', 'engin']","['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'powerbi', 'aw', 'python', 'java', 'tableau', 'r', 'cloud', 'oracl', 'information technology', 'big data', 'basi', 'analyt', 'azur', 'hadoop', 'bi', 'aw', 'python', 'warehous', 'comput', 'big', 'relat', 'engin']"
DE,"2+ yearsrsquo advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns 2+ yearsrsquo experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases) Experience in the financial services, banking and or Insurance industries is a nice to have
","['sql', 'nosql', 'cassandra']",['graph'],999,"['sql', 'nosql', 'cassandra', 'graph']",[None],"['sql', 'nosql', 'cassandra', 'graph', None]"
DE,"Data Engineer
Fast-pace, ground floor opportunity to make a true impact on the bottom line through Data
Upward mobility, career path that will allow this person to develop as a professional
Robust Bonus Opportunity – high performance equals reward
Stock Options
Remote work opportunity!
YOUR TYPICAL DAY…
Collaborate with business and IT to build a roadmap to extract, transform and load data from various sources
Build data models that will fuel business intelligence to increase data access
Champion opportunities to automate processes for great scalability
Foster a culture of utilizing data to influence decisions across the organization
YOU HAVE…
Demonstrated experience in building a data warehouse from scratch in a cloud environment
Ability to connect with the business and guide the internal customer to the solution they seek
Experience in executing ETL solutions, comfortable and thrives in an environment that requires heads down coding
Schema design and data modeling
Bachelor’ s Degree in Computer Engineering, Mathematics, or related field required
EXTRA CREDIT…
Financial Services experience highly preferred
For a Confidential Conversation and/or Personal Meeting regarding this outstanding career opportunity please contact:
Holly Esquivel, CPC | 210.807-5602 | hesquivel@deaconrecruiting.com
",['cloud'],"['etl', 'data modeling']",1,"['cloud', 'etl', 'data modeling']","['relat', 'line', 'comput', 'warehous', 'etl', 'sourc', 'engin']","['cloud', 'etl', 'data modeling', 'relat', 'line', 'comput', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Ability to coordinate with people of many different types of skillsets including systems engineers, network defenders, network engineers, data scientists and analytics developers.
Data types include, but are not limited to, network appliance event logs, system logs, domain logs, firewall logs, Zeek logs, audit logs, vulnerability scans, packet capture, STIX formatted messages, PDF/text files, .csv files.
Statement of Work for Data Engineer
Job Responsibilities:
· Utilize various Big Data Platform technologies, to include but not limited to, Elastic/Lucene databasing, Hadoop Distributed File System (HDFS), Kafka, and Gem to prepare various datasets for use in data analytics.
· Work with 35 IS and external support engineers to develop, adapt, modify, and implement data parsers for analytic use in the Big Data Platform and other Air Force CS&D weapon systems.
· Utilize one or more of several coding languages to include Java, Python, and Scala.
· Develop documentation and comprehensive user manuals for all developed projects, to be understandable by the average analyst familiar with BDP.
· Provide support to training development and instruction focusing on CS&D data analytics development.
Knowledge/Skills Ability:
· Proficiency in one or more big data programming languages, such as R, Python, Scala, or Java.
· Previous experience performing research in Data Analytics or big data.
Minimum Experience/Education:
· Minimum 2 years of recent experience in data engineering
· Programming experience, ideally in Python, Spark, Kafka, or Java
· Experience in data cleaning, wrangling, visualization and reporting
· Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources
· Knowledge of data mining, machine learning, natural language processing, or information retrieval
Highly Desired:
· Education/ Certifications:
· Bachelors Degree or more in Computer Science or related field
· DoD 8570 Sec+
· 4+ years of experience in data analytics or quantitative intelligence analysis
","['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka']","['research', 'natural language processing', 'data mining', 'visual', 'machine learning', 'big data']",1,"['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka', 'research', 'nlp', 'data mining', 'visual', 'machine learning', 'big data']","['analyt', 'machin', 'spark', 'relat', 'visual', 'hadoop', 'provid', 'amount', 'python', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']","['spark', 'scala', 'hadoop', 'java', 'python', 'r', 'kafka', 'research', 'nlp', 'data mining', 'visual', 'machine learning', 'big data', 'analyt', 'machin', 'spark', 'relat', 'visual', 'hadoop', 'provid', 'amount', 'python', 'scientist', 'comput', 'big', 'quantit', 'sourc', 'engin']"
DE,"The Business
GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.
The Role
You are the Scotty Pippin to the Michael Jordans.
You are the Xavi to the Messis.
You'll do things like:
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience
Non negotiables:
SQL
Python
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract
Salary: $100,000.00 /year
Work Remotely:
Yes
","['sql', 'tableau', 'python']","['commun', 'pipelin']",2,"['sql', 'tableau', 'python', 'commun', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']","['sql', 'tableau', 'python', 'commun', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']"
DE,"Skills and Qualifications:
1+ year experience with Snowflake database
AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)
Experience in ETL and ELT workflow management
Experience building internal cloud to cloud integrations is ideal
Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus
3+ years of batch ETL tool experience (DataStage / Informatica / Talend)
2+ years' experience with Hadoop Ecosystem (HDFS/S3, Hive, Spark)
2+ years' experience in a software engineering, leveraging Java, Python, Scala, etc.
2+ years' advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns
2+ years' experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases)
Experience in the financial services, banking and/ or Insurance industries is a nice to have
","['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 'python', 'redshift', 's3', 'nosql', 'lambda', 'hive', 'kafka']","['etl', 'graph']",999,"['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 'python', 'redshift', 's3', 'nosql', 'lambda', 'hive', 'kafka', 'etl', 'graph']","['stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'etl', 'engin', 'integr']","['sql', 'spark', 'cloud', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'java', 'python', 'redshift', 's3', 'nosql', 'lambda', 'hive', 'kafka', 'etl', 'graph', 'stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'etl', 'engin', 'integr']"
DE,"The Senior Data Engineer will play an integral role in this development and will be responsible for building and optimizing the infrastructure and architecture of existing and future data storage and pipelines, as well as optimizing data collection, flow, and delivery across the firm.Responsibilities* Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of internal and external data sources.
* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
* Communicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable language* Utilize and stay current in programming languages and software technology.
* Successful completion of a job-related assessment may be requiredPreferred Skills* Data warehousing and ETL solutions* Schema design and dimensional data modeling* Familiarity with the AWS ecosystem and implementations* Experience working with APIs* Ability to work with any level of stakeholder across the business.
* Experience working with Informatica* Experience working with Tableau or other BI Visualization tools.
","['aw', 'bi', 'tableau']","['pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'etl', 'commun']",999,"['aw', 'powerbi', 'tableau', 'pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'etl', 'commun']","['engin', 'pipelin', 'relat', 'visual', 'infrastructur', 'optim', 'bi', 'aw', 'etl', 'sourc', 'collect', 'integr']","['aw', 'powerbi', 'tableau', 'pipelin', 'visual', 'data modeling', 'optim', 'data warehousing', 'etl', 'commun', 'engin', 'pipelin', 'relat', 'visual', 'infrastructur', 'optim', 'bi', 'aw', 'etl', 'sourc', 'collect', 'integr']"
DE,"Purpose of Job
Data Engineers deliver quality reporting and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions.
Data Engineers (DEs) are engaged in all phases of the data management lifecycle; gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data.
Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored and controlled.
Job Requirements
This singular mission requires a dedication to innovative thinking at every level.
https://www.youtube.com/watch?v=kVCnnaJUH_c
Data Engineer A Realistic Preview
Identifies and manages existing and emerging risks that stem from business activities and the job role.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled.
Follows written risk and compliance policies and procedures for business activities.
Design and implement complex technical solutions.
Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.
Participate in daily standups and lead design reviews.
Breakdown business features into technical stories and approaches.
Analyze data and enable machine learning.
Create proof of concepts and prototypes.
Implement efficient defect management, root cause analysis, and resolution processes.
Assist in setting technical direction for the team.
Mentor and coach junior engineers.
Minimum Experience:
Bachelor's degree in related field of study,
OR
Certification from an approved technical field of study,
OR
4 additional years of related experience beyond the minimum required.
6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)
Deep Knowledge of a technology or product line
*Qualifications may warrant placement in a different job level*
This will take approximately 5 minutes.
Once you begin the questions you will not be able to finish them at a later time and you will not able to change your responses.
Preferred Experience
Extensive experience in ETL tools (DataStage and/or Informatica)
Extensive experience with Unix Shell Scripting
Experience in Python Development
Experience in ServiceNow Development
Experience in SalesForce Development
Strong SQL experience
Netezza Experience
Experience in creation of APIs using openshift and docker
For Internal Candidates:
Must complete 12 months in current position (from date of hire or date of placement), or must have managers approval prior to posting.
","['sql', 'unix', 'salesforc', 'python', 'docker']","['pipelin', 'risk', 'machine learning', 'analyz', 'etl']",1,"['sql', 'unix', 'salesforc', 'python', 'docker', 'pipelin', 'risk', 'machine learning', 'analyz', 'etl']","['machin', 'collect', 'pipelin', 'relat', 'line', 'python', 'etl', 'engin', 'particip']","['sql', 'unix', 'salesforc', 'python', 'docker', 'pipelin', 'risk', 'machine learning', 'analyz', 'etl', 'machin', 'collect', 'pipelin', 'relat', 'line', 'python', 'etl', 'engin', 'particip']"
DE,"H-E-B Digital is seeking new team members (Partners)!
In the Sr. Data Engineer position, that means you have a…
HEART FOR PEOPLE… you can organize multiple engineers, negotiate solutions, and provide upward communication
HEAD FOR BUSINESS… you consistently demonstrate and uphold the standards of codding, infrastructure, and process
What you’ll do
Contribute to existing data platforms and implement new technologies
Ensure data is distributed in a timely and accurate manner
Make data discoverable and accessible to business users
Who You Are
4 years of data engineering experience
Proficient with data technologies (e.g.
Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
Strong understanding of SQL and data modeling
Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
Bachelor's degree in computer science or comparable field or equivalent experience
A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling
What are the Perks?
A robust Benefits plan with coverage starting Day One
Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
Partner Care Team, for any time you have healthcare or coverage questions
Telehealth offers 24/7 access to board-certified doctors by phone
Partner Guidance allows free counselor visits
Funeral leave, jury duty, and military pay (subject to applicable law)
Maternal / paternal leave for new parents, including adoptions
10"" off H-E-B brand products in-store and online
Eligibility to participate in 401(k)
Opportunity to become a “Partner-Owner” after 12 months
H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
04-2019
DASO3232
","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'amazon web services']","['healthcar', 'pipelin', 'data modeling', 'etl', 'commun', 'account']",1,"['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'etl', 'commun', 'account']","['day', 'digit', 'spark', 'pipelin', 'infrastructur', 'python', 'comput', 'texa', 'etl', 'common', 'engin']","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'etl', 'commun', 'account', 'day', 'digit', 'spark', 'pipelin', 'infrastructur', 'python', 'comput', 'texa', 'etl', 'common', 'engin']"
DE,"Job Summary:
Senior Data Engineer / SQL Developer / SQL Analyst.
Responsible for developing and implementing data models for multiple business processes on Acelity’s data warehouse and big data analytics infrastructure.
This particular role will focus on developing a semantics layer that applies the business rules and generates the business metrics required for sales reporting and analytics.
Principle Responsibilities: (essential job duties and responsibilities)
Design and implement data models, datamarts, and datasets on a Hadoop-based data warehouse and data hub
Interface directly with business and systems subject matter experts to understand analytic needs and determine logical data model requirements
Develop and implement ETL processes across Hadoop, BI systems, and databases
Work closely with data architects to identify common data requirements and develop shared solutions
Develop close collaboration with senior analysts and data owners across multiple business domains
Maintain data modeling standards and ETL best practices
Support data model and ETL solutions in production
Skills and Experiences:
Strong data warehouse and ETL background
Advanced SQL programming capabilities.
Hive, Impala, or Snowflake preferred
Strong background in preparing data for analysis and reporting: creating analytical datasets and working with others to define simple to use data models (i.e.
star schema)
Experience with analytical tools for data discovery & modeling, visualization, and analysis
Success in a highly dynamic technology demand driven environment with ability to shift priorities with agility
Ability to go from whiteboard discussion to code
Willingness to explore and implement new ideas and technologies
Ability to effectively communicate with technical and non-technical audiences
Ability to work independently with minimal supervision
Minimum Qualifications:
8+ years experience with SQL
6+ years experience with data modeling design and implementation
4+ years experience working directly with subject matter experts in both business and technology domains
4+ years experience with BI and analytic tools such as Tableau, Datameer, R, or similar
2+ years experience with Ab Initio, Datastage, Informatica, or Talend
Nice-to-have:
Hands-on experience with Hadoop
Familiarity with Agile methodologies
Education:
Bachelor’s in Computer Science, Information Systems, Engineering, science discipline, or similar
The information listed above is not a comprehensive list of all duties/responsibilities performed.
EOE AA M/F/Vet/Disability: Acelity L.P. Inc. and its subsidiaries are an equal opportunity and affirmative action employer and give consideration for employment to qualified applicants without regard to race, ethnicity, color, religion, sex, sexual orientation, gender identity, pregnancy, national origin, age, disability, veteran status, or genetic information or any other legally protected characteristic.
If you'd like more information about your EEO rights as an applicant under the law, please click here: http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf VEVRAA Federal Contractor
","['sql', 'hadoop', 'bi', 'snowflak', 'tableau', 'hive', 'r']","['visual', 'data modeling', 'big data', 'supervis', 'etl']",1,"['sql', 'hadoop', 'powerbi', 'snowflak', 'tableau', 'hive', 'r', 'visual', 'data modeling', 'big data', 'supervis', 'etl']","['analyt', 'essenti', 'visual', 'hadoop', 'infrastructur', 'bi', 'comput', 'warehous', 'action', 'big', 'etl', 'common', 'engin']","['sql', 'hadoop', 'powerbi', 'snowflak', 'tableau', 'hive', 'r', 'visual', 'data modeling', 'big data', 'supervis', 'etl', 'analyt', 'essenti', 'visual', 'hadoop', 'infrastructur', 'bi', 'comput', 'warehous', 'action', 'big', 'etl', 'common', 'engin']"
DE,"This will take approximately 5 minutes.
Once you begin the questions you will not be able to finish them at a later time and you will not be able to change your responses.
**Preferred Experience:**+ IBM DataStage Experience+ 4+ years of Data Warehousing experience+ Unix bash Experience+ Python Experience+ Java Experience+ Hadoop Experience+ Control-m Experiencequalifications:+ Experience level: Experienced+ Minimum 4 years of experience+ Education: Bachelorsskills:+ ETL+ Data Warehouse+ Java+ Hadoop+ PythonEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.
","['java', 'python', 'unix', 'hadoop']",['data warehousing'],999,"['java', 'python', 'unix', 'hadoop', 'data warehousing']","['python', 'hadoop']","['java', 'python', 'unix', 'hadoop', 'data warehousing', 'python', 'hadoop']"
DE,"Hi, Hope You Are Doing Great!!!
I hope you and your friends and family are OK during the COVID crisis.
Below are a few details pertaining to the job.
Please take a look at it and let me know if you would like to be considered for the opportunity.
Please share with me your updated resume.
Data Engineer ndash Contract ndash long term 6+ mo at a time, strong possibility to extend and or convert!
Purpose - Project will include working in teams to pull raw data from several products spread out across the globe in different countries, to then merge into another single system.
The goal is to build out this product.
Requirements ndash MUST HAVE all of the bullets below 5+ years Data Engineering experience Big Data tools experience with SparkHiveImpala Must have 1 of these 2 Data Warehouse builds with TeradataNetezzaGreenplum OR lsquoHadoop Data Lakersquo experience Building Data PipelinesELTETL using Java or Python or Linux scripting Cloud experience AWS or Azure OR GCP Advanced SQL development Bachelorrsquos degree ndash (let me know if you find super strong talent without) Preferred ndash The strongest candidates will have Experience with 1 or more Talend, Ab Initio, Datastage, Informatica Oracle EBS What other ERP systems used to gather data?
Hadoop experience?
All employment is decided on the basis of qualifications, merit, and business need.
Thank You, Hanumesh E Technical Recruiter (O) 312 - 967- 6038 (M) 312-971-5144 Email hanumesh.ersrit.com URL www.rsrit.com
","['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'aw', 'cloud', 'java', 'python', 'oracl']",['big data'],1,"['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'aw', 'cloud', 'java', 'python', 'oracl', 'big data']","['basi', 'azur', 'hadoop', 'look', 'aw', 'python', 'warehous', 'big', 'engin']","['sql', 'erp', 'linux', 'gcp', 'azur', 'hadoop', 'erp system', 'aw', 'cloud', 'java', 'python', 'oracl', 'big data', 'basi', 'azur', 'hadoop', 'look', 'aw', 'python', 'warehous', 'big', 'engin']"
DE,"To be successful in this role, you will be thorough, creative, and exceptionally well-skilled in all phases of the development lifecycle, with a passion for continued learning and collaboration.
*This position is located in San Antonio and requires an active TS w/SCI eligibility clearance*
*Also looking for candidates in these locations: Charlottesville, Fort Bragg (NC), Fort Gordon (GA), Ft. Meade, Hawaii, San Antonio, Germany, Italy, South Korea
5+ years performing data acquisition, identify relevant data sources and sets and shall provide data system enhancements as required, including but not limited to product reformatting and data quality assessments to support the acquisition of new datasets.
Design, develop, test and manage the overall architecture that helps analyze and process data in the way the organization needs it.
Integrate external or new datasets into existing data pipelines.
Process, clean, and verify the integrity, accuracy, completeness, and uniformity.
Assess the effectiveness and accuracy of new data sources and data gathering techniques and perform all network administration and data system operations (e.g., computer and peripheral device operations, system backups) and any related operations associated with data acquisition, data maintenance, maintaining and updating metadata, and other data and information services for stakeholders.
Develop, construct, test and maintain databases.
Build data and analytics tools that will offer deeper insight into the pipeline, allowing for critical discoveries surrounding key performance indicators and customer activity.
Give recommendations and implement ways to improve data reliability, efficiency, and quality: evaluate, compare and improve the different approaches including design patterns innovation, data lifecycle design, data ontology alignment, annotated datasets, and elastic search approaches.
Act as the lead data strategist, identifying and integrating new datasets that can be leveraged.
Document all processes, models and activities.
Research and keep up-to-date with latest tradecraft and technology.
Collaborate with systems architects, data scientists, and analysts to direct and optimize the flow of data within the pipeline and ensure consistency of data delivery and utilization across multiple projects.
Curate and collect the data from a variety of traditional and non-traditional sources: extract data from sources, transform and integrate data in line with existing data, and load data into data stores for access by others.
Languages, Tools, and Techniques:
Data pipeline/workflow management tools such as Azkaban and Airflow AWS cloud services such as EC2, EMR, RDS and Redshift.
Know basics of algorithms and data structures, distributed computing, Hadoop cluster management, HDFS, MapReduce, stream-processing solutions such as Storm or Spark, big data querying tools, frameworks, messaging systems, and big data toolkits.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Knowledge of ETL tools, data APIs, data modeling, and data warehousing solutions.
R, Python, Ruby, C++, Perl, Java, SAS, SPSS, and Matlab.
Demonstrated ability to work with enterprises to develop processes that support data transformation, data structures, metadata, dependency and workload management.
Comfort working in a dynamic environment with several ongoing concurrent projects; able to multitask, prioritize, and manage time effectively.
Creative problem solver who thrives when presented with a challenge; able to analyze problems and strategize for better solutions; strong problem solving skills with an emphasis on production for re-use.
Active TS/SCI clearance
MS in Computer Science, Information Systems or equivalent field and 5+ years of experience in a similar data engineer role; BS in Computer Science, Information Systems or equivalent field and 7 + years of experience in a similar data engineer role; or AA in Computer Science Information Systems or equivalent field and 10+ years of experience in a similar data engineer role
3 years of experience handling databases and software develop is preferred.
Experience working with AWS cloud services such as EC2, EMR, RDS and Redshift
R, Python, Ruby, C++, Perl, Java, SAS, SPSS, and Matlab skills desired
Advanced data engineering experience required
Significant experience with databases
Experience and familiarity with Agile-Scrum software development
Experience gathering and decomposing requirements
Proven record of solution development and deployment
Familiarity with web based application development
Experience with testing, use case, and user stories
Outstanding communication skills, written and verbal
Highly-organized and able to manage multiple projects simultaneously
Team-player mentality with a positive attitude
Keen attention to detail and solid analytical skills
Able to articulate complex, abstract concepts concisely and effectively
","['sql', 'mapreduc', 'spark', 'airflow', 'perl', 'sa', 'spss', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'r', 'matlab', 'rubi']","['recommend', 'research', 'pipelin', 'data modeling', 'clean', 'problem solving', 'data warehousing', 'big data', 'etl', 'commun', 'cluster']",1,"['sql', 'mapreduc', 'spark', 'airflow', 'perl', 'sa', 'spss', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'r', 'matlab', 'rubi', 'recommend', 'research', 'pipelin', 'data modeling', 'clean', 'problem solving', 'data warehousing', 'big data', 'etl', 'commun', 'cluster']","['techniqu', 'stream', 'handl', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'challeng', 'line', 'sa', 'hadoop', 'learn', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'mapreduc', 'spark', 'airflow', 'perl', 'sa', 'spss', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'r', 'matlab', 'rubi', 'recommend', 'research', 'pipelin', 'data modeling', 'clean', 'problem solving', 'data warehousing', 'big data', 'etl', 'commun', 'cluster', 'techniqu', 'stream', 'handl', 'python', 'etl', 'integr', 'sourc', 'algorithm', 'analyt', 'challeng', 'line', 'sa', 'hadoop', 'learn', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Senior Data Engineer
Opportunity to develop professionally through mentorship and project ownership to make the leap to leadership
Highly visible role, this person will be tapped to build relationships across the enterprise to promote a data-driven culture
Lucrative discretionary bonus plan with stock options
Remote work options
YOUR TYPICAL DAY…
Spearhead initiative to build ground up data architecture – bridging data from a high volume of sources
Build data models that will fuel business intelligence to increase data access
Mentor and guide team members to foster a collaborative and open work environment
Remain hands-on in the day-to-day, take a strategic approach while still coding regularly
YOU HAVE…
Built a data warehouse from scratch in a cloud environment
Relationship builder – enjoys serving as a technical liaison to the business and seeks to improve lines of communication
Finds satisfaction performing in high demand environments
ETL/Informatica/Python/AWS/Tableau
Bachelor’ s Degree in Computer Engineering, Mathematics, or related field required
EXTRA CREDIT…
Financial Services experience highly preferred
For a Confidential Conversation and/or Personal Meeting regarding this outstanding career opportunity please contact:
Holly Esquivel, CPC | 210.807-5602 | hesquivel@deaconrecruiting.com
","['aw', 'tableau', 'python', 'cloud']","['etl', 'commun']",1,"['aw', 'tableau', 'python', 'cloud', 'etl', 'commun']","['day', 'relat', 'line', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']","['aw', 'tableau', 'python', 'cloud', 'etl', 'commun', 'day', 'relat', 'line', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Do you have compassion and a
passion to help others?
Transforming healthcare and millions of lives as a
result starts with the values you embrace and the passion you bring to achieve
your lifes best work.
(sm)
The Principal Integration
Developer, Data Services is responsible for engineering efficient and
standardized data transfer methods internally at WellMed, with Optum, and
external entities.
As the subject matter expert on data engineering,
specifically integration, overall responsibilities include: ensuring data
integrity, data transformation & enrichment, capacity planning, performance
tuning, complexity & interdependency elimination, complete Extract,
Transform, and Load (ETL) process architecture & design, championing
quality and reliability improvement processes, continuous integration
optimization and automation.
This position defines strategies and tactical
plans to drive the Data Services mission and will provide direction to senior
leadership & technical stakeholders when considering the integration
components of strategic data initiatives, while keeping a strong focus on
delivering business value.
This position is also responsible for collaborating
with departments across the organization, (including Optum organizations, such
as ECG, eODS, Data Lake, HCE) to analyze, specify and optimize information
delivery solutions based on business needs.
The position will partner with
Optum to ensure integration strategies support the requirements of local care
delivery organizations needs and serves as an escalation point for complex
issues.
The Principal Integration Developer, Data Services will serve as a key
resource of information technology for the organization, by providing guidance,
assistance and consultation to others and has the capability to work on the
full technology stack.
Primary Responsibilities:
Works with and across WellMed system owners & IT
teams, as well as Optum teams and external partners, to define and
optimize data integration standards Builds instrumentation and diagnostics for monitoring
and demonstrating the business value of Continuous Integration and
Continues Delivery (CICD)Partners with application, infrastructure, and
information architecture leaders to develop a cohesive, rational,
scalable, and adaptable future state architecture based on business
strategy and growthManage offshore development and support activities
ensuring their deliverables meet both quality and performance standardsMay manage a staff of 3 to 5 information technology
analysts and developers to deliver effective service and systems to users
and customers.
Maximizes staff performance and technical expertise through
clearly defined objectives, training, skill development and leadership to
ensure quality services to all customersBuilds the standards and processes to ensure data
integrity exceeds business expectationsIdentifies ways to enrich & transform data to
enhance the business value of the data and implements data-driven modelsServes as a consultant to departments in the process of
strategic planning, operational analysis, process design, solution
implementation and project management as it relates to data integrationCoordinates with Optum organizations, teams, and
initiatives as required, facilitating consistent improvement of quality,
performance and use of dataStays current on latest technologies, including
artificial intelligence and machine learning and applies that knowledge
when the business will receive value from itSupports the Director, Data Services in the
development, coordination and management of a responsible operations
budgetWorks with Data Governance to ensure standards are
enforced at the implementation levelDisplays strong customer management skills
The pace is fast and youll need to
be comfortable managing multiple priorities.
You may be required to work in
multiple locations, so scheduling flexibility is essential.
You must also be
able to respond calmly and effectively in emergency situations.
You'll
be rewarded and recognized for your performance in an environment that will
challenge you and give you clear direction on what it takes to succeed in your
role as well as provide development for other roles you may be interested in.
",[None],"['healthcar', 'machine learning', 'optim', 'information technology', 'etl']",999,"['healthcar', 'machine learning', 'optim', 'information technology', 'etl']","['essenti', 'machin', 'learn', 'primari', 'infrastructur', 'provid', 'optim', 'appli', 'etl', 'engin', 'integr']","['healthcar', 'machine learning', 'optim', 'information technology', 'etl', 'essenti', 'machin', 'learn', 'primari', 'infrastructur', 'provid', 'optim', 'appli', 'etl', 'engin', 'integr']"
DE,"Do you have compassion and a passion to help others?
Transforming healthcare and millions of lives as a result starts with the values you embrace and the passion you bring to achieve your lifes best work.
(sm)
The Principal Integration Developer, Data Services is responsible for engineering efficient and standardized data transfer methods internally at WellMed, with Optum, and external entities.
As the subject matter expert on data engineering, specifically integration, overall responsibilities include: ensuring data integrity, data transformation & enrichment, capacity planning, performance tuning, complexity & interdependency elimination, complete Extract, Transform, and Load (ETL) process architecture & design, championing quality and reliability improvement processes, continuous integration optimization and automation.
This position defines strategies and tactical plans to drive the Data Services mission and will provide direction to senior leadership & technical stakeholders when considering the integration components of strategic data initiatives, while keeping a strong focus on delivering business value.
This position is also responsible for collaborating with departments across the organization, (including Optum organizations, such as ECG, eODS, Data Lake, HCE) to analyze, specify and optimize information delivery solutions based on business needs.
The position will partner with Optum to ensure integration strategies support the requirements of local care delivery organizations needs and serves as an escalation point for complex issues.
The Principal Integration Developer, Data Services will serve as a key resource of information technology for the organization, by providing guidance, assistance and consultation to others and has the capability to work on the full technology stack.
Primary Responsibilities:
Works with and across WellMed system owners & IT teams, as well as Optum teams and external partners, to define and optimize data integration standards
Builds instrumentation and diagnostics for monitoring and demonstrating the business value of Continuous Integration and Continues Delivery (CICD)
Partners with application, infrastructure, and information architecture leaders to develop a cohesive, rational, scalable, and adaptable future state architecture based on business strategy and growth
Manage offshore development and support activities ensuring their deliverables meet both quality and performance standards
May manage a staff of 3 to 5 information technology analysts and developers to deliver effective service and systems to users and customers.
Maximizes staff performance and technical expertise through clearly defined objectives, training, skill development and leadership to ensure quality services to all customers
Builds the standards and processes to ensure data integrity exceeds business expectations
Identifies ways to enrich & transform data to enhance the business value of the data and implements data-driven models
Serves as a consultant to departments in the process of strategic planning, operational analysis, process design, solution implementation and project management as it relates to data integration
Coordinates with Optum organizations, teams, and initiatives as required, facilitating consistent improvement of quality, performance and use of data
Stays current on latest technologies, including artificial intelligence and machine learning and applies that knowledge when the business will receive value from it
Supports the Director, Data Services in the development, coordination and management of a responsible operations budget
Works with Data Governance to ensure standards are enforced at the implementation level
Displays strong customer management skills
The pace is fast and youll need to be comfortable managing multiple priorities.
You may be required to work in multiple locations, so scheduling flexibility is essential.
You must also be able to respond calmly and effectively in emergency situations.
You'll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Required Qualifications:
8+ years of experience with the analysis, design, and development of database structures and ETL processes within an enterprise database management system (e.g.
MS SQL Server, Oracle, etc.)
8+ years of experience in MS SQL Server-2008 version or higher
5+ years of experience with MS SSIS
5+ years of experience in Systems Analysis
5+ years of experience in Development of technical requirements
Experience in a lead role within an IT group
Large scale data integration and data management including data profiling, cleansing, parsing/standardization, data enrichment, and mastering
Preferred Qualifications:
Advanced degree in a related discipline
Experience with loading large, analytical and operational data structures including data requirements, data mapping, requirements and specification, data delivery and performance tuning
Experience working with a structured application development process in the health care industry
Experience in a data integrations development and operations department
Experience with healthcare data, applications, business processes and best practices
Strong customer management skills
Careers with WellMed.
Were innovators in preventative health care, striving to change the face of health care for seniors.
Were impacting 380,000+ lives, primarily Medicare eligible seniors in Texas and Florida, through primary and multi-specialty clinics, and contracted medical management services.
For you, that means one incredible team and a singular opportunity to do your lifes best work.
(sm)
Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity / Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
UnitedHealth Group is a drug-free workplace.
Candidates are required to pass a drug test before beginning employment.
Job Keywords: Principal Data Engineering, San Antonio, TX, Texas
","['sql', 'oracl']","['healthcar', 'machine learning', 'optim', 'information technology', 'etl']",2,"['sql', 'oracl', 'healthcar', 'machine learning', 'optim', 'information technology', 'etl']","['analyt', 'essenti', 'machin', 'learn', 'engin', 'relat', 'divers', 'primari', 'infrastructur', 'provid', 'optim', 'appli', 'texa', 'clinic', 'etl', 'action', 'integr']","['sql', 'oracl', 'healthcar', 'machine learning', 'optim', 'information technology', 'etl', 'analyt', 'essenti', 'machin', 'learn', 'engin', 'relat', 'divers', 'primari', 'infrastructur', 'provid', 'optim', 'appli', 'texa', 'clinic', 'etl', 'action', 'integr']"
DE,"Hi, Trust Yoursquore Doing Good.
If you are interested, kindly send me your updated resume along with the expected rate ASAP.
Hadoop experience?
Agile methodology environments BI and Analystic tools Tableau, Datameer, R, or similar Thank You Sai Kiran Sr. Technical Recruiter (D) 248-504-6876 (M) (614) 721-6940 Emailsaikiran.chrsrit.com mailtoEmail3Asaikiran.chrsrit.com 22260 Haggerty Rd, Suite 285,Northville, MI 48167.
","['bi', 'tableau', 'r', 'hadoop']",[None],999,"['powerbi', 'tableau', 'r', 'hadoop']","['bi', 'hadoop']","['powerbi', 'tableau', 'r', 'hadoop', 'bi', 'hadoop']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"Responsibilities The Data Engineer will need to demonstrate experience and knowledge of Activity Management, Requirement Identification, and Project Programming Programs or cross-Directorate programs which are still under development and need support to meet the end-state.
Develop life-cycle sustainment and risk mitigation strategy recommendations for two FYDP's for each portfolio.
Analyze built infrastructure data, aggregate requirements, assesses asset performance predictions from SMS, analyze statistics and trend across portfolios, validate data, perform data management, compile other summarizing reports for each AMP portfolio, and recommend enhancements to processes.
Support AMP Enterprise Managers with the development of portfolio-specific SEED's to support Installation Development Plans (IDPs) and the Enterprise Planning Process.
Maturation of Requirement Identification by analyzing built infrastructure data, aggregating requirements, assessing asset performance predictions from SMS, analyzing statistics and trends across portfolios, validating data, performing data management, compiling other summarizing reports for each AMP portfolio, and recommending enhancements to process.
Assisting in the AFCAMP process, establishment of document templates/tools, logistic support, training, and process refinement recommendations associated with the maturation of the AFCAMP process.
Qualifications * BA/BS or MA/MS degree in the field(s) of Data Engineering and Information Technology * 3 to 10 years of related experience in Web Based Application Design, Data Management, Document Management/Repository * Active Public Trust clearance Qualifications - Desired * Air Force Activity Management Experience Additional Requirements The successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described.
All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.
All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.
",[None],"['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist']",1,"['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist']","['basi', 'asset', 'program', 'public', 'predict', 'infrastructur', 'statist', 'relat', 'engin']","['recommend', 'risk', 'predict', 'statist', 'analyz', 'information technology', 'logist', 'basi', 'asset', 'program', 'public', 'predict', 'infrastructur', 'statist', 'relat', 'engin']"
DE,"Key Skills:
ETL/ELT, Informatica, DBT(Data Build Tool), Snowflake, AWS S3 and Glue
","['aw', 'snowflak', 's3']",['etl'],999,"['aw', 'snowflak', 's3', 'etl']","['aw', 'etl']","['aw', 'snowflak', 's3', 'etl', 'aw', 'etl']"
DE,"Do you believe that people with compassion will support one another to create a better world?
GoFundMe is the largest social fundraising community in the world and is just getting started.
With over $9 billion raised from more than 120 million donations, GoFundMe is the largest social fundraising community in the world and is just getting started.Data is at the center of all decisions and strategy at GoFundMe.
)* Integrate data from data warehouse into 3rd party tools to make data actionable* Develop and maintain REST API endpoints for data science products* Provide ongoing maintenance and enhancements to existing data warehouse solutions* Ensure data quality through automated testing* Collaborate with analysts, engineers and business users to design solutions* Research innovative technologies and make continuous improvementsWhat you bring to the role...* 3+ years as a data engineer designing, developing and maintaining enterprise data warehouse solutions consisting of structured and unstructured data* Proficiency with building data pipelines using ETL/data preparation tools* Experience with web APIs and data integrations across internal and external systems* Expertise in writing and optimizing SQL queries* Knowledge of Python, Java, C++ or other scripting languages* Experience with Spark and Scala* Good understanding of database architecture and best practices* Understanding of data science and machine learning technologies a plus* Experience with event tracking is a plus* Bachelor's degree in Engineering* Ping pong skills, a love for boba tea, and a sense of humorWhy you'll love it here...* Your work has real purpose and will be helping to change lives at a global scale.
* Great perks like lunch, snacks, wellness, company/team activities, and full benefits.
Every day friends, family, and members of the community come together to support one another and the causes they care about most.
","['sql', 'spark', 'scala', 'java', 'python']","['research', 'pipelin', 'machine learning', 'etl', 'commun']",1,"['sql', 'spark', 'scala', 'java', 'python', 'research', 'pipelin', 'machine learning', 'etl', 'commun']","['day', 'machin', 'learn', 'spark', 'pipelin', 'provid', 'python', 'parti', '3rd', 'action', 'warehous', 'etl', 'engin', 'integr']","['sql', 'spark', 'scala', 'java', 'python', 'research', 'pipelin', 'machine learning', 'etl', 'commun', 'day', 'machin', 'learn', 'spark', 'pipelin', 'provid', 'python', 'parti', '3rd', 'action', 'warehous', 'etl', 'engin', 'integr']"
DE,"The Data Engineer will implement and maintain custom ETLs.
You are focused on results, a self-starter, and have demonstrated success in developing and maintaining data infrastructure to ensure your colleagues are empowered with reliable access to data.
Responsibilities
Collaborate with Product Management and Engineering to understand data needs, solve problems, and identify trends and opportunities.
Design, build and launch new data extraction, transformation, and loading processes in production.
Manage data warehouse plans for a group of products.
Work with data infrastructure to triage infrastructure issues and drive to resolution.
Build data expertise and own data quality for allocated areas of ownership.
Support existing processes running in production.
Requirements
Experience with custom ETL design, implementation and maintenance, including schema design and dimensional data modeling.
Significant experience with workflow management engines (i.e.
Airflow, AWS Step Functions, etc).
Expert proficiency in any scripting language (Python, Node.js, R, etc.)
and SQL.
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e., AWS Redshift or similar).
Ability to analyze data to identify deliverables, gaps, and inconsistencies.
Communication skills.
A strong ability to identify and communicate data-driven insights.
Managing and communicating data warehouse plans to internal clients.
4+ years experience working with either a Map Reduce or an MPP system.
Ability to thrive in an unstructured environment, working autonomously to find opportunities to deliver business impact.
Success Drivers And Competencies
Must be able to convey information in a clear, focused, and concise manner.
Experience in planning, coordinating, and executing multiple projects simultaneously.
Ability to think creatively and work in a team environment.
Must work well in a dynamic environment and be able to recommend and implement process improvements, work independently, and handle multiple tasks simultaneously.
Passion for helping others.
Benefits
Compensation Commensurate With Experience.
","['sql', 'airflow', 'aw', 'cloud', 'python', 'redshift', 'r']","['etl', 'data modeling', 'commun', 'big data']",999,"['sql', 'airflow', 'aw', 'cloud', 'python', 'redshift', 'r', 'etl', 'data modeling', 'commun', 'big data']","['analyt', 'infrastructur', 'aw', 'python', 'warehous', 'concis', 'big', 'etl', 'engin']","['sql', 'airflow', 'aw', 'cloud', 'python', 'redshift', 'r', 'etl', 'data modeling', 'commun', 'big data', 'analyt', 'infrastructur', 'aw', 'python', 'warehous', 'concis', 'big', 'etl', 'engin']"
DE,"Offering both free and paid memberships.
About the Role
Responsibilities:
Primary responsibilities include, but are not limited to:
Design and develop resilient pipelines using a variety of different technologies.
Automate, test and harden all data workflows.
Architect logical and physical data models to ensure the needs of the business are met.
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure.
Be part of a fast growing data team handling massive scale with tons of automation.
Provide innovative solutions to challenging data projects and proof of concepts.
Education / Experience Requirements:
Minimum 4+ years of experience working with high volume data infrastructure.
Experience with AWS data related services.
Extensive experience programming in one of the following languages: Python / Java.
Experience in data modeling, optimizing SQL queries, and system performance tuning.
Knowledge and proficiency in the latest open source and data frameworks.
Experience evaluating industry trends and technologies.
Always be learning and staying up to speed with the fast moving data world.
Prefer candidates with AWS certifications.
Perks:
Competitive pay and benefits
Free snacks, drinks, and food in the office
Catered lunches throughout the week
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
This position is located in Encinitas, CA.
It is not a remote role.
","['sql', 'java', 'python', 'aw']","['pipelin', 'data modeling']",999,"['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling']","['program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']","['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling', 'program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']"
DE,"Primary Skills 4+ years working experience in data integration and pipeline development.
BS degree in CS, CE or EE.
2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDBDynamoDB ecosystems Strong real-life experience in python development especially in pySpark in AWS Cloud environment.
Design, develop test, deploy, maintain and improve data integration pipeline.
Experience in Python and common python libraries.
Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.
Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.
Databricks or Apache Spark Experience is a plus.
","['pyspark', 'spark', 'git', 'aw', 'lambda', 'python', 'redshift', 's3', 'cloud', 'kafka']","['optim', 'pipelin']",1,"['pyspark', 'spark', 'git', 'aw', 'lambda', 'python', 'redshift', 's3', 'cloud', 'kafka', 'optim', 'pipelin']","['analyt', 'spark', 'pipelin', 'primari', 'optim', 'aw', 'python', 'integr', 'common', 'sourc']","['pyspark', 'spark', 'git', 'aw', 'lambda', 'python', 'redshift', 's3', 'cloud', 'kafka', 'optim', 'pipelin', 'analyt', 'spark', 'pipelin', 'primari', 'optim', 'aw', 'python', 'integr', 'common', 'sourc']"
DE,"Its video analysis, predictive analytics and personalized performance program help fleets improve driving skills, lower operating costs, and deliver significant ROI.
With an easy-to-use managed service, fleets and drivers can access and self-manage driving performance anytime, anywhere.
Responsibilities:
Data Engineering to the core - analysis, modelling, transformation and visualization of datasets for online products and backend data platform.
Implement data engineering codebase using server-side languages Java/Scala or scripting using Python and Javascript
Design and develop apis for data pipelining frameworks on data collection, processing and storage across data stores.
Understand noSQL and stream programming apis for building data pipelines and micro-service based architecture across products
Design and development of data solutions for fast datastores, large scale data warehousing, machine learning and computer vision analytics.
Minimum Qualifications:
Bachelor's Degree in Computer Science or related discipline
5+ years of software development experience
2+ years of experience as a Data Engineer
Preferred Qualifications
Extensive knowledge of RDBMS (Microsoft SQL Server, Mysql, Postgres or similar)
Proven experience with NoSQL stores (one or more of Cassandra, MongoDB, InfluxDB, HBase/HDFS, ElasticSearch)
Experience in a distributed microservices and/or serverless (Lambda) cloud software architecture
In-Depth knowledge of ETL commercial software products (any of Informatica, Talend, Nifi or SSIS) with hands-on experience designing, implementing, and delivering solutions
Expertise with integration of complex and large data from multiple data sources, data and sensor fusion, and migration to newer methodologies
Prior experience as part of a large group working on massive data engineering pipelines and analytics for machine learning, computer vision
An aggressive problem solver who can provide creative solutions to complex situations and obtain buy-in from those affected
An independent worker who can take the initiative to define and prioritize specific goals and objectives, and to do the same for others
Strong people skills - able to communicate with colleagues while building credibility and rapport, modifying behavioral style to respond to the needs of others while maintaining objectives
An organized individual who is very detail oriented and can document and develop plans necessary for deliverables towards specific product or platform goals.
A team player that works hard, admits his/her strengths and weaknesses, and has the flexibility to improve by learning new things
","['mongodb', 'sql', 'elasticsearch', 'cassandra', 'scala', 'javascript', 'lambda', 'python', 'java', 'hbase', 'nosql', 'microsoft', 'mysql', 'postgr', 'cloud']","['computer vision', 'pipelin', 'visual', 'predict', 'machine learning', 'data warehousing', 'etl']",1,"['mongodb', 'sql', 'elasticsearch', 'cassandra', 'scala', 'javascript', 'lambda', 'python', 'java', 'hbase', 'nosql', 'microsoft', 'cloud', 'computer vision', 'pipelin', 'visual', 'predict', 'machine learning', 'data warehousing', 'etl']","['analyt', 'machin', 'program', 'stream', 'learn', 'engin', 'pipelin', 'relat', 'visual', 'predict', 'python', 'comput', 'etl', 'sourc', 'collect', 'integr']","['mongodb', 'sql', 'elasticsearch', 'cassandra', 'scala', 'javascript', 'lambda', 'python', 'java', 'hbase', 'nosql', 'microsoft', 'cloud', 'computer vision', 'pipelin', 'visual', 'predict', 'machine learning', 'data warehousing', 'etl', 'analyt', 'machin', 'program', 'stream', 'learn', 'engin', 'pipelin', 'relat', 'visual', 'predict', 'python', 'comput', 'etl', 'sourc', 'collect', 'integr']"
DE,"Position OverviewStepStone Portfolio Analytics and Reporting (""SPAR"") is looking for a Data Engineer to join the Analytics team.
The team applies its comprehensive knowledge of private markets to deliver customized performance reports and monitoring services to meet the needs of various types of investors.
The Data Engineer will ensure necessary data is being recorded, validate and maintain a large data pipeline, mine large quantities of data, and build new data infrastructures as needed.We are looking for candidates who are interested in building the team's capabilities around the extract, transform, and load processes of analytic projects.
The ideal candidates will be passionate about creating better standardized processes so that the broader team is able to focus more on the analysis portion of the pipeline.
Minimizing the team's efforts around ETL is the number one priority.
This will be a fast-paced and dynamic environment that is ideal for those who want to continuously tackle challenging problems and learn new things.Essential Job Functions:* Develop highly efficient systems to retrieve, maintain and analyze discrete financial data* Perform data normalization and management ensuring data security and efficiency* Perform ETL tasks and maintenance* Maintain and test data integrity to ensure accuracy and timeliness* Peer review SQL queries for errors and optimization* Define and iterate on development of analytics infrastructure for interactive dashboardsQualifications:* 2+ years' experience programming large complex data sets* Experience revising and optimizing complex queries with SQL* Solid grasp of Python programming* Database management experience is a plus* Good understanding of data modeling and relational databases* Experience working in collaborative environments* Inquisitive and intellectually curious: able to independently learn new technologies, skills, and industry standards* Bachelor's degree from an accredited institution
","['sql', 'python']","['pipelin', 'normal', 'data modeling', 'optim', 'analyz', 'etl']",1,"['sql', 'python', 'pipelin', 'normal', 'data modeling', 'optim', 'analyz', 'etl']","['analyt', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'python', 'etl', 'engin', 'integr']","['sql', 'python', 'pipelin', 'normal', 'data modeling', 'optim', 'analyz', 'etl', 'analyt', 'pipelin', 'set', 'relat', 'infrastructur', 'optim', 'python', 'etl', 'engin', 'integr']"
DE,"Context
Working in teams (consisting of Hadoop data engineers, Hadoop data warehouse engineers, and platform engineers) that are building and managing Hadoop stacks.
The teams install, configure and manage Hadoop ecosystem components.
As Hadoop data engineer, you are responsible for the functional part of provisioning data – e.g.
building data ingestion pipelines and data connectors.
You work closely with the data scientists and business intelligence engineers who are using this data to create analytical models.
Competence
You are well acquainted with the complete Hadoop stack.
In addition, you have practical experience of being part of a DevOps team.
Further requirements:
Bachelor of Science / Master’s degree in Computer Science, System Administration, or any other IT infrastructure or software related study with a passion for the automation side of IT infrastructure
Minimum 2-3 years of relevant work experience
Capable of building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets of structured, semi-structured and unstructured data
Experience in building data products incrementally and integrating and managing data sets from multiple sources
Data quality oriented
Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture
Hortonworks Certified Hadoop Developer and/or Cloudera Certified Hadoop Developer and/or Certified Hadoop Administrator
Knowledge of continuous integration & delivery tooling: e.g.
Jira, Git, Jenkins, Bamboo
Coding proficiency in at least one modern programming language (Python, Ruby, Java)
Strong verbal and written communication skills
Good documenting capabilities
You have a hands-on mindset, a strong customer focus, a problem-solving orientation and can show fast results
You have a clear focus on results and quality.
Willingness to travel to the Netherlands if required for training or project work
Activities
Build efficient and highly reliable data ingestion pipelines for the Hadoop stack
Own data quality and data knowledge around all data that you touch
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production
Understand the entire life cycle of data that flows through any systems for which you are responsible
Pay constant attention and effort to the reliability of your pipelines
Contact
In large organizations, project managers, architects, and business analysts often operate autonomously.
Here these competencies are combined into customer-targeted teams, enabling the rapid delivery of results (within weeks).
This knowledge-intensive approach has a proven direct and positive impact on career development.
","['git', 'jira', 'hadoop', 'java', 'python', 'rubi']","['commun', 'pipelin']",1,"['git', 'jira', 'hadoop', 'java', 'python', 'rubi', 'commun', 'pipelin']","['analyt', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['git', 'jira', 'hadoop', 'java', 'python', 'rubi', 'commun', 'pipelin', 'analyt', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"For more information visitwww.curemetrix.com
ROLE OVERVIEW
Local (San Diego) candidates only, please.
ROLE SPECIFICS
Data Management
Working with a variety of public and private health institutions around the world to gather images and clinical mammography data.
Developing data validation specifications and data management plans
Cleaning and validating incoming data
Normalizing and standardizing data according to set standards
Verifying the integrity of the data
Anonymize data according to set standards
Ensuring the HIPAA compliance of the data at rest and in transit
Working to establish and maintain the security of the data
Curating the data as needed
Designing queries to extract study data as needed for both internal uses and for publications.
Working with internal and external teams to design and execute clinical studies utilizing data
Performing analysis and reporting to help ensure ongoing data integrity
Performing analysis and reporting to support customer audits and consulting engagements
Python Toolset
Minimum of 4 years of Python experience working with development and research teams to build specifications, implement, and maintain data extraction and mining tools.
Cloud Storage
Minimum of 4 years of experience working in the cloud.
Familiarity with AWS S3 storage in a HIPAA compliant environment.
Requires knowledge of boto3 and AWS CLI tools.
Machine Learning Pipeline
Own internal medical image ground truth pipeline from ingestion to training.
Work with radiologists to determine medical image ground truth
Help manage medical image annotation vendors including quality management
Assist the Research team in building datasets for Machine Learning training.
Development
Working with CTO to develop custom tools and applications for data access and management
Supporting bi-directional integrations with 3rd-parties
As needed supporting the entire team with development expertise on specific projects.
QUALIFICATIONS
Bachelor's degree in science, IT, or business-related field and/or equivalent work experience.
Ability to build and maintain a growing study database along with strong SQL skills
Strong experience using python scripting in a technical environment
A Minimum of 5 years of working with large and complex datasets, preferably in a regulated device/diagnostic, laboratory, or CRO setting.
Strong project management skills
Ability to prioritize activities and meet regular deadlines according to reasonable levels of quality.
Ability to work independently.
Have strong writing, verbal communication skills, good organizational, interpersonal and team skills
Used to working in an Agile environment
POSITION TYPE AND HOURS OF WORKFull-time position.
General Hours of work are Monday through Friday, from 8:30 a.m. to 5:30 p.m.
Occasional early, late or weekend work required.
POSITION COLLABORATES WITH AND IS SUPPORTED BY
Research
Software Development
Operations and IT
Business Development
Key Executives
WORK ENVIRONMENTThis job operates in a professional office environment.
This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets, and fax machines.
However, at this time all work is remote in your home office until further notice.
As such candidates must be comfortable with remote work and prepared to contribute full engagement in such an environment.
PHYSICAL DEMANDS:This position requires the ability to navigate throughout a large office, 3-story complex.
This is primarily a sedentary role; however, some walking, standing, bending, lifting up to 20 lbs., and hand/eye coordination for keyboard data entry and viewing data on a computer monitor may be required.
Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
The physical demands described above must be met by an employee to successfully perform the essential functions of this job.
","['sql', 'bi', 'aw', 'python', 's3', 'cloud']","['research', 'pipelin', 'normal', 'machine learning', 'clean', 'commun']",1,"['sql', 'powerbi', 'aw', 'python', 's3', 'cloud', 'research', 'pipelin', 'normal', 'machine learning', 'clean', 'commun']","['essenti', 'machin', 'learn', 'pipelin', 'relat', 'public', 'bi', 'aw', 'python', 'parti', 'employe', '3rd', 'comput', 'clinic', 'integr']","['sql', 'powerbi', 'aw', 'python', 's3', 'cloud', 'research', 'pipelin', 'normal', 'machine learning', 'clean', 'commun', 'essenti', 'machin', 'learn', 'pipelin', 'relat', 'public', 'bi', 'aw', 'python', 'parti', 'employe', '3rd', 'comput', 'clinic', 'integr']"
DE,"Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position.
",[None],[None],999,[],"['employe', 'essenti', 'comput']","['employe', 'essenti', 'comput']"
DE,"Position Summary
Designs, creates, interprets, standardizes, and manages large datasets and data pipelines to achieve business goals.
Responsible for providing database support, maintenance, design, implementation strategy and testing by considering system characteristics to produce optimal performance, reliability and maintainability.
Interprets data results using a variety of techniques, ranging from simple data aggregation via statistical analysis, to visualization, to application of AI/data mining tools.
Works directly with internal and external clients and project and business leaders to identify and address analytical requirements.
Responsibilities
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build tools, frameworks, infrastructure, and services to ensure high quality and availability of data for AI/machine learning purposes.
Identify, design, and implement internal process improvements; automating manual processes, optimizing data delivery, standardizing and re-designing infrastructure for greater scalability.
Develop, design, document, test, implement and manage clinical data collections and reporting systems that optimize statistical efficiency and data quality.
Interpret data, analyze results using statistical techniques and provide clinical reports
Create and write SQL code for reporting database based on customer and internal report specifications.
Perform Oracle database maintenance and install patches and provide Oracle support with internal and external use of the Oracle database
Troubleshoot data and data feed problems related to Oracle and other SQL driven applications
Requirements
Bachelor’s degree in Computer Science or related field, an additional 4 years of related experience in lieu of degree
1-3 years database experience with solid understanding of SQL query development
Experience with UNIX and PC-based information systems, windows servers and workstations, network interfaces and modern office methods.
Strong knowledge of SQL and databases (Oracle, MySQL, SQLite, PostgreSQL).
Proficient with scripting (Bash, Perl, Python, etc.)
and proficiency with MS Office Applications
Successful completion of the security clearance process may not be required for every position but all candidates must be willing to submit to the process if requested by their manager according to the requirements of the position and business needs.
Benefits
100% covered Medical and Dental coverage for you & your family
Generous 401(k) plan and contribution
Events and weekly lunches
Engaging wellness activities
Corporate Social Responsibility Program
So many more to list…
EEO/AA/M/F/Veteran/Disabled
","['sql', 'ms office', 'perl', 'unix', 'python', 'postgresql', 'mysql', 'oracl']","['pipelin', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz']",1,"['sql', 'microsoft', 'perl', 'unix', 'python', 'postgresql', 'oracl', 'pipelin', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz']","['program', 'techniqu', 'visual', 'python', 'collect', 'analyt', 'optim', 'avail', 'statist', 'clinic', 'learn', 'corpor', 'infrastructur', 'set', 'machin', 'pipelin', 'interpret', 'comput', 'relat']","['sql', 'microsoft', 'perl', 'unix', 'python', 'postgresql', 'oracl', 'pipelin', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'program', 'techniqu', 'visual', 'python', 'collect', 'analyt', 'optim', 'avail', 'statist', 'clinic', 'learn', 'corpor', 'infrastructur', 'set', 'machin', 'pipelin', 'interpret', 'comput', 'relat']"
DE,"Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field.
3+ years of relevant experience in data engineering.
Demonstrated ability in data modeling, ETL development, and data warehousing.
Advanced SQL experience is a must.
Data Warehousing.
Experience with SQL Server.
Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch, etc.).
Experience in using DotNet, C#, and/or other data engineering languages.
Knowledge and experience of SQL Server and SSIS.
Preferred Qualifications:
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience building data products integrating and managing datasets from multiple focus.
","['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql']","['data modeling', 'data warehousing', 'statist', 'big data', 'etl']",1,"['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']","['spark', 'relat', 'hadoop', 'comput', 'scientist', 'statist', 'big', 'etl', 'engin']","['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'spark', 'relat', 'hadoop', 'comput', 'scientist', 'statist', 'big', 'etl', 'engin']"
DE,"Offering both free and paid memberships.
About the Role
Responsibilities:
Primary responsibilities include, but are not limited to:
Design and develop resilient pipelines using a variety of different technologies.
Automate, test and harden all data workflows.
Architect logical and physical data models to ensure the needs of the business are met.
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure.
Be part of a fast growing data team handling massive scale with tons of automation.
Provide innovative solutions to challenging data projects and proof of concepts.
Education / Experience Requirements:
Minimum 4+ years of experience working with high volume data infrastructure.
Experience with AWS data related services.
Extensive experience programming in one of the following languages: Python / Java.
Experience in data modeling, optimizing SQL queries, and system performance tuning.
Knowledge and proficiency in the latest open source and data frameworks.
Experience evaluating industry trends and technologies.
Always be learning and staying up to speed with the fast moving data world.
Prefer candidates with AWS certifications.
Perks:
Competitive pay and benefits
Free snacks, drinks, and food in the office
Catered lunches throughout the week
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
This position is located in Encinitas, CA.
It is not a remote role.
","['sql', 'java', 'python', 'aw']","['pipelin', 'data modeling']",999,"['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling']","['program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']","['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling', 'program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']"
DE,"???????????????????????
?Hi Folks ,
Regarding I have an exciting new opportunity??
?s that I wanted to share with you and your network.
Joins their organization.
Job Title: Data Engineer
Work Authorizations: Authorized to Work
Position Type: Contract
No: of Positions: 1
Position Start Date:
Duration: 6+ Months
Primary Skills: ETL, Date warehousing, SQL
Secondary Skills:
Roles and Responsibility:-
???
Interfacing with business customers, gathering requirements and developing new datasets in data platform
???
Building and migrating the complex ETL pipelines from on premise system to cloud
???
Identifying the data quality issues to address them immediately to provide great user experience
???
Extracting and combining data from various heterogeneous data sources
???
Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets
Basic Qualifications:
???
Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field
3+ years relevant experience in data engineering.
???
Demonstrated ability in data modeling, ETL development, and data warehousing.
Advanced SQL experience is a must
???
Data Warehousing Experience with SQL Server.
???
Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elastic Search etc.)
???
Experience in using .Net , C# and/or other data engineering languages
???
Knowledge and experience of SQL Sever and SSIS.
Preferred Qualifications:
???
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
???
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
???
Experience building data products incrementally and integrating and managing datasets from multiple focus.
Regards,
Vinay
4525 Route 27,Princeton, NJ 08540
Ph: 732 512 0009 Ext: 134 Direct : 609 256 4342
Email: vinay@logicplanet.com | www.logicplanet.com
Certified Minority Women Based Enterprise
18 years in IT.
$30M in revenues
","['sql', 'spark', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql']","['pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']",1,"['sql', 'spark', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']","['spark', 'pipelin', 'set', 'relat', 'hadoop', 'primari', 'avail', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'primari', 'avail', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"The Senior Software Engineer codes software applications based on business requirements.
The Senior Software Engineer work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors.
Responsibilities
Role: Senior Software Engineer
The ideal candidate should have an advanced grasp of the software engineering lifecycle and is particularly strong in backend development.
You will join a team that work on the collecting, storing, processing, and analyzing of huge sets of data.
The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
Responsibilities
+ Lead and guide project team(s) as tech lead or key contributor
+ Participate in architectural discussion and product development
+ Evaluate and integrate tools and frameworks required to provide requested capabilities
+ Diagnose system performance issues and advise any necessary infrastructure changes
+ Interact with product and business stakeholders beyond engineering
+ Mentor and develop junior levels of engineers
Skills and Qualifications
+ Bachelor's Degree or above (Computer Science, Bio Engineering, Electronics and Electrical Engineering or any related field)
+ Proficient understanding and tech-lead level experience in one or more programming languages (Java, JavaScript, Python etc.)
+ 5+ years of experience in software development and/or data engineering
+ Experience with JSON, RESTful web services and client-server interactions
+ Knowledge of various persistence (RDBMS, noSQL, HDFS, Cassandra, Redis)
+ Understanding Data Catalog, Data Governance, Data Lineage
+ Experience with security, authentication in data platform
+ Experience with building stream-processing systems, using solutions such as Spark-Streaming or Flink
+ Experience with integration of data from multiple data sources
+ Experience with building data lakes and data warehouses by leveraging any of the major cloud providers (GCP, AWS or Azure) is highly desirable
+ Familiar with Hadoop ecosystem (HDFS, HBase etc.
), especially Spark
+ Good basics of operating systems and network
+ Good knowledge of Big Data querying tools
+ Knowledge of and experience with Azure
+ Knowledge of various ETL techniques
+ Knowledge of messaging systems, such as Kafka or RabbitMQ
Role Desirables
+ Experience with Spring/Spring Boot
+ Experience with highly scalable web services
+ API design and development
+ Environment management/orchestration systems (Kubernetes, etc.)
+ Experience with testing frameworks and code quality tools
+ Experience with continuous integration environment and tools
+ Experience with Oracle, PostgresSQL, Mongo development and optimization
+ Familiarity with Healthcare (FHIR), Clinical or Financial industry
+ Experience with agile development practices
+ Familiarity with Git
+ Knowledge of JIRA and Confluence
Scheduled Weekly Hours
40
Equal Opportunity Employer
View the EEO is the Law poster.
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact mailbox_tas_recruit@humana.com for assistance.
","['gcp', 'spark', 'azur', 'cassandra', 'postgressql', 'jira', 'git', 'javascript', 'aw', 'python', 'java', 'hbase', 'hadoop', 'cloud', 'kubernet', 'kafka', 'oracl']","['healthcar', 'analyz', 'optim', 'big data', 'etl']",1,"['gcp', 'spark', 'azur', 'cassandra', 'postgressql', 'jira', 'git', 'javascript', 'aw', 'python', 'java', 'hbase', 'hadoop', 'cloud', 'kubernet', 'kafka', 'oracl', 'healthcar', 'analyz', 'optim', 'big data', 'etl']","['program', 'techniqu', 'stream', 'provid', 'python', 'integr', 'etl', 'sourc', 'evalu', 'azur', 'hadoop', 'optim', 'clinic', 'primari', 'infrastructur', 'aw', 'big', 'set', 'warehous', 'engin', 'particip', 'spark', 'comput', 'relat']","['gcp', 'spark', 'azur', 'cassandra', 'postgressql', 'jira', 'git', 'javascript', 'aw', 'python', 'java', 'hbase', 'hadoop', 'cloud', 'kubernet', 'kafka', 'oracl', 'healthcar', 'analyz', 'optim', 'big data', 'etl', 'program', 'techniqu', 'stream', 'provid', 'python', 'integr', 'etl', 'sourc', 'evalu', 'azur', 'hadoop', 'optim', 'clinic', 'primari', 'infrastructur', 'aw', 'big', 'set', 'warehous', 'engin', 'particip', 'spark', 'comput', 'relat']"
DE,"A strong candidate will be able to rapidly troubleshoot complex technical problems under pressure, implement solutions that are massively scalable, while managing multiple customer groups.
Deep knowledge of MySQL/Orcale Database concepts, strong administration experience and excellent interpersonal communication skills are required.
In this job there are opportunities to model an Operational Data Store and Data Warehouse.
Essential Duties and Responsibilities:
Production MySQL performance troubleshooting and recommendations in but not restricted to AWS environment
Advise core software development team on best practices for interfacing core product with MySQL
Application schema design/data modeling and implementation using standard database modeling/management tools.
Database performance tuning and configuration in support of developmental software
Work with and mentor other software engineers as it relates to database engineering and development.
Manage AWS database infrastructure.
Education & Experience:
Bachelors Degree in Computer Science or closely related technical field.
Experience in database development, design and software engineering experience.
Experience in SQL/Oracle Server.
Experience in Open Source Database Technology Platforms such as MySQL.
Ability to lead specific strategic engagement involving partner teams.
Experience with migrating critical database environments from one platform to another.
Experience in Shell/Perl/Python Scripting.
Hands-on implementation experience is required.
Excellent SQL programming skills.
Self-starting and ability to work with minimal supervision.
Preferences:
Experience with AWS Platforms and associated technologies.
Experience in database administration.
Experience with data modeling tools.
Knowledge of git for software configuration management.
Experience with ETL and OLAP technologies is a plus.
Benefits and Perks:
Life insurance
Flexible spending account (FSA)
401(k)
Paid time off (PTO)
Casual office environment with flexible schedules
Access to gym facilities in office complex
Break room stocked with snacks!
Work closely with Engineering Executives
Opportunity to make a difference and implement your ideas.
Competitive but positive work environment
Access to gym facilities in office complex
Great customers (Southern California Edison, PG&E, SDG&E, AVANGRID, and more)
NO AGENCIES PLEASE!
","['sql', 'perl', 'git', 'aw', 'python', 'mysql', 'excel', 'oracl']","['recommend', 'tune', 'data modeling', 'supervis', 'etl', 'commun', 'account']",1,"['sql', 'perl', 'git', 'aw', 'python', 'excel', 'oracl', 'recommend', 'tune', 'data modeling', 'supervis', 'etl', 'commun', 'account']","['essenti', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'perl', 'git', 'aw', 'python', 'excel', 'oracl', 'recommend', 'tune', 'data modeling', 'supervis', 'etl', 'commun', 'account', 'essenti', 'relat', 'infrastructur', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']"
DE,")* Experience with Application Performance Management tools (Prometheus, Grafana)* Deep understanding of Agile principles and processes* Thrive in a fast-paced environment with minimal supervision* Experience in IoT, machine learning, computer vision, video solutions* Problem solver who can provide creative and cost effective solutionsEducation:* Bachelor's Degree in either Computer Science or a related scientific discipline or equivalent meaningful experience
",[None],"['machine learning', 'supervis', 'computer vision']",1,"['machine learning', 'supervis', 'computer vision']","['relat', 'comput', 'machin']","['machine learning', 'supervis', 'computer vision', 'relat', 'comput', 'machin']"
DE,"Data Engineer II
Technology San Diego, California
This position will be responsible for delivering best-in-class analytical data solutions.
The primary role will be developing new data assets, reporting system support, and maintenance, with opportunities to develop innovative solutions and explore new technologies.
This position will have direct interaction with customers and end-users for training, demos and requirements gathering.
Responsibilities
Data development, reporting system support, and maintenance.
Participate in the entire lifecycle for Business Intelligence Solution Delivery.
Design, build, document and manage data assets.
Design and develop ETL processing solutions.
Assist with BI Solutions (DBMS, SSIS, SSAS, SSRS, Tableau) development and performance tuning.
Develop, test and deliver error free data.
Stay abreast of BI industry best practices and relevant new technologies.
Design and develop reports/visualizations using SQL Server Reporting Services and Tableau.
Perform requirements gathering and analysis.
Ability to work independently (as well as within a team environment) and unsupervised.
Ability to work a flexible day schedule required including occasional evenings.
Data modeling of enterprise data assets.
Recommend and drive implementation of development techniques and methodologies.
Design and develop high performance reporting assets.
Work with SMEs (both technical and non-technical) for knowledge transfer and documentation.
Work with end-users to support adoption of team deliverables.
Work with junior team members to develop and grow their skill sets.
Qualifications
Bachelor's degree or higher in Computer Science, Computer Engineering, MIS or related field or equivalent work experience is preferred.
If equivalent experience, high school diploma required.
3 years of experience developing Business Intelligence solutions.
6 years of developing solutions using the MS SQL Server BI Stack.
Microsoft data-related certification preferred.
Strong technical skills; able to work with multiple platforms, integrate data with various sources.
Strong knowledge of TSQL.
Strong knowledge of data modeling.
Basic knowledge of data visualization practices and methods (Tableau preferred).
Basic understanding of security implementation in databases and BI solutions.
Solid understanding of Inmon or Kimball methodologies.
Basic understanding of dimensional data structures.
Knowledge of ETL Design Patterns and Configurations.
Basic understanding of multidimensional objects (cubes) and/or columnstore structures.
Ability to guide and mentor junior team members.
Core Competencies
Demonstrated ability to interact in a positive, respectful manner and establish and maintain cooperative working relationships.
Ability to display excellent customer service to meet the needs and expectations of both internal and external customers.
Excellent listening and interpersonal communication skills to identify critical core competencies based on success factors and organizational environment.
Ability to effectively organize, prioritize, multi-task and manage time.
Demonstrated accuracy and productivity in a changing environment with constant interruptions.
Demonstrated ability to analyze information, problems, issues, situations and procedures to develop effective solutions.
Ability to exercise strict confidentiality in all matters.
Mobility
Primarily sedentary, able to sit for long periods of time with ability to travel within and outside the facility.
Physical Requirements
Ability to speak, see and hear other personnel and/or objects.
Ability to communicate both in verbal and written form.
Ability to travel within the facility.
Capable of using a telephone and computer keyboard.
Ability to lift up to 10 lbs.
Environmental Conditions
Within facility, normal office conditions, including lighting and ventilation.
Minor noise from conversations and general office equipment (telephone, printer, etc.).
When required to travel outside the facility, usual weather, traffic, and related conditions are applicable.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran Status, or any other characteristic protected by applicable federal, state, or local law.
Please view Equal Employment Opportunity Posters provided by OFCCP here .
","['sql', 'ssr', 'bi', 'tableau', 'microsoft', 'excel']","['recommend', 'normal', 'visual', 'tune', 'data modeling', 'unsupervis', 'etl', 'commun']",1,"['sql', 'ssr', 'powerbi', 'tableau', 'microsoft', 'excel', 'recommend', 'normal', 'visual', 'tune', 'data modeling', 'unsupervis', 'etl', 'commun']","['day', 'analyt', 'asset', 'techniqu', 'relat', 'visual', 'primari', 'provid', 'bi', 'comput', 'school', 'etl', 'sourc', 'engin', 'particip']","['sql', 'ssr', 'powerbi', 'tableau', 'microsoft', 'excel', 'recommend', 'normal', 'visual', 'tune', 'data modeling', 'unsupervis', 'etl', 'commun', 'day', 'analyt', 'asset', 'techniqu', 'relat', 'visual', 'primari', 'provid', 'bi', 'comput', 'school', 'etl', 'sourc', 'engin', 'particip']"
DE,"Senior Data Engineer (multiple locations)
The ideal candidates for these position would be experienced individuals who are hard-working, have the ability to excel in a fast-paced government contractor environment, and have a positive, energetic attitude.
These positions are located in San Diego, CA; Norfolk/Virginia Beach, VA; Washington D.C. Metro; and Hawaii.
This position will support the Program Executive Office Command, Control, Communications, Computers and Intelligence (PEO C4I) in their work to transform, modernize, improve, and sustain current and future operational maritime tactical Command and Control needs into effective and affordable C2 capabilities for Navy, Marine Corps, Joint, and Coalition warfighters.
Key Responsibilities:
Provides engineering and technical support services to assist in the development, upgrade, review, sustainment, production and delivery of program documentation and data.
Assists with the development of the C4ISR Data Strategy which includes Information Operations (IO), Combat Systems, Tactical Communications, C2 and Intelligence Surveillance and Reconnaissance (ISR) domains.
Assists in the preparation of point papers and support documentation to facilitate the integration of procured technologies with current and future Maritime C2, Support C2, Tactical C2, and, Defensive C2 programs as they relate Navy Tactical Cloud Reference Implementation (NTC RI) utilizing computing component (Cloudera), a Big Data analytic component (Hadoop Distributed Files System, Accumulo, MapReduce, and Storm), and a Data Storage component (Content Zone).
Assists in the establishment of program/project databases/spreadsheets to support system engineering requirements.
This includes: problem resolution, use of data/statistical analysis tools, support for meetings such as Program Office Configuration Change Board (CCB), Platform Technical Review Board (PTRB) and other required Program Office specific meetings.
Supports implementation and use of M&S best practices including cost-benefit tools, standards information, data exchange techniques, authoritative data, and architectures.
Requirements:
Master's degree in technical area.
Active DoD Secret clearance with ability to obtain Top Secret Clearance with Sensitive-Compartmented Information (SCI) access.
Active TS preferred.
Experience designing and developing models using large amounts of structured and unstructured data.
Desire ten (10) years of experience and knowledge in data engineering and design to include:
Intelligence Community Information Technology
Enterprise Government and industry standards (e.g., National Information Exchange Model)
Object-Based Production
Activity-Based Intelligence
Military Force Track Data mining, machine learning, natural language processing, and information retrieval
Extending existing pedigree and provenance data items to a complete logical ontology for use across C2, IO, ISR and data domains
Must be U.S. citizen to obtain and maintain a DoD Security Clearance
","['mapreduc', 'cloud', 'hadoop']","['natural language processing', 'data mining', 'machine learning', 'statist', 'big data', 'information technology', 'commun']",2,"['mapreduc', 'cloud', 'hadoop', 'nlp', 'data mining', 'machine learning', 'statist', 'big data', 'information technology', 'commun']","['analyt', 'machin', 'program', 'techniqu', 'hadoop', 'provid', 'amount', 'comput', 'statist', 'big', 'integr', 'engin']","['mapreduc', 'cloud', 'hadoop', 'nlp', 'data mining', 'machine learning', 'statist', 'big data', 'information technology', 'commun', 'analyt', 'machin', 'program', 'techniqu', 'hadoop', 'provid', 'amount', 'comput', 'statist', 'big', 'integr', 'engin']"
DE,"Posted: Jun 9, 2020
Role Number:
200174814
Meaningful insights require a solid infrastructure that is able to scale with the large amount of data coming in.
Would you like to help understand the challenges of building and maintaining a large-scale analytics infrastructure?
Are you excited about identifying areas for improvement and creating out-of-the-box solutions?
Key Qualifications
Excellent programming skills in C, C++, Python or Java
Prior experience developing production software
2 years minimum experience with Linux system administration and command line tools
Strong analytical thinking
Self-motivated and able to work independently
Excellent spoken and written communication skills
You will enable continued innovation and progress within the infrastructure through research and development.
You will help and support the execution, test and roll-out of solutions.
To be successful in this role, you must have a solid software engineering background and be able to write production level code.
Education & Experience
B.S., M.S.
or Ph.D. in Computer Science, Electrical Engineering or equivalent
Travel of up to 20% will also be required.
","['linux', 'java', 'c', 'python', 'excel']","['research', 'commun']",1,"['linux', 'java', 'c', 'python', 'excel', 'research', 'commun']","['analyt', 'challeng', 'line', 'infrastructur', 'amount', 'python', 'comput', 'engin']","['linux', 'java', 'c', 'python', 'excel', 'research', 'commun', 'analyt', 'challeng', 'line', 'infrastructur', 'amount', 'python', 'comput', 'engin']"
DE,"Summary
As a data scientist, you will work closely with a team of research and production professionals.
This role requires a broad knowledge of cancer biology, oncology, bioinformatics, statistics, and programming, paired with effective communication skills, a strong sense of diligence, and timeliness.
Duties and Tasks
Contribute the design of experiments focused on assay improvement.
Conduct data analysis in support of biopharma clinical trials and collaborations
Support development of novel genomic signatures
Design and implement data reports and dashboards
Contribute to manuscript drafting for peer-reviewed journals and patent filings
Support and assist colleagues and external collaborators in data preparation and analysis
Understanding, maintaining, and improving various databases and schema
Contribute to the maintenance and improvement of the data processing pipelines, automating common data analysis routines into software packages or applications.
Qualifications:
Graduate degree from a quantitative field with strong programming experience (data science, statistics, bioinformatics, or equivalent).
Proven experience in analysis and interpretation of large-scale gene expression data (microarray, sequencing).
1 year + experience in data science and quantitative data analysis.
1 year + experience in working with cloud platform like GCP.
Familiar with a Linux environment and shell scripting.
Experience working with source control tools (Git) in a collaborative programming environment.
Knowledge in cancer biology and genomics.
Work/research experience with public genomic data platforms (GEO, CBioPortal, MSigDB, etc or equivalent) is a plus.
Extensive experience of working with R or Python.
Experience of building and testing customized R packages.
Software engineering experience in a programming language (GO and Javascript is preferred) is a plus.
Experience in performing complex data analysis on large volumes of data and present findings to collaborators.
Experience in drafting and publishing scientific research papers and addressing comments and feedbacks from collaborators and reviewers.
Strong interpersonal and communication skills (both written and verbal); ability to communicate with people in a wide variety of areas and at various levels from technical specialists to executives
Ability to quickly and efficiently adapt to new concepts and collaborate with cross-function teams and business units.
Critical thinking
Curiosity and eager to learn.
Powered by JazzHR
Zp1iPX2t0j
","['linux', 'gcp', 'git', 'javascript', 'cloud', 'python', 'r']","['research', 'bioinformat', 'pipelin', 'dashboard', 'statist', 'commun']",2,"['linux', 'gcp', 'git', 'javascript', 'cloud', 'python', 'r', 'research', 'bioinformat', 'pipelin', 'dashboard', 'statist', 'commun']","['program', 'pipelin', 'power', 'public', 'python', 'interpret', 'packag', 'statist', 'scientist', 'clinic', 'quantit', 'common', 'sourc', 'engin']","['linux', 'gcp', 'git', 'javascript', 'cloud', 'python', 'r', 'research', 'bioinformat', 'pipelin', 'dashboard', 'statist', 'commun', 'program', 'pipelin', 'power', 'public', 'python', 'interpret', 'packag', 'statist', 'scientist', 'clinic', 'quantit', 'common', 'sourc', 'engin']"
DE,"Big Data Engineer
Full-Time.
Multiple Locations
Annual Salary Range: Based on experience
Full benefits including:
Medical/Dental/Vision /Paid travel/Paid Time Off/ 401(k) savings plan
What you will be doing:
The ideal candidate is a data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
They are a team player possessing strong analytical as well as technical skills and are able to communicate the logic behind technical decisions to non-tech stakeholders.
What you need for this position:
Degree in Computer Science, Information Systems or equivalent quantitative field and 3+ years of experience in a similar Big Data Engineer role.
Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management
Strong interpersonal skills and ability to project manage and work with cross-functional teams
Working SQL knowledge with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Bonus points:
Relational SQL and NoSQL databases
Object-oriented/Object function scripting languages such as Python, Scala, etc.
Job Type: Full-time
Multiple locations
Benefits:
Health insurance
Dental insurance
Vision insurance
Retirement plan
Paid time off
","['sql', 'python', 'nosql', 'scala']","['pipelin', 'big data']",2,"['sql', 'python', 'nosql', 'scala', 'pipelin', 'big data']","['analyt', 'pipelin', 'relat', 'python', 'comput', 'big', 'quantit', 'engin']","['sql', 'python', 'nosql', 'scala', 'pipelin', 'big data', 'analyt', 'pipelin', 'relat', 'python', 'comput', 'big', 'quantit', 'engin']"
DE,"Functional Description/Track (Technical Individual Contributor): Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy.
Documents and communicates database design.
Evaluates and installs database management systems.
Codes complex programs and derives logical processes on technical platforms.
Builds windows, screens and reports.
Assists in the design of user interface and business application prototypes.
Participates in quality assurance and develops test application code in client server environment.
Provides expertise in devising, negotiating and defending the tables and fields provided in the database.
Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application.
Demonstrated experience and ability to develop in Java (Scala) Primary role Demonstrated experience as a DBA supporting a Production environment Secondary role Knowledge of traditional and modern data warehouse methodologies Proven ability to learn new tools and technology Expert level SQL with the ability to create and evaluate complex SQL statements, stored procedures, and index tables Experience developing, deploying, and supporting ETL workflows Demonstrated ability to work in a fast paced and changing environment with short deadlines, interruptions, and multiple tasks/projects occurring at once Bachelor's degree in Computer Science, Information Systems, Mathematics or Engineering from an accredited academic university, or an equivalent combination of education and experience Experience with GCP infrastructure deployments application resource management in GCP environment(s) Experience developing and deploying libraries for database(s) interaction layer Experience with PostgreSQL Experience with GCP services (Dataflow/PubSub/Airflow/Spanner) Prior medical device experience Up to 10% Possesses broad understanding of technical principles and theories.
Ability to synthesize external data and research findings for application that may impact technical objectives.
Demonstrates successes in technical proficiency and independent thought.
Works on complex problems in which analysis of situations or data requires an indepth evaluation of various factors.
Exercises judgment within broadly defined practices and policies in selecting methods, techniques and evaluation criteria for obtaining results.
Exercises good judgment in selecting methods and techniques for obtaining solutions.
Normally receives little instruction on daytoday work, general instructions on new assignments.
LICC1
","['sql', 'gcp', 'airflow', 'scala', 'java', 'postgresql']","['research', 'etl', 'normal']",1,"['sql', 'gcp', 'airflow', 'scala', 'java', 'postgresql', 'research', 'etl', 'normal']","['program', 'techniqu', 'judgment', 'primari', 'infrastructur', 'provid', 'comput', 'warehous', 'etl', 'engin', 'evalu']","['sql', 'gcp', 'airflow', 'scala', 'java', 'postgresql', 'research', 'etl', 'normal', 'program', 'techniqu', 'judgment', 'primari', 'infrastructur', 'provid', 'comput', 'warehous', 'etl', 'engin', 'evalu']"
DE,"Hi
Greetings from TekLeaders!!!
Job Title : AWS Data Engineer
Duration of the Project : 06 Month Contract
Interview Process: Zoom Video Interview
Â
In this role the incumbent must know how data flows happen, how to engineer the data to extract it into data warehousing platforms (snowflake, netezza, oracle, Teradata), and have extensive experience with SQL (writing SQL queries).
Partner with the Data architects, Product managers and Scrum Masters to deliver data integrations and BI solutions required for various projects
Ability to develop ETL pipelines on AWS platform using Python and AWS services such as S3 Buckets, Lambda, API Gateway, SQS queues
Ability to write complex SQL scripts and automate them using Python
Ability to build REST APIs to read data from Database via API requests
Ability to consume APIs to get data and load it into database using Python
Ability to write SQL queries against major databases such as Oracle, Snowflake, SQL Server etc.
Ability to integrate on premise infrastructure with public cloud (AWS) infrastructure
Requirements
3-5 years' experience with data engineering using AWS platform and Python
3-5 years' experience AWS cloud and AWS services such as S3 Buckets, Lambda, API Gateway, SQS queues
3-5 years' experience using Python for data processing using CSV, JSON, Delimited, XML, AVRO, Parquet and other file formats
3-5 years' experience with major data warehousing platform such as Oracle, Teradata, Netezza, AWS Redshift, Snowflake etc.
3-5 years' experience developing ETL, ELT and Data Warehousing solutions
3-5 years' experience in loading source system data extracts into data warehouse
3-5 years' experience with batch job scheduling and identifying data/job dependencies
3-5 years' experience with automation of DevOps build using GitLab/Bitbucket/Jenkins/Maven
Strong in Linux experience to for shell scripting and Python scripting
Experience working directly with technical and business teams.Â
Â
Best Regards,
Â
Kevin
Sr. Technical Recruiter
TekLeaders Inc
4975 Preston Park Blvd, Suite 500 Plano, TX 75093
Phone: +1(214) 614-9025 | Text: +1(315) 257-8762
Email: kevin@tekleaders.com
www.tekleaders.com
Â
Â
Â
","['sql', 'linux', 'bi', 'snowflak', 'python', 's3', 'redshift', 'lambda', 'aw', 'cloud', 'oracl']","['data warehousing', 'etl', 'pipelin']",2,"['sql', 'linux', 'powerbi', 'snowflak', 'python', 's3', 'redshift', 'lambda', 'aw', 'cloud', 'oracl', 'data warehousing', 'etl', 'pipelin']","['engin', 'pipelin', 'public', 'infrastructur', 'bi', 'aw', 'python', 'etl', 'sourc', 'â', 'integr']","['sql', 'linux', 'powerbi', 'snowflak', 'python', 's3', 'redshift', 'lambda', 'aw', 'cloud', 'oracl', 'data warehousing', 'etl', 'pipelin', 'engin', 'pipelin', 'public', 'infrastructur', 'bi', 'aw', 'python', 'etl', 'sourc', 'â', 'integr']"
DE,"Job Title: Data Engineer
Work matters.
And the workplace of the future is going to be a great place.
People matter.
Team
This is a new team within the Data and Analytics organization.
The team is fast-paced while managing highly accurate detailed information.
Role
The role entails collaborative engagements with Field & Product Line Sales Teams, Global Services, Alliances & Channels and FP & A to deploy insightful analytic products, establish alignment on processes teams and deliver strategic metrics for current and future business initiatives.
What You Get To Do In This Role
You will provide insights and deep analysis being sought by users/business stakeholders
Work with Cross Functional Analytics team members to curate and assimilate insights
Grow into being SME on business functions
Gather business requirements from stakeholders on various analytics initiatives
Analyze requirements, determine optimal solutions and determine gap from current state, dependencies and ways to mitigate risks
Develop business requirements documentation, process workflow diagrams, functional specifications, user acceptance test scripts and other supporting documentation for Business Intelligence and Analytics initiatives
Assist stakeholders with data analysis, design data models & develop DB Views, procs, models in SAP HANA to meet business need
Develop dashboard and report prototypes and mock-ups with respect to the UX/UI Best Practices and have impactful UI Design
Communicate status regularly with stakeholders
Define required data integration requirements between various systems and work with extended team to get them created
Collaborate with India Development Center BI team to translate business requirements and get appropriate data solutions developed to meet business need
Partner with Global BI team to help implement solutions for end user adoption
Bachelor's Degree in Information System, Analytics, Business Intelligence or related field required
1 to 3+ years of documented experience in writing strong SQL, PLSQL in data warehouse technologies (Hana, Snowflake or any modern database).
Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)
Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools
Working knowledge Tableau, Power BI is a plus
Strong analytical and problem-solving ability and be able dive into technical details and design analytics solutions
Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively
Must have good communication, presentation, and documentation skills
Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products
Business process design, project management, and/or Agile SDLC experience a plus
1-2 years of SAP HANA experience is a plus
","['sql', 'excel', 'bi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'power bi']","['dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun']",1,"['sql', 'excel', 'powerbi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun']","['analyt', 'program', 'relat', 'line', 'power', 'optim', 'bi', 'python', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['sql', 'excel', 'powerbi', 'snowflak', 'python', 'tableau', 'r', 'db', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'end user', 'risk', 'optim', 'analyz', 'commun', 'analyt', 'program', 'relat', 'line', 'power', 'optim', 'bi', 'python', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Data Engineer
· Have a penchant for digging deep and attacking difficult and complex problems.
· Be a self-starter who is able to work alone or as a member of a team.
· Excellent oral and written communication skills as well as strong analytical and problem-solving skills.
· Ability to work on multiple concurrent projects and interface with all levels within the organization.
Key responsibilities include:
· Architect scalable data pipelines that handle millions of events per second in a cost-effective manner using AWS technologies including Airflow, Spark, EMR, Glue, Kinesis, Redshift/Spectrum and Athena.
· Create automated ingestion, processing, cleaning and aggregation of large data sets
· Design user access controls that allows partial and complete data access to large and diverse groups of users
· Work closely with game teams, Twitch, finance and marketing to build data solutions.
· Help continually improve ongoing reporting and analysis processes
Basic Qualifications
· 5+ years of industry experience in data engineering, with a track record of manipulating, processing, and extracting value from large datasets
· Demonstrated strength in data modeling, ETL development, and data warehousing
· Experience using big data technologies (Airflow, EMR, Columnar Data Warehouses, Spark etc.)
· Knowledge of data management fundamentals and data storage principles
· Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
· Experience working with AWS big data technologies (Redshift, S3, EMR)
· Experience with Airflow or other similar ETL tools
· Experience providing technical leadership and mentoring other engineers for best practices on data engineering
· Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
· Familiarity with statistical models and data mining algorithms
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
· Meets/exceeds Amazons leadership principles requirements for this role.
· Meets/exceeds Amazons functional/technical depth and complexity for this role.
","['spark', 'airflow', 'aw', 'redshift', 's3', 'excel']","['big data', 'pipelin', 'data mining', 'data modeling', 'clean', 'data warehousing', 'statist', 'financ', 'etl', 'commun']",999,"['spark', 'airflow', 'aw', 'redshift', 's3', 'excel', 'big data', 'pipelin', 'data mining', 'data modeling', 'clean', 'data warehousing', 'statist', 'financ', 'etl', 'commun']","['analyt', 'spark', 'pipelin', 'handl', 'set', 'divers', 'aw', 'comput', 'warehous', 'statist', 'big', 'etl', 'sourc', 'engin']","['spark', 'airflow', 'aw', 'redshift', 's3', 'excel', 'big data', 'pipelin', 'data mining', 'data modeling', 'clean', 'data warehousing', 'statist', 'financ', 'etl', 'commun', 'analyt', 'spark', 'pipelin', 'handl', 'set', 'divers', 'aw', 'comput', 'warehous', 'statist', 'big', 'etl', 'sourc', 'engin']"
DE,"Digital Health Technology team powers digital experiences and engagement to enhance the lives of millions of people every day through connected care.
The Advanced Analytics team is responsible for leading the way in executing on this promise.
This position will partner with the data science and analyst teams to build the systems and capabilities to discover these insights and deliver them in impactful ways.We are passionate and innovative problem solvers that support business units across the globe, providing on-going opportunities to engage with new and challenging projects.
You are passionate about data and using it in meaningful ways.
You have a deep understanding of a wide-variety of big data tools and technologies which you can implement to achieve desired business outcomes.
Reporting to the Manager of Data Engineering, you will play a key role of developing, constructing, testing and maintaining data on the AWS platform.Let's talk about Responsibilities:* Assemble extensive, complex data that meet functional/non-functional business requirements.
* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing transformation for greater scalability, etc.
* Use the infrastructure/services required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS services.
* Work with stakeholders, including the Product, Data, and Design teams to assist with data-related technical issues and support their data requirement needs.
* Create and maintain optimal data pipeline architecture in AWS.
* Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets* Excellent working knowledge on Linux* Experience working with Data Scientist* Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
* Must have experience working on Spark/Scala, Kafka, Elasticsearch and Python (at least two)* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
* Strong analytic skills related to working with unstructured datasets.
* Experience in manipulating, processing and extracting value from large disconnected datasets.
* Experience with AWS cloud services: (Redshift, RDS, EMR, Kinesis, S3, Glue, DMS-Batch/CDC, Athena, EC2, Lambda, SQS, SNS etc.
)* Experience working on AWS Data Lakes* Experience working on AWS Data Pipeline and CI/CD processes* Exposure to Hadoop Ecosystem preferably on AWS/EMR, NoSQL-based, SQL-like technologies* Experience with Data Science tools & technologies on AWS Cloud is plus* Experience supporting and working with cross-functional teams in a dynamic environment.
It's discovering a career that's challenging, supportive and inspiring.
Where a culture driven by excellence helps you not only meet your goals, but also create new ones.
","['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'excel']","['optim', 'pipelin', 'big data']",999,"['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'excel', 'optim', 'pipelin', 'big data']","['day', 'analyt', 'digit', 'spark', 'challeng', 'pipelin', 'power', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'scientist', 'big', 'relat', 'sourc', 'engin']","['sql', 'linux', 'spark', 'elasticsearch', 'scala', 'ec2', 'hadoop', 'aw', 'lambda', 'python', 's3', 'redshift', 'nosql', 'cloud', 'kafka', 'excel', 'optim', 'pipelin', 'big data', 'day', 'analyt', 'digit', 'spark', 'challeng', 'pipelin', 'power', 'hadoop', 'infrastructur', 'optim', 'aw', 'python', 'scientist', 'big', 'relat', 'sourc', 'engin']"
DE,"Sr. Data Engineer
San Diego, CA
12 months contract
8+ Years of experience in Data and Analytics area
5+ Years of working in Microsoft Azure environment (Azure SQL)
Strong experience with Azure services: Data Factory, Functions, is a must
If you are interested kindly reach me at sathish.r@itresonance.com / 630-345-5252.
","['sql', 'microsoft', 'azur']",[None],999,"['sql', 'microsoft', 'azur']","['analyt', 'engin', 'azur']","['sql', 'microsoft', 'azur', 'analyt', 'engin', 'azur']"
DE,"Hi,
Hope you are doing well.
Job Title: Data Engineer and Visualization Developer
Duration: Long Term Contract
Strong Python development experience
Experience working on Splunk
Tableau development experience
Excellent SQL knowledge
Airflow and PySpark experience
Excellent communication skills
Ability to work in agile project environment
Thanks & Regards
Ranjith Kumar Vemula
Work: 408-648-2170
Mailto: Ranjith.vemula@idctechnologies.com
","['sql', 'pyspark', 'airflow', 'splunk', 'tableau', 'python', 'excel']","['commun', 'visual']",999,"['sql', 'pyspark', 'airflow', 'splunk', 'tableau', 'python', 'excel', 'commun', 'visual']","['python', 'visual', 'engin']","['sql', 'pyspark', 'airflow', 'splunk', 'tableau', 'python', 'excel', 'commun', 'visual', 'python', 'visual', 'engin']"
DE,"for consideration with preference for rehire.
Eligible Special Selection clients should contact their Disability Counselor for assistance.DESCRIPTIONDepartment Overview:The San Diego Supercomputer Center (SDSC) is a world leader in using, innovating and providing cyberinfrastructure to enable advances and new discovery in science and engineering.
The incumbent will need to demonstrate competency in selecting tools, methods and techniques to obtain results, give technical presentations to associated teams and other technical units, and evaluate new technologies including performing moderate to complex cost / benefit analyses.
The incumbent will support resource managers, schedulers and client access to parallel and distributed file systems, conduct multi-faceted analysis, testing, scripting and benchmarking, work with very complex, advanced systems, data and networks in a research and performance evaluation environment, and provide technical expertise in parallel and high-performance filesystems (Lustre, Ceph, GPFS, etc.)
and storage.
S/he will present at national meetings as necessary, work with the Operations group in training their staff and serve as liaison to the computational scientists, work on multiple problems or tasks that are not necessarily well defined and make recommendations that have an impact on an entire project or system, as well as provide advanced technical guidance to others at the same or lower level on an ongoing basis.
Specifically demonstrated through experience administering large-scale HPC clusters and their related filesystems.
* Demonstrated experience with large data storage arrays (more than100TB), and skill necessary to administer, maintain, monitor and upgrade.
Ability to use said knowledge to integrate HPC resources into data center network.SPECIAL CONDITIONS* Job offer is contingent upon satisfactory clearance based on Background Check results.
* Occasional evenings and weekends may be required.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age, protected veteran status, gender identity or sexual orientation.
Please visit smokefree.ucsd.edu for more information.
",[None],"['recommend', 'research', 'cluster']",2,"['recommend', 'research', 'cluster']","['basi', 'techniqu', 'provid', 'comput', 'scientist', 'relat', 'engin', 'evalu']","['recommend', 'research', 'cluster', 'basi', 'techniqu', 'provid', 'comput', 'scientist', 'relat', 'engin', 'evalu']"
DE,"Job Requisition Number:20190821J2
Job Title: Senior Data Engineer
Salary Range or Maximum: Negotiable
Employment Type: Contingent upon award
Security Clearance: TOP SECRET, SCI ELIGBLE
Posted Date: August 21, 2019
Closing Date: When Filled
Desired Skill Requirements:
Desire ten (10) years of experience and knowledge in data engineering and design to include:
Object Based Production Activity Based Intelligence
Military Force Track Data mining, machine learning, natural language processing, and information retrieval
Capability to obtain a TOP SECRET clearance with SCI access
Must demonstrate strong interpersonal skills including ability to communicate, both orally and in writing, and proficiency in writing reports and instructions; be able to brief senior leaders on assignments.
Job Duties:
Support the Navy Government S&T manager through all development phases of Navy C2 systems from RDT&E through delivery and execution of C2 System of Systems.
Responsible to interact directly with all levels of the Government Team and their stakeholders and Customers.
Education/Equivalent: Master's Degree in Technical area.
Travel Required: Possible travel associated with role.
",[None],"['machine learning', 'natural language processing', 'data mining']",2,"['machine learning', 'nlp', 'data mining']","['machin', 'engin']","['machine learning', 'nlp', 'data mining', 'machin', 'engin']"
DE,"Campus:
San Diego
Job ID:
7232
Job Title:
7232 Data Engineer (Analyst/Programmer - Career), Information Technology DevOps
Appointment Type:
Probationary
Time Base:
Full-Time
Date Posted:
March 9, 2020
Closing Date:
Open until filled
Benefits Link:
CSUEU (Unit 2, 5, 7, 9)
https://bfa.sdsu.edu/hr/jobs/job-opportunities.aspx
Campus Employment Homepage:
http://jobs.sdsu.edu
Position Summary
Under the direction of the Director of IT Computing Operations and DevOps, the incumbent will design, architect, develop, implement and maintain systems supporting the campus data lake and data warehouse environments.
This position will develop data pipeline routines to support the ingestion and transformation of data into the campus data lake and data warehouse environments using Amazon Web Services (AWS) services such as Lake Formation, Glue, EMR, Redshift and tools such as Python and Spark.
In addition, the incumbent will work with other team members to develop API solutions to support the exchange of campus data.
This position will work closely with many campus departments to understand their data needs in order to increase data exchange, analysis and reporting capabilities.
For more information regarding the IT Division/IT Finance and Administration department, click here: https://it.sdsu.edu/
This is a full-time (1.0 time-base) benefits eligible, permanent (probationary) position.
This position is designated exempt under FLSA and is not eligible for overtime compensation.
Standard SDSU work hours are Monday – Friday, 8:00 a.m. to 4:30 p.m., but may vary based on operational needs.
Education and Experience
To enter this classification, a basic foundation of knowledge and skills in applications programming and systems analysis and related programming support functions is a prerequisite.
This foundation would normally be obtained through a bachelor’s degree, preferably in computer science or business, or equivalent training and applied experience.
Foundation knowledge and skills for the Analyst/Programmer, depending on the position assignment, may include working knowledge of a specific industry standard applications programming language and knowledge of standard systems analysis techniques.
Preferred Qualifications and Specialized Skills
B.S.
in Computer Science, B.S in related course of study, or equivalent skills and experience
Experience working in a Higher Education environment
Minimum of two (2) years experience with Amazon Web Services (AWS)
Experience with Amazon Web Services (AWS) database services: RDS, Aurora or Redshift
Experience with ETL
Proficiency with Spark in Python
Knowledge of database modeling and design in a data warehousing context
Proficiency with SQL
Experience working with visualization tools such as Tableau or Power BI
Compensation and Benefits
Starting salary upon appointment not expected to exceed $8,333 per month.
Salary placement is determined by the education, experience, and qualifications the candidate brings to the position, internal equity, and the hiring department’s fiscal resources.
For more information regarding SDSU benefits, please click here: https://bfa.sdsu.edu/hr/jobs/benefits
Supplemental Information
Initial review of the required application materials, including cover letters and resumes, will begin on March 19, 2020.
The position will remain open until filled.
Applicants must currently be authorized to work in the United States on a full-time basis.
Offers of employment are contingent upon the presentation of documents that demonstrate a person's identity and authorization to work in the United States, which are consistent with the provisions of the Immigration Reform and Control Act.
SDSU is a smoke-free campus.
For more information, please click here: https://smokefree.sdsu.edu/
SDSU is an equal opportunity employer and does not discriminate against persons on the basis of race, religion, national origin, sexual orientation, gender, gender identity and expression, marital status, age, disability, pregnancy, medical condition, or covered veteran status.
","['sql', 'spark', 'bi', 'aw', 'python', 'redshift', 'tableau', 'power bi', 'amazon web services']","['classif', 'pipelin', 'visual', 'data warehousing', 'financ', 'information technology', 'etl']",1,"['sql', 'spark', 'powerbi', 'aw', 'python', 'redshift', 'tableau', 'classif', 'pipelin', 'visual', 'data warehousing', 'financ', 'information technology', 'etl']","['basi', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'visual', 'power', 'bi', 'aw', 'python', 'comput', 'appli', 'warehous', 'etl', 'engin']","['sql', 'spark', 'powerbi', 'aw', 'python', 'redshift', 'tableau', 'classif', 'pipelin', 'visual', 'data warehousing', 'financ', 'information technology', 'etl', 'basi', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'visual', 'power', 'bi', 'aw', 'python', 'comput', 'appli', 'warehous', 'etl', 'engin']"
DE,"Documents and communicates database design.
Evaluates and installs database management systems.
Codes complex programs and derives logical processes on technical platforms.
Builds windows, screens and reports.
Assists in the design of user interface and business application prototypes.
Participates in quality assurance and develops test application code in client server environment.
Provides expertise in devising, negotiating and defending the tables and fields provided in the database.
Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application.
Possesses broad understanding of technical principles and theories.
Ability to synthesize external data and research findings for application that may impact technical objectives.
Demonstrates successes in technical proficiency and independent thought.
Works on complex problems in which analysis of situations or data requires an indepth evaluation of various factors.
Exercises judgment within broadly defined practices and policies in selecting methods, techniques and evaluation criteria for obtaining results.
Exercises good judgment in selecting methods and techniques for obtaining solutions.
Normally receives little instruction on daytoday work, general instructions on new assignments.
LICC1
",[None],"['research', 'normal', 'commun']",999,"['research', 'normal', 'commun']","['program', 'techniqu', 'judgment', 'provid', 'engin', 'evalu']","['research', 'normal', 'commun', 'program', 'techniqu', 'judgment', 'provid', 'engin', 'evalu']"
DE,"Job Title:- Date Engineer
Duration: 6+ Months
Roles and Responsibility:-
• Interfacing with business customers, gathering requirements and developing new datasets in data platform
• Building and migrating the complex ETL pipelines from on premise system to cloud
• Identifying the data quality issues to address them immediately to provide great user experience
• Extracting and combining data from various heterogeneous data sources
• Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets
Basic Qualifications:
• Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field
3+ years relevant experience in data engineering.
• Demonstrated ability in data modeling, ETL development, and data warehousing.
Advanced SQL experience is a must
• Data Warehousing Experience with SQL Server.
• Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch etc.)
• Experience in using .Net,C# and/or other data engineering languages
• Knowledge and experience of SQL Sever and SSIS.
Preferred Qualifications:
• Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets • Experience building data products incrementally and integrating and managing datasets from multiple focus.
","['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql']","['pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']",1,"['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl']","['spark', 'pipelin', 'set', 'relat', 'hadoop', 'avail', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin']","['sql', 'spark', 'elasticsearch', 'pig', 'hadoop', 'c', 'hive', 'hbase', 'nosql', 'pipelin', 'data modeling', 'data warehousing', 'statist', 'big data', 'etl', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'avail', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"for consideration with preference for rehire.
All layoff applicants should contact their Employment Advisor.
Eligible Special Selection clients should contact their Disability Counselor for assistance.
Job posting will remain open until position is filled.
Department Overview:
Focusing on data-oriented and computational science and engineering applications, SDSC serves as an international resource for data cyberinfrastructure through the provision of software, hardware and human resources in multi-disciplinary science and engineering, and is a leading national cyberinfrastructure center to the National Science Foundation (NSF) and broader community.
SDSC’s High-Performance Systems Group is responsible for and operates SDSC’s high-performance computing clusters and related systems.
The group operates large-scale compute and storage systems funded by the National Science Foundation (currently the XSEDE program), the UCSD campus (e.g., the Triton Shared Compute Cluster) and other entities; these systems support users from campus, national, and international communities across a broad range of scientific disciplines.
The group is part of SDSC’s Data-Enabled Scientific Computing (DESC) Division.
Position Overview:
S/he will resolve a wide range of business processes, system functionality, implementation issues and system and software integration issues.
The incumbent will need to demonstrate competency in selecting tools, methods and techniques to obtain results, give technical presentations to associated teams and other technical units, and evaluate new technologies including performing moderate to complex cost/benefit analyses.
S/he may lead a team of systems/infrastructure professionals.
Additionally, the incumbent will be responsible for the management of national and campus HPC clusters and their related storage systems, such as large parallel file systems, NFS file servers, and the underlying storage technologies.
Responsibilities include but are not limited to systems administration (primarily Linux) with on-call duties, including management of hardware, OS, I/O, and software environment installation and maintenance.
The incumbent will support resource managers, schedulers and client access to parallel and distributed file systems, conduct multi-faceted analysis, testing, scripting and benchmarking, work with very complex, advanced systems, data and networks in a research and performance evaluation environment, and provide technical expertise in parallel and high-performance filesystems (Lustre, Ceph, GPFS, etc.)
and storage.
Also, s/he will be responsible for system internals, data and storage, network and operating systems, emerging technologies, hardware, and architectures and the interrelationship of all the foregoing and contribute to the design, installation, management and upgrade of very large HPC clusters, filesystems, data and storage resources.
The incumbent will work closely with other groups to integrate the HPC systems and storage into the SDSC networking, cloud, and user environments, collaborate on security procedure development and implementation, and provide support to the user services and scientific applications group.
S/he will present at national meetings as necessary, work with the Operations group in training their staff and serve as liaison to the computational scientists, work on multiple problems or tasks that are not necessarily well defined and make recommendations that have an impact on an entire project or system, as well as provide advanced technical guidance to others at the same or lower level on an ongoing basis.
The incumbent needs to work well in a group and collaborative setting, such as national projects like XSEDE and its constituent working groups and be able to exhibit effective communications skills in a professional manner.
For more information, please visit www.sdsc.edu.
QUALIFICATIONS
Advanced knowledge of systems integration and deploying moderately complex systems integration solutions.
Specifically proven through experience administering large-scale HPC clusters and their related filesystems.
Demonstrated experience with large data storage arrays (more than100TB), and skill necessary to administer, maintain, monitor and upgrade.
Ability to install, maintain, upgrade, and troubleshoot large (petabyte scale) high performance parallel and distributed filesystems such as Luster, GPFS and Ceph
Strong knowledge of administering Linux systems, primarily Red Hat and its derivatives (e.g., CentOS).
Proven understanding of high speed interconnects used in HPC systems and storage such as Ethernet and Infiniband including knowledge of TCP/IP, VLANs, Pkeys, subnets and routing.
Ability to use said knowledge to integrate HPC resources into data center network.
SPECIAL CONDITIONS
Job offer is contingent upon a satisfactory clearance based on background check results.
Occasional evenings and weekends may be required.
Overtime and weekends may be required.
","['cloud', 'linux']","['recommend', 'research', 'hardwar', 'commun', 'cluster']",2,"['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster']","['basi', 'program', 'techniqu', 'set', 'relat', 'human', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']","['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster', 'basi', 'program', 'techniqu', 'set', 'relat', 'human', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']"
DE,"LI-AC1
",[None],[None],999,[],[None],[None]
DE,"Summary:
This is a new position on a growing team that will partner with multiple functions (IT, Manufacturing Operations, Quality Assurance, R&D, Finance, Commercial Operations, vendors, contract manufacturers) to implement an analytics function for Supply Chain.
Essential Duties and Responsibilities:
Design, build, and maintain processes to integrate data for supply chain and manufacturing.
Design, build, and maintain processes to transform source data into dimensional data models and views.
Gather business requirements from technical and non-technical stakeholders to develop supply chain key performance indicators for reports and dashboards.
Design, build, and maintain dashboards across multiple supply chain functions.
Partner with cross-functional teams to facilitate data access and deployment.
Document and troubleshoot database performance or availability issues, and data quality issues.
Required Qualifications:
Quick learner
Highly attentive to detail and accuracy
SQL programming skills, including the use of SQL analytic functions
Experience using BI software (i.e., Tableau, Power BI) to create reports and dashboards
Experience with Microsoft Office software (i.e., Excel, PowerPoint, Outlook, Visio)
Understanding of database structures and data integration procedures
Ability to research and communicate data modeling approaches for selected use cases
Ability to research and implement data visualization best practices
Ability to effectively prioritize and execute tasks to meet deadlines
Experience working in a team-oriented, collaborative environment
Strong English written and oral communication skills
Preferred Qualifications:
Experience working with Google Cloud Platform and cloud technologies a plus
Experience working with ERP software tools (i.e., Oracle, SAP) a plus
Experience using and operating one or more data integration tool sets (i.e., SnapLogic, Talend, Informatica, etc.)
a plus
Knowledge of supply chain and manufacturing operations a plus
Experience using document, task, defect, requirement and test case management systems such as Oracle Agile PLM, Jira, GitHub, Jama, Confluence, etc.
a plus
Experience and Education Requirements:
Typically requires a Bachelor’s degree in a technical discipline, and a minimum of 5-8 years related experience or Master’s degree and 2-5 years equivalent industry experience or a PhD and 0-2 years experience.
Certifications and/or degrees in Computer Science or Data Engineering preferred but not required
Travel Required: 0% -25%
Language Skills: Must be able to communicate effectively in English.
Ability to read and interpret documents such as safety rules, operating and maintenance instructions, and procedure manuals.
Ability to write routine reports and correspondence.
Physical Demands: The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
May be required to perform the following Physical Activity: Standing, walking, sitting, using hands, handle or feel, reach with hands and arms, climb or balance, stoop, kneel, crouch, or crawl, talk or hear, taste or smell.
May be required to lift 0 lbs.
to over 100 lbs.
Work Environment: The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.
May be exposed to the following environmental conditions: Wet or humid conditions, work near moving parts, work in high - precarious places, fumes or airborne particles, toxic or caustic chemicals, outdoor weather conditions, extreme heat (non-weather), extreme cold (non-weather), risk of electric shock, work with explosives, risk of radiation, vibration.
The noise level in this work environment can range from quiet to very loud.
Please note: The information contained herein is not intended to be an all-inclusive list of the duties and responsibilities of the job, nor are they intended to be an all-inclusive list of the skills and abilities required to do the job.
Management may, at its discretion, assign or reassign duties and responsibilities to this job at any time.
#LI-EF1
An Equal Opportunity Employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Talent Acquisition at talentacquisition@dexcom.com.
Only authorized staffing and recruiting agencies may use this site or to submit profiles, applications or resumes on specific requisitions.
","['sql', 'erp', 'google cloud', 'jira', 'bi', 'tableau', 'r', 'cloud', 'microsoft', 'github', 'powerpoint', 'excel', 'oracl', 'power bi']","['research', 'dashboard', 'visual', 'risk', 'data modeling', 'financ', 'commun']",1,"['sql', 'erp', 'google cloud', 'jira', 'powerbi', 'tableau', 'r', 'cloud', 'microsoft', 'github', 'powerpoint', 'excel', 'oracl', 'research', 'dashboard', 'visual', 'risk', 'data modeling', 'financ', 'commun']","['basi', 'analyt', 'essenti', 'set', 'relat', 'visual', 'power', 'bi', 'employe', 'avail', 'comput', 'integr', 'sourc', 'engin']","['sql', 'erp', 'google cloud', 'jira', 'powerbi', 'tableau', 'r', 'cloud', 'microsoft', 'github', 'powerpoint', 'excel', 'oracl', 'research', 'dashboard', 'visual', 'risk', 'data modeling', 'financ', 'commun', 'basi', 'analyt', 'essenti', 'set', 'relat', 'visual', 'power', 'bi', 'employe', 'avail', 'comput', 'integr', 'sourc', 'engin']"
DE,"The home services industry remains vast ($700b+) but untouched by technology and unencumbered by a dominant competitor.
The 3M+ mobile businesses in home services are facing dramatic change as the world shifts from offline to online.
The role:
You will actively work in a multi-disciplinary fast-paced environment, drive innovation and satisfy the analytics needs of your stakeholder.
This role requires a broad range of skills and abilities; you will be the functional lead, drive innovation, manage staff and do the work.
Data Warehouse (Snowflake)
ETL system & data pipelines (Segment, Stitchdata, Airflow, Python)
BI system (Tableau Online, Amplitude)
(Current analytics stack in parentheses)
Skills:
You have planned, built & managed data infrastructures in a public cloud.
You have 6+ years of experience as a Data Engineer/Data Architect and have architected distributed data platforms.
You can estimate, design, build and own a business initiative end to end.
You have in-depth experience with MySQL databases and Snowflake's data warehouse
You have managed a business intelligence system
You are proficient in at least one programming languages like Python, Scala and Java
You have familiarity with big data technologies like Hadoop, Spark, Hive
You are comfortable with setting and meeting SLAs for data availability and quality
You have an understanding of Machine Learning / AI principles in data engineering
You have consistently engaged in mentoring, training and reviewing other data engineers on the team.
You've worked in an Agile environment.
You thrive on scrum.
You make opportunities to bring value sooner rather than later.
You value data-driven decisions.
You are always looking for opportunities to quickly produce the right data to make decisions quickly.
You keep cool under pressure.
","['spark', 'airflow', 'scala', 'hadoop', 'bi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'mysql']","['pipelin', 'segment', 'machine learning', 'big data', 'etl']",999,"['spark', 'airflow', 'scala', 'hadoop', 'powerbi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'sql', 'pipelin', 'segment', 'machine learning', 'big data', 'etl']","['analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'public', 'hadoop', 'infrastructur', 'bi', 'python', 'avail', 'warehous', 'big', 'etl', 'engin']","['spark', 'airflow', 'scala', 'hadoop', 'powerbi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'sql', 'pipelin', 'segment', 'machine learning', 'big data', 'etl', 'analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'public', 'hadoop', 'infrastructur', 'bi', 'python', 'avail', 'warehous', 'big', 'etl', 'engin']"
DE,"For this position, Ideal candidate should have strong data engineering using AWS platform and Python.
Python for data processing using CSV, JSON, Delimited, XML, AVRO, Parquet and other file formats is highly preferred with major data warehousing platform such as Oracle, Teradata, Netezza, AWS Redshift, Snowflake etc.
For more details, please contact:
Taruna
(972) 753-6500 x131
jobs@primusglobal.com
","['aw', 'snowflak', 'python', 'redshift', 'oracl']",['data warehousing'],999,"['aw', 'snowflak', 'python', 'redshift', 'oracl', 'data warehousing']","['aw', 'python', 'engin']","['aw', 'snowflak', 'python', 'redshift', 'oracl', 'data warehousing', 'aw', 'python', 'engin']"
DE,"Responsibilities:
Primary responsibilities include, but are not limited to:
Design and develop resilient pipelines using a variety of different technologies.
Automate, test and harden all data workflows.
Architect logical and physical data models to ensure the needs of the business are met.
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure.
Be part of a fast growing data team handling massive scale with tons of automation.
Provide innovative solutions to challenging data projects and proof of concepts.
Education / Experience Requirements:
Minimum 4+ years of experience working with high volume data infrastructure.
Experience with AWS data related services.
Extensive experience programming in one of the following languages: Python / Java.
Experience in data modeling, optimizing SQL queries, and system performance tuning.
Knowledge and proficiency in the latest open source and data frameworks.
Experience evaluating industry trends and technologies.
Always be learning and staying up to speed with the fast moving data world.
Prefer candidates with AWS certifications.
PERKS:
Fridays are Work From Home days at Life360
Competitive pay and benefits
Free snacks, drinks, and food in the office
Catered lunches throughout the week
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
","['sql', 'java', 'python', 'aw']","['pipelin', 'data modeling']",999,"['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling']","['day', 'program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']","['sql', 'java', 'python', 'aw', 'pipelin', 'data modeling', 'day', 'program', 'pipelin', 'primari', 'infrastructur', 'provid', 'aw', 'python', 'relat', 'sourc', 'particip']"
DE,"Title: Sr. Data Engineer
Client: Sempra Energy
Duration: 6+ Months
As part of a team, responsible for designing and implementing data platform for a customer.
Need to migrate the existing on-prem data mart to Azure SQL with new data interfaces.
Build new data interfaces using Azure Data Factory and API services in Azure environment.
Requirements:
10+ Years of experience in Data and Analytics area
5+ Years of working in Microsoft Azure environment (Azure SQL)
5+ data factory pipeline and transformation services with implementation experience in ETL best practices, Messaging Frameworks, APIs/Web Services
Strong experience with Azure services: Data Factory, Functions, Logic Apps is a must
5+ years of experience in working in Agile development for data projects
Azure DevOps experience is preferred
Good team player working with remote teams
Excellent Communication skills and Customer interaction skills
","['sql', 'excel', 'microsoft', 'azur']","['etl', 'commun', 'pipelin']",999,"['sql', 'excel', 'microsoft', 'azur', 'etl', 'commun', 'pipelin']","['analyt', 'pipelin', 'azur', 'etl', 'engin']","['sql', 'excel', 'microsoft', 'azur', 'etl', 'commun', 'pipelin', 'analyt', 'pipelin', 'azur', 'etl', 'engin']"
DE,"for consideration with preference for rehire.
All layoff applicants should contact their Employment Advisor.
Eligible Special Selection clients should contact their Disability Counselor for assistance.
Job posting will remain open until position is filled.
Department Overview:
Focusing on data-oriented and computational science and engineering applications, SDSC serves as an international resource for data cyberinfrastructure through the provision of software, hardware and human resources in multi-disciplinary science and engineering, and is a leading national cyberinfrastructure center to the National Science Foundation (NSF) and broader community.
SDSC's High-Performance Systems Group is responsible for and operates SDSC's high-performance computing clusters and related systems.
The group operates large-scale compute and storage systems funded by the National Science Foundation (currently the XSEDE program), the UCSD campus (e.g., the Triton Shared Compute Cluster) and other entities; these systems support users from campus, national, and international communities across a broad range of scientific disciplines.
The group is part of SDSC's Data-Enabled Scientific Computing (DESC) Division.
Position Overview:
S/he will resolve a wide range of business processes, system functionality, implementation issues and system and software integration issues.
The incumbent will need to demonstrate competency in selecting tools, methods and techniques to obtain results, give technical presentations to associated teams and other technical units, and evaluate new technologies including performing moderate to complex cost/benefit analyses.
S/he may lead a team of systems/infrastructure professionals.
Additionally, the incumbent will be responsible for the management of national and campus HPC clusters and their related storage systems, such as large parallel file systems, NFS file servers, and the underlying storage technologies.
Responsibilities include but are not limited to systems administration (primarily Linux) with on-call duties, including management of hardware, OS, I/O, and software environment installation and maintenance.
The incumbent will support resource managers, schedulers and client access to parallel and distributed file systems, conduct multi-faceted analysis, testing, scripting and benchmarking, work with very complex, advanced systems, data and networks in a research and performance evaluation environment, and provide technical expertise in parallel and high-performance filesystems (Lustre, Ceph, GPFS, etc.)
and storage.
Also, s/he will be responsible for system internals, data and storage, network and operating systems, emerging technologies, hardware, and architectures and the interrelationship of all the foregoing and contribute to the design, installation, management and upgrade of very large HPC clusters, filesystems, data and storage resources.
The incumbent will work closely with other groups to integrate the HPC systems and storage into the SDSC networking, cloud, and user environments, collaborate on security procedure development and implementation, and provide support to the user services and scientific applications group.
S/he will present at national meetings as necessary, work with the Operations group in training their staff and serve as liaison to the computational scientists, work on multiple problems or tasks that are not necessarily well defined and make recommendations that have an impact on an entire project or system, as well as provide advanced technical guidance to others at the same or lower level on an ongoing basis.
The incumbent needs to work well in a group and collaborative setting, such as national projects like XSEDE and its constituent working groups and be able to exhibit effective communications skills in a professional manner.
For more information, please visit www.sdsc.edu.
QUALIFICATIONS
Advanced knowledge of systems integration and deploying moderately complex systems integration solutions.
Specifically proven through experience administering large-scale HPC clusters and their related filesystems.
Demonstrated experience with large data storage arrays (more than100TB), and skill necessary to administer, maintain, monitor and upgrade.
Ability to install, maintain, upgrade, and troubleshoot large (petabyte scale) high performance parallel and distributed filesystems such as Luster, GPFS and Ceph
Strong knowledge of administering Linux systems, primarily Red Hat and its derivatives (e.g., CentOS).
Proven understanding of high speed interconnects used in HPC systems and storage such as Ethernet and Infiniband including knowledge of TCP/IP, VLANs, Pkeys, subnets and routing.
Ability to use said knowledge to integrate HPC resources into data center network.
SPECIAL CONDITIONS
Job offer is contingent upon a satisfactory clearance based on background check results.
Occasional evenings and weekends may be required.
Overtime and weekends may be required.
","['cloud', 'linux']","['recommend', 'research', 'hardwar', 'commun', 'cluster']",2,"['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster']","['basi', 'program', 'techniqu', 'set', 'relat', 'human', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']","['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster', 'basi', 'program', 'techniqu', 'set', 'relat', 'human', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']"
DE,"hiring.
UCSD Campus will continue to recruit for essential positions
process during this unprecedented time.
For more information regarding
time, please click here.
for
consideration with preference for rehire.
All layoff applicants should
contact their Employment Advisor.
Eligible Special
Selection clients should contact their Disability Counselor for
assistance.
Job posting will remain open until position is filled.
===========
Department Overview:
innovating and providing cyberinfrastructure to enable advances and
new discovery in science and engineering.
Focusing on data-oriented
and computational science and engineering applications, SDSC serves as
an international resource for data cyberinfrastructure through the
multi-disciplinary science and engineering, and is a leading national
cyberinfrastructure center to the National Science Foundation (NSF)
and broader community.
SDSC s High-Performance Systems Group is responsible for and operates
SDSC s high-performance computing clusters and related systems.
The
group operates large-scale compute and storage systems funded by the
National Science Foundation (currently the XSEDE program), the UCSD
campus (e.g., the Triton Shared Compute Cluster) and other entities;
these systems support users from campus, national, and international
The group
Position Overview:
software integration concepts to evaluate, resolve and implement
scope and complexity.
processes, system functionality, implementation issues and system and
software integration issues.
The incumbent will need to demonstrate
competency in selecting tools, methods and techniques to obtain
results, give technical presentations to associated teams and other
technical units, and evaluate new technologies including performing
moderate to complex cost/benefit analyses.
systems/infrastructure professionals.
national and campus HPC clusters and their related storage systems,
such as large parallel file systems, NFS file servers, and the
underlying storage technologies.
Responsibilities include but are not
limited to systems administration (primarily Linux) with on-call
environment installation and maintenance.
The incumbent will support
resource managers, schedulers and client access to parallel and
distributed file systems, conduct multi-faceted analysis, testing,
scripting and benchmarking, work with very complex, advanced systems,
data and networks in a research and performance evaluation
environment, and provide technical expertise in parallel and
high-performance filesystems (Lustre, Ceph, GPFS, etc.)
and storage.
Also, s/he will be responsible for system internals, data and storage,
network and operating systems, emerging technologies, hardware, and
large HPC clusters, filesystems, data and storage resources.
The incumbent will work closely with other groups to integrate the HPC
systems and storage into the SDSC networking, cloud, and user
environments, collaborate on security procedure development and
implementation, and provide support to the user services and
scientific applications group.
S/he will present at national meetings
as necessary, work with the Operations group in training their staff
and serve as liaison to the computational scientists, work on multiple
problems or tasks that are not necessarily well defined and make
recommendations that have an impact on an entire project or system, as
well as provide advanced technical guidance to others at the same or
lower level on an ongoing basis.
The incumbent needs to work well in a
group and collaborative setting, such as national projects like XSEDE
and its constituent working groups and be able to exhibit effective
communications skills in a professional manner.
For more information, please visit .
QUALIFICATIONS
==============
complex systems integration solutions.
Specifically proven through
experience administering large-scale HPC clusters and their
related filesystems.
Demonstrated experience with large data storage arrays (more
than100TB), and skill necessary to administer, maintain, monitor
and upgrade.
Ability to install, maintain, upgrade, and troubleshoot large
(petabyte scale) high performance parallel and distributed
filesystems such as Luster, GPFS and Ceph
and its derivatives (e.g., CentOS).
systems and storage such as Ethernet and Infiniband including
Ability to
use said knowledge to integrate HPC resources into data center
network.
SPECIAL CONDITIONS
==================
Job offer is contingent upon a satisfactory clearance based on
background check results.
Occasional evenings and weekends may be required.
Overtime and
weekends may be required.
","['cloud', 'linux']","['recommend', 'research', 'hardwar', 'commun', 'cluster']",2,"['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster']","['basi', 'essenti', 'program', 'techniqu', 'set', 'relat', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']","['cloud', 'linux', 'recommend', 'research', 'hardwar', 'commun', 'cluster', 'basi', 'essenti', 'program', 'techniqu', 'set', 'relat', 'infrastructur', 'provid', 'comput', 'scientist', 'integr', 'engin', 'evalu']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"About the Role:
The Data Engineer provides data and reporting development services or technical support daily.
Also maintain data or reporting solutions (data warehouse/mart/stores/data lake/analytics) using tools and programming languages.
Develop data set processes, assists with design and identify ways to improve data, efficiency and quality.
Responsibilities include requirement analysis, code, test, debug, document and maintain data and analytics solutions.
You Will:
You Have:
• An Associate’s degree, a Bachelor's degree a plus
• Certified Business Intelligence Professional (CBIP) or equivalent certification
• Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs
• Comprehensive healthcare options (Medical, Dental, and Vision)
• 401K match, and a funded pension plan
• Paid vacation, holidays, and volunteer hours; flexible work environment
• Generously subsidized public transportation and free parking; annual tuition reimbursement
• Professional development programs, training and conferences
• And more…
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
",[None],['healthcar'],1,['healthcar'],"['analyt', 'program', 'public', 'set', 'engin']","['healthcar', 'analyt', 'program', 'public', 'set', 'engin']"
DE,"About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality.
Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.
You Will:
You Have:
• An Associate’s degree, a Bachelor's degree a plus
• Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
• Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
• Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
• Comprehensive healthcare options (Medical, Dental, and Vision)
• 401K match, and a funded pension plan
• Paid vacation, holidays, and volunteer hours; flexible work environment
• Generously subsidized public transportation and free parking; annual tuition reimbursement
• Professional development programs, training and conferences
• And more…
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
",[None],"['etl', 'healthcar']",1,"['etl', 'healthcar']","['analyt', 'program', 'relat', 'public', 'etl', 'sourc', 'engin', 'integr']","['etl', 'healthcar', 'analyt', 'program', 'relat', 'public', 'etl', 'sourc', 'engin', 'integr']"
DE,"POSITION SUMMARY
The Cloud Data Engineer relishes working with large volumes of data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics.
He/she is an expert with modern data architectures, data modeling, ETL design, business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure create significant business impact.
He/she needs to possess exceptional technical expertise in large scale data warehouse and BI systems.
RESPONSIBILITIES
Design, implement, and support a product data infrastructure providing ad-hoc access to large datasets and computing power.
Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena
Support and supplement current ETL activities based on Microsoft technologies including SQL Server, SSIS.
Analyze and facilitate the transition from on-prem database solutions to cloud solutions.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
Continual research of the latest big data, elastic search, and visualization technologies to provide new capabilities and increase efficiency
Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
Help continually improve ongoing reporting and analysis processes, automating or simplifying the process.
QUALIFICATIONS
Basic Qualifications
7+ years of industry experience in software development, data engineering, business intelligence, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in coding T-SQL, PowerShell, Python, PySpark and MySQL
Demonstrated strength in data modeling, ETL development, and data warehousing in AWS tools (EMR, Glue, Kinesis, Redshift, Spectrum and Athena etc.)
Demonstrated strength in utilizing Microsoft BI Stack including SSIS, SSAS, SSRS.
Experience using business intelligence reporting tools (Tableau, Power BI, Cognos etc.)
Knowledge of big data technologies (Hadoop, Hive, Hbase, Spark etc.)
Preferred Qualifications
Degree in computer science, engineering, mathematics, or a related technical discipline.
Experience working with AWS big data technologies (Redshift, S3, EMR)
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of data set
Certification on AWS tools and technologies
HD Vest, Inc is an equal opportunity employer and does not unlawfully discriminate on the basis of race, sex, age, color, religion, national origin, marital status, sexual orientation, veteran status, disability status or any other basis prohibited by federal, state or local law.
HD Vest, Inc considers information gathered in the hiring process, including information on this application, confidential, and only shares it on a need-to know basis or as required by law.
","['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'bi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'mysql', 'hbase', 'cloud', 'power bi']","['research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl']",1,"['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'powerbi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'hbase', 'cloud', 'research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl']","['basi', 'visual', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', 'power', 'comput', 'relat']","['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'powerbi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'hbase', 'cloud', 'research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl', 'basi', 'visual', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', 'power', 'comput', 'relat']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data.
This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose.
Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.
Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions.
Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions.
Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve.
This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics.
This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.
The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e.
Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel']","['recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']",2,"['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']","['basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account', 'basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']"
DE,"Data Engineer
ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow
Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle
Experience with cloud-based platforms and tools
Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps
Experience with integrating to both database systems and APIs
Experience with documenting technical requirements, designs and systems
Extensive experience building scalable and resilient data pipelines
Extensive experience writing SQL
Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R
Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
Requirements
3-5+ years of experience in technology and/or consulting
Bachelor’s Degree in CS, MIS, CIS, or a comparable technical degree
Benefits
","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud']","['data warehousing', 'etl', 'pipelin', 'big data']",1,"['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Data Engineer
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team?
You will transform complex analytical models into scalable, production-ready solutions
You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment
Responsibilities:
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages
A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
Bachelors Degree
At least 2 years of experience developing software or data solutions
At least 2 years of experience in Spark
Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL
","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgr']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"About the Role
The team covers data governance, data strategy and partnerships, reporting, machine learning and much more.
You will be the cornerstone for data solutions across each of these areas.
The candidate must be able to communicate, work with and deliver across each discipline as well as the end business user and executive leadership.
Responsibilities:
Create and deliver executive presentations explaining the complex data in simple easy-to-understand terms that resonates with an executive audience.
Must haves:
BS in a STEM field.
Advanced design, coding and analytics skills in a big data ecosystem.
Expert knowledge of SQL.
Experience with other languages such as Python, R, PySpark, Java or Scala.
Strong background of data structures and big data tools (Spark, Hive, HDFS, ect.).
Data wrangling and ETL tooling experience.
Exceptional communication skills between both business and technical teams.
Preferred:
MS or higher in a STEM field.
Experience managing teams or projects.
Demonstrated experience with AWS, GCP or Azure.
Experience handling confidential and sensitive data.
Demonstrated ability to independently influence and drive outputs, meet deadlines, and set clear expectations and roadmaps.
About You:
You take pride and responsibility seeing the product you worked on meet the real world for the first time
You love to learn and embrace the opportunity to contribute in new areas.
Integrity.
Drive to Innovate.
Joyful Work Environment.
Loving where you work isn’t about ping pong tables and free snacks.
It’s the feeling that you wouldn’t want to be on a project with any other team.
It’s the feeling that you can get creative energy just by showing up to work.
It’s the feeling that your entire team respects your life away from the job and understands how work impacts it.
","['sql', 'pyspark', 'gcp', 'spark', 'azur', 'scala', 'aw', 'python', 'hive', 'java', 'r']","['machine learning', 'etl', 'commun', 'big data']",1,"['sql', 'pyspark', 'gcp', 'spark', 'azur', 'scala', 'aw', 'python', 'hive', 'java', 'r', 'machine learning', 'etl', 'commun', 'big data']","['analyt', 'machin', 'learn', 'spark', 'azur', 'set', 'aw', 'python', 'big', 'etl', 'integr']","['sql', 'pyspark', 'gcp', 'spark', 'azur', 'scala', 'aw', 'python', 'hive', 'java', 'r', 'machine learning', 'etl', 'commun', 'big data', 'analyt', 'machin', 'learn', 'spark', 'azur', 'set', 'aw', 'python', 'big', 'etl', 'integr']"
DE,"Position: Data Engineer
Total 4 candidates
VISA: USC and GC ONLY
7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Should also have working experience using the following software/tools:
Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Qualifications
null
Additional Information
All your information will be kept confidential according to EEO guidelines.
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'postgr', 'cloud', 'kafka']","['statist', 'big data']",2,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'cloud', 'kafka', 'statist', 'big data']","['stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'cloud', 'kafka', 'statist', 'big data', 'stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']"
DE,"Minimum Requirements Experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python Experience in the following Big Data frameworks File Format (Parquet, AVRO, ORC etc..) Developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps Experience with SQL and Shell Scripting experience Experience with the Apache Spark Spark Professional work experience in Big Data development Data warehousing ETL design, development and implementation experience Python or Java development experience
","['sql', 'spark', 'scala', 'java', 'python']","['data warehousing', 'etl', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'java', 'python', 'data warehousing', 'etl', 'pipelin', 'big data']","['spark', 'pipelin', 'python', 'big', 'etl']","['sql', 'spark', 'scala', 'java', 'python', 'data warehousing', 'etl', 'pipelin', 'big data', 'spark', 'pipelin', 'python', 'big', 'etl']"
DE,"Hi,
Please forward suitable resumes to rakesh@athreyainc.com or call me at 732-582-4977
Role: Data Engineer
Locations:ÂTexas (Lewisville, Plano, Dallas) & Chicago, IL & Jersey City, NJ & Columbus, OH
Duration: Long Term
Mandatory Skills:
Java - 8+ Years, Spark 4+ Years, Big Data 4+ Years
Desired Skills:
Kafka - 3+ Years andÂSQL - 6+ Years.
Regards
Rakesh Sharma
Direct : (732) 582-4977
Email:Ârakesh@athreyainc.com
100 Jersey Avenue, Suite# B-201,
New Brunswick NJ - 08901.
""Certified Minority Business Enterprise (MBE)""
""Certified Small Business Enterprise (SBE)""
""E-Verify Enrolled Employer""
URL:Âwww.athreyainc.com
STATEMENT OF CONFIDENTIALITY
The information contained in this electronic message and any attachments hereto are intended for the sole and exclusive use of the addressee(s), and contain confidential and/or privileged information.
If you are not the intended recipient of this transmission you are hereby notified that any disclosure, copying, distribution or the taking of any action in reliance on the contents of this transmission is strictly prohibited.
If you have received this in error, please notify the sender of this message immediately, and destroy all copies of this message and any attachments.
""There is no power on earth that can neutralize the influence of a high, simple, and useful life.""
~ Booker T. Washington
Â
","['kafka', 'java', 'spark']",['big data'],999,"['kafka', 'java', 'spark', 'big data']","['spark', 'action', 'power', 'â', 'big', 'engin']","['kafka', 'java', 'spark', 'big data', 'spark', 'action', 'power', 'â', 'big', 'engin']"
DE,"At every touchpoint, customers discover stylish merchandise at incredible value from an extensive portfolio of private, exclusive and national brands.
It is a place where careers prosper, accomplishments are celebrated and diversity flourishes.
You'll take part in a fast-paced, 9 week internship which will consist of projects, activities, and hands-on training to facilitate your learning.
You'll have the opportunity to engage with senior leaders and subject matter experts who will be mentoring and coaching you to success.
Worth Area* Experience and good working knowledge of SQL and Python is key.
Java is a plus* Experience and knowledge of Data Visualization Tools such as Tableau is a plus* Intermediate user of Microsoft Excel* Experience with AWS, Hadoop, Spark, UNIX/Linux, Scala, NoSQL such as Cassandra, Agile framework is a plus* Strong customer service and communication skills and ability to collaborate and contribute as a member of a strong technology team* This position is available only to individuals who are eligible to work for JCPenney in the United States.
JCPenney will not sponsor individuals for immigration benefits for this role.Our corporate office, located within the exciting new development of Legacy West in Plano, Texas, supports JCPenney stores and supply chain facilities nationwide.
On-site campus amenities include health clinic, daycare, full cafeteria service, a 24-hour fitness center and free garage parking.#LI-KR2Job Title: Big Data Engineering InternLocation: Plano, TX, United States -Job ID: 1086005J.C.
","['sql', 'linux', 'spark', 'cassandra', 'scala', 'unix', 'hadoop', 'aw', 'tableau', 'python', 'java', 'nosql', 'microsoft', 'excel']","['commun', 'visual', 'big data']",999,"['sql', 'linux', 'spark', 'cassandra', 'scala', 'unix', 'hadoop', 'aw', 'tableau', 'python', 'java', 'nosql', 'microsoft', 'excel', 'commun', 'visual', 'big data']","['big', 'spark', 'corpor', 'visual', 'divers', 'hadoop', 'aw', 'avail', 'python', 'texa', 'clinic', 'engin']","['sql', 'linux', 'spark', 'cassandra', 'scala', 'unix', 'hadoop', 'aw', 'tableau', 'python', 'java', 'nosql', 'microsoft', 'excel', 'commun', 'visual', 'big data', 'big', 'spark', 'corpor', 'visual', 'divers', 'hadoop', 'aw', 'avail', 'python', 'texa', 'clinic', 'engin']"
DE,"Key Responsibilities / Functions:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Working experience with Tableau, QlikView, Mode, Matplotlib, Jupyter, or similar data visualization tools
Extensive experience analyzing data using SQL
Required Minimum Qualifications: (Education, Technical Skills/Knowledge)
2+ years of Python or Java development experience
2+ years of SQL experience (NoSQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
3+ years of relevant experience such as implementing statistical analysis, developing cloud-based data lakes / data warehouses, managing data science projects, developing APIs, developing machine learning models, creating advanced data visualizations.
Good communication and writing skills to facilitate productive collaboration with other team members and business units;
Strong knowledge of project management principles and concepts;
Experience solving problems with an emphasis on product development
Experience with predictive modeling and dissemination of research results;
","['sql', 'jupyt', 'matplotlib', 'cloud', 'tableau', 'python', 'java', 'nosql']","['research', 'pipelin', 'visual', 'predict', 'machine learning', 'data modeling', 'statist', 'commun']",999,"['sql', 'jupyt', 'matplotlib', 'cloud', 'tableau', 'python', 'java', 'nosql', 'research', 'pipelin', 'visual', 'predict', 'machine learning', 'data modeling', 'statist', 'commun']","['machin', 'learn', 'pipelin', 'visual', 'predict', 'python', 'statist', 'warehous', 'integr']","['sql', 'jupyt', 'matplotlib', 'cloud', 'tableau', 'python', 'java', 'nosql', 'research', 'pipelin', 'visual', 'predict', 'machine learning', 'data modeling', 'statist', 'commun', 'machin', 'learn', 'pipelin', 'visual', 'predict', 'python', 'statist', 'warehous', 'integr']"
DE,"About the Role:The Data Engineer provides data and reporting development services or technical support daily.
Also maintain data or reporting solutions (data warehouse/mart/stores/data lake/analytics) using tools and programming languages.
Develop data set processes, assists with design and identify ways to improve data, efficiency and quality.
You will work independently and oversee your work and work of junior members.
You will also have strong domain knowledge of the relevant industry/s.
Responsibilities include requirement analysis, code, test, debug, document and maintain data and analytics solutions.You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas.
These efforts take a team of dedicated individuals doing many different jobs.
",[None],[None],999,[],"['analyt', 'set', 'engin']","['analyt', 'set', 'engin']"
DE,"Mgr role.
",[None],[None],999,[],[None],[None]
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Senior Software/Data Engineer
Are you a Software Engineer that loves working with Data?
Do you thrive in a vibrant, innovative and collaborative team?
You will transform complex analytical models into scalable, production-ready solutions
You will develop applications from ground up using a modern technology stack such as Scala, Spark, PostgreSQL, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment
Responsibilities:
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages
A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
Bachelors Degree
At least 3 years of experience developing software or data solutions
Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years of experience in Spark
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL
","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"Your Role
Data Acquisition: Analyse data from varied sources like databases, CSVs, XMLs and other formats and write SQL commands/scripts/code to transform them
Data Management: Handle incoming data, validate it to ensure usability, map it to platform specifications and perform verification on the same
Platform ingestion: Get trained on and then build pipelines to import incoming data onto the platform
Issue analysis and resolution: Own the process end-to-end, identify issues & resolve the same
A Day in the Life
Design and build interfaces to facilitate workflows between client third party systems and the Data Activation Platform
Define and document best practices along with thorough message specifications
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
What You Need
3+ years of experience in a Data Engineer role, Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience with relational SQL and NoSQL databases, including Postgres and MongoDB
Experience with data pipeline and workflow management tools: Azkaban
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Scala etc.
Experience in Healthcare Data Analytics with a focus on the understanding of healthcare data formats (CCDA, HL7, etc.)
Preferred Skills
Healthcare Data experience
Data Analytics and Visualization (PowerBI , Sisense)
Presentation and documentation skills
Value-Based Care knowledge
Pet-friendly office and open floor plan.
No mundane cubicles.
Job Title
Data Engineer
Department
Customer Engineering
Employment Type
Full Time
Dallas, TX
","['mongodb', 'sql', 'scala', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'postgr', 'nosql', 'powerbi']","['healthcar', 'visual', 'statist', 'pipelin']",2,"['mongodb', 'sql', 'scala', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'powerbi', 'healthcar', 'visual', 'statist', 'pipelin']","['day', 'analyt', 'pipelin', 'handl', 'set', 'relat', 'visual', 'infrastructur', 'aw', 'python', 'parti', 'statist', 'comput', 'quantit', 'sourc', 'engin']","['mongodb', 'sql', 'scala', 'ec2', 'aw', 'cloud', 'redshift', 'python', 'nosql', 'powerbi', 'healthcar', 'visual', 'statist', 'pipelin', 'day', 'analyt', 'pipelin', 'handl', 'set', 'relat', 'visual', 'infrastructur', 'aw', 'python', 'parti', 'statist', 'comput', 'quantit', 'sourc', 'engin']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Senior Data Engineer
- You will transform complex analytical models into scalable, production-ready solutions
- You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
- You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment
Responsibilities:
- Ability to grasp new technologies rapidly as needed to progress varied initiatives
- Break down data issues and resolve them
- Build robust systems with an eye on the long term maintenance and support of the application
- Leverage reusable code modules to solve problems across the team and organization
- Utilize a working knowledge of multiple development languages
- A startup mindset with the backing of a top 10 bank
- Monthly Innovation
- Days dedicated to test driving cutting edge technologies
- Flexible work schedules
- Convenient office locations
- Generous salary and merit-based pay incentives
- Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
- Bachelors Degree
- At least 2 years in coding in data management, data warehousing or unstructured data environments
- At least 2 years experience working with leading big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
Preferred Qualifications:
- Master's Degree
- 2+ years experience with Agile engineering practices
- 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
- 2+ years experience with NoSQL implementation (Mongo, Cassandra)
- 2+ years experience developing Java based software solutions
- 2+ years experience developing software solutions to solve complex business problems
- 2+ years experience with Relational Database Systems and SQL
- 2+ years experience designing, developing, and implementing ETL
- 2+ years experience with UNIX/Linux including basic commands and shell scripting
","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'postgr']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"Position Role/Tile: Data Engineer
6+ months
Must Have
5+ years of work experience in a Data Engineer role
Advanced knowledge and experience in relational databases (Teradata preferred), Data Warehousing and ETL/ELT technologies
Proficiency with SQL, Hive-QL, UNIX/LINUX scripting
At least 3+ years of experience in Big Data technologies including Hadoop, Data Bricks, etc.
Hands-on experience with one or more cloud service providers (Azure preferred)
Participated in performance Tuning, Data Analysis, Mapping, Loading & Validation
Should be able to work independently
Experience supporting and working with cross-functional teams in a dynamic environment.
Prior experience in working in SCALED AGILE framework
Strong Analytical skills
Nice to Have
Experience in Spark and Python is a plus
Experience with working with Teradata and Teradata utilities such as TPUMP, BTEQ, TPT
Good familiarity with the Software Development Life Cycle (SDLC) and Data Ingestion
Suite #214
Newark, CA 94560
","['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive']","['data warehousing', 'etl', 'tune', 'big data']",999,"['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'data warehousing', 'etl', 'tune', 'big data']","['analyt', 'spark', 'relat', 'hadoop', 'provid', 'python', 'big', 'etl', 'engin', 'particip']","['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'data warehousing', 'etl', 'tune', 'big data', 'analyt', 'spark', 'relat', 'hadoop', 'provid', 'python', 'big', 'etl', 'engin', 'particip']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data.
This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose.
Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.
Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions.
Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions.
Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve.
This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics.
This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.
The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e.
Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel']","['recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']",2,"['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']","['basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account', 'basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Data Engineer
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team?
You will transform complex analytical models into scalable, production-ready solutions
You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment
Responsibilities:
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages
A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
Bachelors Degree
At least 2 years of experience developing software or data solutions
At least 2 years of experience in Spark
Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL
","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgr']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"About the Role:
The Data Engineer provides data and reporting development services or technical support daily.
Also maintain data or reporting solutions (data warehouse/mart/stores/data lake/analytics) using tools and programming languages.
Develop data set processes, assists with design and identify ways to improve data, efficiency and quality.
You will work independently and oversee your work and work of junior members.
You will also have strong domain knowledge of the relevant industry/s.
Responsibilities include requirement analysis, code, test, debug, document and maintain data and analytics solutions.
You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas.
You Will:
Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning (ERP) systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)
Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
You Have:
An Associate's degree, a Bachelor's degree a plus
Certified Business Intelligence Professional (CBIP) or equivalent certification
At least 4 years of experience in Data Engineering with SQL, Python; experience with relational SQL and NoSQL databases; experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc..)
At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)
Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting; experience with AWS, GCP or Azure
Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs
Equivalent education and/or experience may be substituted for any of the above requirements
Why the Dallas Fed?
These efforts take a team of dedicated individuals doing many different jobs.
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking; annual tuition reimbursement
Professional development programs, training and conferences
And more
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.
In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.
If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant.
","['sql', 'erp', 'linux', 'gcp', 'spark', 'azur', 'unix', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'kafka']","['healthcar', 'data mining', 'analyz', 'visual', 'big data']",1,"['sql', 'erp', 'linux', 'gcp', 'spark', 'azur', 'unix', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'kafka', 'healthcar', 'data mining', 'analyz', 'visual', 'big data']","['analyt', 'program', 'spark', 'azur', 'handl', 'relat', 'visual', 'public', 'hadoop', 'amount', 'aw', 'python', 'warehous', 'big', 'set', 'engin']","['sql', 'erp', 'linux', 'gcp', 'spark', 'azur', 'unix', 'hadoop', 'aw', 'python', 'hive', 'nosql', 'kafka', 'healthcar', 'data mining', 'analyz', 'visual', 'big data', 'analyt', 'program', 'spark', 'azur', 'handl', 'relat', 'visual', 'public', 'hadoop', 'amount', 'aw', 'python', 'warehous', 'big', 'set', 'engin']"
DE," 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Job Type: Full-time
Pay: $45.00 per hour
Schedule:
Monday to Friday
Work authorization:
United States (Preferred)
","['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'kafka']","['statist', 'big data']",2,"['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'kafka', 'statist', 'big data']","['stream', 'spark', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']","['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'kafka', 'statist', 'big data', 'stream', 'spark', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']"
DE,"Key Responsibilities:End to end ownership of ETL data pipelines, from ingestion of data to consumption by business intelligence and advanced analytics teams
Experience in Cloud Data Warehouse Platform Snowflake, Spark processing and AWS foundational services.
Understanding of complete data analytics stack and workflow, from ETL to data platform design to BI and analytics tools.
Strong skills in data integration, data warehouses, and data processing
Design and build an automated, self-service data platform, freeing teams to focus on customer features and analysis.
Evolve existing tools and framework to support new scalability requirements as well new functionality as needed.
Identify and drive new solutions to enhance the development cycle to increase development productivity.
Work with product owners to identify and mature upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Work with team to identify and resolve technical debt to improve the teams throughput.
Skills:
Strong communication skills.
Deep experience designing and implementing highly scalable, distributed application systems.
5+ years experience building data pipelines.
5+ years experience programming in Python
Extensive knowledge in fine tuning SQL, understanding optimizers, and execution plans.
Extensive experience architecting complex data models to handle millions of transactions.
Experience in application design and Implementation using agile practices & TDD.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.
Strong understanding of software development life cycle and release management
Self-motivated, independent, team-player
Phone: 949.534.3939 x 406 | Email: krishna@itminds.net |
: 23172 Plaza Pointe Dr, # 265 | Laguna Hills, CA 92653 |
www.itminds.net
","['sql', 'spark', 'bi', 'snowflak', 'python', 'aw', 'cloud', 'kafka']","['optim', 'etl', 'commun', 'pipelin']",999,"['sql', 'spark', 'powerbi', 'snowflak', 'python', 'aw', 'cloud', 'kafka', 'optim', 'etl', 'commun', 'pipelin']","['analyt', 'spark', 'pipelin', 'infrastructur', 'optim', 'bi', 'python', 'aw', 'warehous', 'etl', 'sourc', 'integr']","['sql', 'spark', 'powerbi', 'snowflak', 'python', 'aw', 'cloud', 'kafka', 'optim', 'etl', 'commun', 'pipelin', 'analyt', 'spark', 'pipelin', 'infrastructur', 'optim', 'bi', 'python', 'aw', 'warehous', 'etl', 'sourc', 'integr']"
DE,"Plano, Texas
Skills : Data Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, Elasticsearch
Experience with the following:
Data Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, ElasticsearchData Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, Elasticsearch
","['python', 'elasticsearch']",['machine learning'],999,"['python', 'elasticsearch', 'machine learning']","['machin', 'learn', 'python', 'scientist', 'texa', 'algorithm']","['python', 'elasticsearch', 'machine learning', 'machin', 'learn', 'python', 'scientist', 'texa', 'algorithm']"
DE,"As a ML Data Engineer, you will build a solid data foundation that powers the entire spectrum from Business Intelligence to Artificial Intelligence.
This role will work closely with the Data Science and AI team and will focus on enablement and acceleration of new and existing workflows.
This role is ideal for someone looking to extend software engineering skills into the field of Machine Learning and Artificial Intelligence.
Job Responsibilities
Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation
Work closely with data scientists and analysts to create and deploy new features
Write efficient and well-organized software to ship products in an iterative, continual-release environment
Monitor and plan out core infrastructure enhancements
Contribute to and promote good software engineering practices across the team
Mentor and educate team members to adopt best practices in writing and maintaining production code
Communicate clearly and effectively to technical and non-technical audiences
Actively contribute to and re-use community best practices
Required Skills
University or advanced degree in engineering, computer science, mathematics, or a related field
Strong experience working with a variety of relational SQL and NoSQL databases
Strong experience working with big data tools: Hadoop, Spark, Kafka, etc.
Experience with at least one cloud provider solution (AWS, GCP, Azure)
Strong experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Ability to work in Linux environment
Experience working with APIs
Strong knowledge of data pipeline and workflow management tools
Expertise in standard software engineering methodology, e.g.
unit testing, code reviews, design documentation
Experience creating ETL processes that prepare data for consumption appropriately
Experience in setting up, maintaining and optimizing databases for production usage in reporting, analysis and ML applications
Working in a collaborative environment and interacting effectively with technical and non-technical team members equally well
Relevant working experience with Docker and Kubernetes preferred
Ability to work with ML frameworks preferred
Additional Key Metrics
Preference will be given to candidates either based in, or willing to relocate to, Dallas/Ft.
Worth or San Francisco area.
Client has ability to transfer H1B’s and sponsor.
Start date is ASAP and compensation is negotiable contingent on experience and qualifications.
Job Type: Full-time
Pay: $150,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental Insurance
Employee Discount
Health Insurance
Paid Time Off
Vision Insurance
Supplemental Pay:
Signing Bonus
Experience:
AWS, Azure or GCP: 1 year (Preferred)
Scala: 1 year (Preferred)
Kafka: 1 year (Preferred)
Java: 1 year (Preferred)
SQL: 3 years (Required)
Python: 1 year (Required)
Kubernetes: 1 year (Preferred)
Docker: 1 year (Preferred)
NoSQL: 1 year (Preferred)
Spark: 1 year (Preferred)
Education:
Bachelor's (Required)
Work authorization:
United States (Required)
Additional Compensation:
Bonuses
Store Discounts
Fully Remote
Innovative -- innovative and risk-taking
Stable -- traditional, stable, strong processes
People-oriented -- supportive and fairness-focused
Schedule:
Monday to Friday
Day shift
","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['pipelin', 'risk', 'machine learning', 'big data', 'etl', 'commun']",1,"['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'risk', 'machine learning', 'big data', 'etl', 'commun']","['day', 'provid', 'python', 'etl', 'ml', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'risk', 'machine learning', 'big data', 'etl', 'commun', 'day', 'provid', 'python', 'etl', 'ml', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"SUMMARYThe SQL Database Engineer will provide will have an enterprise background using MS SQL Server and be able to understand complex SQL design concepts, as well as practices and procedures.
This position involves identifying business requirements, developing data models, performing data analysis, writing advanced SQL queries, designing, and coding complex stored procedures and performance tuning existing database processes.
ESSENTIAL FUNCTIONS AND ACCOUNTABILITIES
• Design and develop database objects, stored procedures, views, functions, tables, triggers and SSIS packages; ensure their stability, reliability, and performance
• Troubleshoots escalated, complex problems; recommends, and reviews the implementation of associated fixes
• Design, develop, and maintain SSRS reports based on user requirements
• Write and optimize SQL statements for data access and retention
• Test databases for performance, fine-tune when necessary
• Collaborate with developers on database design, query tuning, and schema refinement
• Independently analyze, solve, and correct database related issues in real time while providing efficient resolutions
• Respond to feedback from users regarding performance and work with developers to improve the performance of queries and indexes
• Tune stored procedures and T-SQL queries to improve performance and sustainability
• Provide support for the deployment of database scripts in development, test, pre-production and production environments
• Study the technical requirements provided by product team and design databases to fulfill the requirements
• Migrate on-premises SQL databases and related workloads to Azure
• Ensures that the environment is suitable for any proposed business applications, or propose scaling the environment to meet requirements
• Utilize techniques to minimize operational costs
• Ensures that the operational team is actively monitoring the backup environment for all applicable instances
• Ensures that the operational team maintains compliance with established service level agreements
WORK EXPERIENCE
• In-depth knowledge of standard concepts, practices and procedures related to database management.
• 7+ years of experience in software development using Microsoft SQL Server 2008 R2, T-SQL and T-SQL language (queries, views, procedures), SQL Server database design, stored procedure design and implementation.
• Experience coding complex stored procedures (using T-SQL).
• Experience developing SSIS packages
• Experience with migrating on-premises databases to IaaS-based and PaaS-based SQL Databases
• Experience with Azure cloud environment strongly preferred including:
Azure Databricks
Azure Databox
Azure Synapse Analytics/SQL Data Warehouse
Azure Event Hubs
Azure Data Factory
Azure Data Lake Storage
Migrating On-Premises Databases to Azure IaaS and PaaS
• Proven analytical problem solving and debugging skills
• Experience in troubleshooting and resolving database integrity issues, performance issues, blocking and deadlocking issues, etc.
• Experience with performance tuning, query optimization, using Performance Monitor, SQL Profiler and other related monitoring and troubleshooting tools.
• Familiarity with BI technologies (e.g.
Microsoft Power BI/Tableau)
• Excellent communications skills (written and verbal)
Job Requirements:
","['sql', 'azur', 'ssr', 'bi', 'tableau', 'microsoft', 'excel', 'power bi']","['recommend', 'tune', 'optim', 'problem solving', 'commun', 'account']",999,"['sql', 'azur', 'ssr', 'powerbi', 'tableau', 'microsoft', 'excel', 'recommend', 'tune', 'optim', 'problem solving', 'commun', 'account']","['analyt', 'essenti', 'techniqu', 'azur', 'relat', 'power', 'provid', 'optim', 'bi', 'packag', 'warehous', 'integr', 'engin']","['sql', 'azur', 'ssr', 'powerbi', 'tableau', 'microsoft', 'excel', 'recommend', 'tune', 'optim', 'problem solving', 'commun', 'account', 'analyt', 'essenti', 'techniqu', 'azur', 'relat', 'power', 'provid', 'optim', 'bi', 'packag', 'warehous', 'integr', 'engin']"
DE,"JD:
ETL/ELT, Informatica, DBT(Data Build Tool), Snowflake, AWS S3 and Glue
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['aw', 'snowflak', 's3']",['etl'],999,"['aw', 'snowflak', 's3', 'etl']","['basi', 'aw', 'etl', 'evalu']","['aw', 'snowflak', 's3', 'etl', 'basi', 'aw', 'etl', 'evalu']"
DE,"AWS WWRO is hiring for a Sr. Data Engineer to play a key role in building their industry leading Analytics Platform.
Are you passionate about Big Data and highly scalable data platforms?
Do you enjoy building end to end Analytics solutions to help drive business decisions?
The full stack Data Engineer will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for analytics and deep learning.
Implement data ingestion routines both real time and batch using best practices in data modeling, ETL/ELT processes leveraging AWS technologies and Big data tools.
Provide on-line reporting and analysis using business intelligence tools and a logical abstraction layer against large, multi-dimensional datasets and multiple sources.
Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture.
Produce comprehensive, usable dataset documentation and metadata.
Provides input and recommendations on technical issues to the project manager.
Basic Qualifications
· 7+ years of experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
· 4+ years of experience data modeling concepts
· 3+ years of Python and/or Java development experience
· 3+ years experience in Big Data stack environments (EMR, Hadoop, MapReduce, Hive)
· Experience with Kafka, Flume and AWS tool stack such as Redshift and Kinesis are preferred.
· Experience building on AWS using S3, EC2, Redshift, DynamoDB, Lambda, QuickSight, etc.
Preferred Qualifications
·.
Meets/exceeds Amazons leadership principles requirements for this role
.
Meets/exceeds Amazons functional/technical depth and complexity for this role
Experience in gathering requirements and formulating business metrics for reporting.
· Experience with Kafka, Flume and AWS tool stack such as Redshift and Kinesis are preferred.
· Experience building on AWS using S3, EC2, Redshift, DynamoDB, Lambda, QuickSight, etc.
· Experience using software version control tools (Git, Jenkins, Apache Subversion)
· AWS certifications or other related professional technical certifications
· Experience with cloud or on-premise middleware and other enterprise integration technologies
· Experience in writing MapReduce and/or Spark jobs
· Demonstrated strength in architecting data warehouse solutions and integrating technical components
· Excellent communication skills, both written and verbal
Amazon.com is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.
","['mapreduc', 'spark', 'ec2', 'git', 'hadoop', 'aw', 'lambda', 'redshift', 'python', 's3', 'hive', 'java', 'cloud', 'kafka', 'excel']","['recommend', 'data modeling', 'deep learning', 'big data', 'etl', 'commun']",999,"['mapreduc', 'spark', 'ec2', 'git', 'hadoop', 'aw', 'lambda', 'redshift', 'python', 's3', 'hive', 'java', 'cloud', 'kafka', 'excel', 'recommend', 'data modeling', 'deep learning', 'big data', 'etl', 'commun']","['analyt', 'learn', 'spark', 'input', 'line', 'relat', 'hadoop', 'infrastructur', 'provid', 'aw', 'python', 'warehous', 'action', 'big', 'etl', 'sourc', 'engin', 'integr']","['mapreduc', 'spark', 'ec2', 'git', 'hadoop', 'aw', 'lambda', 'redshift', 'python', 's3', 'hive', 'java', 'cloud', 'kafka', 'excel', 'recommend', 'data modeling', 'deep learning', 'big data', 'etl', 'commun', 'analyt', 'learn', 'spark', 'input', 'line', 'relat', 'hadoop', 'infrastructur', 'provid', 'aw', 'python', 'warehous', 'action', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"Position Role/Tile: Data Engineer
6+ months
Must Have
5+ years of work experience in a Data Engineer role
Advanced knowledge and experience in relational databases (Teradata preferred), Data Warehousing and ETL/ELT technologies
Proficiency with SQL, Hive-QL, UNIX/LINUX scripting
At least 3+ years of experience in Big Data technologies including Hadoop, Data Bricks, etc.
Hands-on experience with one or more cloud service providers (Azure preferred)
Participated in performance Tuning, Data Analysis, Mapping, Loading & Validation
Should be able to work independently
Experience supporting and working with cross-functional teams in a dynamic environment.
Prior experience in working in SCALED AGILE framework
Strong Analytical skills
Nice to Have
Experience in Spark and Python is a plus
Experience with working with Teradata and Teradata utilities such as TPUMP, BTEQ, TPT
Good familiarity with the Software Development Life Cycle (SDLC) and Data Ingestion
Suite #214
Newark, CA 94560
","['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive']","['data warehousing', 'etl', 'tune', 'big data']",999,"['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'data warehousing', 'etl', 'tune', 'big data']","['analyt', 'spark', 'relat', 'hadoop', 'provid', 'python', 'big', 'etl', 'engin', 'particip']","['sql', 'linux', 'spark', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'data warehousing', 'etl', 'tune', 'big data', 'analyt', 'spark', 'relat', 'hadoop', 'provid', 'python', 'big', 'etl', 'engin', 'particip']"
DE,"At least 1-2 years of hands on experience in building distributed data processing application leveraging spark with Scalajava or python.
At least 1-2 years of hands on experience in building application on AWS (certified preferred) Experience around building data processing platformsapplications on Cloud (AWS) Good knowledge around data warehousing concept Experience around Devops leveraging Jenkins, Git and automated test-driven development approach Quick adoption to the emerging open source Technologies and perform proof of ConceptPilots to evaluate the tech stacks middot Excellent communication skills.
","['spark', 'git', 'aw', 'cloud', 'python', 'excel']","['data warehousing', 'commun']",999,"['spark', 'git', 'aw', 'cloud', 'python', 'excel', 'data warehousing', 'commun']","['spark', 'aw', 'python', 'sourc']","['spark', 'git', 'aw', 'cloud', 'python', 'excel', 'data warehousing', 'commun', 'spark', 'aw', 'python', 'sourc']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
Key Responsibilities
Involved in the design, prototyping and delivery of software solutions within the big data eco-system
Work independently on big data projects and/or serving as analytics SME to provide new or enhanced data to the business
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, learns, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e.
Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required.
Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
At least 2 years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow is a plus
Working knowledge of Tableau a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka']","['recommend', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']"
DE,"Innovators Wanted!!
",[None],[None],999,[],[None],[None]
DE,"Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered.
",[None],[None],999,[],[None],[None]
DE,"Data Engineer Requirements:
Â8 + years of professional experience
Â 3+ years of experience with Big data technology and analytics
Â
Â
Experience working with traditional warehouse and correlation into hive warehouse on big data technologies
Experience setting data modeling standards in Hive
Â
Â
Â
Â
Â Proficiency in using query languages such as SQL, Hive
Â Understanding of data preparation and manipulation using Datameer tool
Â Knowledge of SOA, IaaS, and Cloud Computing technologies, particularly in the AWS environment
Â Knowledge of setting standards around data dictionary and tagging data assets within DataLake for business consumption.
Â
Â Experience with data visualization tools like Tableau
Â Identifying technical implementation options and issues
Â Partners wand communicates cross-functionally across the enterprise
Â Foster the continuous evolution of best practices within the development team to ensure data standardization and consistency
Â Experience in agile software development paradigm (e.g., Scrum, Kanban)
Â Strong written and verbal communication
Â
","['sql', 'aw', 'tableau', 'cloud', 'hive']","['correl', 'visual', 'data modeling', 'big data', 'commun']",999,"['sql', 'aw', 'tableau', 'cloud', 'hive', 'correl', 'visual', 'data modeling', 'big data', 'commun']","['analyt', 'asset', 'engin', 'visual', 'aw', 'comput', 'warehous', 'big', 'â']","['sql', 'aw', 'tableau', 'cloud', 'hive', 'correl', 'visual', 'data modeling', 'big data', 'commun', 'analyt', 'asset', 'engin', 'visual', 'aw', 'comput', 'warehous', 'big', 'â']"
DE,"Data Engineer Ideal candidates should have experience with Data Ingestion and Consumption.
That is transforming from source raw data, cleansing missing data and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST.
In depth knowledge of preparing large scale data analytics for consumption's.
Key knowledge on HIVE and query optimization in HIVE Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']"
DE,"• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry* Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster* Proven Experience in handling variety of data formats* Experience in building large scale Data Lake Environment* Troubleshooting Hive Performance issues and developing HQL queries* Experience with Spark and PySpark* Experience in implementing CI/CD Process and Job Automation through Autosys* Experience in Hadoop Cluster Administration is a big plus* Experience with integration of data from multiple data sources* Assist Analytics and Data Scientist team and Business Users* Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations* The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow* Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus* Strong situational analysis and decision making abilitiesLI-AG1
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc']"
DE,"*What if there was a different way of looking at money?
This is a great opportunity to join a Fortune 500 and not-for-profit organization that gives you a great sense of purpose!As a full-time Data Engineer you will work closely with the Data Architect as they provide guidance and vision so you can develop, construct and maintain the data architecture for the enterprise data professionals.
You will work with large scale data processing systems that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
Communication is key in this role as you will work with users of all levels across the organization.Job DescriptionJob Duties and Responsibilities:* Oversee and direct efforts to identify information and technology solutions that enable business needs and strategies.
* Lead efforts to analyze IT industry and market trends and determine potential impacts.
* Develop concepts and constructs necessary to create technology-enabled business systems.
* Influence technology direction and provide thought leadership and execution to large complex efforts.
* Utilize breadth of technical understanding and dive deep when necessary.
* Consult on and manage initiatives to ensure alignment across multiple business and IT areas.
* Proactively mitigate risks across multiple assets, information domains, technologies and platforms.
* Provide leadership, mentoring and technical guidance to others to drive initiatives.
* Facilitate communications that involve obtaining cooperation and agreement on issues that may be complex or controversial.
* Utilize negotiation and persuasion to come to agreement and to effectively form partnerships.
* Act as a change agent to continuously improve and move the organization forward.
* Accountable to successfully deliver the right results on initiatives in a timely and effective manner.
* Direct the work of others to lead initiatives that cross multiple assets, technologies, platforms, departments and vendors.
* Ability to work within a diverse team of skillsets and experience levels to deliver results.Required Job Qualifications:* Bachelor's degree or equivalent experience in MIS, Computer Science, Mathematics, Business, or related field.
* 10+ years of experience in Technology related field including 3+ years prior lead experience.
* Expert knowledge of predictive analytics, statistical modeling, advanced mathematics, data integration concepts, business intelligence and data warehousing and implementing large systems* Implement and configure data platforms including but not limited to Hadoop, Spark, Kafka and batch integration is preferred.
","['kafka', 'spark', 'hadoop']","['risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']",1,"['kafka', 'spark', 'hadoop', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']","['analyt', 'asset', 'techniqu', 'spark', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'comput', 'statist', 'integr', 'sourc', 'engin']","['kafka', 'spark', 'hadoop', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account', 'analyt', 'asset', 'techniqu', 'spark', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'comput', 'statist', 'integr', 'sourc', 'engin']"
DE,"Title: Data EngineerLocation: Plano, TX
Duration: 12 Months
Responsibilities for Data Engineer:
Minimum Requirements:
At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc..)
At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 3 years of experience with SQL and Shell Scripting experience
At least 2 years of experience with software design and must have an understanding of cross systems usage and impact
Nice to Have qualifications:
2+ years of experience working with Dimensional Data Model and pipelines in relation with the same
2+ years experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service
2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
Hands on design experience with data pipelines, joining data between structured and unstructured data
Regards,
Karthik (KP)
Resource Development Manager
Direct: 469-533-7270
Cell: 469-717-0141
Email: karthik@infovision.com
","['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'amazon web services']","['pipelin', 'big data']",999,"['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'pipelin', 'big data']","['spark', 'pipelin', 'azur', 'public', 'aw', 'python', 'big', 'relat', 'engin']","['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'pipelin', 'big data', 'spark', 'pipelin', 'azur', 'public', 'aw', 'python', 'big', 'relat', 'engin']"
DE,"About the Role:The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality.
The Engineer's also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry.
Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas.
These efforts take a team of dedicated individuals doing many different jobs.
",[None],[None],2,[],"['analyt', 'set', 'engin']","['analyt', 'set', 'engin']"
DE,"Role: Hadoop Data Engineer
Duration: 3 - 6 + Months
Responsibilities
â Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.
â Support/Enhance data pipelines and ETL using heterogeneous sources
â Transform data using data mapping and data processing capabilities like Spark, Spark SQL, Impala etc.
â Expands and grows data platform capabilities to solve new data problems and challenges
â Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project
Basic Skill Requirements
â 7+ years Enterprise Development
â 2+ Years Design with Big Data Strategies.
Other Skills
â Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, Hive, Impala, Spark,Â
â Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types (Parquet, Avro, ORC) for specific use cases etc.
â Experience with Query languages â SQL, Hive, Impala, Drill etc.
â Solid hands on experience of development/automation in Unix OS
â Experience in agile(scrum) development methodology
â Exposure to Data ingestion methodologies such as Kafka, Sqoop, Storm, Web Scrapping, etc.
â Experience with development/automation skills.
Must be very comfortable with reading and writing Python/Shell script and SQL code
â Experience with Hadoop open source distributions - Cloudera
In addition, all colleagues are eligible for a number of rewards and recognition programs.
The Client retains the discretion to add or change the duties of the position at any time.
","['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka']","['etl', 'pipelin', 'big data']",999,"['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka', 'etl', 'pipelin', 'big data']","['program', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'etl', 'sourc', 'â']","['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka', 'etl', 'pipelin', 'big data', 'program', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'etl', 'sourc', 'â']"
DE,"Overview
You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.
What You'll Be Doing
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
It's time to rethink the possible.
Are you ready?
","['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'mysql', 'kafka', 'excel', 'oracl']","['pipelin', 'etl', 'commun', 'math']",1,"['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'kafka', 'excel', 'oracl', 'pipelin', 'etl', 'commun', 'math']","['analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'etl', 'engin']","['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'kafka', 'excel', 'oracl', 'pipelin', 'etl', 'commun', 'math', 'analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'etl', 'engin']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Senior Data Engineer
- You will transform complex analytical models into scalable, production-ready solutions
- You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
- You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment
Responsibilities:
- Ability to grasp new technologies rapidly as needed to progress varied initiatives
- Break down data issues and resolve them
- Build robust systems with an eye on the long term maintenance and support of the application
- Leverage reusable code modules to solve problems across the team and organization
- Utilize a working knowledge of multiple development languages
- A startup mindset with the backing of a top 10 bank
- Monthly Innovation
- Days dedicated to test driving cutting edge technologies
- Flexible work schedules
- Convenient office locations
- Generous salary and merit-based pay incentives
- Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
- Bachelors Degree
- At least 2 years in coding in data management, data warehousing or unstructured data environments
- At least 2 years experience working with leading big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
Preferred Qualifications:
- Master's Degree
- 2+ years experience with Agile engineering practices
- 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
- 2+ years experience with NoSQL implementation (Mongo, Cassandra)
- 2+ years experience developing Java based software solutions
- 2+ years experience developing software solutions to solve complex business problems
- 2+ years experience with Relational Database Systems and SQL
- 2+ years experience designing, developing, and implementing ETL
- 2+ years experience with UNIX/Linux including basic commands and shell scripting
","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'postgr']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Senior Software/Data Engineer
Are you a Software Engineer that loves working with Data?
Do you thrive in a vibrant, innovative and collaborative team?
You will transform complex analytical models into scalable, production-ready solutions
You will develop applications from ground up using a modern technology stack such as Scala, Spark, PostgreSQL, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment
Responsibilities:
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages
A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)
Basic Qualifications:
Bachelors Degree
At least 3 years of experience developing software or data solutions
Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years of experience in Spark
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL
","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql', 'data warehousing', 'etl', 'big data']","['day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'angular', 'hadoop', 'hive', 'hbase', 'nosql', 'postgresql', 'data warehousing', 'etl', 'big data', 'day', 'analyt', 'spark', 'relat', 'hadoop', 'texa', 'big', 'etl', 'engin']"
DE,"Req ID: 90378
Duties:
Required Skills:
2 years' experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB
2 years of Python
2 years' experience using frameworks/libs like TensorFlow, Keras, or Spacy
2 years' experience using Hadoop, Kafka, or HDFS
2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming
2 years' experience using Spark
Preferred Skills:
Experience with various messaging systems, such as MQTT or RabbitMQ
Experience working in public cloud environments like AWS, or Azure.
Good understanding of Lambda Architecture, along with its advantages and drawbacks
A strong team-oriented mindset
Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered.
Visit nttdataservices.com to learn more.
","['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka']",[None],999,"['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka']","['stream', 'spark', 'azur', 'public', 'hadoop', 'aw', 'python']","['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka', 'stream', 'spark', 'azur', 'public', 'hadoop', 'aw', 'python']"
DE,"You are highly motivated and have a passion for Data Warehousing and the value it brings to an organization.
Plan and execute secure, good practice data integration strategies and approaches Acquire, ingest, and process data from multiple sources and systems into Big Data platforms Involved in end-to-end data management bringing a desire to help move forward in best practices Optimization and maintenance of existing data pipelines Develop data integration mappingsworkflows Data mart development Add calculations, columns and tables to meet front end demand.
Document work according to best practices Collaborate with other teams to map data fields to hypotheses and curate, wrangle, and prepare data for use in advanced analytical models Size and scope development needs for both operational and project initiatives Required Skills 5+ years development experience working with advanced SQL 1+ year experience working with Python scripting 5+ years building operational ETL data pipelines across several sources, and constructing relational and dimensional data models 5+ years Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift 5+ years working with on premise and cloud base Data Warehouse solutions
","['sql', 'azur', 'cloud', 'snowflak', 'python', 'redshift']","['pipelin', 'optim', 'data warehousing', 'big data', 'etl']",999,"['sql', 'azur', 'cloud', 'snowflak', 'python', 'redshift', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl']","['analyt', 'pipelin', 'azur', 'relat', 'etl', 'optim', 'python', 'warehous', 'big', 'integr', 'sourc']","['sql', 'azur', 'cloud', 'snowflak', 'python', 'redshift', 'pipelin', 'optim', 'data warehousing', 'big data', 'etl', 'analyt', 'pipelin', 'azur', 'relat', 'etl', 'optim', 'python', 'warehous', 'big', 'integr', 'sourc']"
DE,"Develops code that reuses objects, is well structured, includes sufficient comments, and is easy to maintain.
Writes programs and reportsElevates code into the development, test, and Production environments on schedule.
Participates in design, code, and test inspections throughout life cycle to identify issues and ensure methodology compliance.
Participates in systems analysis activities, including system requirements analysis and definition, e.g.
prototyping.
",[None],[None],999,[],"['program', 'particip']","['program', 'particip']"
DE,"Title Big Data Engineer Summary The Big Data Engineer provides data and reporting development services andor technical support daily.
Develop data set processes, assist with design and identify ways to improve data, efficiency and quality.
Responsibilities include requirement analysis, code, test, debug, document and maintain data and analytics solutions.
Responsibilities bull Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning ERP systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function) bull Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns Requirements bull 4+ years of experience in Data Engineering with SQL, Python experience with relational SQL and NoSQL databases experience in the following Big Data frameworks File Format (Parquet, AVRO, ORC etc.)
bull 3+ years working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)
bull Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and UnixLinux shell scripting experience with AWS, GCP or Azure bull Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs bull An Associatersquos degree andor Bachelor's degree a plus bull Certified Business Intelligence Professional (CBIP) or equivalent certification bull Equivalent education andor experience may be substituted for any of the above requirements Benefits Competitive Overall Benefit Plan, 401K Match, Pension, flexible work environment, free parking and more!
","['sql', 'erp', 'gcp', 'spark', 'azur', 'kafka', 'hadoop', 'aw', 'erp system', 'python', 'hive', 'nosql', 'unixlinux']","['big data', 'visual', 'data mining', 'analyz']",1,"['sql', 'erp', 'gcp', 'spark', 'azur', 'kafka', 'hadoop', 'aw', 'erp system', 'python', 'hive', 'nosql', 'unixlinux', 'big data', 'visual', 'data mining', 'analyz']","['analyt', 'program', 'spark', 'azur', 'relat', 'visual', 'hadoop', 'amount', 'aw', 'python', 'warehous', 'big', 'set', 'engin']","['sql', 'erp', 'gcp', 'spark', 'azur', 'kafka', 'hadoop', 'aw', 'erp system', 'python', 'hive', 'nosql', 'unixlinux', 'big data', 'visual', 'data mining', 'analyz', 'analyt', 'program', 'spark', 'azur', 'relat', 'visual', 'hadoop', 'amount', 'aw', 'python', 'warehous', 'big', 'set', 'engin']"
DE," 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Job Type: Full-time
Pay: $40.00 per hour
Schedule:
Monday to Friday
Work authorization:
United States (Preferred)
Visa Sponsorship Potentially Available:
Yes: H-1B work authorization
Work Remotely:
Temporarily due to COVID-19
","['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'hive', 'kafka']","['statist', 'big data']",2,"['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'hive', 'kafka', 'statist', 'big data']","['stream', 'spark', 'hadoop', 'aw', 'avail', 'python', 'statist', 'comput', 'big', 'quantit', 'engin']","['pyspark', 'spark', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 's3', 'hive', 'kafka', 'statist', 'big data', 'stream', 'spark', 'hadoop', 'aw', 'avail', 'python', 'statist', 'comput', 'big', 'quantit', 'engin']"
DE,"The Data Engineer will be an integral part of the development team designing strategies for database systems and setting standards for operations, programming and security.
This role comes with 100% of both individual and family health premiums covered.
Looking for a team member with a hunger to constantly improve and suggest innovative solutions.
This person will be a team player open to collaborating and sharing resources with other team members.
Roles
• Design databases and ensure their stability, reliability, and performance
• Design, create, and implement database systems based on end user requirements
• Design and create large relational databases and database tables to store applicant data
• Investigate exceptions with application, refine system performance and functionality
• Prepare documentation for database applications
• Develop database schemas, tables and dictionaries
• Ensure the data quality and integrity in databases
• Fix any issues related to database performance and provide corrective measures
• Create complex functions, scripts, stored procedures and triggers to support application development
• Review and analyze existing SQL queries for performance improvement, optimize code and suggest new queries
• Write queries used by applications and work with application developers to create optimized queries
• Perform data modeling to visualize database structure
• Integrate new systems with existing in-house structure
• Program views, stored procedures, and functions
• Develop, implement and optimize stored procedures and functions using T-SQL
• Review and interpret ongoing business report requirements, build appropriate and useful reporting deliverables and provide scheduled reports to management
• Develop procedures and scripts for data migration
• Perform security assessment for potential risks
• Provide architecture guidance and support to technical leads
• Develop best practices for database design and development activities
• Research new technologies and develop proofs of concept
Requirements
• BS/MS degree in Computer Science, Engineering or a closely related subject
• At least 5 years of experience as a Data Engineer, SQL Developer, or similar role
• At least 5 years of experience working with SQL Server Reporting Services and SQL Server Integration Services
• Excellent understanding of T-SQL programming and Microsoft SQL Server
• Knowledge of HTML and C#, .Net, Visual Studio
• Critical thinking and problem-solving skills
• Team player with strong interpersonal and communication skills
• Effective time-management skills and deadline-oriented
","['sql', 'c', 'excel', 'microsoft']","['research', 'end user', 'visual', 'risk', 'data modeling', 'optim', 'commun']",1,"['sql', 'c', 'excel', 'microsoft', 'research', 'end user', 'visual', 'risk', 'data modeling', 'optim', 'commun']","['program', 'set', 'relat', 'visual', 'optim', 'provid', 'interpret', 'comput', 'integr', 'engin']","['sql', 'c', 'excel', 'microsoft', 'research', 'end user', 'visual', 'risk', 'data modeling', 'optim', 'commun', 'program', 'set', 'relat', 'visual', 'optim', 'provid', 'interpret', 'comput', 'integr', 'engin']"
DE,"Dallas, New York, Pittsburgh
Geography:
North America
Capabilities:
Technology & digital
Industries:
Technology industries
To succeed, organizations must blend digital and human capabilities.
Practice Area Profile
Role Profile
Iterative.
They are excited to prototype at all levels of fidelity—and have the humility to walk away from ideas when they fail.
Collaborative.
They have the ability and enthusiasm to work with researchers, engineers, business consultants, and other designers who will challenge and support one another.
Comfortable with ambiguity.
They know projects and businesses move fast.
That means the path forward isn’t always well-defined.
Interdisciplinary.
They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need.
Working with a diverse set of clients across domains and industries
Implement data orchestration pipelines, data sourcing, cleansing, and augmentation and quality control processes
Deploying machine learning models in production
Leading data architects in designing data architectures
Design flexible and scalable data architectures tailor made for the client
Mentoring data engineers to further their personal and professional growth
Leading other engineering staff on projects
Developing team’s talent by providing direction and facilitating technical architectural discussions
Assisting with business development through writing proposals, scoping projects
Translating business needs into solutions
Designing overall data solution, integration, and enterprise architectures
Your Qualifications
You’ll Bring:
6+ years of experience working on large scale, full lifecycle data implementation projects
BS/BA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience
2+ years professional development experience with some of the AWS/Azure/GCP data stack:
S3
Redshift
AWS glue
EMR
Azure Data Warehouse
Azure Blob Store
Google Big Query
5+ years of experience in a client facing role
Subject matter expert in at least one area related to data management
An RDBMS technology
A Big Data technology
Enterprise Data Management, Governance, Strategy, etc.
A deep knowledge of performant SQL and understanding of relational database technology
Hands-on RDBMS experience (data modeling, analysis, programming, stored procedures)
Expertise in developing ETL/ELT workflows with one or more of the following:
Python
Scala
Java
Deployment of data pipelines in the Cloud in at least AWS, Azure, or GCP
A deep understanding of relational and warehousing database technology, working with at least one of the major databases platforms (Oracle, SQLServer, Teradata, MySQL, Postgres)
Additional consideration to candidates who possess some of the following criteria:
Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto
A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills
Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment
Strong interpersonal and analytical skills
Intellectual curiosity and an ability to execute projects
An understanding of “big picture” business requirements that drive architecture and design decisions
DevOps and DataOps skills including “infrastructure as code” systems like CloudFormation or Terraform
Data system performance tuning
Implementation of predictive analytics and machine learning models (MLlib, scikit-learn, etc)
At times, this role involves significant travel to client sites.
The amount of travel will depend on client needs and nature of projects
What to include in your application:
A link to your portfolio that demonstrates your affinity for data engineering and shows how you approach digital challenges
Date Posted:
14-Jan-2020
All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
Click here for more information on E-Verify.
","['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'mysql', 'scikit', 'postgr', 'oracl']","['research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl']",1,"['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'scikit', 'oracl', 'research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl']","['program', 'predict', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'human', 'appli', 'learn', 'infrastructur', 'amount', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']","['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'scikit', 'oracl', 'research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl', 'program', 'predict', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'human', 'appli', 'learn', 'infrastructur', 'amount', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']"
DE,"PRINCIPAL DATA ENGINEERRESPONSIBILITIESPrincipal Data Engineer will be part of a team of data experts to architect, build and operate cloud scale big data platform to deliver data analytics & AI/ML capability.
In this role you will be working with team of Architects, Data Engineers and Data Experts who are Data ninjas.
You will be working with new generation data streaming technologies like CDC, Kafka, Apache Pulsar , cloud scale data warehouses like AWS Redshift, Azure Synapse, Snowflake, Data lake technologies like ADLF, Lake Formation or Databricks Delta Lake, Data catalogs like Azure data catalog or AWS Glue or Apache Atlas.
The data pipelines and data flow technologies like Apache Nifi, Airflow, Azure Data Factory, AWS Data Pipelines.ESSENTIAL JOB DUTIES* Follow agile and SAFe methodologies, Collaborate with SCRUM team consisting of product and engineering members* Communicate effectively with team members, internal and external customers* Mentor other engineers on database design standards* Work with enterprise DBAs, Architecture, and Engineering to drive a database architecture roadmap* Troubleshoot and solve database problems quickly and efficientlyQUALIFICATIONSEducation* Bachelor's Degree in Computer Science or equivalent experience and knowledgeExperience* 10+ Years Database Engineering, data platform* Engineering and Implementing enterprise level software applicationsSkills & Expertise (Intermediate Level)* 5+ years of Experience working with Data ware housing & ETL development* 5+ years of Experience with a BI tool like PowerBI, Tableau, Spotfire.
* 2+ years of Experience with cloud-based data platformso Azure SQL Managed Instance, Azure SQL DW, AWS RDS, AWS Redshifto Azure Data Lake Storage, AWS Lake Formation, Databricks Delta Lake* Data modeling (ER/Studio, ERWin)* Proficiency in MS SQL development (SSRS, SSIS, SSAS)* Strong MS SQL Server experience with SQL Server 2016/2019 versions.
* Knowledge of programming languages like C#, Python or R.* Understanding and implementation of REST APIs.
* Experience with index design and T-SQL performance tuning techniques.
* Experience in automated deployment and CI/CD pipelines for Database codebase.
* Experience with any NoSQL based solutions will be a Plus* Experience in Build and integrate transactional and data warehousing systems* Experience Asp.net and Net core will be an added PlusOther Abilities* Team player who is detailed oriented* Strong logical & analytical thinking, verbal and written communication skills* Works efficiently across distributed teams* Aptitude for learning new technologies and concepts* Excellent time management skills and ability to prioritize* Manage several activities in parallel* Strong analytical, verbal, and written communication skills* Able to translate customer and business needs to technical solutionsPhysical Demands (Travel, etc.
)* Available to solve critical issues as necessaryPERKS & BENEFITS* Medical, Dental, Vision, 401k* Company-paid Life & Disability Insurance* Paid Vacation, Sick Days & Holidays* Paid Parental Leave* Collaborative & Open Office EnvironmentABOUT DEALERSOCKETDealerSocket is a leading provider of software for the automotive industry.
","['sql', 'airflow', 'azur', 'net', 'ssr', 'bi', 'snowflak', 'python', 'redshift', 'c', 'nosql', 'aw', 'tableau', 'cloud', 'kafka', 'excel', 'powerbi']","['pipelin', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'airflow', 'azur', 'net', 'ssr', 'powerbi', 'snowflak', 'python', 'redshift', 'c', 'nosql', 'aw', 'tableau', 'cloud', 'kafka', 'excel', 'powerbi', 'pipelin', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']","['day', 'analyt', 'techniqu', 'pipelin', 'ml', 'azur', 'provid', 'bi', 'python', 'avail', 'aw', 'warehous', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'airflow', 'azur', 'net', 'ssr', 'powerbi', 'snowflak', 'python', 'redshift', 'c', 'nosql', 'aw', 'tableau', 'cloud', 'kafka', 'excel', 'powerbi', 'pipelin', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'day', 'analyt', 'techniqu', 'pipelin', 'ml', 'azur', 'provid', 'bi', 'python', 'avail', 'aw', 'warehous', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"The Azure Data Engineer will be responsible for the development, deployment, maintenance, diagnostics, and support of spark ETL jobs on the MS Databricks platform.
The ideal candidate will have:
• Experience in development of apache Spark code using PySpark or Scala
• Experience in using Azure Databricks Platform
• Experience in using Azure Data Factory to call Azure Databricks notebook activities
• Experience in using Git based repositories
• Good fundamental knowledge about distributed computing, RDBMS and Dimensional modeling concepts
• Exposure to Azure DevOps
• Exposure to Azure Storage (Blob/Data Lake Store), Azure Catalog, Databricks Delta
It is an extraordinary time to be in business.
Be part of building one of the largest independent technology and business services firms in the world.
No unsolicited agency referrals please.
Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, or any other legally protected status or characteristics.
You will need to reference the requisition number of the position in which you are interested.
Your message will be routed to the appropriate recruiter who will assist you.
Emails for any other reason or those that do not include a requisition number will not be returned.
Your future duties and responsibilities
In parallel, you will build detailed architecture diagrams or Entity Relationship Diagrams for all workflows and processes.
All code and diagrams should be maintained in a code bank or repository.
You will work with cloud architects to establish a pipeline to Azure data stores using either NiFi or DataBricks as well as build templates for engineering processes in the cloud.
Required qualifications to be successful in this role
Skills: SQL, TeraData SQL, scripting, Hive.
Must have experience in NiFi or DataBricks.
#LI-JS2
","['sql', 'pyspark', 'spark', 'azur', 'scala', 'git', 'cloud', 'hive']","['etl', 'pipelin']",2,"['sql', 'pyspark', 'spark', 'azur', 'scala', 'git', 'cloud', 'hive', 'etl', 'pipelin']","['spark', 'pipelin', 'azur', 'comput', 'etl', 'engin']","['sql', 'pyspark', 'spark', 'azur', 'scala', 'git', 'cloud', 'hive', 'etl', 'pipelin', 'spark', 'pipelin', 'azur', 'comput', 'etl', 'engin']"
DE,"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Experience Required - 6+ years
Hadoop/HDFS.
Spark is a must.
Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.
Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.
Position 3: Lead Big Data Engineer
Experience Required - 8-15 years
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus.
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'java', 'hive', 'nosql', 'kafka']",['big data'],999,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'java', 'hive', 'nosql', 'kafka', 'big data']","['stream', 'spark', 'ml', 'hadoop', 'primari', 'aw', 'big', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'java', 'hive', 'nosql', 'kafka', 'big data', 'stream', 'spark', 'ml', 'hadoop', 'primari', 'aw', 'big', 'engin']"
DE,"* Experience in developing, deploying and operating on large scale distributed systems on a commercial scale* Experience working in Cloud-based Big Data Infrastructure eg: Azure, AWS* Good working experience on Cloud, Delta Lake, ETL processing.
* Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.
* Working knowledge on Python and PySpark Programming.
* Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)* Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.
* Work in a highly agile environment* Excellent communication and teamwork skills.
The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
","['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana']","['etl', 'commun', 'big data']",1,"['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana', 'etl', 'commun', 'big data']","['program', 'spark', 'challeng', 'azur', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'etl', 'sourc']","['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana', 'etl', 'commun', 'big data', 'program', 'spark', 'challeng', 'azur', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'etl', 'sourc']"
DE,"mello™, the Greek word for “future,” was the product of a recent $80+ million dollar investment in research & development to transform & streamline the home buying process into a digital experience like no other competitor offers.
Position Summary:
The job duties and requirements are defined for the backend.
The senior role provides technical leadership and mentorship to junior team members.
Responsibilities:
Designs, develops and delivers solutions that meet business line and enterprise requirements.
Creates enterprise-grade application services.
Participates in rapid prototyping and POC development efforts.
Advances overall enterprise technical architecture and implementation best practices.
Assists in efforts to develop and refine functional and non-functional requirements.
Participates in iteration and release planning.
Performs functional and non-functional testing.
Contributes to overall enterprise technical architecture and implementation best practices.
Informs efforts to develop and refine functional and non-functional requirements.
Demonstrates knowledge of, adherence to, monitoring and responsibility for compliance with state and federal regulations and laws as they pertain to this position.
Strong ability to produce high-quality, properly functioning deliverables the first time.
Delivers work product according to established deadlines.
Estimates tasks with a level of granularity and accuracy commensurate with the information provided.
Works collaboratively in a small team.
Excels in a rapid iteration environment with short turnaround times.
Deals positively with high levels of uncertainty, ambiguity, and shifting priorities.
Accepts a wide variety of tasks and pitches in wherever needed.
Constructively presents, discusses and debates alternatives.
Takes shared ownership of the product.
Communicates effectively both verbally and in writing.
Takes direction from team leads and upper management.
Ability to work with little to no supervision while performing duties.
Experience designing enterprise database systems using Microsoft SQL Server preferred.
Experience with advanced queries, stored procedures, views, triggers, etc.
Experience with indexing and normalization.
Experience of performance tuning queries.
Experience of both DDL and DML.
Experience of database administration
Experience in Visual Studio 2013/2015 and SSIS to develop enterprise ETL processes.
Understanding of data mart and data warehousing concepts including variant schemas (Star, Snowflake).
Deep understanding of one or more source/version control systems.
Develops branching and merging strategies.
Working understanding of Web API, REST, JSON.
Working understanding of unit testing creation.
Knowledge of cubes and SSAS is a plus.
Requirements:
Experience in the Mortgage industry preferred.
Bachelor’s Degree preferred, and/or a minimum of four (4) + related work experience.
The Perks:
Competitive compensation reliant on ability & experience.
Excellent benefits package including multiple health, dental & vision options.
The opportunity to work for America’s Lender under the vision of industry legend, Anthony Hsieh.
","['sql', 'snowflak', 'excel', 'microsoft']","['research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl']",1,"['sql', 'snowflak', 'excel', 'microsoft', 'research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl']","['digit', 'relat', 'line', 'visual', 'provid', 'packag', 'releas', 'etl', 'sourc']","['sql', 'snowflak', 'excel', 'microsoft', 'research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl', 'digit', 'relat', 'line', 'visual', 'provid', 'packag', 'releas', 'etl', 'sourc']"
DE,"About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality.
The Engineer's also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry.
Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.
You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas.
You Will:
Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning (ERP) systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)
Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
You Have:
An Associate's degree, a Bachelor's degree a plus
Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
At least seven years of verifiable Data Integration (a.k.a.
Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
At least 5 years of experience with Relational Database Systems, SQL, Database Architecture, Data Warehousing, Modeling and Mining, Data Analysis, Scripting Languages, Oracle
Equivalent education and/or experience may be substituted for any of the above requirements
Why the Dallas Fed?
These efforts take a team of dedicated individuals doing many different jobs.
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking; annual tuition reimbursement
Professional development programs, training and conferences
And more
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.
In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.
If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant.
","['sql', 'erp', 'oracl']","['healthcar', 'data mining', 'data warehousing', 'analyz', 'etl']",1,"['sql', 'erp', 'oracl', 'healthcar', 'data mining', 'data warehousing', 'analyz', 'etl']","['analyt', 'program', 'handl', 'set', 'relat', 'public', 'amount', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['sql', 'erp', 'oracl', 'healthcar', 'data mining', 'data warehousing', 'analyz', 'etl', 'analyt', 'program', 'handl', 'set', 'relat', 'public', 'amount', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"Job Summary:
Support the data architect, data analysts, and mobile development team, and other cross functional teams on their data initiatives.
Essential Functions:
· Create, Improve, and modernize existing SSIS packages
· Working understanding of big data and big data modeling concepts
· Work closely with the Principal Data Architect to Develop, construct, test and maintain various data models for OLTP and OLAP Systems
· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies
· Provide SQL tuning suggestions when necessary
· Build data pipelines from multiple 3rd part sources utilizing cloud based technology
Non-essential Job Functions:
· Contributes to a positive work environment by demonstrating cultural expectations and influencing others to reward performance and value ""can do"" people, accountability, diversity and inclusion, flexibility, continuous improvement, collaboration, creativity and fun.
· Performs other duties as assigned.
Required Knowledge, Skills and Abilities:
· Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
· Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
· Ability to communicate effectively
· Ability to follow tasks assigned in Agile environment
· Strong analytic skills related to working with unstructured datasets.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· Demonstrates high personal integrity and ability to earn trust from others
Required Education/Experience:
· Bachelor’s degree in computer science or a related field or equivalent work experience.
· 7+ years’ experience in Data Engineering or Database Development
· 2+ years’ experience in Cloud Services such as Azure/AWS/Google
Physical Environment:
· This position is primarily an in office position
Normal office environment
•Entrepreneurial branch platforms
•Extensive loan products and programs
•Competitive and flexible pricing
•Fast Underwriting turn times
•Exceptional Rush Team
•Knowledgeable Support Teams
•Employee Stock Ownership Plan (ESOP)
•Cutting-edge marketing resources, including free access to Home Buyers Marketing (HBM)
","['sql', 'aw', 'cloud', 'azur']","['pipelin', 'normal', 'data modeling', 'optim', 'big data']",1,"['sql', 'aw', 'cloud', 'azur', 'pipelin', 'normal', 'data modeling', 'optim', 'big data']","['program', 'provid', 'packag', 'integr', 'sourc', 'analyt', 'essenti', 'azur', 'optim', 'infrastructur', 'aw', 'big', 'set', 'engin', 'pipelin', 'divers', '3rd', 'comput', 'relat']","['sql', 'aw', 'cloud', 'azur', 'pipelin', 'normal', 'data modeling', 'optim', 'big data', 'program', 'provid', 'packag', 'integr', 'sourc', 'analyt', 'essenti', 'azur', 'optim', 'infrastructur', 'aw', 'big', 'set', 'engin', 'pipelin', 'divers', '3rd', 'comput', 'relat']"
DE,"Role: Sr.
Application Data Engineer
Duration: 12+ Months onsite contract
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Job Types: Full-time, Contract
Pay: $70.00 - $75.00 per hour
Experience:
Data Management: 10 years (Required)
Hive: 6 years (Required)
Kafka: 6 years (Required)
Java: 5 years (Required)
C#: 5 years (Required)
SQL: 8 years (Required)
Azure: 6 years (Required)
Python: 5 years (Required)
Work Remotely:
No
","['sql', 'azur', 'scala', 'python', 'hive', 'java', 'c', 'kafka', 'excel']","['pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']",999,"['sql', 'azur', 'scala', 'python', 'hive', 'java', 'c', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']","['analyt', 'pipelin', 'azur', 'avail', 'python', 'engin']","['sql', 'azur', 'scala', 'python', 'hive', 'java', 'c', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun', 'analyt', 'pipelin', 'azur', 'avail', 'python', 'engin']"
DE,"Plano, Texas
Skills : Algorithms ,Data Structures,data science,linux,python
Urgent Hiring Data Scientist/Data Engineer
Details
Industry- Banking
Duration: 12 month contract to hire
Interview times: Thursday 7/23 and Friday 7/24.
Data Engineer – Top Skills
Data Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, Elasticsearch
Required Skills:
Good understanding of object oriented programing principles
Good understanding of Algorithms and Data Structures
Good interpersonal communication skills for tech and business conversations
Good analytical skills to break down requirements, solve complex problems
Algorithms ,Data Structures,data science,linux,python
","['linux', 'python', 'elasticsearch']","['machine learning', 'commun']",999,"['linux', 'python', 'elasticsearch', 'machine learning', 'commun']","['analyt', 'machin', 'program', 'learn', 'python', 'scientist', 'texa', 'engin', 'algorithm']","['linux', 'python', 'elasticsearch', 'machine learning', 'commun', 'analyt', 'machin', 'program', 'learn', 'python', 'scientist', 'texa', 'engin', 'algorithm']"
DE,"Position-Data Engineer
Client: USAA (IBM)
Duration: 04 May 2020 - 27 Nov 2020
Experience in working Data Integration teams on Data Analytics and Data Warehousing engagements.
Minimum of 5 years Data Integration experience working in medium to large sized projects
Strong understanding of data warehousing, data integration, reporting and advanced analytics technologies
Strong communication skills
Experience managing client relationship and expectations
Specific knowledge of Datastage 9.X/11.x and UNIX
Prior Data integration experience with the following IBM software products:
DB2
SQL Server
UNIX scripting
Scheduling tool Control-M
Experience in Unix/Linux/Redhat
Experience leading Technical teams
Experience with ETL tool Datastage
","['sql', 'linux', 'unix']","['data warehousing', 'etl', 'commun']",999,"['sql', 'linux', 'unix', 'data warehousing', 'etl', 'commun']","['analyt', 'integr', 'engin', 'etl']","['sql', 'linux', 'unix', 'data warehousing', 'etl', 'commun', 'analyt', 'integr', 'engin', 'etl']"
DE,"Note Candidate can start remotely and till COVID situation can do remote from home after that candidate need to be onsite on TXhellip.if situationoffice gets open.
Candidates with the following must haves Strong experience in SQL Programming Strong experience in Python Experience Analytical and problem solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig..
Minimum 2+ years of demonstrated technical proficiency with Hadoop and big data projects..
Need Shell Scripting.. JavaJ2EE coding experience is plus not mandatory
","['sql', 'pig', 'hadoop', 'python', 'hive']","['problem solving', 'big data']",999,"['sql', 'pig', 'hadoop', 'python', 'hive', 'problem solving', 'big data']","['analyt', 'program', 'hadoop', 'python', 'appli', 'big']","['sql', 'pig', 'hadoop', 'python', 'hive', 'problem solving', 'big data', 'analyt', 'program', 'hadoop', 'python', 'appli', 'big']"
DE,"As a part of a dynamic team working in the Agile methodology you will have the opportunity to contribute to multiple phases of the solution lifecycle.Essential Functions* Gather requirements, perform data analysis, and profile data to design appropriate solutions* Develop detailed data models and data architecture for enterprise data warehouse projects* Create and maintain processes to load the data lake utilizing DataBricks Notebooks and Python* Create and maintain processes to build Analysis Services DAX cubes from the data lake tables* Design and maintain Power BI and SSRS reports that meet business requirements* Performance tuning to ensure reports meet SLA targets* Troubleshoot issues and create solutions in a timely mannerJob Requirements* Bachelors degree in Computer Science or Information Systems or equivalent experience required* 8-10 years of general technology experience (required)* 5-7 years of Analytics/Business Intelligence solution delivery experience* Power BI visualization, Power Query data integration, and DAX experience required* DataBricks and Python experience required* Understanding of dimensional modeling concepts* Strong T-SQL skills required* SQL Server Analysis Services Tabular model experience required* SQL Server Reporting Service experience required* Azure SQL Data Warehouse or other MPP data warehouse experience preferred* Experience with Microsoft Azure-based solutions preferred* Must have advanced skills with the ETL Tools, BI & Analytics Tools and Cloud software utilized to perform their daily duties, e.g., Azure ADF, Informatica ICS, Python, Analysis Services, Power BI, SSRS,* Demonstrable experience with development lifecycle (development, testing, deployment, source control, etc.
PlainsCapital Bank is a leading commercial bank with locations throughout Texas.
PrimeLending is a national mortgage provider focused on purchase mortgage originations.
HilltopSecurities provides financial advisory, clearing, retail brokerage, and other investment banking services.
To learn more, please visit www.hilltop-holdings.com.
","['sql', 'azur', 'ssr', 'bi', 'cloud', 'python', 'microsoft', 'power bi']","['etl', 'visual']",1,"['sql', 'azur', 'ssr', 'powerbi', 'cloud', 'python', 'microsoft', 'etl', 'visual']","['analyt', 'azur', 'visual', 'power', 'provid', 'bi', 'python', 'comput', 'texa', 'warehous', 'etl', 'sourc', 'integr']","['sql', 'azur', 'ssr', 'powerbi', 'cloud', 'python', 'microsoft', 'etl', 'visual', 'analyt', 'azur', 'visual', 'power', 'provid', 'bi', 'python', 'comput', 'texa', 'warehous', 'etl', 'sourc', 'integr']"
DE,"NEED_USC_&_GC_Candidates_only
Job Title: Data Engineer
Job Duration: 6-9 Months CTH
Qualifications:
5 years of experience in a Data Engineer role - Strong analytic skills
related to working with unstructured datasets
Strong project management and organizational skills - Experience with big
data tools: Hadoop, Spark, Kafka, etc.
Experience building and optimizing - big data- data pipelines, architectures
and data sets
Experience working on, and deploying, large-scale systems in Python or Go,
Scala/Java, or other similar languages
Experience performing root cause analysis on internal and external data and
processes to answer specific business questions and identify opportunities for
improvement
Build processes supporting data transformation, data structures, metadata,
dependency and workload management
Working knowledge of message queuing, stream processing, and highly scalable,
bigdata- data stores
Advanced working SQL knowledge and experience working with relational
databases, query authoring (SQL) as well as working familiarity with a variety
of databases
Experience with data pipeline and workflow management tools
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Thanks and Regards,
Pravin Shejwal
Saicon Consultants, Inc.
x 137 (Work)
(Fax)
URL:
Email:
Show moreShow less
","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka']","['pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'python', 'big', 'set', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'python', 'big', 'set', 'engin']"
DE,"Analyze and understand data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Design and Develop SQL , Hive queries, APIs to extract data from the store
Work closely with data Scientists to ensure the source data is aggregated and cleansed
Work with product managers to understand the business objectives
Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
Work with DevOps to build automated data pipelines
Total Experience Required
4 years <10>
The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
Big Data-Hadoop, NoSQL, Hive, Apache Spark
Python
Java & REST
GIT and Version Control
Desirable Technical Skills
Familiarity with HTTP and invoking web-APIs
Exposure to machine learning engineering
Exposure to NLP and text processing
Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
Experience working in SCRUM methodology
Proven sense of high accountability and self-drive to take on and see through big challenges
Confident, takes ownership, willingness to get the job done
Excellent verbal communications and cross group collaboration skills
Job Type: Contract
Salary: $55.00 per hour
Schedule:
Monday to Friday
Experience:
Hive: 2 years (Required)
Apache Spark: 2 years (Required)
REST: 1 year (Required)
NoSQL: 1 year (Required)
Java: 1 year (Required)
Git: 1 year (Required)
Hadoop: 4 years (Required)
SQL: 1 year (Required)
Python: 1 year (Required)
API: 1 year (Required)
Contract Length:
5 - 6 months
Work Remotely:
Temporarily due to COVID-19
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
Key Responsibilities
Involved in the design, prototyping and delivery of software solutions within the big data eco-system
Work independently on big data projects and/or serving as analytics SME to provide new or enhanced data to the business
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, learns, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e.
Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required.
Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
At least 2 years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow is a plus
Working knowledge of Tableau a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka']","['recommend', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']"
DE,"Job Title AWS/Big Data Engineer
Domain Financial Services
Career Stream Data Science, Client, Python, Spark, H2O.ai
Qualifications
Graduate with excellent communication skills
Candidate should have 3-4 years of experience in Data Science and Client.
Responsibilities Collaborate with team to provide data-driven recommendations and solutions to clients by clearly articulating complex technical concepts
Work with team to quickly understand client needs, develop solutions, and collaboratively present findings to client executives
Analyze and model both structured and unstructured data to generate value
Use industry expertise and data science experience to uncover new opportunities with data
Design and build scalable machine learning models to meet customer requirement
Develop machine learning models to automate business processes and decisions
Productionalize models into a variety of platform architectures
Coordinate with multiple internal and external functional teams to implement models and monitor outcomes
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Help drive business results and participate in the account and department growth
Experience Highly technical with hand-on experience using Apache Spark and python
Thorough understanding of data science concepts and toolsets such as H2O.ai
Knowledge of common data science languages (Python, R etc.)
Proficiency with Apache Spark (via Python, R or Java)
Training machine learning models (H2O.ai)
Hands-on experience with industry-standard predictive modeling solutions such as Spark Client, and H2O.ai in a production setting
Cleaning, blending and visualizing data (Pandas, Spark, Matplotlib)
Cloud infrastructure and services (AWS, Azure)
SQL and NoSQL databases (MySQL, PostgreSQL, Cassandra, MongoDB)
Experience working with big data distributed programming languages, and ecosystems: Spark, Hadoop, MapReduce, Kafka
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'mysql', 'excel', 'spark', 'cloud', 'postgresql']","['recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']",2,"['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']","['basi', 'machin', 'program', 'techniqu', 'stream', 'spark', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc', 'engin', 'evalu']","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account', 'basi', 'machin', 'program', 'techniqu', 'stream', 'spark', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc', 'engin', 'evalu']"
DE,"Job Title: Data Engineer
Type: Contract
Remote
Experience in working Data Integration teams on Data Analytics and Data Warehousing engagements.
Minimum of 5 years Data Integration experience working in medium to large sized projects
Strong understanding of data warehousing, data integration, reporting and advanced analytics technologies
Strong communication skills
Experience managing client relationship and expectations
Specific knowledge of Datastage 9.X/11.x and UNIX
Prior Data integration experience with the following IBM software products:
DB2
SQL Server
UNIX scripting
Scheduling tool Control-M
Experience in Unix/Linux/Redhat
Experience leading Technical teams
Experience with ETL tool Datastage
","['sql', 'linux', 'unix']","['data warehousing', 'etl', 'commun']",999,"['sql', 'linux', 'unix', 'data warehousing', 'etl', 'commun']","['analyt', 'integr', 'engin', 'etl']","['sql', 'linux', 'unix', 'data warehousing', 'etl', 'commun', 'analyt', 'integr', 'engin', 'etl']"
DE,"Level-row"">Level
Experienced
Irving, TX - , TX
Position Type
Full Time
Education Level
Graduate Degree
Salary Range
Undisclosed
Travel Percentage
Negligible
Job Shift
Undisclosed
Job Category
Science
Title: Data Engineer
Position Summary
The successful candidate will design, implement, and maintain data storage and data flow solutions for structured and non-structured multi-model data in support of data science and machine learning pipelines.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Must know and adhere to best practices and possess knowledge of current state of the art data platforms and solutions.
Job Responsibilities
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create data tools for analytics and data scientist team members that assist them in building and optimizing data science products
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Requirements
Masters in Computer Science, Engineering or a related field with exposure to cancer biology
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience with relational SQL and NoSQL databases, including MongoDB, Cassandra, etc.
Strong analytic skills related to working with unstructured datasets
Experience with microservices architecture
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Proficiency in Python, Pandas, PySpark, Dask, Ray, etc.
Experience writing RESTful APIs
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Proficient verbal and written communication skills to explain complex technical details in clear language
Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team
Demonstrate a focus on listening to and understanding user needs and then delighting the customer by exceeding service and quality expectations
#LI-DG1
Qualifications
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""First Name"" minlength=""1"" required=""required"" value="""">First Name*
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""Last Name"" minlength=""1"" required=""required"" value="""">Last Name*
"" class="" input--dark-grey"" data-parsley-type=""email"" aria-label=""Email"" minlength=""1"" required=""required"" value="""">Email*
"" data-parsley-pattern=""/^\d*$/"" data-parsley-pattern-message=""Only numbers are allowed.""
aria-label=""Phone"" minlength=""1"" required=""required"" value="""">Phone*
"" id=""quick-apply-desktop__application-form__resume__input"" accept="".bmp,.accdb,.xls,.xlsx,.gif,.html,.jpe,.jpg,.jpeg,.pdf,.png,.ppt,.pptx,.rtf,.tiff,.txt,.csv,.doc,.docx,.zip"" required=""required"" style=""width: 0; height: 0; opacity: 0; overflow: hidden; position: absolute; z-index: -1; "" data-parsley-errors-container=""#quick-apply-desktop__application-form__resume__errors-container"">
Attach Resume*
Submit
chevron_left
Sign In
Welcome !
It looks like you have an account with this email.
Please enter your password to complete your application.
""
class="" input--dark-grey"" minlength=""8"" required=""required"" value="""">Password*
Sign In
Forgot password?
Thanks!
An email has been sent to you with a password to access your account.
Ready for the next step?
Add your availability!
View More Listings
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""First Name"" minlength=""1"" required=""required"" value="""">First Name*
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""Last Name"" minlength=""1"" required=""required"" value="""">Last Name*
"" class="" input--dark-grey"" data-parsley-type=""email"" aria-label=""Email"" minlength=""1"" required=""required"" value="""">Email*
"" data-parsley-pattern=""/^\d*$/"" data-parsley-pattern-message=""Only numbers are allowed.""
aria-label=""Phone"" minlength=""1"" required=""required"" value="""">Phone*
"" id=""quick-apply-mobile__application-form__resume__input"" accept="".bmp,.accdb,.xls,.xlsx,.gif,.html,.jpe,.jpg,.jpeg,.pdf,.png,.ppt,.pptx,.rtf,.tiff,.txt,.csv,.doc,.docx,.zip"" required=""required"" style=""width: 0; height: 0; opacity: 0; overflow: hidden; position: absolute; z-index: -1; "" data-parsley-errors-container=""#quick-apply-mobile__application-form__resume__errors-container"">
Attach Resume*
Submit
chevron_left
Sign In
Welcome !
It looks like you have an account with this email.
Please enter your password to complete your application.
""
class="" input--dark-grey"" minlength=""8"" required=""required"" value="""">Password*
Sign In
Forgot password?
Thanks!
An email has been sent to you with a password to access your account.
Ready for the next step?
Add your availability!
View More Listings
Terms of Use |
Paycom Privacy Policy |
© 2020 Paycom | All Rights Reserved.
This website uses cookies to customize and improve your experience.
If you are a California resident, you may be entitled to certain rights regarding your personal information.
Accept Cookies
","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka']","['pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account']",2,"['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account']","['python', 'analyt', 'input', 'hadoop', 'optim', 'avail', 'appli', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account', 'python', 'analyt', 'input', 'hadoop', 'optim', 'avail', 'appli', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Job Title: Data Engineer
Duration: 6+ Months Contract
Data Engineer- Big data (Hadoop, map reduce Spark, HIVE) with data warehousing background
Equal Employment Opportunity Commission
The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.
Inception in 2007, privately held, Debt free
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
Awards and Accolades:
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 Fastest 50 by NJBiz
2018 Techserve Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1
Regards,
Kapil Sharma
Account Manager: Client Delivery Services
Corp. HQ'S: 270 Davidson Ave., Suite 704, Somerset, NJ 08873, USA
T: (201) 340.8700 Ext.
440| D: (201) 479 2141| F: (201) 221.8131
Email: Kapil.sharma@net2source.com | Web: www.net2source.com
LinkedIn: http:// www.linkedin.com/in/kapil-sharma-2b1519b2/
","['hive', 'excel', 'spark', 'hadoop']","['data warehousing', 'big data', 'account']",999,"['hive', 'excel', 'spark', 'hadoop', 'data warehousing', 'big data', 'account']","['basi', 'spark', 'hadoop', 'big', 'employe', 'engin']","['hive', 'excel', 'spark', 'hadoop', 'data warehousing', 'big data', 'account', 'basi', 'spark', 'hadoop', 'big', 'employe', 'engin']"
DE,"•
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala,
Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera
Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through
Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data
Scientist team and Business Users
• Exceptional communication skills and the ability to communicate
appropriately at all levels of the organization; this includes written and
verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business
to IT and data constraints to the business; applies equal conveyance
regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization
with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
#LI-AG1
Job Function
TECHNOLOGY
Role
Developer
Job Id
158552
Desired Skills
Big Data
Desired Candidate Profile
Qualifications :
BACHELOR OF ENGINEERING
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster', 'big data']",1,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']"
DE,"Position: Big Data Engineer
Total 4 candidates
 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Mandatory Technology to look for :
Python, Hadoop, Scala, Hive
Job Type: Full-time
Schedule:
Monday to Friday
Experience:
S3, EC2, EMR, RDS, Redshift: 3 years (Required)
Hadoop, Apache Spark, Kafka, Hive: 3 years (Required)
relational SQL, Snowflake and NoSQL databases: 3 years (Required)
Storm, Spark-Streaming: 3 years (Required)
Postgres and Cassandra.
: 3 years (Required)
Data Engineer: 7 years (Required)
Python, PySpark, Scala: 3 years (Required)
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'postgr', 'cloud', 'kafka']","['statist', 'big data']",2,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'cloud', 'kafka', 'statist', 'big data']","['stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'cloud', 'kafka', 'statist', 'big data', 'stream', 'spark', 'relat', 'hadoop', 'aw', 'python', 'comput', 'statist', 'big', 'quantit', 'engin']"
DE,"Position: Data Engineer
Total 4 candidates
Must Have : Python, Hadoop, Scala, Hive
Detailed overview of functional and technical role expectations:
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive ,etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Job Type: Contract
Pay: $45.00 per hour
Schedule:
Monday to Friday
Experience:
AWS: 6 years (Required)
Hive: 6 years (Required)
Scala: 6 years (Required)
Hadoop: 6 years (Required)
Python: 7 years (Required)
Dallas, TX (Required)
Work authorization:
United States (Required)
Work Remotely:
Temporarily due to COVID-19
","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'postgr', 'cloud', 'kafka']",['big data'],999,"['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'cloud', 'kafka', 'big data']","['spark', 'hadoop', 'aw', 'python', 'big', 'relat', 'engin']","['sql', 'pyspark', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 's3', 'nosql', 'hive', 'cloud', 'kafka', 'big data', 'spark', 'hadoop', 'aw', 'python', 'big', 'relat', 'engin']"
DE,"You will be responsible for working closely with Data Architect's and Big Data Engineers and their vision to help design, build and provide governance of large-scale streaming architecture solution that delivers business value across the organization.
You will have the opportunity to both lead and execute on projects and always consider the bigger picture proactively with anticipating any performance issues, troubleshooting, monitoring, quality, etc This role will require experience with hybrid platforms, cloud migration, publishing, development with newer technologies such as Spring Boot, KSQL/Stream Processing, data connectors such as Kafka, performance tuning, data quality and data visualization knowledge.
* Revenue generated* Budget responsibilities* Leads the delivery, support and maintenance of solutions with one or more business and technology areas.
* Organizational impact results from mid-large sized projectsRequired Job Qualifications* Bachelor's degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field* 5+ years of experience in Technology related field including prior lead experience* Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools* Strong organizational, analytical, critical thinking and leadership skills* Demonstrated leadership on mid-large-scale project impacting strategic partnersThrivent provides Equal Employment Opportunity (EEO) without regard to race, religion, color, sex, gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state, or federal law.
",['kafka'],"['big data', 'visual', 'statist', 'predict']",1,"['kafka', 'big data', 'visual', 'statist', 'predict']","['analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']","['kafka', 'big data', 'visual', 'statist', 'predict', 'analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']"
DE,"Should have working experience in Python and Spark(Pyspark)
Experience in AWS preferably in Lambda, S3, EMR and EC2 is must.
Experience in SQL and should have data warehousing concepts.
Experience in building data pipeline and framework development using above technology is preferred.
","['sql', 'pyspark', 'spark', 'ec2', 'aw', 'lambda', 'python', 's3']","['data warehousing', 'pipelin']",999,"['sql', 'pyspark', 'spark', 'ec2', 'aw', 'lambda', 'python', 's3', 'data warehousing', 'pipelin']","['aw', 'python', 'spark', 'pipelin']","['sql', 'pyspark', 'spark', 'ec2', 'aw', 'lambda', 'python', 's3', 'data warehousing', 'pipelin', 'aw', 'python', 'spark', 'pipelin']"
DE,"Senior Data Engineer
TX-Dallas
Full-time
Regular
Exempt
Experienced
Day Job
M-F, 8 am to 5 pm, with flexibility
Tier II - Credit Check
About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality.
Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.
You Will:
You Have:
An Associate's degree, a Bachelor's degree a plus
Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
Why the Dallas Fed?
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking; annual tuition reimbursement
Professional development programs, training and conferences
And more...
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
",[None],"['etl', 'healthcar']",1,"['etl', 'healthcar']","['day', 'analyt', 'program', 'relat', 'credit', 'public', 'etl', 'sourc', 'engin', 'integr']","['etl', 'healthcar', 'day', 'analyt', 'program', 'relat', 'credit', 'public', 'etl', 'sourc', 'engin', 'integr']"
DE,"Position Summary:
Bi-Lingual (Korean).
Under limited supervision, has the ability of coordinating and managing service engineering product requirements.
In addition, this individual will coordinate any additional technical service issue with the HQ Engineering Team to ensure a successful product launch while meeting all deliverables.
Responsibilities also include the establishment of effective working relationships with SEA Product Management Teams, Sales, and field testing personnel in order to drive product development and the product schedule successes.
Interface with major customer and end users to ensure customer satisfaction.
Analyze return rates and trends and plan future requirements.
Develop test and repair strategies for new products.
Auditing and training of out-sourced repair product to third party vendors.
Essential Duties And Responsibilities include the following: Other duties may be assigned.
In this position, either directly or through others, the incumbent will:
• Develops product inspection, tests, diagnostics, and repair procedures.
Identifies and initiates improvements in product testing techniques, tools and repair processes.
Provides product test and repair standards for new products.
Works closely with HQ engineering to resolve technical issues and to develop product test and repair modifications.
• Designs and implements production floor plans to maximize production output.
Establishes and maintains test equipment within calibration to ensure quality test and repair performance within specification standards
• Ability to determine necessary resources, and project timelines while communicating project status to all levels of management.
• Gathers repair service data, analyzes repair data for repair rates and trends.
Prepares and presents statistical reports and information.
Projects and plans future requirements, budgets and resources.
• Responsible for the establishment of new processes and improve existing processes to improve overall reverse logistics performance.
• Provides guidance, direction and decision control of RB-MES systems and application enhancements for the repair department.
Ensures and recommends enhancements that improve operation efficiency, provide availability and integrity of critical repair service information and product performance/failure data.
Manages systems and information projects against plans and performance expectations.
Background/Experience to qualify for this position, the following minimal background and skill levels are required:
• Bachelors Degree in Engineering, from an accredited college or university, or equivalent work experience in the telecommunications industry.
• Should have a minimum of 3-5 years experience in a project development with 1-3 years experience in the wireless telecommunications position requiring engineering direct project development experience and long-term customer contact and relationship building.
• Experience researching and resolving customer technical complaints and issues.
• Previous experience with Mobile or Network Field.
• Comprehensive use of Microsoft Office applications operating in a Windows XP or NT LAN/WAN environment.
Necessary Skills/Attributes for this position the following skills and abilities must be demonstrated at a proficient level:
• Plan, organize and prioritize multiple complex assignments and projects.
• Read and interpret detailed and complex customer requirements.
• Demonstrated competency in both oral and written communication modes for both internal and external personnel at various levels, especially in logistical or financial areas of clients, prospects, and SEA.
• Work independently and in a team environment, in order to achieve personal and team goals and complete assignments within established time frames.
• Easily takes direction and follows up as needed to ensure project completion.
• Use of MS Office (Word, Excel, PowerPoint and Access)
• Bi-Lingual Korean Speaking in order to communicate effectively with HQ on a regular basis.
Physical/Mental Demands and Working Conditions: The position requires the ability to perform the essential duties and responsibilities in the following environment:
• Operate a computer keyboard and view a video display terminal more than 25% of work time.
• Lift, move, or adjust general office equipment, boxes of marketing presentation or media materials using proper materials handling equipment and procedures.
• Physically make product marketing presentations or demonstrations to customers, internal and external groups using verbal and graphics communication modes.
• Travel to customer locations, trade shows, etc.
requires maximum 30% of the time.
• Occasionally work additional hours beyond normal schedule
","['microsoft', 'bi', 'ms office', 'powerpoint', 'excel']","['research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun']",1,"['microsoft', 'powerbi', 'powerpoint', 'excel', 'research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun']","['basi', 'essenti', 'techniqu', 'provid', 'bi', 'avail', 'parti', 'statist', 'comput', 'integr', 'sourc', 'engin']","['microsoft', 'powerbi', 'powerpoint', 'excel', 'research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun', 'basi', 'essenti', 'techniqu', 'provid', 'bi', 'avail', 'parti', 'statist', 'comput', 'integr', 'sourc', 'engin']"
DE,"Long term contract with possible conversion to full time
Primary responsibility is to advance the PHM analysis capability Fleet Analytics Search Tool by implementing solutions utilizing systems engineering principles to design and implement the data life-cycle to; collect, ingest, process, store, persist, access, and deliver data at scale and at speed to enable team members to efficiently conduct analyses and mature PHM performance.
The engineer will clearly document and define PHM data available for analysis and maintain standards and best practices to ensure the architecture can efficiently support required data growth.
The applicant will be responsible for designing, coding, testing and integrating engineering algorithms in Python.
The engineer will develop and support efficient methods of sustaining engineering tools and processes to grow data available for analysis.
Additional tasks include development of visualizations, applications, and job aids to enable the efficient review of isolated events of interest contained within large data sets.
Position may be remote/virtual
Skills & Tools:
Senior Level skills with the following Languages or Tools: SQL & Python
Database systems (SQL/NO SQL)
Demonstrated experience with modern enterprise data architectures (i.e.
distributed data architecture) and the associated data toolsets (i.e.
Hadoop)
Distributed Data Systems (e.g.
Hadoop, HBase, Cassandra, Spark)
Expertise in Data modeling
Demonstrated experience with ETL process and data validation tasks
Comfortable manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources
Expertise in Extraction, Transformation and Load tools
Nice to have: Experience with the following: Scripting, Java, Ruby, C++, Perl, SAS, Matlab, Hive, Pig, SPSS, R, XML, Impala, Flume, Oracle, SQL Server
Education:
Bachelor's degree in Computer Science, Electrical Engineering, Mechanical Engineering, Systems Engineering, or related field.
","['sql', 'spark', 'perl', 'cassandra', 'pig', 'sa', 'spss', 'hadoop', 'python', 'hive', 'java', 'hbase', 'r', 'oracl', 'matlab', 'rubi']","['etl', 'visual', 'data modeling']",1,"['sql', 'spark', 'perl', 'cassandra', 'pig', 'sa', 'spss', 'hadoop', 'python', 'hive', 'java', 'hbase', 'r', 'oracl', 'matlab', 'rubi', 'etl', 'visual', 'data modeling']","['analyt', 'spark', 'set', 'relat', 'sa', 'visual', 'hadoop', 'primari', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin', 'algorithm']","['sql', 'spark', 'perl', 'cassandra', 'pig', 'sa', 'spss', 'hadoop', 'python', 'hive', 'java', 'hbase', 'r', 'oracl', 'matlab', 'rubi', 'etl', 'visual', 'data modeling', 'analyt', 'spark', 'set', 'relat', 'sa', 'visual', 'hadoop', 'primari', 'python', 'avail', 'comput', 'etl', 'sourc', 'engin', 'algorithm']"
DE,"bull You will work closely with business partners and technology leaders to evaluate new BI tools that can further enhance the business' ability to analyze data bull You should be able to commercialize data products and influence leadership on the value of these products QUALIFICATIONS bull Advanced degree in Engineering, Computer Science or related disciplines bull Hands-on experience with the Hadoop or Spark big data platform and one or more mainstream programming languages (Python, Java, C, Scala, C++, etc.)
bull Experience in building data products from ideation to implementation bull At least 2 years experience in a visualization tool such as Tableau or QlikView bull At least 3 years experience dealing with data (structured or unstructured) bull Ability to explain data analysis to drive business ideas bull Strong project management skills bull Experience working in a start-up business or a new business line within a larger organization is preferred
","['spark', 'scala', 'hadoop', 'bi', 'tableau', 'java', 'python', 'c']","['visual', 'big data']",2,"['spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'java', 'python', 'c', 'visual', 'big data']","['program', 'spark', 'line', 'visual', 'hadoop', 'bi', 'python', 'comput', 'big', 'relat', 'engin']","['spark', 'scala', 'hadoop', 'powerbi', 'tableau', 'java', 'python', 'c', 'visual', 'big data', 'program', 'spark', 'line', 'visual', 'hadoop', 'bi', 'python', 'comput', 'big', 'relat', 'engin']"
DE,"Sr. Data Engineer Irving, TX Ideal candidates should have experience with Data Ingestion and Consumptions.
That is transforming from source raw data, cleansing missing data, and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of the Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST.
Note that this requirement is 4 technologies stack in ONE PROJECT, not in multiple projects In-depth knowledge of preparing large scale data analytics for consumptions.
Candidate MUST HAVE EXPERIENCE with Large Fortune 500 Data Analytics Key knowledge on HIVE and query optimization in HIVE The candidate must have done data transformation, cleansing, matching and standardization as part of the ingestion Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']"
DE,"Dallas, New York, Pittsburgh
Geography:
North America
Capabilities:
Technology & digital
Industries:
Technology industries
To succeed, organizations must blend digital and human capabilities.
Practice Area Profile
Role Profile
Iterative.
They are excited to prototype at all levels of fidelity—and have the humility to walk away from ideas when they fail.
Collaborative.
They have the ability and enthusiasm to work with researchers, engineers, business consultants, and other designers who will challenge and support one another.
Comfortable with ambiguity.
They know projects and businesses move fast.
That means the path forward isn’t always well-defined.
Interdisciplinary.
They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need.
Working with a diverse set of clients across domains and industries
Implementing data orchestration pipelines, data sourcing, cleansing, and augmentation and quality control processes
Deploying machine learning models in production
Supporting data architects in designing data architectures
Assisting in mentoring data engineers to further their personal and professional growth
Supporting project management operations of a project
Translating business needs into solutions
Contributing to overall solution, integration, and enterprise architecture
Your Qualifications
You’ll Bring:
2+ years of experience working on large scale, full lifecycle data implementation projects
BS/BA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience
2+ years professional development experience with some of the AWS/Azure/GCP data stack:
S3
Redshift
AWS glue
EMR
Azure Data Warehouse
Azure Blob Store
Google Big Query
A deep knowledge of performant SQL and understanding of relational database technology
Hands-on RDBMS experience (data modeling, analysis, programming, stored procedures)
Expertise in developing ETL/ELT workflows with one or more of the following:
Python
Scala
Java
Deployment of data pipelines in the Cloud in at least AWS, Azure, or GCP
A deep understanding of relational and warehousing database technology, working with at least one of the major databases platforms (Oracle, SQLServer, Teradata, MySQL, Postgres)
Additional consideration to candidates who possess some of the following criteria:
Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto
A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills
Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment
Strong interpersonal and analytical skills
Intellectual curiosity and an ability to execute projects
An understanding of “big picture” business requirements that drive architecture and design decisions
DevOps and DataOps skills including “infrastructure as code” systems like CloudFormation or Terraform
Data system performance tuning
Implementation of predictive analytics and machine learning models (MLlib, scikit-learn, etc)
At times, this role involves significant travel to client sites.
The amount of travel will depend on client needs and nature of projects
What to include in your application:
A link to your portfolio that demonstrates your affinity for data engineering and shows how you approach digital challenges
Date Posted:
14-Jan-2020
All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
Click here for more information on E-Verify.
","['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'mysql', 'scikit', 'postgr', 'oracl']","['research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl']",1,"['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'scikit', 'oracl', 'research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl']","['program', 'predict', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'human', 'appli', 'learn', 'infrastructur', 'amount', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']","['sql', 'gcp', 'spark', 'azur', 'scala', 'mllib', 'aw', 'cloud', 'python', 's3', 'redshift', 'hive', 'java', 'scikit', 'oracl', 'research', 'pipelin', 'cleans', 'predict', 'machine learning', 'data modeling', 'big data', 'etl', 'program', 'predict', 'python', 'etl', 'integr', 'algorithm', 'analyt', 'challeng', 'azur', 'human', 'appli', 'learn', 'infrastructur', 'amount', 'aw', 'warehous', 'set', 'big', 'engin', 'digit', 'machin', 'spark', 'pipelin', 'divers', 'comput', 'relat']"
DE,"JD: Python Lambda, building API integration on AWS platform (with Python as the language) and hands-on developer.
Disclaimer: The above statements are not intended to be a complete statement of job content, rather act as a guide to the essential functions performed by the employee assigned to this classification.
Management retains the discretion to add or change the duties of the position at any time
","['aw', 'lambda', 'python']",['classif'],999,"['aw', 'lambda', 'python', 'classif']","['essenti', 'aw', 'employe', 'python', 'integr']","['aw', 'lambda', 'python', 'classif', 'essenti', 'aw', 'employe', 'python', 'integr']"
DE,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale
Experience working in Cloud-based Big Data Infrastructure eg: Azure, AWS
Good working experience on Cloud, Delta Lake, ETL processing.
Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.
Working knowledge on Python and PySpark Programming.
Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)
Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.
Work in a highly agile environment
Excellent communication and teamwork skills.
Knowledge on Data Governance & Security Principles
Bachelor's degree in Computer Science or closely related
Benefits
The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
","['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana']","['etl', 'commun', 'big data']",1,"['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana', 'etl', 'commun', 'big data']","['program', 'spark', 'challeng', 'azur', 'relat', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'etl', 'sourc']","['sql', 'pyspark', 'spark', 'azur', 'pig', 'excel', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'kafka', 'hana', 'etl', 'commun', 'big data', 'program', 'spark', 'challeng', 'azur', 'relat', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'etl', 'sourc']"
DE,"Summary:
The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization s data assets.
Job Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
5-7 years of experience required.
Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
**Please submit candidates with the following must haves***
Analytical and problem solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
5-8 years of SQL, Hive, Hadoop and Python, Shell (4-5 years)
Java/J2EE development knowledge
3+ years of demonstrated technical proficiency with Hadoop and big data projects
Day to Day duties:
Take requirements from Business Analyst and Stakeholders Analyze and see where changes are needed; perform impact analysis (if pipeline available); make sure proper connectivity to source & right credentials to connect to source; create pipelines (if not available)
Data collection gather information and required data fields.
Pull data from Oracle database (from different databases) put into Hive tables
Data manipulation Join data from multiple data sources and build ETLs to be sent to Tableau (front-end dashboard) for reporting purpose
Measure & Improve - Implement success indicators to continuously measure and improve, while providing relevant insight and reporting to leadership and teams.
Must be able to optimize performance tuning/monitoring and development at the same time.
(A lot of times candidates screened have been really good in SQL but not in python/shell/Hadoop or vice versa.)
All backend is in Hadoop/AWS.
The other 30% is Python, shell script, oozie, spark, etc.
Hive/SQL
Hadoop
Python/Shell Scripting (exchanging data between UNIX and other sources into Hadoop.
Interview:
Ashok is first interview - ZOOM (video isn't necessary) - live coding/logic
Gopi/Ran - second interview - ZOOM VIDEO
Koga & Kayla - third interview - ZOOM VIDEO
Onsite/Virtual with Koga/whole Big Data team
Interviews focus on conceptual questions around python, shell, hadoop, hive.
After that will be coding questions in SQL.
After that, it's coding in Python and Shell scripting.
Team: 2 Data Engineer, 1 BI Engineer (front-end development), 5 Biz Analyst, 1 PM, 1 Director
Comments for Suppliers: Please DO NOT submit candidates interviewed on #569-1 and #1765-1 (same position/HM).
","['sql', 'spark', 'pig', 'unix', 'hadoop', 'bi', 'aw', 'python', 'hive', 'java', 'tableau', 'microsoft', 'excel', 'oracl']","['big data', 'pipelin', 'dashboard', 'predict', 'problem solving', 'analyz', 'etl', 'commun']",1,"['sql', 'spark', 'pig', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'hive', 'java', 'tableau', 'microsoft', 'excel', 'oracl', 'big data', 'pipelin', 'dashboard', 'predict', 'problem solving', 'analyz', 'etl', 'commun']","['day', 'predict', 'python', 'etl', 'sourc', 'collect', 'algorithm', 'analyt', 'hadoop', 'avail', 'appli', 'asset', 'bi', 'aw', 'big', 'engin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'spark', 'pig', 'unix', 'hadoop', 'powerbi', 'aw', 'python', 'hive', 'java', 'tableau', 'microsoft', 'excel', 'oracl', 'big data', 'pipelin', 'dashboard', 'predict', 'problem solving', 'analyz', 'etl', 'commun', 'day', 'predict', 'python', 'etl', 'sourc', 'collect', 'algorithm', 'analyt', 'hadoop', 'avail', 'appli', 'asset', 'bi', 'aw', 'big', 'engin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"mello, the Greek word for ""future,"" was the product of a recent $80+ million dollar investment in research & development to transform & streamline the home buying process into a digital experience like no other competitor offers.
The job duties and requirements are defined for the backend.
The senior role provides technical leadership and mentorship to junior team members.
* Creates enterprise-grade application services.
* Participates in rapid prototyping and POC development efforts.
* Advances overall enterprise technical architecture and implementation best practices.
* Assists in efforts to develop and refine functional and non-functional requirements.
* Participates in iteration and release planning.
* Performs functional and non-functional testing.
* Contributes to overall enterprise technical architecture and implementation best practices.
* Informs efforts to develop and refine functional and non-functional requirements.
* Demonstrates knowledge of, adherence to, monitoring and responsibility for compliance with state and federal regulations and laws as they pertain to this position.
* Strong ability to produce high-quality, properly functioning deliverables the first time.
* Delivers work product according to established deadlines.
* Estimates tasks with a level of granularity and accuracy commensurate with the information provided.
* Works collaboratively in a small team.
* Excels in a rapid iteration environment with short turnaround times.
* Deals positively with high levels of uncertainty, ambiguity, and shifting priorities.
* Accepts a wide variety of tasks and pitches in wherever needed.
* Constructively presents, discusses and debates alternatives.
* Takes shared ownership of the product.
* Communicates effectively both verbally and in writing.
* Takes direction from team leads and upper management.
* Ability to work with little to no supervision while performing duties.
* Experience designing enterprise database systems using Microsoft SQL Server preferred.
* Experience with advanced queries, stored procedures, views, triggers, etc.
* Experience with indexing and normalization.
* Experience of performance tuning queries.
* Experience of both DDL and DML.
* Experience of database administration* Experience in Visual Studio 2013/2015 and SSIS to develop enterprise ETL processes.
* Understanding of data mart and data warehousing concepts including variant schemas (Star, Snowflake).
* Deep understanding of one or more source/version control systems.
Develops branching and merging strategies.
* Working understanding of Web API, REST, JSON.
* Working understanding of unit testing creation.
* Knowledge of cubes and SSAS is a plus.Requirements:* Experience in the Mortgage industry preferred.
* Bachelor's Degree preferred, and/or a minimum of four (4) + related work experience.The Perks:* Competitive compensation reliant on ability & experience.
* Excellent benefits package including multiple health, dental & vision options.
","['sql', 'snowflak', 'excel', 'microsoft']","['research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl']",1,"['sql', 'snowflak', 'excel', 'microsoft', 'research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl']","['digit', 'relat', 'visual', 'provid', 'packag', 'releas', 'etl', 'sourc']","['sql', 'snowflak', 'excel', 'microsoft', 'research', 'normal', 'visual', 'data warehousing', 'supervis', 'etl', 'digit', 'relat', 'visual', 'provid', 'packag', 'releas', 'etl', 'sourc']"
DE,"and rural communities in 25 states.
residential customers over its FiOS and Vantage fiber-optic and copper
Secure digital protection solutions.
Big_Data_Engineer_-_Job_Description
etc.
You will be architecting a ""big data"" platform to store, process these
data to be used by data scientists and machine learning engineers.
You will
have the opportunity to work with a small team of data scientists and machine
learning engineers to build products and services to improve the state of the
You will design, develop, test, and maintain big data infrastructure in
the cloud and on-prem locations.
You will develop ETL pipelines to collect data from various sources,
transform and store them, and enable stakeholders to consume it.
You will be developing pipelines to support machine learning application
development processes.
You will be working with different parts of a large organization to
locate, understand, and extract data from a diverse variety of systems
and transform them into a big data platform.
Monitoring data performance and modifying infrastructure as
Define data retention policies
Qualifications
Minimum:
o Computer Science degree or relevant experience.
o 5+ years of industry experience in Data Engineering
o Strong experience with MapReduce development for large datasets
(Hadoop, HDFS, YARN).
o Strong background in Linux
o 5+ years of experience in python
o Industry experience in developing ETL pipelines to manage large
datasets.
o Working knowledge of machine learning development process
Preferred:
o Master s degree in computer science
o Experience in terabyte scale data manipulation.
o Experience in ingestion tools such as sqoop, and flume is a plus.
o Experience in Kafka and airflow is a plus.
o Processing framework such as Spark and Hive is a plus
o Experience in Apache HBase is a plus
o AWS big data certification is a plus
Employer.
All qualified applicants will receive consideration for employment
without regard to race, color, religion, sex, sexual orientation, gender
identity, national origin, or protected veteran status and will not be
discriminated against on the basis of disability.
","['mapreduc', 'linux', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'hbase', 'kafka']","['pipelin', 'machine learning', 'big data', 'etl', 'commun']",2,"['mapreduc', 'linux', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'hbase', 'kafka', 'pipelin', 'machine learning', 'big data', 'etl', 'commun']","['basi', 'digit', 'machin', 'learn', 'spark', 'pipelin', 'divers', 'hadoop', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin']","['mapreduc', 'linux', 'spark', 'airflow', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'hbase', 'kafka', 'pipelin', 'machine learning', 'big data', 'etl', 'commun', 'basi', 'digit', 'machin', 'learn', 'spark', 'pipelin', 'divers', 'hadoop', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'big', 'etl', 'sourc', 'engin']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
The Big Data Engineer - Senior Consultant II is accountable for leading a technical team of developers on the Enterprise Data Quality team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data.
This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose.
Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.
Key Responsibilities
Serve as a subject matter expert and an advocate of the data quality strategic vision and enterprise data strategy
Design and drive the implementation of end-to-end data quality solutions for multiple data ecosystems that leverage non-relational data
Consistently contribute to process improvements and help lead the team to deliver working solutions and data quality standards
Participate and review documentation for self-service data quality practices, the process framework, and internal guides
Collaborate with various business and technical teams to gather requirements around data quality rules and propose the optimization of these rules if applicable, then design and develop these rules in Informatica Data Quality, Informatica Data Engineering Quality (DEQ), or other home-grown technologies
Perform thorough data profiling with multiple usage patterns, root cause analysis and data cleansing and develop scorecards utilizing Informatica, Excel and other data quality tools
Configure and manage connection process
Develop ""matching"" plans, help determine best matching algorithm, configure identity matching and analyze duplicates
Build workflows scorecards in Informatica Data Engineering Quality (DEQ) to support data remediation
Run data quality jobs (address standardization and validation, email cleanups, name cleanup, parsing, etc.)
utilizing DEQ and other ETL tools
Serve as the primary resource to team members and data stewards for training, problem resolution, data profiling etc.
Analyze and provide data metrics to target personas to provide insight into data quality and help prioritize remediation efforts
Identify and resolve issues, bugs, and impediments
Job Qualifications
Education and Experience
Bachelors Degree in Math, Statistics, Finance, Business, Economics, IT or related field, or equivalent experience
5-7+ years of data and/or analytics experience
Required Skills
Experience with various data types (i.e.
relational, unstructured, semi-structured, hierarchical, linked graph data, streaming, biometrics)
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations
Job Qualifications
Nice-to-Have
Experience working within agile methodologies
Working knowledge of cloud infrastructure (AWS/Azure/Google)
Intermediate technical knowledge related to various data platforms
Administration experience on Hadoop, HDFS, YARN, Spark, Sentry/Ranger, HBase and Zookeeper
Experience using Informatica Data Quality, Informatica Data Engineering Quality, Informatica Analyst
Experience in configuring & troubleshooting the components in the Hadoop ecosystem Spark, Solr, Scala, Kafka etc.
Ability to communicate and present advanced technical topics to general audiences
Ability to code in languages such as Python, Perl, Java, C, R, SQL, XSLT
Understanding of analytics techniques such as predictive modeling, machine learning, neural networks
Certificates, Licenses, Registrations
Preferred: Certified Data Management Professional Certification
Preferred: Informatica Data Quality or Data Engineering Quality (or other Informatica tool) Certification
Preferred: non-relational data store certification
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel']","['recommend', 'big data', 'graph', 'cleans', 'analyz', 'neural network', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account']",1,"['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel', 'recommend', 'big data', 'graph', 'cleans', 'analyz', 'nn', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account']","['basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'avail', 'statist', 'public', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'particip', 'machin', 'spark', 'relat']","['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel', 'recommend', 'big data', 'graph', 'cleans', 'analyz', 'nn', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account', 'basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'avail', 'statist', 'public', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'particip', 'machin', 'spark', 'relat']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Data Engineering -Team Lead
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team?
What youll do:
Grasp / master new technologies rapidly as needed to progress varied initiatives
Break down complex data issues and resolve them
Build robust systems with an eye on automation and the long-term maintenance and support of the application
Work directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment
Who you are:
Someone with a strong sense of engineering craftsmanship, and takes pride in designing work products (code, etc.)
A believer that good development includes good testing, documentation, and collaboration
A good communicator with strong reasoning skills, who can make a case for technology choices
Curious.
Ask questions and desire to understand the Why to drive the How or What of a solution
Self-driven, actively looking for ways to contribute, and know how to get things done
Basic Qualifications:
Bachelors Degree
At least 4 years of Data Engineering experience with Big Data Technologies: Apache Spark, Hadoop, or Kafka
At least 3 years of microservices development experience: Python, Java or Scala
At least 2 years of experience building data pipelines, CICD pipelines, and fit for purpose data stores
At least 1 year of experience in Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform
Preferred Qualifications:
Masters Degree
6+ years of experience with Data Architecture and Design
4+ years of experience working with AWS: S3, EMR, or EC2
4+ years of experience working with data consumption patterns in SQL or Python applications
3+ years of experience working with automated build and continuous integration systems
2+ years of experience with Linux systems
2+ years of experience with NOSQL Graph Databases
","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker']","['commun', 'graph', 'pipelin', 'big data']",1,"['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data']","['spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data', 'spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']"
DE,"You will work with cloud architects to establish a pipeline to Azure data stores using either NiFi or DataBricks as well as build templates for engineering processes in the cloud.
**Required qualifications to be successful in this role:** Skills: SQL, TeraData SQL, scripting, Hive.
Must have experience in NiFi or DataBricks.
** It is an extraordinary time to be in business.
No unsolicited agency referrals please.
You will need to reference the requisition number of the position in which you are interested.
Your message will be routed to the appropriate recruiter who will assist you.
Emails for any other reason or those that do not include a requisition number will not be returned** .
","['sql', 'cloud', 'hive', 'azur']",['pipelin'],999,"['sql', 'cloud', 'hive', 'azur', 'pipelin']","['engin', 'pipelin', 'azur']","['sql', 'cloud', 'hive', 'azur', 'pipelin', 'engin', 'pipelin', 'azur']"
DE,"You will be asked to not only write significant portions of new code bases but maintain and integrate that code into existing code bases as well.
Expect your abilities and scope of work to expand in a fast paced and highly innovative environment.
Be curious and willing to learn.
As a Data Engineer, you will work with product owners to evaluate both high-level requirements, and user stories.
3+ years of experience in a data warehousing, data engineering, or data architect role
Experience with full life cycle development, architecture and implementation, of an enterprise data warehouse.
Experience working with AWS data technologies such as S3, Redshift Spectrum, Athena, Data Pipeline, Glue, EMR, RDS, and Kinesis
Expert level SQL skills
Data modeling experience for both transactional and data warehousing environments.
Experience working with a variety of data sources such as MySQL, Oracle, SQL Server, PostgreSQL, S3, HDFS, and MongoDB
Strong interpersonal skills and problem-solving ability
Build new and extend existing data structures to support real-time analytics for Orion services
Maintain large, multi-terabyte data warehouse which includes performance tuning and data retention/purge processes
Research and troubleshoot data quality issues, providing fixes and proposing both short- and long-term solutions
Prepare designs for database systems and recommend improvements for performance.
Maintain and develop various database scripts and tools to facilitate automation process
Provide support to all data warehouse initiatives.
Experience with infrastructure as code to support systems developed
Experience with Python and Linux shell scripting
Experience using source control systems (Git, Perforce, SVN, etc.)
CCPA Applicant Privacy Notice
","['mongodb', 'sql', 'linux', 'git', 'aw', 'redshift', 's3', 'python', 'postgresql', 'mysql', 'oracl']","['recommend', 'research', 'pipelin', 'tune', 'data modeling', 'data warehousing']",999,"['mongodb', 'sql', 'linux', 'git', 'aw', 'redshift', 's3', 'python', 'postgresql', 'oracl', 'recommend', 'research', 'pipelin', 'tune', 'data modeling', 'data warehousing']","['analyt', 'pipelin', 'infrastructur', 'provid', 'aw', 'python', 'warehous', 'integr', 'sourc', 'engin']","['mongodb', 'sql', 'linux', 'git', 'aw', 'redshift', 's3', 'python', 'postgresql', 'oracl', 'recommend', 'research', 'pipelin', 'tune', 'data modeling', 'data warehousing', 'analyt', 'pipelin', 'infrastructur', 'provid', 'aw', 'python', 'warehous', 'integr', 'sourc', 'engin']"
DE,"Sr. Big Data Engineer with 7+ years of programming experience with 1 or more programming language such as: Java (preferred), Python or Scala is workable
Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity
Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
Job Type: Contract
Schedule:
Monday to Friday
Experience:
Programming: 7 years (Required)
Big Data: 5 years (Required)
snowflake: 1 year (Preferred)
Plano, TX 75024 (Preferred)
Contract Length:
1 year
Work Remotely:
Temporarily due to COVID-19
","['scala', 'java', 'python']","['commun', 'big data']",999,"['scala', 'java', 'python', 'commun', 'big data']","['program', 'divers', 'python', 'big', 'engin']","['scala', 'java', 'python', 'commun', 'big data', 'program', 'divers', 'python', 'big', 'engin']"
DE,"All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
",[None],[None],999,[],"['basi', 'evalu']","['basi', 'evalu']"
DE,"Work will also be used by other cross functional groups such as Finance, Supply Chain, Marketing, etc.
* Enable analytic capabilities within team to inform business decisions.
* Will be required to support team by creating dynamic visualizations in Tableau and analytic apps in Alteryx.
* Work with IT to develop data governance framework.
* Role may also include work from conducting minor to advanced data analysis, wire-framing and developing dashboards and visualizations* Communicate insights and recommendations to business.
* Participating in helping to build a Data Driven CultureQualifications:* BA Degree in Business Administration, Computer Science, Accounting or Finance, or equivalent work experience* 2-3 years of experience in similar role* Experience with Tableau and Alteryx* Strong Computer Science fundamentals in Algorithms and Data Structures* Strong experience in system analysts, data modeling, data architecture, data warehousing tools MPP/Cloud Data Warehouses, such as Snowflake* Excellent verbal and written communication skills* Excellent organizational skillsScope Data:* Work with other Technical resources including and not limited to Project Managers, Development Managers, QA, Tech Services, Business Analysts, the User Community, and others in requirements gathering endeavors and problem solving.
* Judgment/Decision Making - Ability to think logically and practically before making decisions.
Use of independent thought, originality, and reasoning.
","['cloud', 'snowflak', 'excel', 'tableau']","['recommend', 'dashboard', 'visual', 'data modeling', 'data warehousing', 'problem solving', 'financ', 'commun', 'account']",1,"['cloud', 'snowflak', 'excel', 'tableau', 'recommend', 'dashboard', 'visual', 'data modeling', 'data warehousing', 'problem solving', 'financ', 'commun', 'account']","['analyt', 'visual', 'judgment', 'comput', 'warehous', 'algorithm']","['cloud', 'snowflak', 'excel', 'tableau', 'recommend', 'dashboard', 'visual', 'data modeling', 'data warehousing', 'problem solving', 'financ', 'commun', 'account', 'analyt', 'visual', 'judgment', 'comput', 'warehous', 'algorithm']"
DE,"The Applications Development Senior Programmer Analyst is an intermediate level position responsible for participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team.
The overall objective of this role is to contribute to applications systems analysis and programming activities.
Responsibilities: Conduct tasks related to feasibility studies, time and cost estimates, IT planning, risk technology, applications development, model development, and establish and implement new or revised applications systems and programs to meet specific business needs or user areas Monitor and control all phases of development process and analysis, design, construction, testing, and implementation as well as provide user and operational support on applications to business users Utilize in-depth specialty knowledge of applications development to analyze complex problems/issues, provide evaluation of business process, system process, and industry standards, and make evaluative judgement Recommend and develop security measures in post implementation analysis of business usage to ensure successful system design and functionality Consult with users/clients and other technology groups on issues, recommend advanced programming solutions, and install and assist customer exposure systems Ensure essential procedures are followed and help define operating standards and processes Serve as advisor or coach to new or lower level analysts Has the ability to operate with a limited level of direct supervision.
Can exercise independence of judgement and autonomy.
Acts as SME to senior stakeholders and /or other team members.
Other job-related duties may be assigned as required.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
To view the ""EEO is the Law"" poster CLICK HERE .
To view the EEO is the Law Supplement CLICK HERE .
To view the EEO Policy Statement CLICK HERE .
To view the Pay Transparency Posting CLICK HERE .
",[None],"['recommend', 'supervis', 'risk']",2,"['recommend', 'supervis', 'risk']","['essenti', 'program', 'provid', 'relat', 'particip', 'evalu']","['recommend', 'supervis', 'risk', 'essenti', 'program', 'provid', 'relat', 'particip', 'evalu']"
DE,"Job Title Big Data EngineerArchitectLocation Dallas, TXDuration Long Term Contract Requirements Bachelorrsquos degree or equivalent in computer scienceengineering, computer information systems or information technology.
At least 5 yearsrsquo experience in Big DataHadoop based Infrastructure.
At least 6 yearsrsquo experience in Developing and Designing Software Applications.
Experience with multiple SQL flavors, MapReducePigHive, Sparkscala, Python, Kafka, HBasePhoenix, Druid, Airflow.
Developing petabyte-scale data-processing workflows with complicated business requirements covering various search, visualization, analytics, and reporting functions Certified Big Data Architect.
Expertise and experience in software architecture, development and on-prem application design and migration.
Key Responsibilities Subject matter expert for data processing and serving systems using legacy and state-ofthe-art frameworks, systems, and languages including but not limited to multiple SQL flavors, MapreducePigHive, Sparkscala, Python, Kafka, HBasePhoenix, Druid, Airflow Production support of two data-processing Hadoop based infrastructures that power web applications used by security groups.
Developing petabyte-scale data-processing workflows with complicated business requirements covering various search, visualization, analytics, and reporting functions Subject matter expert for the few dozen data sources that the System collects Ensuring data integrity for processed data sources All development related to data processing job orchestration New development, including bug fixes, feature enhancements, hotfixes Automation of builds and deployments Researching and prototyping new technologies and features.
Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, disability, military status, national origin or any other characteristic protected under federal, state, or applicable local law.
","['sql', 'airflow', 'hadoop', 'python', 'kafka']","['information technology', 'visual', 'big data']",2,"['sql', 'airflow', 'hadoop', 'python', 'kafka', 'information technology', 'visual', 'big data']","['analyt', 'relat', 'visual', 'power', 'hadoop', 'infrastructur', 'python', 'comput', 'big', 'integr', 'sourc']","['sql', 'airflow', 'hadoop', 'python', 'kafka', 'information technology', 'visual', 'big data', 'analyt', 'relat', 'visual', 'power', 'hadoop', 'infrastructur', 'python', 'comput', 'big', 'integr', 'sourc']"
DE,"Minimum 12 years of IT experience in building data lake in Hadoop ecosystem.
Hands on experience in design and development of ingestion framework, Data quality , Audit Balancing and Egress framework using Hive/Spark .
Good working knowledge in Hdfs, Hive, Spark, Sqoop, Unix technology
Experience on Hive/Spark and data pipeline creation with Unix knowledge.
Writing highly standard HQL and identifying performance bottleneck.
Hands on with loading and manipulating large data sets using Hive & SparkSQL.
Knowledge on performance optimization, debugging and troubleshooting Hadoop jobs.
Preparing unit test cases and ensure defect free code delivery.
Prior project experience on Agile methodology and DevOps technology.
Good understanding of project management tools such as JIRA, GitHub etc.
Good knowledge on Health Insure Domain( Specifically PAYER Industry is added advantage).
Very good communication and customer interaction skills.
Experience in onsite/offshore delivery model.
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19
","['spark', 'unix', 'jira', 'hadoop', 'hive', 'github']","['optim', 'commun', 'pipelin']",999,"['spark', 'unix', 'jira', 'hadoop', 'hive', 'github', 'optim', 'commun', 'pipelin']","['spark', 'pipelin', 'hadoop', 'optim', 'set']","['spark', 'unix', 'jira', 'hadoop', 'hive', 'github', 'optim', 'commun', 'pipelin', 'spark', 'pipelin', 'hadoop', 'optim', 'set']"
DE,"Job Title: Big Data Engineer
Duration: Full Time
Technical & Functional Skills :
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
Proven Experience in handling variety of data formats
Experience in building large scale Data Lake Environment
Troubleshooting Hive Performance issues and developing HQL queries
Experience with Spark and PySpark
Experience in implementing CI/CD Process and Job Automation through Autosys
Experience in Hadoop Cluster Administration is a big plus
Experience with integration of data from multiple data sources
Assist Analytics and Data Scientist team and Business Users
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster', 'big data']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']"
DE,"Required Education:4 Year Degree
The position will be responsible to:
Qualifications for Senior Data Engineer
Education:
A Bachelor's Degree from an accredited university or college in science, technology, engineering, mathematics, or a related, quantitative field.
Experience:
NOTES: Telecommuting is allowed.
Additional Salary Information: Competitive benefits
Create a Job Alert for Similar Jobs Connections working at Silicon Valley Clean EnergyMore Jobs from This EmployerMore Jobs Like This
Traffic Engineer Technician Overland Park, Kansas
Senior Construction Materials Testing Technician El Paso, Texas
Wood Yesterday
BACK TO TOP
",[None],['clean'],1,['clean'],"['quantit', 'engin', 'relat', 'texa']","['clean', 'quantit', 'engin', 'relat', 'texa']"
DE,"They are team player who is passionate about building team culture.
They enjoy a fast-paced environment, solving business problems with the use of data.
The data engineer will be comfortable applying their expertise across many use cases in various industries.
QUALIFICATIONS:
Candidate with 3+ years of experience in a Data Engineer role
Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience building and optimizing data pipelines and data architecture
Experience with wrangling, exploring, and analyzing data to answer specific business questions and identify opportunities for improvement
Strong project management and organizational skills
Consulting experience is preferred
The ideal candidate will have working knowledge of the following:
Big data tools (e.g.
Hadoop, Spark, Kafka, etc.)
Relational SQL and NoSQL databases (e.g.
Postgres, MySQL, SQL Server, Cassandra, MongoDB, etc.)
Data pipeline and workflow management tools (e.g.
Azkaban, Oozie, Luigi, Airflow, etc.)
Stream-processing systems (e.g.
Storm, Spark-Streaming, etc.)
Scripting languages (e.g.
Python, Java, C++, Scala, etc.)
Container Orchestration (e.g.
Kubernetes, Docker, etc.)
Experience with one or more of the following cloud service providers:
AWS cloud services
Google Cloud Platform
Azure cloud services
Travel: Up to 25%
LEARN MORE:
More information is available at www.credera.com.
This position is an exempt position.
U.S.
Equal Opportunity Employment Information (Completion is voluntary)
Any information that you do provide will be recorded and maintained in a confidential file.
","['sql', 'python', 'java', 'nosql', 'kafka', 'azur', 'hadoop', 'postgr', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'mysql', 'spark', 'airflow', 'cloud']","['statist', 'pipelin', 'big data']",2,"['sql', 'python', 'java', 'nosql', 'kafka', 'azur', 'hadoop', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'spark', 'airflow', 'cloud', 'statist', 'pipelin', 'big data']","['stream', 'learn', 'spark', 'azur', 'pipelin', 'relat', 'hadoop', 'provid', 'aw', 'python', 'avail', 'statist', 'comput', 'big', 'quantit', 'engin']","['sql', 'python', 'java', 'nosql', 'kafka', 'azur', 'hadoop', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'spark', 'airflow', 'cloud', 'statist', 'pipelin', 'big data', 'stream', 'learn', 'spark', 'azur', 'pipelin', 'relat', 'hadoop', 'provid', 'aw', 'python', 'avail', 'statist', 'comput', 'big', 'quantit', 'engin']"
DE,"Minimum 3+ years of experience with programming languages like Java, Scala, PythonHands on development experience in Hadoop Spark and Kafka, Experience with cloud solutions like AWS, Azure Experience with NoSql and traditional databases like Cassandra, MongoDB, MSSQL Experience with enterprise application integration and with approaches in one of the leading tool suites (e.g.
Kibana, Solr, ElasticSearch, R, Python)
","['mongodb', 'spark', 'azur', 'cassandra', 'scala', 'elasticsearch', 'solr', 'hadoop', 'aw', 'cloud', 'java', 'python', 'r', 'nosql', 'kafka']",[None],999,"['mongodb', 'spark', 'azur', 'cassandra', 'scala', 'elasticsearch', 'solr', 'hadoop', 'aw', 'cloud', 'java', 'python', 'r', 'nosql', 'kafka']","['spark', 'azur', 'hadoop', 'aw', 'python', 'integr']","['mongodb', 'spark', 'azur', 'cassandra', 'scala', 'elasticsearch', 'solr', 'hadoop', 'aw', 'cloud', 'java', 'python', 'r', 'nosql', 'kafka', 'spark', 'azur', 'hadoop', 'aw', 'python', 'integr']"
DE,"bull Analyze and understand data sources APIs bull Design and Develop methods to connect collect data from different data sources bull Design and Develop methods to filtercleanse the data bull Design and Develop SQL , Hive queries, APIs to extract data from the store bull Work closely with data Scientists to ensure the source data is aggregated and cleansed bull Work with product managers to understand the business objectives bull Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows bull Work with DevOps to build automated data pipelines Total Experience Required bull 4 years 10 of relevant experience bull The candidate should have performed client facing roles and possess excellent communication skills Business Domain knowledge Finance banking systems, Fraud, Payments Required Technical Skills bull Big Data-Hadoop, NoSQL, Hive, Apache Spark bull Python bull Java REST bull GIT and Version Control Desirable Technical Skills bull Familiarity with HTTP and invoking web-APIs bull Exposure to machine learning engineering bull Exposure to NLP and text processing bull Experience with pipelines, job scheduling and workflow management
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun']","['machin', 'engin', 'spark', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'machin', 'engin', 'spark', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Hi,
Â
Please find the below details
Â
Azure Data Engineer
Â
Irving,TX
Â
Need 10+ years of experience
Â
Need experience with ADF V2,Azure Devops and CI/CD
Â
Need Azure Data Catalog,Azure event hub
Â
Need Azure Synapse & Snowflake
Â
Experience with Scrum/Safe and Azure Storage
Â
","['snowflak', 'azur']",[None],999,"['snowflak', 'azur']","['engin', 'â', 'azur']","['snowflak', 'azur', 'engin', 'â', 'azur']"
DE,"Innovators Wanted!!
",[None],[None],999,[],[None],[None]
DE,"8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus)
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Job Type: Contract
Pay: $80.00 per hour
Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19
","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel']","['pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']",999,"['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']","['analyt', 'pipelin', 'azur', 'avail', 'python', 'warehous', 'engin']","['sql', 'azur', 'scala', 'cloud', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun', 'analyt', 'pipelin', 'azur', 'avail', 'python', 'warehous', 'engin']"
DE,"Client- City Bank
Location- Irving, TX
FullTime Role
• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data Scientist team and Business Users
• Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster']","['basi', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'evalu']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'basi', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'evalu']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Lead Data Engineer
Responsibilities:
- Design robust systems with an eye on the long term maintenance and support of the application
- Leverage reusable code modules to solve problems across the team and organization
- Exhibits mastery over multiple domains
- Handle multiple functions / roles for the projects / Agile teams
- Has an intrinsic knowledge of standards and languages
- Leverage and establish standards across the team and organization
- Provide technical guidance to team members
- Develop a framework of a significant complexity
Basic Qualifications:
- Bachelors Degree
- At least 5 years coding in data management, data warehousing or in unstructured data environments
- At least 2 years experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
Preferred Qualifications:
- Master's Degree
- 2+ years of experience with Agile engineering practices
- 5+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
- 5+ years experience with NoSQL implementation (Mongo, Cassandra)
- 5+ years experience developing Java based software solutions
- 5+ years experience developing software solutions to solve complex business problems
- 5+ years experience with Relational Database Systems and SQL
- 5+ years experience designing, developing, and implementing ETL
- 5+ years experience with UNIX/Linux including basic commands and shell scripting
- 5+ years of experience providing technical leadership on relevant applications
","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'pig', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql']","['data warehousing', 'etl', 'big data']",1,"['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'pig', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data']","['spark', 'handl', 'relat', 'hadoop', 'provid', 'texa', 'big', 'etl', 'engin']","['mongodb', 'sql', 'linux', 'mapreduc', 'spark', 'cassandra', 'pig', 'unix', 'hadoop', 'java', 'hive', 'hbase', 'nosql', 'data warehousing', 'etl', 'big data', 'spark', 'handl', 'relat', 'hadoop', 'provid', 'texa', 'big', 'etl', 'engin']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Manager, Data Engineering
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team?
What youll do:
Lead and develop other up and coming engineers
Grasp / master new technologies rapidly as needed to progress varied initiatives
Break down complex data issues and resolve them
Build robust systems with an eye on automation and the long-term maintenance and support of the application
Work directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment
Who you are:
Someone with a strong sense of engineering craftsmanship, and takes pride in designing work products (code, etc.)
A believer that good development includes good testing, documentation, and collaboration
A good communicator with strong reasoning skills, who can make a case for technology choices
Curious.
Ask questions and desire to understand the Why to drive the How or What of a solution
Self-driven, actively looking for ways to contribute, and know how to get things done
A leader that enjoys mentoring others and helping to guide the organization to continuously improve
Basic Qualifications:
Bachelors Degree
At least 2 years of experience leading a development team and managing other engineers
At least 4 years of Data Engineering experience with Big Data Technologies: Apache Spark, Hadoop, or Kafka
At least 3 years of microservices development experience: Python, Java or Scala
At least 2 years of experience building data pipelines, CICD pipelines, and fit for purpose data stores
At least 1 year of experience in Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform
Preferred Qualifications:
Masters Degree
6+ years of experience with Data Architecture and Design
4+ years of experience working with AWS: S3, EMR, or EC2
4+ years of experience working with data consumption patterns in SQL or Python applications
3+ years of experience working with automated build and continuous integration systems
2+ years of experience with Linux systems
2+ years of experience with NOSQL Graph Databases
","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker']","['commun', 'graph', 'pipelin', 'big data']",1,"['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data']","['spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data', 'spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']"
DE,"Hi ,Hope you are doing great,
Role : Data Engineer
• Strong experience in chatbots and natural language processing (NLP).
• Good understanding of training classical Machine learning algorithms along with an understanding of choosing the right evaluation metric.
• Ability to use pretrained models and fine tune them if required.
• Experience with REST APIs and other web services.
• Perform keyword and topic extraction from chat logs.
• Solid knowledge of training and tuning topic modelling algorithms like LDA and NMF.
• Strong written communication skills.
• Ability to learn the latest technologies.
• Good problem-solving ability.
Nice to have skills:
• Experience in working with any AI/NLP platform (DialogFlow/ Alexa/ Converse.ai/ Amazon Lex etc.)
for building chatbots Experience with any one of the technology (JavaScript, Node.js or Python).
• Understanding of conversational UI, voiced based processing (text to speech, speech to text) and voice apps built on Amazon Alexa or Google Home is a plus.
• Experience in Test Driven Development & Agile methodologies.
• Hands on experience of using frameworks like nltk and spacy.
• Experience with configuring, support and integrating various software systems, API’s, etc.
Thanks & Regards
shobana@infowaygroup.com
Cell : (315) 974 0117
Powered by JazzHR
kCU6bP1zPh
","['javascript', 'python', 'spaci', 'nltk']","['natural language processing', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']",999,"['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']","['machin', 'learn', 'power', 'evalu', 'python', 'engin', 'algorithm']","['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun', 'machin', 'learn', 'power', 'evalu', 'python', 'engin', 'algorithm']"
DE,"Hadoop Data EngineerWe invite you to join the GIS team at Client as a Hadoop Programmer/Developer (Hadoop Data Engineer).
In this role, you will be helping to build new data pipelines, identifying existing data gaps, and providing automated solutions to deliver advanced analytical capabilities and enriched data to applications that are supporting the operations team.
You will also be responsible for obtaining data from the System of Record and establishing real-time data feed to provide analysis in an automated fashion.
Desired Skills:
• Experience working with Hadoop/Big Data related field
• Working experience on tools like Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce, etc.
• Hands on programming experience in perhaps Java, Scala, Python, or Shell Scripting, to name a few
• Experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines
• Strong experience with SQL and Data modelling
• Experience working in Agile development process and deep understanding of various phases of the Software Development Life Cycle
• Experience using Source Code and Version Control systems like SVN, Git, etc.
• Deep understanding of the Hadoop ecosystem and strong conceptual knowledge in Hadoop architecture components
• Self-starter who works with minimal supervision and the ability to work in a team of diverse skill sets
• Ability to comprehend customer requests and provide the correct solution
• Strong analytical mind to help take on complicated problems
• Desire to resolve issues and dive into potential issues
Required Skills:
• Hadoop, SPARK, Scala, HIVE, Impala, Sqoop, MapReduce, Kafka
• MySQL, Linux
• SVN
• PHP programming is a plus
• Top 3 Job Qualifications:
• Data Engineering
• ETL experience
• Agile experience
Job Requirements:
","['sql', 'mapreduc', 'linux', 'spark', 'scala', 'git', 'hadoop', 'python', 'hive', 'java', 'php', 'hbase', 'mysql', 'kafka']","['pipelin', 'gi', 'big data', 'supervis', 'etl']",999,"['sql', 'mapreduc', 'linux', 'spark', 'scala', 'git', 'hadoop', 'python', 'hive', 'java', 'php', 'hbase', 'kafka', 'pipelin', 'gi', 'big data', 'supervis', 'etl']","['analyt', 'program', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'python', 'big', 'etl', 'sourc', 'engin']","['sql', 'mapreduc', 'linux', 'spark', 'scala', 'git', 'hadoop', 'python', 'hive', 'java', 'php', 'hbase', 'kafka', 'pipelin', 'gi', 'big data', 'supervis', 'etl', 'analyt', 'program', 'spark', 'pipelin', 'relat', 'divers', 'hadoop', 'python', 'big', 'etl', 'sourc', 'engin']"
DE,"Job Title AWS/Big Data Engineer
Domain Financial Services
Career Stream Data Science, ML, Python, Spark, H2O.ai
Qualifications
Graduate with excellent communication skills
Candidate should have 3-4 years of experience in Data Science and ML.
Responsibilities
Collaborate with team to provide data-driven recommendations and solutions to clients by clearly articulating complex technical concepts
Work with team to quickly understand client needs, develop solutions, and collaboratively present findings to client executives
Analyze and model both structured and unstructured data to generate value
Use industry expertise and data science experience to uncover new opportunities with data
Design and build scalable machine learning models to meet customer requirement
Develop machine learning models to automate business processes and decisions
Productionalize models into a variety of platform architectures
Coordinate with multiple internal and external functional teams to implement models and monitor outcomes
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Help drive business results and participate in the account and department growth
Experience
Highly technical with hand-on experience using Apache Spark and python
Thorough understanding of data science concepts and toolsets such as H2O.ai
Knowledge of common data science languages (Python, R etc.)
Proficiency with Apache Spark (via Python, R or Java)
Training machine learning models (H2O.ai)
Hands-on experience with industry-standard predictive modeling solutions such as Spark ML, and H2O.ai in a production setting
Cleaning, blending and visualizing data (Pandas, Spark, Matplotlib)
Cloud infrastructure and services (AWS, Azure)
SQL and NoSQL databases (MySQL, PostgreSQL, Cassandra, MongoDB)
Experience working with big data distributed programming languages, and ecosystems: Spark, Hadoop, MapReduce, Kafka
","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'mysql', 'excel', 'spark', 'cloud', 'postgresql']","['recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']",2,"['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']","['machin', 'program', 'techniqu', 'stream', 'spark', 'ml', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc', 'engin']","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account', 'machin', 'program', 'techniqu', 'stream', 'spark', 'ml', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc', 'engin']"
DE,"Job Code: **4423**
\# of Openings: **1**
Methods+Mastery Dallas is seeking a Data Engineer who is experienced with building robust, scalable data pipelines for internal and client-facing data science products.
Methods+Mastery offers a fast-paced environment that encourages personal and professional growth and the opportunity to have a panoramic view of Dallas-34 floors up to be exact.
You'll have the opportunity to contribute against the full spectrum of communication mediums and disciplines including interactive, mobile, social, advertising, branding, experiential, video, motion, presentations, environmental and print.
*Responsibilities include:**
+ Support the development of data science products by building data pipelines using cloud computing and APIs
+ Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
+ Ensure that all systems meet the business/company requirements as well as industry practices
+ Identify ways to improve data reliability, efficiency and quality
+ Work closely with all team members to develop strategy for long term data platform architecture
*Key Qualifications**
The ideal candidate is keenly strategic, versatile and insights-driven; someone who thrives in an energetic, fast-paced entrepreneurial environment.
+ Bachelor's degree in Mathematics, Statistics, Economics, Computer Science, Engineering, or a related quantitative discipline
+ 2+ years professional experience building data pipelines using cloud computing platforms (GCP preferred)
+ 2+ years professional experience using Python
+ 2+ years professional experience using big data tools (Apache Hadoop, Apache Spark, etc.)
+ 2+ years experience with relational databases and data manipulation using SQL
+ 2+ years professional experience working with multi-disciplinary teams
To be successful, the candidate must have a desire to deliver quality work for clients/brands, be a team player and demonstrate the ability to get the job done.
You'll need a serious interest of trends, best practices, technology and business applications on the Internet and within Methods+Mastery.
*About Omnicom Public Relations Group**
Omnicom Public Relations Group at http://www.omnicomprgroup.com/ is a global collective of three of the top global public relations agencies worldwide and eight specialist agencies in public affairs, marketing to women, fashion, global health strategy and corporate social responsibility.
It encompasses more than 6,000 public relations professionals in more than 330 offices worldwide who provide their expertise to companies, government agencies, NGOs and nonprofits across a wide range of industries.
Omnicom Public Relations Group delivers for clients through a relentless focus on talent, continuous pursuit of innovation and a culture steeped in collaboration.
Omnicom Public Relations Group is part of the DAS Group of Companies, a division of Omnicom Group Inc. that includes more than 200 companies in a wide range of marketing disciplines including advertising, public relations, healthcare, customer relationship management, events, promotional marketing, branding and research.
","['sql', 'gcp', 'spark', 'hadoop', 'cloud', 'python']","['research', 'pipelin', 'statist', 'big data', 'commun', 'econom']",1,"['sql', 'gcp', 'spark', 'hadoop', 'cloud', 'python', 'research', 'pipelin', 'statist', 'big data', 'commun', 'econom']","['engin', 'spark', 'pipelin', 'corpor', 'relat', 'public', 'hadoop', 'avail', 'python', 'statist', 'comput', 'big', 'quantit', 'collect']","['sql', 'gcp', 'spark', 'hadoop', 'cloud', 'python', 'research', 'pipelin', 'statist', 'big data', 'commun', 'econom', 'engin', 'spark', 'pipelin', 'corpor', 'relat', 'public', 'hadoop', 'avail', 'python', 'statist', 'comput', 'big', 'quantit', 'collect']"
DE,"JD Skills : Senior Data Engineer
Â
At least 8 years of experience in software application development
At least 3 years' experience with Big Data / Hadoop architecture and related technologies
Hands-on experience with Spark â RDDs, Datasets, Dataframes, Spark SQL,
Hands-on experience with streaming technologies such Spark Streaming and Kafka
Hands-on experience using SQL, Spark SQL, HiveQL and performance tuning for big data operations
Hands-on experience with Java 8 and use of IDEs for the same
Hands-on experience using technologies such as Hive, Pig, Sqoop,
Experience building micro-services based application
Experience dealing with SQL and NoSQL databases such as Oracle, DB2, Teradata, Cassandra
Experience using CI/CD processes for application software integration and deployment using Maven, Git, Jenkins, Jules
Experience building scalable and resilient applications in private or public cloud environments and cloud technologies
Experience using SDLC and Agile software development practices
Experience building enterprise applications enabled for logging, monitoring, alerting and operational control
Experience enabling scheduling for big data jobs
Hands-on experience working in unix environment
Good written, verbal, presentation and interpersonal communication skills, given an opportunity willing to work in a challenging and cross platform environment.
Strong Analytical and problem-solving skills.
Ability to quickly master new concepts and applications
Preferable â experience in Financial industry
Preferable â experience in Data Science, Machine Learning, Deep learning, Business Intelligence and Visualization.
Â
Â
","['sql', 'spark', 'cassandra', 'pig', 'unix', 'git', 'hadoop', 'cloud', 'java', 'hive', 'nosql', 'kafka', 'oracl']","['visual', 'tune', 'deep learning', 'machine learning', 'big data', 'commun']",999,"['sql', 'spark', 'cassandra', 'pig', 'unix', 'git', 'hadoop', 'cloud', 'java', 'hive', 'nosql', 'kafka', 'oracl', 'visual', 'tune', 'deep learning', 'machine learning', 'big data', 'commun']","['analyt', 'machin', 'stream', 'learn', 'challeng', 'engin', 'spark', 'relat', 'visual', 'public', 'hadoop', 'big', 'integr', 'â']","['sql', 'spark', 'cassandra', 'pig', 'unix', 'git', 'hadoop', 'cloud', 'java', 'hive', 'nosql', 'kafka', 'oracl', 'visual', 'tune', 'deep learning', 'machine learning', 'big data', 'commun', 'analyt', 'machin', 'stream', 'learn', 'challeng', 'engin', 'spark', 'relat', 'visual', 'public', 'hadoop', 'big', 'integr', 'â']"
DE,"Interview Logistics:
WebEx Interview
Required Skills Set:
Years of Experience: 4+
Education Required: Bachelor’s Degree
BS/MS degree in Computer Science, Mathematics, or other relevant science and engineering discipline.
4+ years working as a software engineer.
2+ years working within an enterprise data lake/warehouse environment or big data architecture.
Excellent programming skills with experience in at least one of Python, Scala, Java, Node.js.
Great communication skills.
Proficiency in testing frameworks and writing unit/integration tests
Proficiency in Unix-based operating systems and bash scripts.
Additional Preferred Skills:
Experience with working in Spark
Experience with AWS
Experience with monitoring and visualization tools such as Grafana, Prometheus, Data Dog, and Cloudwatch.
Experience with NoSQL databases, such as DynamoDB, MongoDB, Redis, Cassandra, or HBase
Job Type: Full-time
Benefits:
Health Insurance
Schedule:
Monday to Friday
Education:
Bachelor's (Required)
Visa Sponsorship Potentially Available:
Yes: H-1B work authorization
https://www.vdriveitsolutions.com/
Work Remotely:
Temporarily due to COVID-19
","['mongodb', 'spark', 'cassandra', 'scala', 'unix', 'aw', 'python', 'java', 'hbase', 'nosql', 'excel']","['commun', 'logist', 'visual', 'big data']",1,"['mongodb', 'spark', 'cassandra', 'scala', 'unix', 'aw', 'python', 'java', 'hbase', 'nosql', 'excel', 'commun', 'logist', 'visual', 'big data']","['spark', 'set', 'visual', 'aw', 'avail', 'python', 'warehous', 'comput', 'big', 'integr', 'engin']","['mongodb', 'spark', 'cassandra', 'scala', 'unix', 'aw', 'python', 'java', 'hbase', 'nosql', 'excel', 'commun', 'logist', 'visual', 'big data', 'spark', 'set', 'visual', 'aw', 'avail', 'python', 'warehous', 'comput', 'big', 'integr', 'engin']"
DE,"Hi Friends,
Hope you are doing great,
Kindly share the suitable profile with contact details
Job Role : Data Engineer/Machine learning
Duration : Contract to Hire (6+ Months contract)
Essential Skills:
• Strong experience in chatbots and natural language processing (NLP).
• Good understanding of training classical Machine learning algorithms along with an understanding of choosing the right evaluation metric.
• Ability to use pretrained models and fine tune them if required.
• Experience with REST APIs and other web services.
• Perform keyword and topic extraction from chat logs.
• Solid knowledge of training and tuning topic modelling algorithms like LDA and NMF.
• Strong written communication skills.
• Ability to learn the latest technologies.
• Good problem-solving ability.
Nice to have skills:
• Experience in working with any AI/NLP platform (DialogFlow/ Alexa/ Converse.ai/ Amazon Lex etc.)
for building chatbots Experience with any one of the technology (JavaScript, Node.js or Python).
• Understanding of conversational UI, voiced based processing (text to speech, speech to text) and voice apps built on Amazon Alexa or Google Home is a plus.
• Experience in Test Driven Development & Agile methodologies.
• Hands on experience of using frameworks like nltk and spacy.
• Experience with configuring, support and integrating various software systems, API's, etc.
Thanks & Regards
Direct: (925)-464-1116 / 510-335-6561
Powered by JazzHR
d6x8xli4KE
","['javascript', 'python', 'spaci', 'nltk']","['natural language processing', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']",999,"['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']","['essenti', 'machin', 'learn', 'power', 'evalu', 'python', 'engin', 'algorithm']","['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun', 'essenti', 'machin', 'learn', 'power', 'evalu', 'python', 'engin', 'algorithm']"
DE,"Hello Associates,
ÂWe hope that you are safe and in good health during these unprecedented times.
Â
Position:ÂNetwork Data Engineer
Duration:Â12+ Months
Â
5+ years progressively responsible experience in network engineering/architecture.
Experience working on teams that are responsible for large scale distributed IP Network
Hands on experience on High end Switches (Cisco Nexus7K, 5K, 2K, Cisco 6800, 6500, 4500, 3850, HP switches) and Routers (ASR 1001-X, Cisco Platform).
Detailed knowledge and experience in implementing and troubleshooting VSS and VPC in 6800s and Nexus switches respectively.
Experience in upgrading the higher end catalyst switches like 6800, 6500.
Experience in Racking, Stacking, configuring and determining UPS requirements.
Experience working with project managers, network architecture group and end users.
Networking: Switches, Routers, Servers, Cables, Racks, ASA Firewalls, LAN, WAN, TCP/IP, DNS, UDP, Latency, VoIP, QoS, EIGRP, BGP, OSPF, NHRP, MPLS, VTP, VRF.
Detailed knowledge and troubleshooting of network concepts AAA, SNMP and protocols such as EIGRP, BGP, OSPF, MPLS, STP, HSRP, Load Balancing.
Must have experience with 5520 WLC, Cisco Prime Infrastructure, creating heat-maps and configuring and installing wireless access points.
Good understanding on ASA and Palo Alto firewalls and concepts such as VPN, DMVPN, NAT.
WAN design and Implementation (Multi-Vendor-MPLS for redundancy)
Experience with monitoring and troubleshooting tools like Solar winds.
Experience in creating BOM, documentation, determining transceivers.
Good understanding of Incident and Change Management procedures.
Excellent time management and communications skills, ability to work with remote teams, with minimal supervision.
CCNP certification is preferred.
",['excel'],"['supervis', 'end user', 'commun']",999,"['excel', 'supervis', 'end user', 'commun']","['infrastructur', 'engin', 'â']","['excel', 'supervis', 'end user', 'commun', 'infrastructur', 'engin', 'â']"
DE,"Hii All,,
Pls Find JD for the below Requirement ,
Position 2
Client : PWC
Role : Developer/Consultant level
Type : Contract to Hire (6+ Months contract)
Essential Skills:
• Strong experience in chatbots and natural language processing (NLP).
• Good understanding of training classical Machine learning algorithms along with an understanding of choosing the right evaluation metric.
• Ability to use pretrained models and fine tune them if required.
• Experience with REST APIs and other web services.
• Perform keyword and topic extraction from chat logs.
• Solid knowledge of training and tuning topic modelling algorithms like LDA and NMF.
• Strong written communication skills.
• Ability to learn the latest technologies.
• Good problem-solving ability.
Nice to have skills:
• Experience in working with any AI/NLP platform (DialogFlow/ Alexa/ Converse.ai/ Amazon Lex etc.)
for building chatbots Experience with any one of the technology (JavaScript, Node.js or Python).
• Understanding of conversational UI, voiced based processing (text to speech, speech to text) and voice apps built on Amazon Alexa or Google Home is a plus.
• Experience in Test Driven Development & Agile methodologies.
• Hands on experience of using frameworks like nltk and spacy.
• Experience with configuring, support and integrating various software systems, API’s, etc.
Thanks & Regards,
Chandi Kumar
Connections: | 925.271.9083 | 650.644.3713 |chandikumar@infowaygroup.com |
Skype :cid.61d8016473cb78b3 | linkedin.com/in/chandi-kumar-raghu-056333196
Powered by JazzHR
G07WKrVZCL
","['javascript', 'python', 'spaci', 'nltk']","['natural language processing', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']",999,"['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun']","['essenti', 'machin', 'learn', 'power', 'evalu', 'python', 'algorithm']","['javascript', 'python', 'spaci', 'nltk', 'nlp', 'tune', 'machine learning', 'nlp', 'chatbot', 'commun', 'essenti', 'machin', 'learn', 'power', 'evalu', 'python', 'algorithm']"
DE,"Position:Sr. Data Engineer/ Big Data Engineer
Duration: Long term
Big Data, Cloud, Azure, AWS .
Thanks & Regards
Sai Krishna | Sr.
Address: 4701 Patrick Henry Dr Ste 8, Santa Clara, CA 95054
Work : 805-500-4366 x 104
","['aw', 'cloud', 'azur']",['big data'],999,"['aw', 'cloud', 'azur', 'big data']","['big', 'aw', 'engin', 'azur']","['aw', 'cloud', 'azur', 'big data', 'big', 'aw', 'engin', 'azur']"
DE,"Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction.
Defines scope, plans, and deliverables for assigned components.
Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems.
Applies metrics to monitor performance and measure key project parameters.
Prepares system documentation.
Conforms to security and quality standards.
Stays current on emerging tools, techniques, and technologies.Responsibilities:* Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives* Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams* Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.
* Develop visualization, user experience and configuration elements of solution design.
* Execute and coordinate requirements management and change management processes.
Participates as a member of and leads development teams.
* Designs units for others.
* Completes development to implement complex components.
* Designs solutions for others to develop.
* Participates in cross-functional teams.
* Leads design activities and provides mentoring and guidance to developers.
* Designs, prepares and executes unit tests.
* Demonstrates technical leadership and exerts influence outside of immediate team.
* Develops innovative team solutions to complex problems.
* Contributes to strategic direction for teams.
* Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g.
Power BI and Power App development).
* Integrates technical expertise and business understanding to create superior solutions for clients.
* Consults with team members and other organizations, clients and vendors on complex issues.Education and Experience Required:* Typically, a technical bachelors degree or equivalent experience and 3 years of related experienceKnowledge and Skills:* 2 or more years experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, VB.Net.
* 2 or more years programming in one of the following: Python, Java, C++, C#,* Advanced understanding of RDBMS databases such SQL Server and Oracle.
* Advanced understanding of modern software design and development methodologies.
* Experience on multiple full release project life cycles.
* Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies* Good verbal and written communication and negotiation skills.
* Ability to work effectively in a globally dispersed team and with clients and vendors.
* Demonstrated technical leadership skills.
* Data engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business desired.
* Experience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment* Drives the construction of highly innovative statistical and financial models to analyze new aspects of business performance.
* Establishes the metrics required to measure business performance and recommends the go-forward strategy to address performance gaps.
Determination on requests for reasonable accommodation are considered on a case-by-case basis.
This contact information (email and phone) is intended for application assistance and accommodation requests only.
","['sql', 'bi', 'tableau', 'python', 'c', 'java', 'oracl', 'power bi']","['commun', 'visual', 'statist']",1,"['sql', 'powerbi', 'tableau', 'python', 'c', 'java', 'oracl', 'commun', 'visual', 'statist']","['basi', 'analyt', 'techniqu', 'corpor', 'relat', 'visual', 'power', 'bi', 'python', 'appli', 'statist', 'releas', 'integr', 'engin', 'particip']","['sql', 'powerbi', 'tableau', 'python', 'c', 'java', 'oracl', 'commun', 'visual', 'statist', 'basi', 'analyt', 'techniqu', 'corpor', 'relat', 'visual', 'power', 'bi', 'python', 'appli', 'statist', 'releas', 'integr', 'engin', 'particip']"
DE,"Role: Lead Data Engineer
Duration: 12+ Months onsite contract
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
","['sql', 'azur', 'scala', 'java', 'python', 'kafka', 'excel']","['pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']",999,"['sql', 'azur', 'scala', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun']","['analyt', 'pipelin', 'azur', 'avail', 'python', 'engin']","['sql', 'azur', 'scala', 'java', 'python', 'kafka', 'excel', 'pipelin', 'data modeling', 'problem solving', 'information technology', 'commun', 'analyt', 'pipelin', 'azur', 'avail', 'python', 'engin']"
DE,"This customer is the world's largest manufacturer of wood and laminate lockers.
Examples of successful clients have been the Dallas Cowboys, Texas Rangers, Dallas Mavericks, New Orleans Saints, TCU, UNT and more!
The IT Data Engineer will work in tandem with the current App Integration Specialist to create analytics and automate reports for the 3 main business units: Sales, Operations and Manufacturing.
Top 3 skills:
) Power BI
) SQL DB (Optimization and performance tuning)
) Scripting using stored procedures
This customer offers a variety of perks including:
Benefits
Medical, Dental
401K
FREE COLLEGE tuition and books at North Lake College
Company-sponsored getaways (after three years of employment)
Monthly food trucks provide free lunch
Refreshments every day, including birthday cakes (everyone celebrates together!)
Company-sponsored activities: fantasy football league, K1 Racing Day, and more
Holiday celebrations and year-end Christmas party
This person will need to be a self-starter with a creative mindset.
This person will need to be able to take a few directions and work independently without hesitation.
#ZR
Your Name:
Email Address:
Phone Number:
Upload Resume File:
Upload Resume File: …
Attach a resume file.
Accepted file types are DOC, DOCX, PDF, HTML, and TXT.
information, and use it in the consideration of your fitness for the position,
People looking for jobs should not put anything here.
It may take few moments to read your resume.
Please wait!
","['sql', 'bi', 'db', 'power bi']",['optim'],999,"['sql', 'powerbi', 'db', 'optim']","['day', 'analyt', 'power', 'optim', 'bi', 'parti', 'texa', 'integr', 'engin']","['sql', 'powerbi', 'db', 'optim', 'day', 'analyt', 'power', 'optim', 'bi', 'parti', 'texa', 'integr', 'engin']"
DE,"Analyze and understand data sources & APIs
• Design and Develop methods to connect & collect data from different data sources
• Design and Develop methods to filter/cleanse the data
• Design and Develop SQL , Hive queries, APIs to extract data from the store
• Work closely with data Scientists to ensure the source data is aggregated and cleansed
• Work with product managers to understand the business objectives
• Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
• Work with DevOps to build automated data pipelines
Total Experience Required
• 4
• The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
• Big Data-Hadoop, NoSQL, Hive, Apache Spark
• Python
• Java & REST
• GIT and Version Control
Desirable Technical Skills
• Familiarity with HTTP and invoking web-APIs
• Exposure to machine learning engineering
• Exposure to NLP and text processing
• Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
• Experience working in SCRUM methodology
• Proven sense of high accountability and self-drive to take on and see through big challenges
• Confident, takes ownership, willingness to get the job done
• Excellent verbal communications and cross group collaboration skills
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"be your best self
This Data Engineer role will focus on using Alteryx and Tableau SQL to create analytic deliverables to provide operational reporting, drive process improvement, and inform management decisions.
Job Components:
Developing scalable data solutions for the Commercial Analytics Team.
Work will also be used by other cross functional groups such as Finance, Supply Chain, Marketing, etc.
Enable analytic capabilities within team to inform business decisions.
Will be required to support team by creating dynamic visualizations in Tableau and analytic apps in Alteryx.
Work with IT to develop data governance framework.
Role may also include work from conducting minor to advanced data analysis, wire-framing and developing dashboards and visualizations
Communicate insights and recommendations to business.
Participating in helping to build a Data Driven Culture
Qualifications :
BA Degree in Business Administration, Computer Science, Accounting or Finance, or equivalent work experience
Experience with Tableau and Alteryx
Strong Computer Science fundamentals in Algorithms and Data Structures
Excellent verbal and written communication skills
Excellent organizational skills
Scope Data:
Work with other Technical resources including and not limited to Project Managers, Development Managers, QA, Tech Services, Business Analysts, the User Community, and others in requirements gathering endeavors and problem solving.
Judgment/Decision Making - Ability to think logically and practically before making decisions.
Ability to handle confidential information.
","['sql', 'tableau', 'excel']","['recommend', 'dashboard', 'visual', 'problem solving', 'financ', 'commun', 'account']",1,"['sql', 'tableau', 'excel', 'recommend', 'dashboard', 'visual', 'problem solving', 'financ', 'commun', 'account']","['analyt', 'visual', 'judgment', 'comput', 'engin', 'algorithm']","['sql', 'tableau', 'excel', 'recommend', 'dashboard', 'visual', 'problem solving', 'financ', 'commun', 'account', 'analyt', 'visual', 'judgment', 'comput', 'engin', 'algorithm']"
DE,"12+ Months Contract Role
Graduate with excellent communication skills
Candidate should have 3-4 years of experience in Data Science and ML.
Responsibilities
Collaborate with team to provide data-driven recommendations and solutions to clients by clearly articulating complex technical concepts
Work with team to quickly understand client needs, develop solutions, and collaboratively present findings to client executives
Analyze and model both structured and unstructured data to generate value
Use industry expertise and data science experience to uncover new opportunities with data
Design and build scalable machine learning models to meet customer requirement
Develop machine learning models to automate business processes and decisions
Productionalize models into a variety of platform architectures
Coordinate with multiple internal and external functional teams to implement models and monitor outcomes
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Help drive business results and participate in the account and department growth
Qualifications
Highly technical with hand-on experience using Apache Spark and python
Thorough understanding of data science concepts and toolsets such as H2O.ai
Knowledge of common data science languages (Python, R etc.)
Proficiency with Apache Spark (via Python, R or Java)
Training machine learning models (H2O.ai)
Hands-on experience with industry-standard predictive modeling solutions such as Spark ML, and H2O.ai in a production setting
Cleaning, blending and visualizing data (Pandas, Spark, Matplotlib)
Cloud infrastructure and services (AWS, Azure)
SQL and NoSQL databases (MySQL, PostgreSQL, Cassandra, MongoDB)
Experience working with big data distributed programming languages, and ecosystems: Spark, Hadoop, MapReduce, Kafka
Additional Information
Additional Information
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.All your information will be kept confidential according to EEO guidelines.
","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'mysql', 'excel', 'spark', 'cloud', 'postgresql']","['recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']",2,"['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account']","['machin', 'program', 'techniqu', 'spark', 'ml', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc']","['sql', 'mapreduc', 'python', 'java', 'nosql', 'kafka', 'azur', 'matplotlib', 'hadoop', 'mongodb', 'cassandra', 'panda', 'aw', 'r', 'excel', 'spark', 'cloud', 'postgresql', 'recommend', 'big data', 'predict', 'machine learning', 'clean', 'analyz', 'commun', 'account', 'machin', 'program', 'techniqu', 'spark', 'ml', 'azur', 'predict', 'hadoop', 'infrastructur', 'aw', 'python', 'big', 'common', 'sourc']"
DE,"Analyze and understand data sources & APIs• Design and Develop methods to connect & collect data from different data sources
• Design and Develop methods to filter/cleanse the data
• Design and Develop SQL , Hive queries, APIs to extract data from the store
• Work closely with data Scientists to ensure the source data is aggregated and cleansed
• Work with product managers to understand the business objectives
• Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
• Work with DevOps to build automated data pipelines
Total Experience Required
• 4
• The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
• Big Data-Hadoop, NoSQL, Hive, Apache Spark
• Python
• Java & REST
• GIT and Version Control
Desirable Technical Skills
• Familiarity with HTTP and invoking web-APIs
• Exposure to machine learning engineering
• Exposure to NLP and text processing
• Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
• Experience working in SCRUM methodology
• Proven sense of high accountability and self-drive to take on and see through big challenges
• Confident, takes ownership, willingness to get the job done
• Excellent verbal communications and cross group collaboration skills
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Summary:
The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organizations data assets.
Job Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
5-7 years of experience required.
Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
***Please submit candidates with the following must haves***
- Analytical and problem solving skills, applied to Big Data domain
- Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
- 5-8 years of Python or Java/J2EE development experience
- 3+ years of demonstrated technical proficiency with Hadoop and big data projects
Day to Day duties:
- Data collection – gather information and required data fields.
- Data manipulation – Join data from multiple data sources and build ETLs to be sent to Tableau for reporting purpose
- Measure & Improve - Implement success indicators to continuously measure and improve, while providing relevant insight and reporting to leadership and teams.
A lot of times candidates screened have been really good in SQL but not in python/shell/Hadoop or vice versa.
","['sql', 'spark', 'pig', 'hadoop', 'tableau', 'java', 'hive', 'python', 'microsoft', 'excel']","['predict', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'pig', 'hadoop', 'tableau', 'java', 'hive', 'python', 'microsoft', 'excel', 'predict', 'problem solving', 'big data', 'etl', 'commun']","['day', 'analyt', 'asset', 'engin', 'spark', 'relat', 'predict', 'hadoop', 'python', 'comput', 'appli', 'big', 'etl', 'sourc', 'collect', 'algorithm']","['sql', 'spark', 'pig', 'hadoop', 'tableau', 'java', 'hive', 'python', 'microsoft', 'excel', 'predict', 'problem solving', 'big data', 'etl', 'commun', 'day', 'analyt', 'asset', 'engin', 'spark', 'relat', 'predict', 'hadoop', 'python', 'comput', 'appli', 'big', 'etl', 'sourc', 'collect', 'algorithm']"
DE,"Each employee is hand-picked not only for their skills, but for their personality and broad expertise.
Build and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).
Uncover and recommend remediations for data quality anomalies.
Investigate, recommend and implement data ingestion and ETL performance improvements.
Document data ingestion and ETL program designs, present findings, conduct peer code reviews.
Develop and execute test plans to validate code.
Your Expertise:
4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.
3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.
Experience configuring big data solutions in a cloud environment (AWS, Azure or GCP).
Experience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.
Experience developing complex technical and ETL programs within a Hadoop ecosystem.
Must have a bachelors degree in Computer Science, Technology, Computer Information Systems, Computer Applications, Engineering, or a related field.
Your X-Factor:
Aptitude - You have an innate capacity to transition from project to project without skipping a beat.
Communication - You have excellent written and verbal communication skills for coordination across projects and teams.
Impact - You are a critical thinker with an emphasis on creativity and innovation.
Passion - You have the drive to succeed paired with a continuous hunger to learn.
Leadership - You are trusted, empathetic, accountable, and empower others around you.
Why Were Proud To Be Mavens!
Google Cloud North America Services Partner of the Year 2019, 2018
#21 Best Workplaces in Chicago, FORTUNE, 2018
Great Place To Work Certification, Great Place to Work, 2017 & 2018
Fast Fifty, Crain's Chicago Business
101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)
Top Google Cloud Partner, Clutch
Fastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine
Top IT Services Companies, Clutch
Google Global Rising Star Partner of the Year
Ready to Learn More?
Check out the Data Team
See what Glassdoor has to say
Real Customer Stories
","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel']","['recommend', 'anomali', 'big data', 'etl', 'commun', 'account']",1,"['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account']","['program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']","['sql', 'google cloud', 'gcp', 'spark', 'azur', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'redshift', 'python', 'java', 'cloud', 'microsoft', 'excel', 'recommend', 'anomali', 'big data', 'etl', 'commun', 'account', 'program', 'spark', 'azur', 'relat', 'hadoop', 'aw', 'employe', 'python', 'comput', 'big', 'etl', 'engin']"
DE,"Note Candidate can start remotely and till COVID situation can do remote from home after that candidate need to be onsite on TXhellip.if situationoffice gets open.
Candidates with the following must haves Strong experience in SQL Programming Strong experience in Python Experience Analytical and problem solving skills, applied to Big Data domain Proven understanding and hands on experience with Hadoop, Hive, Pig..
Minimum 2+ years of demonstrated technical proficiency with Hadoop and big data projects..
Need Shell Scripting.. JavaJ2EE coding experience is plus not mandatory
","['sql', 'pig', 'hadoop', 'python', 'hive']","['problem solving', 'big data']",999,"['sql', 'pig', 'hadoop', 'python', 'hive', 'problem solving', 'big data']","['analyt', 'program', 'hadoop', 'python', 'appli', 'big']","['sql', 'pig', 'hadoop', 'python', 'hive', 'problem solving', 'big data', 'analyt', 'program', 'hadoop', 'python', 'appli', 'big']"
DE,"Title: Big Data EngineerLocation: Plano, TX
Duration: 12+ Months contract
Minimum 10 Years of IT experience required
At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc..)
At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 3 years of experience with SQL and Shell Scripting experience
At least 2 years of experience with software design and must have an understanding of cross systems usage and impact
2+ years of experience working with Dimensional Data Model and pipelines in relation with the same
2+ years experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service
2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
Hands on design experience with data pipelines, joining data between structured and unstructured data
Thanks & Regards
Kumar Beeram
Manager- Resource Development
D: 469- 533- 7235
T: 972-234-0058 X 7235
E: kumar.beeram@infovision.com
","['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'amazon web services']","['pipelin', 'big data']",999,"['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'pipelin', 'big data']","['spark', 'pipelin', 'azur', 'public', 'aw', 'python', 'big', 'relat']","['sql', 'spark', 'perl', 'azur', 'scala', 'javascript', 'aw', 'python', 'java', 'cloud', 'nosql', 'microsoft', 'kafka', 'pipelin', 'big data', 'spark', 'pipelin', 'azur', 'public', 'aw', 'python', 'big', 'relat']"
DE,"Purpose of Job
Data Engineers deliver quality reporting and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions.
Data Engineers (DEs) are engaged in all phases of the data management lifecycle; gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data.
Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored and controlled.
Job Requirements
This singular mission requires a dedication to innovative thinking at every level.
https://www.youtube.com/watch?v=kVCnnaJUH_c
Data Engineer A Realistic Preview
Identifies and manages existing and emerging risks that stem from business activities and the job role.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled.
Follows written risk and compliance policies and procedures for business activities.
Design and implement complex technical solutions.
Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management.
Participate in daily standups and lead design reviews.
Breakdown business features into technical stories and approaches.
Analyze data and enable machine learning.
Create proof of concepts and prototypes.
Implement efficient defect management, root cause analysis, and resolution processes.
Assist in setting technical direction for the team.
Mentor and coach junior engineers.
Minimum Experience:
Bachelor's degree in related field of study,
OR
Certification from an approved technical field of study,
OR
4 additional years of related experience beyond the minimum required.
6 years of data management experience implementing data solutions demonstrating depth of technical understanding within a specific discipline(s)/technology(s)
Deep Knowledge of a technology or product line
*Qualifications may warrant placement in a different job level*
This will take approximately 5 minutes.
Once you begin the questions you will not be able to finish them at a later time and you will not able to change your responses.
Preferred Experience
Extensive experience in ETL tools (DataStage and/or Informatica)
Extensive experience with Unix Shell Scripting
Experience in Python Development
Experience in ServiceNow Development
Experience in SalesForce Development
Strong SQL experience
Netezza Experience
Experience in creation of APIs using openshift and docker
For Internal Candidates:
Must complete 12 months in current position (from date of hire or date of placement), or must have managers approval prior to posting.
","['sql', 'unix', 'salesforc', 'python', 'docker']","['pipelin', 'risk', 'machine learning', 'analyz', 'etl']",1,"['sql', 'unix', 'salesforc', 'python', 'docker', 'pipelin', 'risk', 'machine learning', 'analyz', 'etl']","['machin', 'collect', 'pipelin', 'relat', 'line', 'python', 'etl', 'engin', 'particip']","['sql', 'unix', 'salesforc', 'python', 'docker', 'pipelin', 'risk', 'machine learning', 'analyz', 'etl', 'machin', 'collect', 'pipelin', 'relat', 'line', 'python', 'etl', 'engin', 'particip']"
DE,"Locations: TX - Plano, United States of America, Plano, Texas
Manager, Data Engineer
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team?
What youll do:
Lead and develop other up and coming engineers
Grasp / master new technologies rapidly as needed to progress varied initiatives
Break down complex data issues and resolve them
Build robust systems with an eye on automation and the long-term maintenance and support of the application
Work directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment
Who you are:
Someone with a strong sense of engineering craftsmanship, and takes pride in designing work products (code, etc.)
A believer that good development includes good testing, documentation, and collaboration
A good communicator with strong reasoning skills, who can make a case for technology choices
Curious.
Ask questions and desire to understand the Why to drive the How or What of a solution
Self-driven, actively looking for ways to contribute, and know how to get things done
A leader that enjoys mentoring others and helping to guide the organization to continuously improve
Basic Qualifications:
Bachelors Degree
At least 2 years of experience leading a development team and managing other software and/or data engineers
At least 4 years of Data Engineering experience with Big Data Technologies: Apache Spark, Hadoop, or Kafka
At least 3 years of microservices development experience: Python, Java or Scala
At least 2 years of experience building data pipelines, CICD pipelines, and fit for purpose data stores
At least 1 year of experience in Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform
Preferred Qualifications:
Masters Degree
6+ years of experience with Data Architecture and Design
4+ years of experience working with AWS: S3, EMR, or EC2
4+ years of experience working with data consumption patterns in SQL or Python applications
3+ years of experience working with automated build and continuous integration systems
2+ years of experience with Linux systems
2+ years of experience with NOSQL Graph Databases
","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker']","['commun', 'graph', 'pipelin', 'big data']",1,"['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data']","['spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']","['sql', 'linux', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 's3', 'java', 'nosql', 'kafka', 'docker', 'commun', 'graph', 'pipelin', 'big data', 'spark', 'pipelin', 'azur', 'hadoop', 'aw', 'python', 'texa', 'big', 'integr', 'engin']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"ÂBigData Engineers:
Cassandra, Spark and Big Data TechnologiesÂ
Length: + 6 MonthsÂ
","['spark', 'cassandra']",['big data'],999,"['spark', 'cassandra', 'big data']","['big', 'spark', 'engin']","['spark', 'cassandra', 'big data', 'big', 'spark', 'engin']"
DE,"""Candidates should possess strong knowledge and interest across big data technologies and have a background in data engineering.
• Transform complex analytical models in scalable, production-ready solutions
• Provide support and enhancements for an advanced anomaly detection machine learning platform
• Develop cloud based applications from the ground up using a modern technology stack
• Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment
Responsibilities:
• Building data APIs and data delivery services to support critical operational and analytical applications
• Contributing to the design of robust systems with an eye on the long-term maintenance and support of the application
• Leveraging reusable code modules to solve problems across the team and organization
• Handling multiple functions and roles for the projects and Agile teams
• Being a technology thought leader and strategist
Required:
At least 4 years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least4 years of experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS
At least 4 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 2 years of experience with SQL and Shell Scripting experience
Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Apache Beam, Apache Airflow etc).
Experience with Spark programming (pyspark or scala or java).
Experience troubleshooting JVM-related issues.
Experience and strategies to deal with mutable data in Hadoop.
Familiarity with Spark Structure Streaming and/or Kafka Streams.
Familiarity with machine learning implementation using PySpark.
Experience in data visualization tools like Cognos, Arcadia, Tableau.
Experience in Ab Initio technologies including, but not limited to Ab Initio graph development, EME, Co-Op, BRE, Continuous flow)
Preferred:
• Angular.JS 4 Development and React.JS Development expertise in a up to date Java Development Environment with Cloud Technologies
• 1 years’ experience with Amazon Web Services (AWS), Google Compute or another public cloud service
• 2 years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
• 2 years of experience working with Dimensional Data Model and pipelines in relation with the same
• Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
• Hands on design experience with data pipelines, joining data between structured and unstructured data
• Familiarity of SAS programming will be a plus
• Experience implementing open source frameworks & exposure to various open source & package software architectures (AngularJS, ReactJS, Node, Elastic Search, Spark, Scala, Splunk, Apigee, and Jenkins etc.).
• Experience with various noSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j) will be a plus
Other:
• Successfully complete assessment tests offered in Pluralsight, Udemy, etc.
or complete certifications to demonstrate technical expertise on more than one development platform.
""
","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'amazon web services', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk']","['graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']",999,"['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']","['program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data', 'program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Title: Software Engineering - Big Data
Duration: 12+ months
Expert in Big Data and design technique as well as experience working across large environments with multiple operating systems/infrastructure for large-scale programs (e.g., Expert Engineers) starting to be firm-wide resources working on projects across Client.
§ Is multi-skilled with expertise across software development lifecycle and toolset
§ May be recognized as a leader in Agile and cultivating teams working in Agile frameworks
§ Sought out as coach for at least one technical skill
§ Strong understanding of techniques such as Continuous Integration, Continuous Delivery, Test Driven Development, Cloud Development, resiliency, security
Job Type: Contract
Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19
",['cloud'],['big data'],2,"['cloud', 'big data']","['program', 'techniqu', 'infrastructur', 'big', 'integr', 'engin']","['cloud', 'big data', 'program', 'techniqu', 'infrastructur', 'big', 'integr', 'engin']"
DE,"Title Senior Data Engineer City Plano, TX Duration 12+ Months Must have skills AWS certified SQL Spark Nice to have Tableau reporting or any reporting tools Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS lsquobig datarsquo technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Qualifications for Data Engineer Advanced working SQL knowledge and experience working with relational databases,query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing lsquobig datarsquo data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable lsquobig datarsquo data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Best regards httpslh5.googleusercontent.comb-g1iUrSj7YML7Un0xN0QD622MGpMiHBQIi4haC--CddN3uyfsUcIw3g httpswww.infovision.com httpslh5.googleusercontent.comynd3Hsuw2UVGJVZ6mf0CAuN1X8y2jHP0JA- httpswww.ivlglobal.com cidimage003.png01D5B023.1A61D220 httpsinfovisionsocial.com httpslh6.googleusercontent.comwNRdyop1uFlrgojoRrfmKVVmpChOM4sPrI-hEkPf2sq2eT4PCushelGGXu65Ws7GFEE--Cgxw8FJ httpswww.vcollab.com Charan Varanasi Resource Development Manager c 972-792-9969 e charaninfovision.com mailtocharaninfovision.com cidimage002.png01D54227.DDA0D340 httpswa.me19724271954 httpswa.me19727929969 httpswa.me19727929969 cidimage003.png01D54227.DDA0D340 httpwww.linkedin.cominvvdcharan www.linkedin.cominvvdcharan httpwww.linkedin.cominvvdcharan
","['sql', 'spark', 'aw', 'tableau', 'c']","['optim', 'pipelin', 'big data']",999,"['sql', 'spark', 'aw', 'tableau', 'c', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'action', 'set', 'sourc', 'engin']","['sql', 'spark', 'aw', 'tableau', 'c', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'infrastructur', 'optim', 'aw', 'action', 'set', 'sourc', 'engin']"
DE,"Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services) cloud environment.
Essential Responsibilities:
Design, implement, and test major subsystems of AWS cloud platform and core service offerings using the Scrum agile framework
Develop and follow best practices relative to design, implementation, and testing
Prototype new ideas or technologies to prove efficacy and usefulness in production
Develop SOAP (Simple Object Access Protocol) and REST (Representational State Transfer) APIs (Application Programming Interfaces) to integrate with partners and customers in real-time
Build a service structure on AWS capable of being deployed and scaled to run a variety of platform components dynamically
Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS cloud
Construct a state-of-the-art data lake on AWS using Amazon EMR, and Apache Spark, NiFi, Kafka, and Cassandra
Design & develop data pipelines for batch & streaming data sets using open source/ AWS tech stack
Mentor junior team members as a senior member of the Engineering team
Non-Essential Responsibilities:
Other duties as assigned
Knowledge, Skills and Abilities:
Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Basic knowledge for distributed computing
Knowledge of the challenges associated with “big data” and how to build systems that scale seamlessly
Ability to communicate well
Work Conditions and Physical Demands:
Primarily sedentary work in a general office environment
Ability to communicate and exchange information
Ability to comprehend and interpret documents and data
Requires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)
Requires manual dexterity to use computer, telephone and peripherals
May be required to work extended hours for special business needs
May be required to travel at least 10"" of time based on business needs
Minimum Education:
The knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required.
A combination of education and experience may be used in lieu of a diploma.
Minimum Related Work Experience:
6 years’ experience designing and delivering production software
5 years’ experience designing and implementing big data high performance operational systems
Proven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc
EOE including disability/veteran.
","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'amazon web services']","['pipelin', 'big data']",1,"['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'pipelin', 'big data']","['essenti', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'pipelin', 'big data', 'essenti', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']"
DE,"Must have skills*
Minimum 8+ years of experience with 4+ years of experience with building Big Data Solutions and Machine Learning Models.
Key responsibilities*
Understand Machine Learning Models in SAS and R and Convert them to Spark Jobs
Enhance Machine Learning Models using Pyspark or Scala
Build ML Models based on Business Requirements and Follow ML Cycle to Deploy them all the way to Production Environment
Participate Feature Engineering, Training Models, Scoring and retraining
Architect Data Pipeline and Automate Data Ingestion and Model Jobs
Experience in Cloudera Hadoop Eco System.
Expertise in Spark Framework using Python/Scala
Experience with SAS Modeling and converting them to Pyspark models is big Plus
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities
","['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r']","['pipelin', 'visual', 'machine learning', 'big data', 'commun']",999,"['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r', 'pipelin', 'visual', 'machine learning', 'big data', 'commun']","['machin', 'learn', 'spark', 'pipelin', 'ml', 'sa', 'visual', 'hadoop', 'python', 'appli', 'big', 'engin', 'particip']","['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r', 'pipelin', 'visual', 'machine learning', 'big data', 'commun', 'machin', 'learn', 'spark', 'pipelin', 'ml', 'sa', 'visual', 'hadoop', 'python', 'appli', 'big', 'engin', 'particip']"
DE,"A qualified applicant will have 5-7 years of experience in a Data Engineer position; as well as a background in supporting data transformation, data structures, metadata, dependency, and workload management.
Expert experience with Talend and SQL is highly desired.
Here's what you can expect from the job and what you need to be successful:
Job Duties
Create and maintain optimal data pipeline architecture
Assemble large and complex data sets to meet functional/non-functional business requirements
Build a data pipeline infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, cloud based relational, and/or non-relational databases employing Talend and/or scripting languages like Perl/Python
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability
Work with key stakeholders, including members of the executive, product, data and design teams to assist with data-related technical issues and support data infrastructure needs
Partner with data and analytics experts to ensure greater functionality of data integration platform
Follow established configuration/change control processes
Identify options for ETL, data, and data warehouse potential solutions and assess for technical and business suitability
Essential Skills
5-7 years of experience in a Data Engineer or similar position
Minimum 3 years of developing ETL processes using Talend, informatica cloud including the processing of NoSQL and JSON formats
Talend experience is a requirement
Advanced working knowledge in employing Talend tool to build robust data pipelines.
Talend certification is preferred
Advanced working knowledge in scripting languages, including Perl/Shell& Python
Advanced working SQL knowledge and experience working with relational/non-relational databases, query authoring (SQL), and excellent SQL troubleshooting skills
Experience performing root cause analysis on internal and external data and processes to respond to specific business questions and identify opportunities for improvement
Experience in building processes to support data transformation, data structures, metadata, dependency, and workload management
Experience in working with an agile project management environment
Exposure to basic issues in working within a Cloud (Azure/AWS ) environment
Experience in building data pipelines in a Big Data environment
Excellent problem solving and critical thinking skills with the ability to carry out assigned tasks with limited oversight
Superb verbal and written communication skills
Bachelor’s degree in Business Administration, Computer Science or other related fields of study or equivalent work experience.
Experience in financial services industry is preferred
HP123
","['sql', 'perl', 'azur', 'aw', 'cloud', 'python', 'nosql', 'excel']","['pipelin', 'optim', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'perl', 'azur', 'aw', 'cloud', 'python', 'nosql', 'excel', 'pipelin', 'optim', 'problem solving', 'big data', 'etl', 'commun']","['analyt', 'essenti', 'pipelin', 'azur', 'set', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']","['sql', 'perl', 'azur', 'aw', 'cloud', 'python', 'nosql', 'excel', 'pipelin', 'optim', 'problem solving', 'big data', 'etl', 'commun', 'analyt', 'essenti', 'pipelin', 'azur', 'set', 'relat', 'infrastructur', 'optim', 'aw', 'python', 'comput', 'big', 'etl', 'sourc', 'engin', 'integr']"
DE,"About the Position
Mist is the new global standard for many fortune 500 companies.
The mission of the Data Science team at Mist is to deliver AI driven self-driving network solution.
Data Engineers on the team will be the enabler and amplifiers.
This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth team.
He/she is encouraged to think out of the box and play with the latest technologies while exploring their limits.
Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Minimum Qualifications:
Fluent with Python, Java, or Scala
5+ years of experience of developing and managing streaming or batch data pipelines.
Hands on experience with either one or all, Storm, Spark, Kafka and Flink .
Familiarity with cloud based platforms - AWS or GCP
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Highly motivated to build a great product and great codebase in a fast-paced environment
Strong communication skills with a growth and learning mindset
Preferred Qualifications:
MS in Computer Science, Electrical Engineering or other Engineering majors with 10+ years of total experience.
Knowledge and experiences of using machine learning tools such as Numpy, ScikitLearn, MLlib, Tensorflow
www.dhs.gov/E-Verify
","['gcp', 'spark', 'numpi', 'scala', 'mllib', 'kafka', 'aw', 'cloud', 'python', 'java', 'tensorflow', 'scikitlearn']","['machine learning', 'commun', 'pipelin', 'big data']",999,"['gcp', 'spark', 'numpi', 'scala', 'mllib', 'kafka', 'aw', 'cloud', 'python', 'java', 'tensorflow', 'scikitlearn', 'machine learning', 'commun', 'pipelin', 'big data']","['machin', 'stream', 'learn', 'spark', 'pipelin', 'aw', 'python', 'comput', 'big', 'engin']","['gcp', 'spark', 'numpi', 'scala', 'mllib', 'kafka', 'aw', 'cloud', 'python', 'java', 'tensorflow', 'scikitlearn', 'machine learning', 'commun', 'pipelin', 'big data', 'machin', 'stream', 'learn', 'spark', 'pipelin', 'aw', 'python', 'comput', 'big', 'engin']"
DE,"Job Summary:
Responsibilities:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for team members to assist them in building and optimizing analytics production.
Other duties as required.
This list is not meant to be a comprehensive inventory of all responsibilities assigned to this position
Required Qualifications/Skills:
Bachelor’s degree (B.S/B.A) from four-college or university and 5 to 8 years’ related experience and/or training; or equivalent combination of education and experience
Networks with senior internal and external personnel in own area of expertise
Demonstrates good judgment in selecting methods and techniques for obtaining solutions
Proficiency in languages SAS, Python, Java, and MatLab
ETL (Extract, Transform, and Load)
Database - SQL and NoSQL
Data Warehousing – Hadoop, MapReduce, HIVE, Presto
Operating system: UNIX, Linux
Follows standard practice and procedures when analyzing situations or data
Preferred Qualifications:
Programming language: Javascript, R
Basic Machine Learning
Scope:
Resolves a wide range of issues in creative ways
Seasoned, experienced professional with a full understanding of their speciality
Works on problems of a diverse scope
Receives little instruction on day to day work, general instruction on new assignments
Employment in the fast-growing IT space providing you with brilliant career options for years to come
Introduction to the new ways of working and awesome technologies
Career paths to help you establish where you want to go
A company-wide mentoring program to advise you along the way
Online training courses through CBT-nuggets to upskill you
Performance management system to provide you with meaningful, actionable feedback
Dedicated management to provide you with a point of leadership and care
Internal promotion focus.
Numerous on-the-job perks
Peer Recognition
Market competitive rates and benefits
","['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab']","['pipelin', 'machine learning', 'optim', 'data warehousing', 'etl']",1,"['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab', 'pipelin', 'machine learning', 'optim', 'data warehousing', 'etl']","['day', 'program', 'techniqu', 'python', 'etl', 'sourc', 'analyt', 'sa', 'judgment', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'set', 'machin', 'pipelin', 'divers', 'relat']","['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab', 'pipelin', 'machine learning', 'optim', 'data warehousing', 'etl', 'day', 'program', 'techniqu', 'python', 'etl', 'sourc', 'analyt', 'sa', 'judgment', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'set', 'machin', 'pipelin', 'divers', 'relat']"
DE,"Its studios create popular, immersive mobile games and its technology brings games to more players around the world.
Learn more at applovin.com.
About You:
Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science
Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding
Experience independently creating and maintaining projects
Product focused mindset
Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka)
Experience designing, building, and maintaining data processing systems
Experience with a backend language such as Java or Scala
About the Role:
Collaborate with various engineering teams to meet a wide range of technological challenges
Work closely with product and business teams to improve data models that feed business intelligence tools
Perks:
Free medical, dental, and vision insurance
Daily lunches and fully stocked kitchen
Free public transit
Free laundry service (wash/dry clean)
Free gym membership
401k matching
Flexible Time Off - work hard and take time when you need it
Interested?
#LI-JZ1
","['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel']","['clean', 'big data']",1,"['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'clean', 'big data']","['learn', 'spark', 'challeng', 'public', 'hadoop', 'comput', 'big', 'engin', 'algorithm']","['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'clean', 'big data', 'learn', 'spark', 'challeng', 'public', 'hadoop', 'comput', 'big', 'engin', 'algorithm']"
DE,"The Opportunity:
What you will do:
Build infrastructure and abstractions that can enable anyone (engineer or data scientist) to craft a scalable ETL pipeline for whatever the purpose is: metrics, analysis, machine learning, dashboard visualizations
Work closely with ops team to monitor and tune existing infrastructure.
Make intuitive decisions about what services, frameworks, and capabilities need to be in place before they are needed.
Build and maintain a data collection system that robustly extracts meaningful data from multiple sources and data stores.
Build analytics and Machine learning platforms to collect, store, process, and analyze huge sets of data
Who you are:
BA/BS in Computer Science, Information Systems or related technical field.
3+ years of experience in Data Infrastructure, with Cloud SW experience a plus.
Experience in crafting and scaling data infrastructure, models, and pipelines
Hands-on experience with a variety of data infrastructures, such as:
Processing: Spark, Flink, Hadoop, Lambda
Messaging: Kafka, Zookeeper, Pulsar
Storage: Hive, Mongo DB, Athena, Phoenix, Splice, Redshift, DynamoDB
Machine Learning: Sagemaker, H2O, Keras, NumPy
Open and active in sharing knowledge as well as excellent communication skills
Programming experience in one or more application or systems languages including Scala, Java, or Python
Have an ability to own a project from inception to completion
","['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel']","['pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun']",1,"['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel', 'pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'spark', 'pipelin', 'set', 'relat', 'visual', 'hadoop', 'infrastructur', 'python', 'scientist', 'comput', 'etl', 'sourc', 'collect']","['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel', 'pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'spark', 'pipelin', 'set', 'relat', 'visual', 'hadoop', 'infrastructur', 'python', 'scientist', 'comput', 'etl', 'sourc', 'collect']"
DE,"Responsibilities:
Develop highly reliable data pipelines using technologies like Apache Spark, PySpark.
Follow AWS best practices and industry standards.
Ensure high quality on the business deliverables and raise the bar on quality of data in Datawarehouse.
Contribute to the development of event processing pipelines for near real-time analytics use cases.
Flawless communication within internal teams and across the organization.
Work independently on business deliverables and mentor team on near real-time data pipelines.
REQUIRES BACHELOR'S DEGREE IN COMPUTER SCIENCE OR RELATED FIELD PLUS 6 YEARS OF EXPERIENCE.
3 YEARS OF EXPERIENCE WORKING AS A DEVELOPER IN DATA ENGINEERING WITH DATA PIPELINES AND DATA PROCESSING.
ANY AMOUNT OF HANDS-ON EXPERIENCE WITH TECHNOLOGIES LIKE APACHE SPARK, PYSPARK, AND OTHER STREAM PROCESSING FRAMEWORKS.
ANY AMOUNT OF EXPERIENCE WORKING WITH DATA FORMATS SUCH AS APACHE AVRO, APACHE PARQUET, AND COMMON METHODS IN DATA TRANSFORMATION.
ANY AMOUNT OF EXPERIENCE WITH AWS BIG DATA TECHNOLOGY STACK.
Must also have authority to work permanently in the U.S.
","['aw', 'pyspark', 'spark']","['commun', 'pipelin', 'big data']",1,"['aw', 'pyspark', 'spark', 'commun', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'aw', 'amount', 'comput', 'big', 'relat', 'common', 'engin']","['aw', 'pyspark', 'spark', 'commun', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'aw', 'amount', 'comput', 'big', 'relat', 'common', 'engin']"
DE,"CAREERS
Data Engineer – Sunnyvale, CA
Sunnyvale, CA
You will also provide business insights, while leveraging internal tools and systems, databases and industry data.
THE IDEAL CANDIDATE WILL
Very Strong engineering skills.
Should have an analytical approach and have good programming skills.
Provide business insights, while leveraging internal tools and systems, databases and industry data
Minimum of 5+ years’ experience.
Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter
Ability to document requirements, data lineage, subject matter in both business and technical terminology.
Guide and learn from other team members.
Demonstrated ability to transform business requirements to code, specific analytical reports and tools
This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
Must Have
Strong analytical background
Self-starter
Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights
ELIGIBILITY CRITERIA
Technical Requirements
Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata)
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes
Real time data ingestion (Kafka)
Nice to Have
Development experience with Java, Scala, Flume, Python
Cassandra
Automic scheduler
R/R studio, SAS experience a plus
Presto
Hbase
Tableau or similar reporting/dash boarding tool
Modeling and Data Science background
Retail industry background
Education
BS degree in specific technical fields like computer science, math, statistics preferred
Send your CV to careers@tredence.com
","['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark']","['optim', 'statist', 'big data', 'etl', 'commun', 'math']",1,"['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark', 'optim', 'statist', 'big data', 'etl', 'commun', 'math']","['analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']","['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark', 'optim', 'statist', 'big data', 'etl', 'commun', 'math', 'analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']"
DE,"eHealthInsurance has many exciting career opportunities in a number of locations, across various functions.
Data Engineer
This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.
Responsibilities:
Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Work with data infrastructure team to triage issues and support issue resolution.
Minimum Qualifications:
Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
2+ years of experience with designing, implementing and maintaining scalable and reliable data pipelines
Mastery of SQL in writing complex and high performance queries
Working experience with MPP systems (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Production coding experience with Python, Scala or Java and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Knowledge of AWS data tools.
Excellent communication skills.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
Nice to Have:
Experience collaborating with data science team.
Strong experience in designing and implementing data APIs.
Product familiarity with Adobe Analytics, Cisco systems, Snowflake.
Familiarity with workflow management tools (Airflow).
Working experience with data warehousing.
Ability to create beautiful data visualizations using D3, Tableau, or similar tools.
Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys.
Experience in building healthcare data pipelines would be a big plus.
Knowledge of healthcare insurance industry, products, systems, business strategies, and products.
Experience working with call center operations.
","['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho']","['healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun']",1,"['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho', 'healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun']","['visual', 'python', 'etl', 'quantit', 'integr', 'analyt', 'statist', 'learn', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'pipelin', '3rd', 'scientist', 'comput', 'relat']","['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho', 'healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun', 'visual', 'python', 'etl', 'quantit', 'integr', 'analyt', 'statist', 'learn', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'pipelin', '3rd', 'scientist', 'comput', 'relat']"
DE,"Your primary role will be to design and build data pipelines.
You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive.
In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies.
If you believe you have these skills please email your resume to info@springml.com.
Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR
","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker']","['clean', 'visual', 'pipelin', 'big data']",1,"['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data']","['program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data', 'program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']"
DE,"As a Product Data Engineer, you will craft and develop infrastructure to store, process, and analyze large data sets.
The product insights gained from these data sets will improve manufacturing and impact designs for future products.
What you’ll be doing:
Crafting and building a data platform to handle growth in data scale and analytics scope
Working with Product Engineers to understand analytics use cases and develop data pipeline processes to support them
Building tools for efficient product characterization during NPI
Supporting the training and release of machine learning algorithms in product manufacturing
Bachelors in Computer Science or equivalent experience
2 - 4 years of proven experience
Strong programming skills in Python, Java, Scala, or Go
Advanced understanding of SQL
Experience in building and maintaining data infrastructure to handle large datasets
Experience with relational databases MySQL or PostgreSQL and data stores such as Hadoop, HBase, or Cassandra
Background with compute frameworks and data warehouses like Hive, Redshift, Spark, and Dask
Experience with cloud providers such as AWS or Azure
Stellar partnership and interpersonal skills
Ways to stand out from the crowd:
Masters in CS or Computer Engineering
Experience with ETL frameworks like Airflow and Luigi
Familiarity with machine learning tools such as SVMs, Kernel Functions, gradient descent, back-propagation
Display familiarity with chip and board testing data
Experience with Tableau, Looker, Kibana or other data visualization platforms
Hardworking and passionate about the industry
Show willingness to collaborate with teams across the globe
","['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'mysql', 'spark', 'airflow', 'cloud', 'postgresql']","['pipelin', 'visual', 'machine learning', 'etl', 'svm']",1,"['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'spark', 'airflow', 'cloud', 'postgresql', 'pipelin', 'visual', 'machine learning', 'etl', 'svm']","['program', 'visual', 'provid', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'warehous', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'spark', 'airflow', 'cloud', 'postgresql', 'pipelin', 'visual', 'machine learning', 'etl', 'svm', 'program', 'visual', 'provid', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'warehous', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"When you come across a sea of data, is your first instinct to put on your software diving suit and go deep?
Requirements
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing
Advanced knowledge of system architecture and database design
Proficiency in Big Data tools: Spark, Hadoop, Kafka, etc.
Advanced experience with SQL and NoSQL database architecture and implementation (hands-on experience with PostgreSQL, Elasticsearch, and Cassandra a plus)
Demonstrable experience designing, developing, and implementing ETL processes
Experience working with private cloud infrastructure
Demonstrable experience building and optimizing Big Data pipelines and architecture
Proficiency in Python, Java, C++
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Ability to collaborate well with others
Strong communication skills
Visit www.entefy.com and www.blog.entefy.com
","['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka']","['etl', 'commun', 'pipelin', 'big data']",999,"['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka', 'etl', 'commun', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'python', 'big', 'etl']","['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka', 'etl', 'commun', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'python', 'big', 'etl']"
DE,"The ticker symbol for the combined public entity will remain NYSE: RUBI.
Why You'll Be Excited
Having a large stake and impact on the product and business direction and bottom-line
Collaborating with innovative and goal-focused engineering and business teams
Working with data scientists, data analysts, and product managers to identify and use the data that is most relevant to the problem at hand
Building systems that can effectively stream, store, and crunch vast amounts of data to help inform customers and power business analytics
Solving complex problems revolving around real-time strategic decision-making and large data systems
Developing, deploying, and maintaining robust and high-performance systems and features
You have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams
You are passionate about learning different technologies, exploring engineering challenges, and working in a dynamic and collaborative environment
You have working experience and skills designing and coding in Java/Scala and/or Python
You are proficient in writing efficient and well-structured SQL queries and have experience with database schemas and design
You have experience with big data technologies (Spark, Presto, Druid, etc.)
You have knowledge of UNIX/Linux and scripting with Perl, Shell, etc.
Degree in Computer Science or a related field
Bonus: Experience working in a data science / machine learning environment
Bonus: Experience working with AWS Services (Redshift, Kinesis, Glue, etc.)
Perks and Benefits:
open vacation policy!
), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more!
All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.
","['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi']","['machine learning', 'commun', 'big data']",1,"['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi', 'machine learning', 'commun', 'big data']","['analyt', 'machin', 'program', 'spark', 'challeng', 'relat', 'line', 'power', 'public', 'amount', 'aw', 'python', 'scientist', 'comput', 'big', 'employe', 'engin']","['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi', 'machine learning', 'commun', 'big data', 'analyt', 'machin', 'program', 'spark', 'challeng', 'relat', 'line', 'power', 'public', 'amount', 'aw', 'python', 'scientist', 'comput', 'big', 'employe', 'engin']"
DE,"Lead Data Engineer - (Elasticsearch, Logstash, Kibana)
Successful candidates would bring a data driven mindset to ensure customer success.
Ability to manage the ELK infrastructure using kubernetes on a global scale.
You will be responsible for architecture and driving roadmap execution in a matrix cross functional organization.
Have strong written and communication skills and a sense of ownership and drive towards business objectives.
Responsibilities:
• SLDC process involving scrum agile process from inception to product delivery.
• Elasticsearch Engineer will work closely with product manager, architects, engineers, and integrators to assess customer requirements and to design and support an Elasticsearch Stack solution to ensure data is useable for internal and external customers.
• Deploy and maintain ELK and Prometheus infrastructure on development and production environments.
• Testing data flows, troubleshooting issues, and monitoring the health of the solution and servers to maximize performance and minimize downtime
• Securing environments with TLS, certificates, SSO authentication
Minimum qualifications:
• 3+ years' experience in technical leadership leading a team of engineers.
• 3+ years' experience contributing as a team member using Scrumban/Kanban agile model
• 3+ years hands on experience in performance profiling and optimization in a distributed environment
• 5+ years hands on experience with Prometheus, Alertmanager, ELK, Grafana, and fluentd
• BS degree in Computer Science or equivalent
• 2-4 years of cloud engineering experience with at least one of the leading public cloud
•
Additional Qualifications:
• Working experience development using Python/Go
• Experience working with fault tolerant and highly-available distributed systems
• Experience with big data systems and/or database administration (e.g.
PostgresSQL, Cassandra, etc) a plus;
• Strong linux environment / OS performance and troubleshooting skills
","['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet']","['optim', 'commun', 'big data']",1,"['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet', 'optim', 'commun', 'big data']","['public', 'infrastructur', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet', 'optim', 'commun', 'big data', 'public', 'infrastructur', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Team Overview
Far too many lives are lost in vehicle accidents each year.
Because when real people’s lives are at stake, driving safely 99% of the time isn't good enough.
Responsibilities
Implementation of BigData infrastructure and services
Quality control of data infrastructure and data service development
Automation of data infrastructure
Implementation of data transformation and streaming services
24/7 operation and site reliability of data services
Minimum Qualifications
MS in Computer Science or a related field
4+ years of relevant industry experience
Already shipped and operated Big Data systems in production environments
Experience in scalability of data systems
Hands-on Big Data software and infrastructure development skills
Hands-on experience of data storage and schemes like Avro, Parquet
Preferred Qualifications
Expert knowledge in rational, no-sql and distributed data stores
Operational experience with on-prem and cloud data systems
Very strong coding skills in Java, Python, Scala
Experience with large-scale ingestion architectures
Experience in cloud and on-premises data systems
Benefits & Perks
Timing: A start-up backed by industry leaders, at a critical stage of growth
Compensation: Competitive salaries and meaningful equity
Benefits: Comprehensive package (medical, dental, vision, and more)
Other: Paid lunch and dinner, team fitness, and fun team off-sites
All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws.
","['sql', 'scala', 'cloud', 'java', 'python']",['big data'],999,"['sql', 'scala', 'cloud', 'java', 'python', 'big data']","['stream', 'infrastructur', 'python', 'comput', 'packag', 'big', 'relat']","['sql', 'scala', 'cloud', 'java', 'python', 'big data', 'stream', 'infrastructur', 'python', 'comput', 'packag', 'big', 'relat']"
DE,"Job Summary:
Responsibilities:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for team members to assist them in building and optimizing analytics production.
Other duties as required.
This list is not meant to be a comprehensive inventory of all responsibilities assigned to this position
Required Qualifications/Skills:
Bachelor’s degree (B.S/B.A) from four-college or university and 5 to 8 years’ related experience and/or training; or equivalent combination of education and experience
Networks with senior internal and external personnel in own area of expertise
Demonstrates good judgment in selecting methods and techniques for obtaining solutions
Proficiency in languages SAS, Python, Java, and MatLab
ETL (Extract, Transform, and Load)
Database - SQL and NoSQL
Data Warehousing – Hadoop, MapReduce, HIVE, Presto
Operating system: UNIX, Linux
Follows standard practice and procedures when analyzing situations or data
Preferred Qualifications:
Programming language: Javascript, R
Basic Machine Learning
Scope:
Resolves a wide range of issues in creative ways
Seasoned, experienced professional with a full understanding of their speciality
Works on problems of a diverse scope
Receives little instruction on day to day work, general instruction on new assignments
Employment in the fast-growing IT space providing you with brilliant career options for years to come
Introduction to the new ways of working and awesome technologies
Career paths to help you establish where you want to go
A company-wide mentoring program to advise you along the way
Online training courses through CBT-nuggets to upskill you
Performance management system to provide you with meaningful, actionable feedback
Dedicated management to provide you with a point of leadership and care
Internal promotion focus.
Numerous on-the-job perks
Peer Recognition
Market competitive rates and benefits
","['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab']","['pipelin', 'machine learning', 'optim', 'data warehousing', 'etl']",1,"['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab', 'pipelin', 'machine learning', 'optim', 'data warehousing', 'etl']","['day', 'program', 'techniqu', 'python', 'etl', 'sourc', 'analyt', 'sa', 'judgment', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'set', 'machin', 'pipelin', 'divers', 'relat']","['sql', 'mapreduc', 'linux', 'sa', 'unix', 'hadoop', 'javascript', 'python', 'hive', 'java', 'nosql', 'r', 'matlab', 'pipelin', 'machine learning', 'optim', 'data warehousing', 'etl', 'day', 'program', 'techniqu', 'python', 'etl', 'sourc', 'analyt', 'sa', 'judgment', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'set', 'machin', 'pipelin', 'divers', 'relat']"
DE,"Responsibilities
Design, construct, test and maintain robust, reliable, and scalable data pipeline infrastructure
Ensure all systems meet the project requirements as well as industry practices
Investigate and integrate up-and-coming big data technologies into existing requirements
Recommend different ways to constantly improve data reliability and quality
Install/update system and component fault-tolerant procedures
Employ an array of technological tools to integrate with 3rd-party data systems
Research new uses for existing data
Develop set processes for data mining, data modeling, and data production
Collaborate with members of your team (e.g., data architects, data scientists, security researchers) on the project’s goals
Qualifications
5+ years of experience in design and implementation of robust and reliable big data infrastructure at scale, especially in supporting inline data ingestion, correlation, and aggregation of data from different data sources
Experience with designing real time and batch log analytics and search solutions using ELK stack
Experience with design, implementation, deployment and management of large Elasticsearch clusters and ELK solutions
Experience with performance tuning and optimization of Elasticsearch clusters
Experience with integration of Elasticsearch with Hadoop, RDBMS, Streaming technologies such as Kafka, Redis as well as and Data Science/Machine Learning frameworks at scale
Current hands-on experience with at least two coding and scripting languages e.g.
Python, Java, Scala, Shell Scripting
","['elasticsearch', 'scala', 'hadoop', 'java', 'python', 'kafka']","['recommend', 'research', 'pipelin', 'data mining', 'correl', 'tune', 'data modeling', 'machine learning', 'optim', 'big data', 'cluster']",999,"['elasticsearch', 'scala', 'hadoop', 'java', 'python', 'kafka', 'recommend', 'research', 'pipelin', 'data mining', 'correl', 'tune', 'data modeling', 'machine learning', 'optim', 'big data', 'cluster']","['analyt', 'machin', 'stream', 'learn', 'pipelin', 'set', 'hadoop', 'infrastructur', 'optim', 'python', 'parti', '3rd', 'scientist', 'big', 'integr', 'sourc']","['elasticsearch', 'scala', 'hadoop', 'java', 'python', 'kafka', 'recommend', 'research', 'pipelin', 'data mining', 'correl', 'tune', 'data modeling', 'machine learning', 'optim', 'big data', 'cluster', 'analyt', 'machin', 'stream', 'learn', 'pipelin', 'set', 'hadoop', 'infrastructur', 'optim', 'python', 'parti', '3rd', 'scientist', 'big', 'integr', 'sourc']"
DE,")* Has experience with different data storages (e.g., relational and NoSQL)* Has experience with containerization and other modern software development workflows* Takes initiative and ownership in a fast-paced environmentNice to have:* Expertise with multiple modern programming languages (Python, C++, Go, etc.
","['python', 'nosql']",[None],999,"['python', 'nosql']","['relat', 'program', 'python']","['python', 'nosql', 'relat', 'program', 'python']"
DE,"When applying for a job you are required to create an account, if you have already created an account - click Sign In.
Creating an account will allow you to follow the progress of your applications.
Please Capitalize first letter of your First and Last Name.
Please avoid using fully capitalized text for your First and/or Last Name.
NOTE: If your name is hyphenated or has multiple capitalization, please use the same format as your government ID.
Its vision is to “Provide Enterprise Product Data at lower cost and better quality”.
This position is focused on delivering Core Data solutions using modern technology to serve the various needs of the business.
The scope of the organization is global, and its data platforms serve a wide array of functions including Merchant, Partner, Operations, and Compliance business operations.
In this position you will also have the opportunity to work with stakeholders and users to understand their needs and partner with engineering to deliver the solution.
This position requires an individual who is comfortable working in cross-functional teams with a very high degree of analytical and technical skills.
In this role, the individual will be part of the engineering team in Global Product Data Services Organization and will be responsible for.
Participating and collaborating with Product Owner/ Cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.
Creativity and out of the box thinking is required.
Proactively anticipating problems and keeping the team and management informed in a timely manner.
Being flexible and being able to support all functions of product life cycle when required.
Ability to use data to draw insights or drive decisions.
Be able to explain complex technical concepts to management, product managers and other engineers.
Motivated and interested in delivering results, especially in the area of writing high-performance, reliable and maintainable code.
Ability to adapt to new development environments, changing business requirements and learning new systems highly desired.
Good team player, able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment
Excellent verbal and written communication skills.
Skills and Experience
8+ years of experience in the IT industry, experience in data engg is Mandatory.
Shell/ Perl scripting experience or proficiency in any programming language like Java/C/ C++
Hands on in Java programming
Proficient in Frameworks – Spring, Maven, Hibernate
Knowledge Of Real time Analytics
Strong fundamentals of object-oriented design, data structures, algorithms and design patterns
Expert in software engineering tools and best practices
Expert in design/implementation for reliability, availability, scalability and performance
Should have strong SQL programming skills
Knowledge of data warehousing concepts
Proficient in Big data Environments – Pig,Hive,MR
Excellent written and oral communication skills
Working experience in an Agile methodology is highly preferred.
Experience with Tableau or other visualization tools is a plus.
Knowledge of Scheduling Tools is a plus
Intermediate level knowledge on following technologies, with expertise on few of them:
Knowledge in MPP Databases/ Distributed systems
Knowledge on Data Encryption Standards is a huge plus
Exposure to Data Quality and Profiling tools is a plus.
Exposure BI tools desired, but not required (Micro strategy, Business Objects)
Basic level knowledge on following business domains is a plus:
Payments and banking
Subsidiary:
Travel Percent:
0
San Jose, California, United States of America
Additional Locations:
","['sql', 'perl', 'pig', 'bi', 'tableau', 'java', 'hive', 'c', 'excel']","['visual', 'data warehousing', 'big data', 'commun', 'account']",1,"['sql', 'perl', 'pig', 'powerbi', 'tableau', 'java', 'hive', 'c', 'excel', 'visual', 'data warehousing', 'big data', 'commun', 'account']","['analyt', 'program', 'visual', 'bi', 'avail', 'big', 'engin', 'algorithm']","['sql', 'perl', 'pig', 'powerbi', 'tableau', 'java', 'hive', 'c', 'excel', 'visual', 'data warehousing', 'big data', 'commun', 'account', 'analyt', 'program', 'visual', 'bi', 'avail', 'big', 'engin', 'algorithm']"
DE,"The Role
Responsibilities:
Work on ETL tools like SSIS and Informatica, Business Intelligence & Reporting tools like SSRS, SSAS and Tableau
Work with systems that handle sensitive data with strict SOX controls and change management processes
Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.
Provide timely and accurate estimates for newly proposed functionality enhancements
critical situation
Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.
Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.
Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.
Qualifications:
Minimum Qualifications:
5+ years of experience in SSIS, Informatica 9.X/10.X on in large/medium scale implementations
Must have strong experience in Data Warehouse ETL design and development, methodologies, tools, processes and best practices
Experience in Finance functional areas like planning and budgeting, accounting, business intelligence, procure-to-pay, order-to-cash
Understanding of SOX controls and audits procedures
Strong experience in stellar dashboards and reports creation for C-level executives
Preferred Qualifications:
3+ years of development experience in Open Source technologies like Python, Java
Experience in Big Data processing using Apache Hadoop/Spark ecosystem applications like Hadoop, Hive, Spark, Kafka and HDFS preferable
Excellent query writing skill and communication skills
Familiarity with common API's: REST, SOAP
","['spark', 'ssr', 'hadoop', 'tableau', 'c', 'hive', 'python', 'java', 'kafka', 'excel']","['big data', 'dashboard', 'financ', 'etl', 'commun', 'account']",1,"['spark', 'ssr', 'hadoop', 'tableau', 'c', 'hive', 'python', 'java', 'kafka', 'excel', 'big data', 'dashboard', 'financ', 'etl', 'commun', 'account']","['spark', 'hadoop', 'provid', 'python', 'warehous', 'big', 'etl', 'common', 'sourc']","['spark', 'ssr', 'hadoop', 'tableau', 'c', 'hive', 'python', 'java', 'kafka', 'excel', 'big data', 'dashboard', 'financ', 'etl', 'commun', 'account', 'spark', 'hadoop', 'provid', 'python', 'warehous', 'big', 'etl', 'common', 'sourc']"
DE,"Role: Data Engineer Intern
Experience: Internship
Must have skills :
Hands-on experience in database systems (Structured and Unstructured).
Programming in Python, R, SAS.
Overall knowledge and exposure on how to architect solutions in cloud platforms like GCP, AWS, Microsoft Azure.
Develop and maintain scalable data pipelines, with a focus on writing clean, fault-tolerant code.
Hands-on experience in data model design, developing Big Query/SQL (any variant) stored.
Optimize data structures for efficient querying of those systems.
Collaborate with internal and external data sources to ensure integrations are accurate, scalable and maintainable.
Collaborate with business intelligence/analytics teams on data mart optimizations, query tuning and database design.
Execute proof of concepts to assess strategic opportunities and future data extraction and integration capabilities..
Data extraction, Data cleansing and transformation.
Strong knowledge on REST APIs, Http Server, MVC architecture.
Knowledge on continuous integration/continuous deployment.
Preferred but not required:
Machine learning and Deep learning experience
Certification on any cloud platform is preferred.
Experience of data migration from On-Prem to Cloud environment.
Exceptional analytical, quantitative, problem-solving, and critical thinking skills
Excellent verbal and written communication skills
","['sql', 'gcp', 'azur', 'sa', 'aw', 'cloud', 'python', 'r', 'microsoft', 'excel']","['pipelin', 'cleans', 'machine learning', 'optim', 'clean', 'deep learning', 'commun']",999,"['sql', 'gcp', 'azur', 'sa', 'aw', 'cloud', 'python', 'r', 'microsoft', 'excel', 'pipelin', 'cleans', 'machine learning', 'optim', 'clean', 'deep learning', 'commun']","['analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'sa', 'optim', 'aw', 'python', 'big', 'quantit', 'sourc', 'engin', 'integr']","['sql', 'gcp', 'azur', 'sa', 'aw', 'cloud', 'python', 'r', 'microsoft', 'excel', 'pipelin', 'cleans', 'machine learning', 'optim', 'clean', 'deep learning', 'commun', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'sa', 'optim', 'aw', 'python', 'big', 'quantit', 'sourc', 'engin', 'integr']"
DE,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive.
Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership.
","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r']","['recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']",999,"['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl']","['predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']","['sql', 'linux', 'gcp', 'spark', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'r', 'recommend', 'pipelin', 'cleans', 'segment', 'predict', 'machine learning', 'optim', 'data warehousing', 'data modeling', 'big data', 'supervis', 'etl', 'predict', 'python', 'packag', 'etl', 'sourc', 'algorithm', 'analyt', 'ml', 'hadoop', 'optim', 'action', 'learn', 'infrastructur', 'amount', 'aw', 'big', 'set', 'engin', 'particip', 'machin', 'spark', 'pipelin']"
DE,"Job TITLE: Data Engineer
Term: Contract
Skill: Client is looking for a Data engineer who has passion and can drive for customer success.
Your primary focus will be on improving customer business outcomes through optimization and automation of data operations and data engineering.
Strong verbal and written communications as well as troubleshooting skills are critical for success in this role.
• Contribute in data engineering efforts
• Develop ETL and data integration processes
• Ensure that critical customer operational issues are addressed quickly and effectively
• Understand existing data environment, variations of implementation and develop effective triage mechanisms and tools
• Coordinate and collaborate with development team to remain current on code and technology
• Analyze individual issues in the context of overall platform to proactively identify larger problems
• Differentiate between issues that arise in operations, user code, third party libraries or product
• Plan and implement upgrades, changes for various implementations
• Maintain and manage SLA
• Work closely with off-shore team
• Drive for success across teams and customers
• Documentation of issues, resolutions and processes
• Develop Splunk Queries, Splunk custom inputs
Experience: • 5+ years of strong SQL and data pipeline development experience
• 3+ years of data pipeline development experience using any programming language like Python, C++, Java, Ruby
• 3+ years of experience developing backend platform services using RoR
• 2+ years of experience working with AWS and various services
• 1+ years of experience with Splunk
• Desire and ability to learn new technologies
• Strong written and verbal communications skills
• Experience working with off-shore teams
Education: • Prior data operations experience
• Splunk administration experience
• 1+ years development experience with map-reduce paradigm with hadoop-hive environment
• Any type of contribution to open source community
• Experience with any of the reporting tools
• Experience working with Elastic Search, Redis or other noSQL data source.
• Prior experience of Infrastructure Systems Engineer Jobs
","['sql', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'splunk', 'rubi']","['pipelin', 'optim', 'analyz', 'etl', 'commun']",999,"['sql', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'splunk', 'rubi', 'pipelin', 'optim', 'analyz', 'etl', 'commun']","['program', 'pipelin', 'input', 'hadoop', 'primari', 'optim', 'infrastructur', 'python', 'parti', 'aw', 'etl', 'sourc', 'engin', 'integr']","['sql', 'hadoop', 'aw', 'python', 'hive', 'java', 'nosql', 'splunk', 'rubi', 'pipelin', 'optim', 'analyz', 'etl', 'commun', 'program', 'pipelin', 'input', 'hadoop', 'primari', 'optim', 'infrastructur', 'python', 'parti', 'aw', 'etl', 'sourc', 'engin', 'integr']"
DE,"Posted: Apr 22, 2019
Role Number:
200054902
Do you thrive on working at analyzing state-of-the-art deep learning algorithms?
Do you have a passion for constructing automation pipelines?
Are you highly organized and detail-oriented?
This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality.
This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
The responsibilities of this position includes the following for current and future products:
Implement algorithm evaluation methods
Analyze data and build data analysis tools
Deep-dive failure analysis
Discover new perspectives for old data
Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
PhD or Masters in Computer Science
","['cloud', 'tableau', 'c', 'python', 'excel']","['computer vision', 'pipelin', 'data mining', 'visual', 'machine learning', 'deep learning', 'problem solving', 'statist', 'analyz', 'commun']",2,"['cloud', 'tableau', 'c', 'python', 'excel', 'computer vision', 'pipelin', 'data mining', 'visual', 'machine learning', 'deep learning', 'problem solving', 'statist', 'analyz', 'commun']","['machin', 'program', 'learn', 'pipelin', 'visual', 'evalu', 'infrastructur', 'python', 'comput', 'statist', 'set', 'engin', 'algorithm']","['cloud', 'tableau', 'c', 'python', 'excel', 'computer vision', 'pipelin', 'data mining', 'visual', 'machine learning', 'deep learning', 'problem solving', 'statist', 'analyz', 'commun', 'machin', 'program', 'learn', 'pipelin', 'visual', 'evalu', 'infrastructur', 'python', 'comput', 'statist', 'set', 'engin', 'algorithm']"
DE,"Note: By applying to this position your application is automatically submitted to the following locations: Mountain View, CA, USA; Boulder, CO, USA
Minimum qualifications:
Bachelor's degree in Computer Science, related technical field or equivalent practical experience.
Experience with one general purpose programming language (e.g., Java, C/C++, Python).
Experience in data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).
Experience designing data models and data warehouses and using SQL and NoSQL database management systems.
Preferred qualifications:
Advanced degree in engineering or technical/scientific field of study.
Experience designing data models and data warehouses and with non-relational data storage systems (NoSQL and distributed database management systems).
Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.
Experience in large scale distributed data processing.
Experience with Unix or GNU/Linux systems.
Excellent communication, organizational, and analytical skills.
About the job
As a Data Engineer, you’ll design and develop data systems and reporting tools to ensure that the Users and Products team members have the product, support, and operations data they need to make crucial business decisions.
Responsibilities
Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users and product problems.
Create extract, transform, and load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems.
Work closely with analysts to productionize various statistical and machine learning models using data processing pipelines.
Write and review technical documents, including design, development, and revision documents.
","['sql', 'linux', 'spark', 'airflow', 'unix', 'hadoop', 'python', 'java', 'c', 'nosql', 'excel']","['pipelin', 'machine learning', 'statist', 'etl', 'commun']",1,"['sql', 'linux', 'spark', 'airflow', 'unix', 'hadoop', 'python', 'java', 'c', 'nosql', 'excel', 'pipelin', 'machine learning', 'statist', 'etl', 'commun']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'python', 'comput', 'statist', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'linux', 'spark', 'airflow', 'unix', 'hadoop', 'python', 'java', 'c', 'nosql', 'excel', 'pipelin', 'machine learning', 'statist', 'etl', 'commun', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'relat', 'hadoop', 'python', 'comput', 'statist', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Data Engineer Consultant
You will design and build highly scalable and reliable modern data platforms including data lakes and data warehouse using Amazon Web Services, Azure, Google Cloud.
Your work will include a variety of core data warehousing tools, Hadoop, Spark, event stream platforms, and ETL tools such as Airflow.
In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.
Who are you?
You have passion for data!
You embrace a continuous learner mentality.
What technologies will you be using?
Every element of a modern data & analytics stack.
Qualifications:
Bachelor’s degree in Computer Engineering, Computer Science, Information Systems or related discipline
3+ years relevant experience
Experience in capturing end users requirements and align technical solutions to the business objectives
Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality
3+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
1+ years of experience working on AWS, GCP or Azure
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow
Experience working with relational databases
Experience with data serialization languages such as JSON, XML, YAML
Experience with code management tools (e.g.
Git, SVN) and DevOps tools (e.g.
Docker, Bamboo, Jenkins)
Strong analytical problem-solving ability
Great presentation skills, written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
","['sql', 'google cloud', 'gcp', 'spark', 'airflow', 'azur', 'bigqueri', 'git', 'hadoop', 'aw', 'snowflak', 'python', 'redshift', 'cloud', 'nosql', 'docker', 'amazon web services']","['pipelin', 'end user', 'data modeling', 'data warehousing', 'etl', 'commun']",1,"['sql', 'google cloud', 'gcp', 'spark', 'airflow', 'azur', 'bigqueri', 'git', 'hadoop', 'aw', 'snowflak', 'python', 'redshift', 'cloud', 'nosql', 'docker', 'pipelin', 'end user', 'data modeling', 'data warehousing', 'etl', 'commun']","['analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'google cloud', 'gcp', 'spark', 'airflow', 'azur', 'bigqueri', 'git', 'hadoop', 'aw', 'snowflak', 'python', 'redshift', 'cloud', 'nosql', 'docker', 'pipelin', 'end user', 'data modeling', 'data warehousing', 'etl', 'commun', 'analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'aw', 'python', 'comput', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Department: Data Engineering
Hours/Shift: Full Time
Reports To: VP, Data Engineering
The position is based in San Jose, CA., and reports into the Data Engineering division.
Responsibilities:
Develop high quality analytical data assets with an eye towards process efficiency and automation through scripting.
Experience in building data marts is a plus.
Build automated QA process to validate the quality of the data and report on data quality
Work closely with a dynamic and growing team of account managers, data engineers and data scientists to perform quantitative analysis of customer data, including gathering data requirements and validate data, applying judgement and statistical analysis to assist with planning and decision making.
Other responsibilities include but not limited to - data validation, troubleshooting issues, and process documentation.
Qualifications:
At least 3 years of hands-on experience in designing and building data pipelines, analytical data applications and BI Reporting.
Proficient in SQL and Tableau, familiar with at least one coding language in Python/Shell scripting.
Experience in using Cloud based managed services and Big Data Environments for data warehousing/analytics is a big plus – e.g.
Amazon RedShift, Google BigQuery, Spark, MapR etc.,
Very strong written and verbal communication skills; Ability to tell a story with the data
Analytical thinker, with an ability to evaluate multiple products/technologies to address various aspects of a big data platform.
Experience working on UNIX / Linux development and production environments
Experience working in Agile software development environments
Strong organization skills with attention to detail is a must.
Ability to manage multiple conflicting priorities, take proactive ownership of problems and outcomes, think outside the box
Knowledge of Retail and Financial verticals is useful but not required.
","['sql', 'linux', 'spark', 'bigqueri', 'unix', 'bi', 'tableau', 'python', 'redshift', 'cloud']","['pipelin', 'data warehousing', 'statist', 'big data', 'commun', 'account']",999,"['sql', 'linux', 'spark', 'bigqueri', 'unix', 'powerbi', 'tableau', 'python', 'redshift', 'cloud', 'pipelin', 'data warehousing', 'statist', 'big data', 'commun', 'account']","['analyt', 'asset', 'spark', 'pipelin', 'bi', 'python', 'scientist', 'statist', 'big', 'quantit', 'engin']","['sql', 'linux', 'spark', 'bigqueri', 'unix', 'powerbi', 'tableau', 'python', 'redshift', 'cloud', 'pipelin', 'data warehousing', 'statist', 'big data', 'commun', 'account', 'analyt', 'asset', 'spark', 'pipelin', 'bi', 'python', 'scientist', 'statist', 'big', 'quantit', 'engin']"
DE,"Title: Data Engineer
Duration: 6 months+
Key Responsibilities:
• Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
• Design and development of data warehouse using T-SQL, SQL, and python
• Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
• Implement data structures using best practices in data modeling, processes, and technologies.
• Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
• Writing analytics programs (transformations/calculations) in T-SQL,R, Python or comparable
• Knowledge and understanding of Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
• Knowledge and functional understanding of Finance, Global Supply Chain business processes
• Development with one or more data visualization/reporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI)
• Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
• Continually recommend, develop, and implement process improvements and tools to collect and analyze data, and visualize/present insights..
Skill/Job Requirements:
• Bachelor's degree in Business, MIS or related area.
Master's degree a plus.
• 8+ years Business Intelligence / Data Warehouse development experience
• 3+ years of experience in ETL development tools, preferably with knowledge of Microsoft Integration Services 2005 or greater (SSIS), SAP Data Services, SAP SLT and Python.
• 5+ years of experience in design and development using Microsoft SQL Server, SAP HANA and Snowflake databases.
• Strong experience in full life cycle development, implementation, management and performance tuning of the Enterprise Data Warehouse
• Experience in database development (T-SQL, PLSQL, and/or SQL scripts)
• Experience in building data pipelines using python, C# and JSON
• Experience in Microsoft BI development in Integration Services (SSIS), Analysis Services (SSAS) or Reporting Services (SSRS)
• Experience building and managing data flows to and from cloud applications
• Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses
• Experience visualizing data in business intelligence tools such as Tableau, Business Objects or Hana Analytics
• Experience and functional understanding with Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
• Strong experience with performance and scalability design and testing
• Experience creating test plans, testing and resolving data discrepancies
• Must be a self-motivated, energetic, detail oriented team player passionate about producing high quality BI & Analytics deliverables
• Strong sense of customer service for internal customers
• Medical robotics has unique characteristics that will require immersion in clinical and technical training and he or she must come up to speed quickly– an interest and desire to learn are critical
#LI-FRESH
","['sql', 'spss', 'ssr', 'salesforc', 'bi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi']","['pipelin', 'visual', 'tune', 'data modeling', 'financ', 'etl']",1,"['sql', 'spss', 'ssr', 'salesforc', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'pipelin', 'visual', 'tune', 'data modeling', 'financ', 'etl']","['analyt', 'program', 'pipelin', 'relat', 'visual', 'bi', 'python', 'warehous', 'clinic', 'etl', 'engin', 'integr']","['sql', 'spss', 'ssr', 'salesforc', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'pipelin', 'visual', 'tune', 'data modeling', 'financ', 'etl', 'analyt', 'program', 'pipelin', 'relat', 'visual', 'bi', 'python', 'warehous', 'clinic', 'etl', 'engin', 'integr']"
DE,"By pioneering a new technology category with an event streaming platform, which enables companies to leverage their data as a continually updating stream of events, not as static snapshots.
This innovation has led Coatue Management, Altimeter Capital and Franklin Templeton to join earlier investors Sequoia Capital, Benchmark, and Index Ventures in the recent Series E financing of a combined $250 million at a $4.5B valuation.
About the Team:
Data Engineers on the team will be the enabler and amplifiers.
This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.
About the Role:
They will also partner closely with Data Scientists and cross functional leaders to develop internal data products.
Data Engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits.
Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Responsibilities:
Implementing a solid, robust, extensible data warehousing design that supports key business flows
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; deploy inclusive data quality checks to ensure high quality of data
Developing strong subject matter expertise and manage the SLAs for those data pipelines
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with Data Scientists and business partners to develop internal data products to improve operational efficiencies organizationally
Building and growing partnership with cross functional teams, and evangelize data-driven culture
4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
Highly proficient in Python and SQL coding
Highly proficient with tuning and optimizing data models and pipelines
Experience in developing data apps with Python, Javascript, high charts etc.
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
What Gives You An Edge:
Experience with Apache Kafka
Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc.
Experience in developing data apps with Python, Javascript, high charts, etc.
#LI-MT1
Come As You Are
","['sql', 'salesforc', 'bi', 'javascript', 'python', 'kafka']","['pipelin', 'tune', 'optim', 'data warehousing', 'statist', 'financ', 'commun']",1,"['sql', 'salesforc', 'powerbi', 'javascript', 'python', 'kafka', 'pipelin', 'tune', 'optim', 'data warehousing', 'statist', 'financ', 'commun']","['stream', 'pipelin', 'relat', 'optim', 'bi', 'python', 'scientist', 'statist', 'comput', 'warehous', 'set', 'sourc', 'engin']","['sql', 'salesforc', 'powerbi', 'javascript', 'python', 'kafka', 'pipelin', 'tune', 'optim', 'data warehousing', 'statist', 'financ', 'commun', 'stream', 'pipelin', 'relat', 'optim', 'bi', 'python', 'scientist', 'statist', 'comput', 'warehous', 'set', 'sourc', 'engin']"
DE,"Minimum qualifications:
Bachelor’s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, a related quantitative field, or equivalent practical experience.
6 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position with experience in SQL and an additional object-oriented programming language (e.g., Python, Java).
Preferred qualifications:
Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.
Effective problem solving and analytical skills.
Ability to manage multiple projects and report simultaneously across different stakeholders.
Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs.
Attention to detail and effective verbal/written communication skills.
About the job
The Business Strategy & Operations organization provides business critical insights using analytics, ensures cross functional alignment of goals and execution, and helps teams drive strategic partnerships and new initiatives forward.
As a Data Engineer, you will take on big data challenges in an agile way.
You will build data pipelines that enable engineers, analysts and other stakeholders across the organization.
You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity.
Responsibilities
Build data pipelines and reports that enable analysts and other stakeholders across the organization.
Build data models to deliver insightful analytics while ensuring the highest standard in data integrity.
","['sql', 'java', 'python']","['big data', 'pipelin', 'data modeling', 'problem solving', 'statist', 'financ', 'commun', 'econom']",1,"['sql', 'java', 'python', 'big data', 'pipelin', 'data modeling', 'problem solving', 'statist', 'financ', 'commun', 'econom']","['analyt', 'program', 'challeng', 'pipelin', 'relat', 'python', 'comput', 'statist', 'big', 'quantit', 'engin', 'integr']","['sql', 'java', 'python', 'big data', 'pipelin', 'data modeling', 'problem solving', 'statist', 'financ', 'commun', 'econom', 'analyt', 'program', 'challeng', 'pipelin', 'relat', 'python', 'comput', 'statist', 'big', 'quantit', 'engin', 'integr']"
DE,"Title: Data Engineer
Duration: 6+ Months Contract
Interview Process: Phone & Skype
Must-Have:
2+ yearsexperience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase
Designs develop, and implement Hadoop eco-system-based applications to support business requirements.
Follows approved life cycle methodologies, creates design documents, and performs program coding and testing.
Resolves technical issues through debugging, research, and investigation.
Experience/Skills Required:
Bachelors degree in computer science, Information Technology, or related field and 5 years experience in computer programming, software development or related
2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase
Hands-on experience with Unix, Teradata and other relational databases.
Thanks & Regards!
Andy
Sr. Technical Recruiter
D: (848) 200 2354
andy(at)eateam.com
","['spark', 'unix', 'hadoop', 'hive', 'hbase']","['research', 'information technology', 'big data']",1,"['spark', 'unix', 'hadoop', 'hive', 'hbase', 'research', 'information technology', 'big data']","['program', 'spark', 'hadoop', 'comput', 'big', 'relat', 'engin']","['spark', 'unix', 'hadoop', 'hive', 'hbase', 'research', 'information technology', 'big data', 'program', 'spark', 'hadoop', 'comput', 'big', 'relat', 'engin']"
DE,"Data Engineer, Infrastructure Analytics
Mountain ViewR&D - DevOpsExperienced
Responsibilities
1.
Build and scale critical data pipelines for use-cases like performance monitoring, cost analysis, capacity planning, and other analytical scenarios.
Design and develop visualizations, monitors, and alerting systems to catch system issues and data anomalies, build automation to handle these issues intelligently.
Work with performance engineers and product engineering teams to analyze data from billions of mobile clients and hundreds of thousands of servers and systems to provide insightful and valuable information on an ongoing basis.
Qualifications
1.
3+ years of experience with Data Warehouse and open-source Big Data technologies
Experience building infrastructure to support real-time or offline data pipelines processing petabytes of data
Experience with Spark, Hive, Hadoop, SQL, Kafka, Parquet, HDFS, or HBase
Proficiency in multiple systems languages (Scala, Java, Python etc).
Desired qualification:
Strong ability to interact, communicate, present and influence within multiple levels of the organization using data
Experience in continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Share to
","['sql', 'spark', 'scala', 'hadoop', 'python', 'hive', 'java', 'hbase', 'kafka']","['pipelin', 'anomali', 'visual', 'big data', 'commun']",999,"['sql', 'spark', 'scala', 'hadoop', 'python', 'hive', 'java', 'hbase', 'kafka', 'pipelin', 'anomali', 'visual', 'big data', 'commun']","['basi', 'analyt', 'spark', 'pipelin', 'visual', 'hadoop', 'infrastructur', 'python', 'avail', 'warehous', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'python', 'hive', 'java', 'hbase', 'kafka', 'pipelin', 'anomali', 'visual', 'big data', 'commun', 'basi', 'analyt', 'spark', 'pipelin', 'visual', 'hadoop', 'infrastructur', 'python', 'avail', 'warehous', 'big', 'set', 'sourc', 'engin']"
DE,"Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!
Position Summary
Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies.
Shape provides industry-leading web and mobile security services designed to protect organizations against automated attacks that evade traditional security defenses.
The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB
Upbeat, positive teammate.
Qualifications
Bachelors degree in Computer science or similar relevant field with 2 + years of experience; or equivalent experience.
Equal Employment Opportunity
This policy applies to all aspects of employment, including, but not limited to, hiring, job assignment, compensation, promotion, benefits, training, discipline, and termination.
","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python']","['pipelin', 'dashboard', 'visual', 'optim', 'big data']",1,"['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data']","['pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'comput', 'big']","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data', 'pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'comput', 'big']"
DE,"About the Team:
The data engineering team is on a mission to create a hyper scale data lake, which helps finding bad actors and stop breaches.
The team builds and operates systems to centralize all of the data the falcon platform collects, making it easy for internal and external customers to transform and access the data for analytics, machine learning, and threat hunting.
Job Responsibilities :
Design, develop, and maintain a data platform that processes petabytes of data
Qualifications for Data Engineer :
They should also have experience with the following software/tools -
A solid understanding of algorithms, distributed systems design and the software development lifecycle
Solid background in Java/Scala and a scripting language like Python
Experience building large scale data pipelines
Strong familiarity with the Apache Hadoop ecosystem including : Spark, Kafka, Hive, Apache Presto, etc.
Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB
Good test driven development discipline
Reasonable proficiency with Linux administration tools
Proven ability to work effectively with remote teams
Experience with the following tools is desirable :
Go
Kubernetes
Jenkins
Parquet
Protocol Buffers/GRPC
#LI-JF1
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.
Notice of E-Verify Participation
Right to Work
","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'python', 'hive', 'java', 'nosql', 'postgr', 'mysql', 'kubernet', 'kafka']","['machine learning', 'pipelin']",999,"['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'python', 'hive', 'java', 'nosql', 'kubernet', 'kafka', 'machine learning', 'pipelin']","['analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'hadoop', 'python', 'relat', 'engin', 'particip', 'algorithm']","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'python', 'hive', 'java', 'nosql', 'kubernet', 'kafka', 'machine learning', 'pipelin', 'analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'hadoop', 'python', 'relat', 'engin', 'particip', 'algorithm']"
DE,"The next big thing in
data is you!
This
team focus on the Industrial internet of things and advanced analytics, includes
hardware engineers, data analysts, data engineers, and machine learning experts
who develop the IIoT platform and perform advanced analysis on engineering
data.
The Data Management Engineer will be responsible for
developing predictive analytic platforms.
This individual will work closely
with the data scientists, business users, and IT to identify, evaluate, design
and implement statistical and machine learning solutions.
The ideal candidate demonstrates a deep passion for applying
advanced analytic approaches, an eagerness to dig into large data sets, and a
vision for turning disparate data streams into a cohesive view for empowering a
diverse engineering community.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Focus on developing and implementing data
analytics solutions to production.
Be able to integrate data, analytics
algorithms and factory systems by using state of art IT infrastructure and open
source solutions, such as Kafka, Kubernetes and Jenkins etc.
Understand challenging business problems and
develop tools & techniques to find patterns and insights within structured
and unstructured data generated in nanoscale manufacturing environment
Prototype creative solutions for improving
product performance predictability, and be able to lead others (Domain/IT
stakeholders) in crafting and implementing smart factory solutions.
REQUIRED:
BS or MS in computer science, computer
engineering, or management of information systems.
New graduates are welcome to
Previous project management experience and
willingness to develop project management skills is a major plus.
Possesses ability to mathematically model
complex problems in machine vision, learning, and automation
Familiarity with open source software tools for
machine learning, deep learning and image analysis.
Prior container, Kafka and/or
Spark experience is a plus.
Experience/proficiency in at least one programming
language e.g.
Java, C++, and Python
Proficiency with statistical analysis tools
(e.g.
JMP, R, SAS)
Experience with analytics on Hadoop, Teradata
(MPP), and AWS Redshift desired.
Highly motivated, team player with strong
communication and collaboration skills, self-starter with willingness to learn,
master new technologies and clearly communicate results to technical and
non-technical audience.
The future.
It’s on you.
You &
storing the world’s data for more than 50 years.
Once, it was the most
The most game-changing companies,
solutions they need to capture, preserve, access, and transform their data.
Today’s
exceptional data challenges require your exceptional skills.
Digital® data-centric solutions are found under the G-Technology™, HGST,
SanDisk®, Tegile™, Upthere™, and WD® brands.
does not discriminate on the basis of race, color, ancestry, religion
(including religious dress and grooming standards), sex (including pregnancy,
childbirth or related medical conditions, breastfeeding or related medical
conditions), gender (including a person’s gender identity, gender expression,
and gender-related appearance and behavior, whether or not stereotypically
associated with the person’s assigned sex at birth), age, national origin,
sexual orientation, medical condition, marital status (including domestic
partnership status), physical disability, mental disability, medical condition,
genetic information, protected medical and family care leave, Civil Air Patrol
status, military and veteran status, or other legally protected characteristics.
listed above.
employment.
Employment Opportunity is the Law"" poster.
Federal and state laws require employers to provide reasonable accommodation to
job.
Examples of reasonable accommodation include making a change to
the application process or work procedures, providing documents in an alternate
format, using a sign language interpreter, or using specialized equipment.
If
For more information
click here.
Este empleador participa in E-Verify.
","['spark', 'sa', 'hadoop', 'aw', 'java', 'python', 'redshift', 'r', 'kubernet', 'kafka']","['hardwar', 'predict', 'machine learning', 'deep learning', 'statist', 'commun']",1,"['spark', 'sa', 'hadoop', 'aw', 'java', 'python', 'redshift', 'r', 'kubernet', 'kafka', 'hardwar', 'predict', 'machine learning', 'deep learning', 'statist', 'commun']","['basi', 'program', 'techniqu', 'stream', 'predict', 'python', 'sourc', 'algorithm', 'analyt', 'evalu', 'essenti', 'challeng', 'sa', 'hadoop', 'statist', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'divers', 'interpret', 'scientist', 'comput', 'relat']","['spark', 'sa', 'hadoop', 'aw', 'java', 'python', 'redshift', 'r', 'kubernet', 'kafka', 'hardwar', 'predict', 'machine learning', 'deep learning', 'statist', 'commun', 'basi', 'program', 'techniqu', 'stream', 'predict', 'python', 'sourc', 'algorithm', 'analyt', 'evalu', 'essenti', 'challeng', 'sa', 'hadoop', 'statist', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'divers', 'interpret', 'scientist', 'comput', 'relat']"
DE,"The Infrastructure Strategy group is responsible for the strategic analysis to support and enable the continued growth critical to Facebooks infrastructure organization.
This is a partnership-heavy role.
As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Facebooks Infrastructure organization.
Projects include analytics, ML modeling, tooling, services, and more.
The broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.
Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to see whats missing, identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.
Mininum Qualifications:
5+ years of Python development experience.
5+ years of SQL experience.
3+ years of experience with workflow management engines (i.e.
Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e.
Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Preferred Qualifications:
Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and E2E process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive']","['pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun']",999,"['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive', 'pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun']","['analyt', 'program', 'spark', 'pipelin', 'ml', 'azur', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'scientist', 'big', 'etl', 'engin']","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive', 'pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun', 'analyt', 'program', 'spark', 'pipelin', 'ml', 'azur', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'scientist', 'big', 'etl', 'engin']"
DE,"This is a full-time contract with negotiable pay rate.
Job responsibilities :
Craft machine learning and predictive models to drive intelligent product features.
Extend existing ML libraries and frameworks.
Research and implement appropriate ML algorithms and tools.
Provide technical leadership and influence data-driven optimization efforts.
Challenge and enrich yourself in an environment of like-minded engineers and data scientists, and most importantly have fun!
Requirements:
MS or PhD in Computer Science or a related quantitive field.
5+ years of related industry experience in a data science or engineering domain.
Development experience in a Python/ Java/ Scala.
Experience working in an Agile environment.
Extensive data modeling and data architecture skills.
Knowledge of Spark or other distributed cloud computing systems.
Experience developing, building and scaling machine learning models in business applications using large amounts of data Strong written and verbal communication skills
Thank you for applying!
","['spark', 'scala', 'cloud', 'java', 'python']","['research', 'predict', 'machine learning', 'optim', 'data modeling', 'commun']",2,"['spark', 'scala', 'cloud', 'java', 'python', 'research', 'predict', 'machine learning', 'optim', 'data modeling', 'commun']","['machin', 'learn', 'spark', 'challeng', 'ml', 'relat', 'predict', 'provid', 'optim', 'python', 'amount', 'scientist', 'comput', 'quantit', 'engin', 'algorithm']","['spark', 'scala', 'cloud', 'java', 'python', 'research', 'predict', 'machine learning', 'optim', 'data modeling', 'commun', 'machin', 'learn', 'spark', 'challeng', 'ml', 'relat', 'predict', 'provid', 'optim', 'python', 'amount', 'scientist', 'comput', 'quantit', 'engin', 'algorithm']"
DE,"Job Title: Data EngineerLocation: Santa Clara, CACompanyWork matters.
And the workplace of the future is going to be a great place.
The team is fast-paced while managing highly accurate detailed information.
* Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)* Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools* Working knowledge Tableau, Power BI is a plus* Strong analytical and problem-solving ability and be able to dive into technical details and design analytics solutions* Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively* Must have good communication, presentation, and documentation skills* Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products* Business process design, project management, and/or Agile SDLC experience a plus* 1-2 years of SAP HANA experience is a plus
","['sql', 'excel', 'bi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'power bi']","['dashboard', 'optim', 'commun', 'analyz']",999,"['sql', 'excel', 'powerbi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'optim', 'commun', 'analyz']","['analyt', 'program', 'power', 'optim', 'bi', 'python', 'comput', 'sourc']","['sql', 'excel', 'powerbi', 'tableau', 'python', 'r', 'microsoft', 'powerpoint', 'hana', 'dashboard', 'optim', 'commun', 'analyz', 'analyt', 'program', 'power', 'optim', 'bi', 'python', 'comput', 'sourc']"
DE,"Change control documentation Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
Design and development of data warehouse using T-SQL, SQL, and python Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Implement data structures using best practices in data modeling, processes, and technologies.
Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
Development with one or more data visualizationreporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI) Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
Continually recommend, develop, and implement process improvements and tools to collect and analyze data, and visualizepresent insights Skills Qualifications Bachelorrsquos degree in Business, MIS or related area.
Masterrsquos degree a plus.
8+ years Data engineering experience Has worked on 5-6 data source projects at a time Python experience preferred 3+ years of experience in ETL development tools, preferably with knowledge of Microsoft Integration Services 2005 or greater (SSIS), SAP Data Services, SAP SLT and Python.
SQL developer background is also accepted Documentation skills and can to handle heavy documentation Able to be versatile Able to read and understand codes and can make adjustment to parameters 5+ years of experience in design and development using Microsoft SQL Server, SAP HANA and Snowflake databases.
Experience in full life cycle development, implementation, management and performance tuning of the Enterprise Data Warehouse Experience in database development (T-SQL, PLSQL, andor SQL scripts) Experience in building data pipelines using python, C and JSON Experience in Microsoft BI development in Integration Services (SSIS), Analysis Services (SSAS) or Reporting Services (SSRS) Experience building and managing data flows to and from cloud applications Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses Experience visualizing data in business intelligence tools such as Tableau, Business Objects or Hana Analytics
","['sql', 'spss', 'ssr', 'bi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi']","['tune', 'etl', 'pipelin', 'data modeling']",1,"['sql', 'spss', 'ssr', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'tune', 'etl', 'pipelin', 'data modeling']","['analyt', 'pipelin', 'relat', 'bi', 'python', 'warehous', 'etl', 'sourc', 'engin', 'integr']","['sql', 'spss', 'ssr', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'tune', 'etl', 'pipelin', 'data modeling', 'analyt', 'pipelin', 'relat', 'bi', 'python', 'warehous', 'etl', 'sourc', 'engin', 'integr']"
DE,"About HouzzHouzz is the leading home renovation and design platform in the world.
You will also work with data infrastructure to build the necessary platform to house the data and intuitive tools to leverage the data.
or M.S.
",[None],[None],2,[],['infrastructur'],['infrastructur']
DE,"middot Strong SQL skills middot Can build pipelines and do project managementprogram management.
(but more technical than a program manager.
middot Good Python skills middot Having a strong technical background middot Excellent Comm skills
","['sql', 'python', 'excel']",['pipelin'],999,"['sql', 'python', 'excel', 'pipelin']","['program', 'python', 'pipelin']","['sql', 'python', 'excel', 'pipelin', 'program', 'python', 'pipelin']"
DE,"Its studios create popular, immersive mobile games and its technology brings games to more players around the world.
Learn more at applovin.com.
About You:
Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science
Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding
Experience independently creating and maintaining projects
Product focused mindset
Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka)
Experience designing, building, and maintaining data processing systems
Experience with a backend language such as Java or Scala
About the Role:
Collaborate with various engineering teams to meet a wide range of technological challenges
Work closely with product and business teams to improve data models that feed business intelligence tools
Perks:
Free medical, dental, and vision insurance
Daily lunches and fully stocked kitchen
Free public transit
Free laundry service (wash/dry clean)
Free gym membership
401k matching
Flexible Time Off - work hard and take time when you need it
Interested?
#LI-JZ1
","['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel']","['clean', 'big data']",1,"['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'clean', 'big data']","['learn', 'spark', 'challeng', 'public', 'hadoop', 'comput', 'big', 'engin', 'algorithm']","['spark', 'scala', 'pig', 'hadoop', 'java', 'kafka', 'excel', 'clean', 'big data', 'learn', 'spark', 'challeng', 'public', 'hadoop', 'comput', 'big', 'engin', 'algorithm']"
DE,"CAREERS
Data Engineer – Sunnyvale, CA
Sunnyvale, CA
You will also provide business insights, while leveraging internal tools and systems, databases and industry data.
THE IDEAL CANDIDATE WILL
Very Strong engineering skills.
Should have an analytical approach and have good programming skills.
Provide business insights, while leveraging internal tools and systems, databases and industry data
Minimum of 5+ years’ experience.
Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter
Ability to document requirements, data lineage, subject matter in both business and technical terminology.
Guide and learn from other team members.
Demonstrated ability to transform business requirements to code, specific analytical reports and tools
This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
Must Have
Strong analytical background
Self-starter
Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights
ELIGIBILITY CRITERIA
Technical Requirements
Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata)
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes
Real time data ingestion (Kafka)
Nice to Have
Development experience with Java, Scala, Flume, Python
Cassandra
Automic scheduler
R/R studio, SAS experience a plus
Presto
Hbase
Tableau or similar reporting/dash boarding tool
Modeling and Data Science background
Retail industry background
Education
BS degree in specific technical fields like computer science, math, statistics preferred
Send your CV to careers@tredence.com
","['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark']","['optim', 'statist', 'big data', 'etl', 'commun', 'math']",1,"['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark', 'optim', 'statist', 'big data', 'etl', 'commun', 'math']","['analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']","['sql', 'mapreduc', 'linux', 'python', 'java', 'hbase', 'kafka', 'pig', 'sa', 'unix', 'hadoop', 'hive', 'cassandra', 'scala', 'tableau', 'r', 'dash', 'excel', 'spark', 'optim', 'statist', 'big data', 'etl', 'commun', 'math', 'analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']"
DE,"middot Strong SQL skills middot Can build pipelines and do project managementprogram management.
(but more technical than a program manager.
middot Good Python skills middot Having a strong technical background middot Excellent Comm skills
","['sql', 'python', 'excel']",['pipelin'],999,"['sql', 'python', 'excel', 'pipelin']","['program', 'python', 'pipelin']","['sql', 'python', 'excel', 'pipelin', 'program', 'python', 'pipelin']"
DE,"Role and Responsibilities:
5-10 years of engineering experience as a Developer, Tester, DE-Manager.
Deep Understanding of Product Lifecycle, tools, and processes.
Deep understanding of product development in regards to the engineering aspects.
Good understanding of Machine Learning and Predictive Analytics fundamentals.
Set long term goals and drive a team to deliver on them.
Willing to tackle high-level objectives and translate them into meaningful outcomes in the direction of the team's charter.
Ability to lead and architect data modeling, to help predict areas for Software improvement.
Proven experience with Modeling and Simulation tools.
Proficient in R or Similar Statistical Package is a plus.
Understanding of high scale SW development models.
Good understanding of Machine Learning and Predictive Analytics fundamentals.
Python/R, Data Science, AI/ML.
Analytics, visualization, and dashboards (Grafana/Kibana, InfluxDB, Bokeh, D3.js, etc.).
Streaming data pipelines (e.g., Kafka, Apache Spark, Apache Beam).
Big-data stores (e.g., Apache Cassandra).
Data processing workflows (Luigi, Apache Airflow.
Education:
Bachelor's degree.
","['spark', 'airflow', 'cassandra', 'python', 'r', 'bokeh', 'kafka']","['pipelin', 'dashboard', 'visual', 'predict', 'machine learning', 'data modeling', 'statist']",1,"['spark', 'airflow', 'cassandra', 'python', 'r', 'bokeh', 'kafka', 'pipelin', 'dashboard', 'visual', 'predict', 'machine learning', 'data modeling', 'statist']","['analyt', 'machin', 'stream', 'learn', 'spark', 'ml', 'pipelin', 'visual', 'predict', 'python', 'packag', 'statist', 'big', 'set', 'engin']","['spark', 'airflow', 'cassandra', 'python', 'r', 'bokeh', 'kafka', 'pipelin', 'dashboard', 'visual', 'predict', 'machine learning', 'data modeling', 'statist', 'analyt', 'machin', 'stream', 'learn', 'spark', 'ml', 'pipelin', 'visual', 'predict', 'python', 'packag', 'statist', 'big', 'set', 'engin']"
DE,"If you are an independent problem solver, have a strong drive to excel, and are looking for opportunities to make a big impact, this is the right place for you.
Job Responsibilities:
Work closely across various development teams to support data processing pipelines
Write python, shell scripts and tools to facilitate customer data processing and product delivery
Proactively identify and fix sensor data and operational errors to minimize workflow latency
Monitor, troubleshoot and fix jobs running in cloud environment
Perform log analysis and present insightful conclusions/suggestions
Requirements:
BS, MS degree or higher in Computer Science, GIS or related field
More than 1 year working experience with Python and Shell scripts
A self-starter with the ability to work independently
Experience with AWS cloud platform and virtualization technologies like Docker is a plus
Experience with GIS or sensor data is plus
","['aw', 'python', 'docker', 'cloud']","['gi', 'pipelin']",1,"['aw', 'python', 'docker', 'cloud', 'gi', 'pipelin']","['pipelin', 'aw', 'python', 'comput', 'big', 'relat']","['aw', 'python', 'docker', 'cloud', 'gi', 'pipelin', 'pipelin', 'aw', 'python', 'comput', 'big', 'relat']"
DE,"intelligence (AI) based solutions that dramatically reduce claims costs by anticipating the needs of
claimants and helping align the best resources to meet those needs.
claims teams efficiently manage claims, reduce escalations and understand the drivers of complexity.
organizations.
developing the next generation of truly game-changing products in the Insurance Industry.
sense out of structured and unstructured data.
Data engineer will work with a senior member of the
team on the collecting, storing, processing, and analyzing huge sets of data.
The primary focus will be on
master data repositories, then maintaining, implementing, and monitoring.
You will also be responsible
Unique skills expected for this job is the ability to translate Python code into clean, high-quality Scala
Ability to create orchestration workflows that
ingest structured and unstructured data, enhances them and makes them available for use throughout
the platform.
Requirements:
 The ideal candidate will be focused on a mix of data engineering, software engineering and data
science knowledge
 Currently pursuing a MS or BS in Computer Science or a related field or equivalent experience
 Proficiency with Python or Scala or Java
 Knowledge/ development experience of popular Big Data and Machine Learning frameworks
like: Spark, Hadoop, Hive, Zeppelin
 Experience working with relational databases, query authoring (SQL) as well as familiarity with a
variety of databases
 Familiarity with OO design and implementation
 Excellent analytical and problem-solving skills
 Work in a dynamic, fast-paced, creative, collaborative and data driven environment
Nice to have skills:
 Experience in implementing ETL process
 Experience with Scala build systems: maven, ant, sbt
 Familiarity with AWS lambda, Athena & s3
 Flexible to work with multiple programming languages
","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 's3', 'hive', 'java', 'excel']","['machine learning', 'clean', 'big data', 'etl']",1,"['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 's3', 'hive', 'java', 'excel', 'machine learning', 'clean', 'big data', 'etl']","['analyt', 'machin', 'program', 'learn', 'spark', 'set', 'relat', 'hadoop', 'primari', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'aw', 'python', 's3', 'hive', 'java', 'excel', 'machine learning', 'clean', 'big data', 'etl', 'analyt', 'machin', 'program', 'learn', 'spark', 'set', 'relat', 'hadoop', 'primari', 'aw', 'python', 'avail', 'comput', 'big', 'etl', 'engin']"
DE,"Data Engineer
Team: Data Engineering
The Role:
The Data Engineer should be an expert familiar with all of the data warehousing technical components (e.g.
ETL, Reporting, Data Model), infrastructure (e.g.
hardware and software) and their integration.
The ideal candidate will be responsible for developing overall architecture, high-level design, building the data pipeline and implementing data preprocessing components.
The candidate must have extensive experience with Star Schemas, Dimensional Models, Data Marts, and Data Lake infrastructure.
Excellent written and verbal communication skills are required as the candidate will work very closely with diverse teams.
What You Will Do:
Drive for key results as an individual contributor.
Work with team members to implement features.
Follow team engineering process and contribute to the development of sound engineering infrastructure.
Maintain high-quality code base which is well factored, performant and robust.
Be able to learn and grow in a dynamic and fast growing start up environment.
Be familiar with the mainstream data processing and AI/ML technologies.
Other duties as assigned.
What You've Already Done:
You have a Bachelor's Degree in Computer Science or related quantitative field.
An advanced degree or equivalent practical work experience is a plus.
You have 4 + years of work experience.
2-4 years of software development experience in one or more object-oriented languages, including Python, Java, C#, or C++.
Demonstrated strength in data modeling, ETL development, and Data warehousing.
Experience with Relational Database like Oracle, MS SQL, MySQL, PostgreSQL
Experience with NoSQL and Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience with AWS services including S3, Glue, Redshift, Athena, QuickSight, EMR, Kinesis and RDS.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing
Experience with data pipeline, server architectures, and distributed systems is a plus.
Startup experience is a plus.
What You Already Know:
Languages: Python and SQL.
Experience with Bash/Shell, Java, C#/C++, R or Scala is a plus.
Frameworks/Libraries: Spark, Hadoop.
Experience with TensorFlow, Torch/PyTorch, Caffe, CNTK, Scikit-Learn, Samza, or Storm is a plus.
Databases: Experience with SQL Server, MongoDB, AWS Cloud Storage, Microsoft Azure Storage, Google Cloud Storage, Cassandra, Amazon Redshift, Apache Hive, or Google BigQuery is a plus.
Platforms: Experience with Windows Desktop/Server, Openstack/GCS/AWS, Firebase, Azure, Google Cloud Platform/App Engine.
Tools: Experience with Docker, Kubernetes, GitHub, Flink, Pulsar, or Kafka is a plus.
","['sql', 'firebas', 'python', 's3', 'redshift', 'nosql', 'tensorflow', 'java', 'scikit', 'pytorch', 'c', 'hbase', 'oracl', 'kafka', 'azur', 'pig', 'bigqueri', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'r', 'microsoft', 'mysql', 'excel', 'spark', 'cntk', 'cloud', 'postgresql', 'github', 'caff']","['pipelin', 'hardwar', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']",1,"['sql', 'firebas', 'python', 's3', 'redshift', 'nosql', 'tensorflow', 'java', 'scikit', 'pytorch', 'c', 'hbase', 'oracl', 'kafka', 'azur', 'pig', 'bigqueri', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'r', 'microsoft', 'excel', 'spark', 'cntk', 'cloud', 'postgresql', 'github', 'caff', 'pipelin', 'hardwar', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']","['learn', 'spark', 'pipelin', 'ml', 'azur', 'relat', 'divers', 'etl', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'quantit', 'engin', 'integr']","['sql', 'firebas', 'python', 's3', 'redshift', 'nosql', 'tensorflow', 'java', 'scikit', 'pytorch', 'c', 'hbase', 'oracl', 'kafka', 'azur', 'pig', 'bigqueri', 'hadoop', 'hive', 'kubernet', 'docker', 'mongodb', 'google cloud', 'cassandra', 'scala', 'aw', 'r', 'microsoft', 'excel', 'spark', 'cntk', 'cloud', 'postgresql', 'github', 'caff', 'pipelin', 'hardwar', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'learn', 'spark', 'pipelin', 'ml', 'azur', 'relat', 'divers', 'etl', 'hadoop', 'infrastructur', 'aw', 'python', 'comput', 'big', 'quantit', 'engin', 'integr']"
DE,"Posted: May 26, 2020
Weekly Hours: 40
Role Number:
200172241
Be at the forefront of the fight to improve health outcomes.
As a data engineer, you will play a key role in architecting, implementing, and managing big data pipelines, ML and analytic functions, and experimentation frameworks.
Key Qualifications
Workflow scheduling/orchestration such as Airflow or Oozie
Big data warehousing (RDB or MPP DB) such as Oracle, Teradata, Postgress, Hive
Strong Python programming skills with an understanding of data analytics, linear algebra, and ML libraries such as Numpy, Scipy
Experience with query APIs using JSON, ProtocolBuffers, or XML
ML model deployment, serving, and performance monitoring
As a data engineer, your day-to-day tasks will be the following.
Design, implement, and maintain end-to-end data pipelines with an understanding of ML lifecycles
Drive development of data products in collaboration with data scientists and analysts
Education & Experience
A background in computer science, mathematics, or similar quantitative field with a minimum of 2 years of professional experience; Masters preferred
Experience with AI/ML modeling using classical or deep learning a plus
Experience with on-device model deployment using CoreML or TFLite a plus
iOS app development skills a plus
Data visualization using Python, Tableau, or similar technologies a plus
","['airflow', 'numpi', 'db', 'tableau', 'python', 'hive', 'scipi', 'oracl']","['pipelin', 'visual', 'deep learning', 'data warehousing', 'big data']",2,"['airflow', 'numpi', 'db', 'tableau', 'python', 'hive', 'scipi', 'oracl', 'pipelin', 'visual', 'deep learning', 'data warehousing', 'big data']","['day', 'analyt', 'engin', 'pipelin', 'ml', 'visual', 'python', 'scientist', 'comput', 'big', 'quantit', 'linear']","['airflow', 'numpi', 'db', 'tableau', 'python', 'hive', 'scipi', 'oracl', 'pipelin', 'visual', 'deep learning', 'data warehousing', 'big data', 'day', 'analyt', 'engin', 'pipelin', 'ml', 'visual', 'python', 'scientist', 'comput', 'big', 'quantit', 'linear']"
DE,"* Process unstructured data into structured data, manage schema of new data.
* Manage data access to protect data in a safe way.
* Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
* Perform tasks such as writing scripts, write SQL queries, etc.
* Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis.
* Work closely with the engineering team to monitor product performance and track product quality trends.
* Analyze processed data.
* Monitoring data performance and modifying infrastructure as needed.
* Define data retention policies.Job Requirements:* 5 + years of recent experience in data engineering.
* Bachelor's Degree or more in Computer Science or a related field.
* Experiences on Cloudear CDH platform,Spark programming,Impala SQL Lauguage, Analyze data via Hive,etc.
* A solid track record of data management showing your flawless execution and attention to detail.
* Strong knowledge of and experience with statistics.
* Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives.
* Experience in C, Linux Shell, JavaScript or other programming languages is a plus.
* Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.
* Deep knowledge of data features engineering, data mining, machine learning, or information retrieval.
* Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
* Experience in production support and troubleshooting.
* You find satisfaction in a job well done and thrive on solving head-scratching problems.
* Language requirement: English, Mandarin is plus
","['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c']","['data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl']",1,"['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl']","['machin', 'program', 'learn', 'spark', 'relat', 'visual', 'infrastructur', 'optim', 'amount', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin']","['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'infrastructur', 'optim', 'amount', 'python', 'comput', 'statist', 'etl', 'sourc', 'engin']"
DE,"Posted: Jun 29, 2020
Role Number:
200178117
This is a unique opportunity to join a focused team and work collaboratively with other groups to make a significant impact.
Key Qualifications
Experience in high level programming languages such as Java, Scala, or Python.
Proficiency with databases and SQL is required.
Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience with large scale data warehousing, mining or analytic systems.
Ability to work with analysts to gather requirements and translate them into data engineering tasks
Aptitude to independently learn new technologies.
As a member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction.
This role is inherently cross-functional and the ideal candidate will work across disciplines.
This position involves working on a small team to develop large scale data pipelines and analytical solutions using Big Data technologies.
Successful candidates will have strong engineering skills and communication, as well as, a belief that data driven processes lead to great products.
You will need to have a passion for quality and an ability to understand complex systems.
Education & Experience
Bachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems.
","['sql', 'spark', 'scala', 'pig', 'hadoop', 'python', 'hive', 'java', 'kafka']","['data warehousing', 'commun', 'pipelin', 'big data']",1,"['sql', 'spark', 'scala', 'pig', 'hadoop', 'python', 'hive', 'java', 'kafka', 'data warehousing', 'commun', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'big', 'relat', 'engin']","['sql', 'spark', 'scala', 'pig', 'hadoop', 'python', 'hive', 'java', 'kafka', 'data warehousing', 'commun', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'big', 'relat', 'engin']"
DE,"About the Role
You'll create new, impactful ways for teachers and students to use online learning in the classroom.
You'll help build cutting-edge infrastructure to enable students of all backgrounds to succeed on high-stakes assessments like the SAT and LSAT.
Ultimately, you'll be part of helping millions of learners around the world unlock their full potential.
What you'll do:
Work on code that is used by millions of monthly active learners from all over the .
Model problems, then design, write, test and release code to solve them!
Collaborate with your peers via code reviews to level up yourself and others, to improve your ability to provide effective feedback, and to achieve higher quality code.
Ship products with a cross-functional team of engineers, designers, analysts and product managers.
Contribute to open source!
A few highlights:
Go + Services = One Goliath Project
Kotlin for Python Developers
You need:
At least 5 years experience building world-class product experiences for an interactive web-app
Experience building and maintaining complex software.
You'll write testable, quality code.
You'll push the team and the mission forward with your contributions.
Strong communication, thoughtfulness, and desire to give and receive regular feedback
Empathy for learners around the world.
You love learning and are excited about helping others learn to love learning.
You're motivated to learn new things and share what you learn with the world.
Experience/Interest in DevOps, specifically Jenkins build, Kubernetes, and some system configuration is a plus!
You don't need:
Highly competitive salaries and annual bonuses
Generous parental leave
Flexible work and time-off schedules to encourage work-family balance and holidays
Delicious catered lunch daily plus lots of snacks and beverages
A fun, high-caliber team that trusts you and gives you the freedom to be brilliant
The ability to improve real lives and the opportunity to work on high-impact software and programs that are already defining the future of education
Affinity groups where parents, Black and Latinx, women and gender minorities, and LGBT+ identified folks support one another
Briefly describe your experience building backend web services that deal with large volumes of requests per second.
Optional:
Links to projects or previous work.
No project is too small if it's something you care about.
Sal's TED talk from 2011
Sal's TED talk from 2015
You Can Learn Anything
","['kubernet', 'python']",['commun'],999,"['kubernet', 'python', 'commun']","['program', 'learn', 'infrastructur', 'python', 'releas', 'sourc', 'engin']","['kubernet', 'python', 'commun', 'program', 'learn', 'infrastructur', 'python', 'releas', 'sourc', 'engin']"
DE,"As a data engineer, you will build a solid data foundation that powers the entire spectrum from Business Intelligence to Artificial Intelligence.
This role will work closely with the Data Science and AI team and will focus on the enablement and acceleration of new and existing workflows.
This role is ideal for someone looking to extend software engineering skills into the field of Machine Learning and Artificial Intelligence.
Responsibilities:
• Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation
• Work closely with data scientists and analysts to create and deploy new features
• Write efficient and well-organized software to ship products in an iterative, continual-release environment
• Monitor and plan out core infrastructure enhancements
• Contribute to and promote good software engineering practices across the team
• Mentor and educate team members to adopt best practices in writing and maintaining production code
• Communicate clearly and effectively to technical and non-technical audiences
• Actively contribute to and re-use community best practices
Requirements:
• University or advanced degree in engineering, computer science, mathematics, or a related field
• Strong experience working with a variety of relational SQL and NoSQL databases
• Strong experience working with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with at least one cloud provider solution (AWS, GCP, Azure)
• Strong experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
• Ability to work in Linux environment
• Experience working with APIs
• Strong knowledge of data pipeline and workflow management tools
• Expertise in standard software engineering methodology, e.g.
unit testing, code reviews, design documentation
• Experience creating ETL processes that prepare data for consumption appropriately
• Experience in setting up, maintaining and optimizing databases for production usage in reporting, analysis and ML applications
• Working in a collaborative environment and interacting effectively with technical and non-technical team members equally well
• Relevant working experience with Docker and Kubernetes preferred
• Ability to work with ML frameworks preferred
Will be a plus:
Knowledge of CI/CI processes and components
Experience with OKTA and Optimizely
NB:
","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['pipelin', 'machine learning', 'big data', 'etl', 'commun']",2,"['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'machine learning', 'big data', 'etl', 'commun']","['machin', 'learn', 'spark', 'pipelin', 'azur', 'ml', 'relat', 'hadoop', 'infrastructur', 'provid', 'aw', 'python', 'scientist', 'comput', 'releas', 'big', 'etl', 'engin']","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'machine learning', 'big data', 'etl', 'commun', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'ml', 'relat', 'hadoop', 'infrastructur', 'provid', 'aw', 'python', 'scientist', 'comput', 'releas', 'big', 'etl', 'engin']"
DE,"XMotors.ai provides autonomous self-driving features for the Xiaopeng EV vehicles in China market.
You will be working with a team of data platform for the big data analysis, AI model validation, report visualization and deployment of various systems.
Responsibilities:
Collaborate with algorithm designers to specify required data sets that include both training and testing of ML/DL/CV algorithms
Analyze the data/results from AI algorithms and generate report
Request specific data sets with various driving scenarios by adding requests to recording plan
Responsible for quality of available data and maintenance of ground truth
Work with algorithms team to test algorithms based on the collected data
Experience and Skills:
2+ years of working experience in big data / deep learning analysis and validation.
Should have good Python programming skills, experience of vision libraries such as OpenCV will be a plus.
Should have worked with data science and computer vision, machine learning/deep learning teams, especially in the areas of data acquisition, data pipeline and data management.
Should have extensive experience with one or more of the following big data frameworks: Spark, Hadoop, HDFS, Kafka, DataBase, etc.
Should have experience with DevOps tools (GitHub, JIRA, Jenkins)
Experience in developing CV/ML algorithms is a plus.
Should be willing to work as part of teams.
Good communication skills
Good written and spoken English is a requirement
A fun, supportive and engaging environment
Opportunities to pursue and work on cutting edge technologies.
Snacks, lunches and fun activities.
Competitive salary
","['spark', 'jira', 'hadoop', 'python', 'github', 'kafka']","['computer vision', 'big data', 'pipelin', 'visual', 'machine learning', 'deep learning', 'analyz', 'commun']",999,"['spark', 'jira', 'hadoop', 'python', 'github', 'kafka', 'computer vision', 'big data', 'pipelin', 'visual', 'machine learning', 'deep learning', 'analyz', 'commun']","['machin', 'learn', 'spark', 'pipelin', 'ml', 'visual', 'hadoop', 'python', 'avail', 'comput', 'big', 'set', 'collect', 'algorithm']","['spark', 'jira', 'hadoop', 'python', 'github', 'kafka', 'computer vision', 'big data', 'pipelin', 'visual', 'machine learning', 'deep learning', 'analyz', 'commun', 'machin', 'learn', 'spark', 'pipelin', 'ml', 'visual', 'hadoop', 'python', 'avail', 'comput', 'big', 'set', 'collect', 'algorithm']"
DE,"Data Engineer
If you are a Data Engineer with experience, please read on!
+ Biotech/Pharma/Life Science experience strongly preferred +
A huge goal that other academic groups and companies are also pursuing, with large clinical trials enrolling many thousands of patients already underway.
+ Be part of a 2-year-old start-up that is growing exponentially over the next few years.
+ Competitive pay and full benefits package
What You Will Be Doing
- Design, develop, and maintain performance measure visualizations and reports using data visualization tools.
- Work with the Data Science team to understand the business needs to design an effective and dynamic visualization.
- Work the Product Management team to understand and gather the user experience data.
What You Need for this Position
3-5 years of experience in the following required:
- Python, Django, AWS, SQL
- Data Visualization tools such as Qlik Sense, Power BI, Tableau.
What's In It for You
Competitive Pay
Full Benefits Package
1.
Or
2.
E-mail directly for more information to Marcus.Quigley@cybercoders.com
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'qlik', 'django', 'bi', 'aw', 'python', 'tableau', 'power bi']",['visual'],999,"['sql', 'qlik', 'django', 'powerbi', 'aw', 'python', 'tableau', 'visual']","['visual', 'power', 'bi', 'aw', 'python', 'packag', 'clinic', 'engin']","['sql', 'qlik', 'django', 'powerbi', 'aw', 'python', 'tableau', 'visual', 'visual', 'power', 'bi', 'aw', 'python', 'packag', 'clinic', 'engin']"
DE,"Big Data Engineer
Job Summary:
Job Responsibilities:
Gather and process raw data at scale.
Process unstructured data into structured data, manage schema of new data.
Manage data access to protect data in a safe way.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Perform tasks such as writing scripts, write SQL queries, etc.
Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis.
Work closely with the engineering team to monitor product performance and track product quality trends.
Analyze processed data.
Monitoring data performance and modifying infrastructure as needed.
Define data retention policies.
Job Requirements:
3 + years of recent experience in data engineering.
Bachelor's Degree or more in Computer Science or a related field.
Experiences on Cloudear CDH platform,Spark programmingImpala SQL Lauguage, Analyze data via Hive,etc.
A solid track record of data management showing your flawless execution and attention to detail.
Strong knowledge of and experience with statistics.
Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives.
Experience in C, Linux Shell, JavaScript or other programming languages is a plus.
Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.
Deep knowledge of data features engineering, data mining, machine learning, or information retrieval.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
Experience in production support and troubleshooting.
You find satisfaction in a job well done and thrive on solving head-scratching problems.
Language requirement: English, Mandarin is plus
","['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c']","['big data', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl']",1,"['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c', 'big data', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl']","['machin', 'program', 'learn', 'spark', 'relat', 'visual', 'infrastructur', 'optim', 'amount', 'python', 'comput', 'statist', 'big', 'etl', 'sourc', 'engin']","['sql', 'linux', 'spark', 'scala', 'javascript', 'python', 'hive', 'java', 'c', 'big data', 'data mining', 'visual', 'machine learning', 'optim', 'statist', 'analyz', 'etl', 'machin', 'program', 'learn', 'spark', 'relat', 'visual', 'infrastructur', 'optim', 'amount', 'python', 'comput', 'statist', 'big', 'etl', 'sourc', 'engin']"
DE,"Data Engineer
Terms: Full-time
Data Engineer
About the Role
Strong data engineer able to:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements,
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources,
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs,
Experience building and optimizing data pipelines, architectures and data sets,
A successful history of manipulating, processing and extracting value from large disconnected datasets.
A plus:
Experience in creating reports and dashboards in Tableau.
Dataswarm (data pipeline framework in Python), Hive, Presto, Python, Scuba (in-memory database), SQL, Oracle, Tableau
Need more details?
For more information, visit www.trianz.com.
Job Type: Full-time
Experience:
relevant: 3 years (Preferred)
","['sql', 'tableau', 'python', 'hive', 'oracl']","['dashboard', 'optim', 'pipelin']",999,"['sql', 'tableau', 'python', 'hive', 'oracl', 'dashboard', 'optim', 'pipelin']","['pipelin', 'relat', 'infrastructur', 'optim', 'python', 'set', 'sourc', 'engin']","['sql', 'tableau', 'python', 'hive', 'oracl', 'dashboard', 'optim', 'pipelin', 'pipelin', 'relat', 'infrastructur', 'optim', 'python', 'set', 'sourc', 'engin']"
DE,"Responsibilities Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements.
Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture.
Skills Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new toolslanguages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity.
","['sql', 'linux', 'python', 'excel']","['etl', 'commun', 'pipelin', 'regress']",999,"['sql', 'linux', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress']","['pipelin', 'relat', 'integr', 'python', 'comput', 'etl', 'engin', 'algorithm']","['sql', 'linux', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress', 'pipelin', 'relat', 'integr', 'python', 'comput', 'etl', 'engin', 'algorithm']"
DE,"NG - 012
Data Engineer
San Jose, California, USA
JOB TITLE
Data Engineer
Job Duties
- Work with different data platforms to provide the most applicable storage and access protocol.
- Use ETL tools or workflow libraries to extract and transform data for ingestion into data platforms.
- Integration of data from multiple data sources.
- Design the solution.
- Enable analytics on the data that was extracted and transformed.
- Data Exploration.
Reporting.
- Development of the data set as per business userâ€™s expectations.
- Align stakeholders with their objectives, assist in data literacy and provide data access respective to their needs.
- Automate mundane work.
- Train users and establish agreements on the requirements
- Resolve bugs and/issues.
- Document all relevant artifacts.
Job Requirements
Required Bachelors or foreign equivalent in CS, CA, CIS, IT, MIS, Engineering (Any), or any related field.
Must be able to travel/relocate to various client sites throughout the U.S.
San Jose, CA.
",[None],['etl'],1,['etl'],"['analyt', 'relat', 'etl', 'set', 'sourc', 'engin', 'integr']","['etl', 'analyt', 'relat', 'etl', 'set', 'sourc', 'engin', 'integr']"
DE,"Posted: Jun 18, 2020
Weekly Hours: 40
Role Number:
200176045
Imagine what you could do here.
Bring passion and dedication to your job and there's no telling what you could accomplish.
The Product Marketing Customer Analytics team is seeking a data engineer to support customer analytics with advanced, scalable and robust architecture, tools, data products, and critical data pipelines that are optimized for rapid business intelligence, data analysis, and data science.
Key Qualifications
Proficient in SQL and programming (Python preferred)
Experience with MPP databases preferred
6+ years of experience in data engineering and ETL pipeline development.
3+ years of Spark development.
6+ years of experience in Big Data Technologies (Hadoop, MapReduce, Hive etc…).
Spark experience preferred.
Experience on Kubernetes, Docker preferred.
Technical
Experience in designing, developing, and managing a highly optimized, flexible, and scalable data platform for customer analytics.
Experience building a connected low latency data platform (highly distributed, scalable with high availability), and stitching together various large and disparate data sources for data analysis.
Deep experience in Big Data, Cloud and programming.
Deep experience in developing custom ETL frameworks and developing robust, low latency and fault tolerant data pipelines dealing with very high volumes.
Deep experience with relational databases and data warehouses (preferably MPP system such as Teradata), and optimizing SQL statements on large data set.
Deploy inclusive data quality checks to ensure high quality of data
Problem Solving
Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs.
Project Management / Product Design
Significant experience managing data engineering projects through all phases, including requirements, ETL, data quality assessments, and data exploration.
Communication
Strong documentation and technical writing skills.
Attention to detail and effective verbal/written communication skills.
Environment / Culture
Can work effectively on sometimes ambiguous data and constructs within a fast changing environment, tight deadlines and priority changes.
Education & Experience
Prefer:
BS/MS in Computer Science Quantitative Finance, Math, Physics or a related Engineering degree
","['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker']","['big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math']",1,"['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker', 'big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math']","['analyt', 'spark', 'pipelin', 'set', 'relat', 'etl', 'hadoop', 'optim', 'python', 'avail', 'warehous', 'comput', 'big', 'quantit', 'sourc', 'engin']","['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker', 'big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math', 'analyt', 'spark', 'pipelin', 'set', 'relat', 'etl', 'hadoop', 'optim', 'python', 'avail', 'warehous', 'comput', 'big', 'quantit', 'sourc', 'engin']"
DE,"Â
Duration: Long Term Contract
Relevant Experience: 8+ Years
$DOE/hr.
Â
Â
Experience with Bigdata ecosystem EMR, Hive, Spark/PySpark is a must.
Exposure to Cloud environment is required preferably AWS Discovering, analyzing, structuring and mining data Developing models supporting business use cases Deep understanding of data mining algorithms and statistical methods
Experience in successfully applying machine learning to real-world problems Proficient in python and packages for data analysis ( numpy, pandas, matplotlib)Â
Strong knowledge of extracting and processing data with RDBMS/MPP's Good communication skills and team player attitude
Thanks and look forward to working with you,
","['pyspark', 'spark', 'numpi', 'panda', 'matplotlib', 'aw', 'cloud', 'python', 'hive']","['machine learning', 'commun', 'statist', 'data mining']",2,"['pyspark', 'spark', 'numpi', 'panda', 'matplotlib', 'aw', 'cloud', 'python', 'hive', 'machine learning', 'commun', 'statist', 'data mining']","['machin', 'learn', 'spark', 'aw', 'python', 'statist', 'packag', 'â', 'algorithm']","['pyspark', 'spark', 'numpi', 'panda', 'matplotlib', 'aw', 'cloud', 'python', 'hive', 'machine learning', 'commun', 'statist', 'data mining', 'machin', 'learn', 'spark', 'aw', 'python', 'statist', 'packag', 'â', 'algorithm']"
DE,"What your role and responsibilities will be
Collaborate with Integration team to build ETL processes to ingest data into BI stores.
Work with the internal teams in understanding the client requirements and convert
them into technical solutions.
Be a team player in performing development work during the production life cycle.
Experience with building stream and batch data processing systems.
Gather and process complex raw data at scale (including writing scripts, calling APIs,
write SQL queries, etc.).
Design and develop data processing solutions that support high performing and scalable
analytic solutions.
What you'll need to succeed
3+ years in a data engineering role.
Advanced knowledge of SQL and SQL queries performance tuning.
Good experience with RDBMS (Potgres, MS SQL Server, Oracle, DB2 ... etc)
Good experience with REST APIs
Good Experience with NOSQL databases (HBase, Mongo DB etc)
Experience with Impala, Hive & Presto is an asset.
Good knowledge of the Spark/Hadoop ecosystem.
Good knowledge of Scala/Java is an asset
Good knowledge of Python and Shell scripting.
Familiarity with micro-services and lambda architecture is an asset.
","['sql', 'spark', 'scala', 'hadoop', 'bi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl']",['etl'],999,"['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl', 'etl']","['analyt', 'asset', 'stream', 'spark', 'hadoop', 'bi', 'python', 'etl', 'engin', 'integr']","['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl', 'etl', 'analyt', 'asset', 'stream', 'spark', 'hadoop', 'bi', 'python', 'etl', 'engin', 'integr']"
DE,"Duties:
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions.
Build data expertise and own data quality for allocated areas of ownership.
Design, build, optimize, launch and support new and existing data models and ETL processes in production.
Monitor and manage the SLA for all data sets and systems in allocated areas of ownership.
Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.
Skills:
5+ years hands-on experience with Linux and shell scripting 2+ years hands-on experience in MySQL database administration, implementation and maintenance 5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Data architecture, data modeling and schema design skills Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus Need to be able to travel to China periodically Mandarin language a plus
","['sql', 'linux', 'python', 'mysql', 'excel']","['optim', 'etl', 'commun', 'data modeling']",999,"['sql', 'linux', 'python', 'excel', 'optim', 'etl', 'commun', 'data modeling']","['set', 'infrastructur', 'optim', 'python', 'interpret', 'etl', 'engin']","['sql', 'linux', 'python', 'excel', 'optim', 'etl', 'commun', 'data modeling', 'set', 'infrastructur', 'optim', 'python', 'interpret', 'etl', 'engin']"
DE,"Job Responsibilities:
Nice to have (but not required) beginner or intermediate level java experience.
Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.
Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements.
Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements
Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture.
Skills:
Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new tools/languages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity.
Powered by JazzHR
N0WQZGog2K
","['sql', 'linux', 'java', 'python', 'excel']","['etl', 'commun', 'pipelin', 'regress']",999,"['sql', 'linux', 'java', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress']","['pipelin', 'relat', 'power', 'integr', 'primari', 'python', 'comput', 'etl', 'engin', 'algorithm']","['sql', 'linux', 'java', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress', 'pipelin', 'relat', 'power', 'integr', 'primari', 'python', 'comput', 'etl', 'engin', 'algorithm']"
DE,"Hello,
Â
Position:ÂData EngineerÂ
Duration:Â12+ months
Â
Â
Job Descrption:
Must-Have strong experienceÂonÂSQLÂ,Python,ÂETL.
RequiredÂSkills:
Â
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, and hands-on SQL coding.
4+ years of experience designing and developing complex ETL/ELT programs with the following Matillion, Python, etc
8+ year's experience developing complex SQL
3+ years' experience programming in Python, and/or Java
2+ year's experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with integration of data from multiple data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)
","['sql', 'java', 'python', 'aw']","['etl', 'data modeling']",999,"['sql', 'java', 'python', 'aw', 'etl', 'data modeling']","['analyt', 'program', 'infrastructur', 'aw', 'python', 'etl', 'sourc', 'â', 'integr']","['sql', 'java', 'python', 'aw', 'etl', 'data modeling', 'analyt', 'program', 'infrastructur', 'aw', 'python', 'etl', 'sourc', 'â', 'integr']"
DE,"The Opportunity:
What you will do:
Build infrastructure and abstractions that can enable anyone (engineer or data scientist) to craft a scalable ETL pipeline for whatever the purpose is: metrics, analysis, machine learning, dashboard visualizations
Work closely with ops team to monitor and tune existing infrastructure.
Make intuitive decisions about what services, frameworks, and capabilities need to be in place before they are needed.
Build and maintain a data collection system that robustly extracts meaningful data from multiple sources and data stores.
Build analytics and Machine learning platforms to collect, store, process, and analyze huge sets of data
Who you are:
BA/BS in Computer Science, Information Systems or related technical field.
3+ years of experience in Data Infrastructure, with Cloud SW experience a plus.
Experience in crafting and scaling data infrastructure, models, and pipelines
Hands-on experience with a variety of data infrastructures, such as:
Processing: Spark, Flink, Hadoop, Lambda
Messaging: Kafka, Zookeeper, Pulsar
Storage: Hive, Mongo DB, Athena, Phoenix, Splice, Redshift, DynamoDB
Machine Learning: Sagemaker, H2O, Keras, NumPy
Open and active in sharing knowledge as well as excellent communication skills
Programming experience in one or more application or systems languages including Scala, Java, or Python
Have an ability to own a project from inception to completion
","['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel']","['pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun']",1,"['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel', 'pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun']","['analyt', 'machin', 'learn', 'engin', 'spark', 'pipelin', 'set', 'relat', 'visual', 'hadoop', 'infrastructur', 'python', 'scientist', 'comput', 'etl', 'sourc', 'collect']","['kera', 'spark', 'numpi', 'scala', 'sagemak', 'h2o', 'hadoop', 'cloud', 'lambda', 'python', 'redshift', 'hive', 'java', 'db', 'kafka', 'excel', 'pipelin', 'dashboard', 'visual', 'machine learning', 'analyz', 'etl', 'commun', 'analyt', 'machin', 'learn', 'engin', 'spark', 'pipelin', 'set', 'relat', 'visual', 'hadoop', 'infrastructur', 'python', 'scientist', 'comput', 'etl', 'sourc', 'collect']"
DE,"Responsibilities:
Develop highly reliable data pipelines using technologies like Apache Spark, PySpark.
Follow AWS best practices and industry standards.
Ensure high quality on the business deliverables and raise the bar on quality of data in Datawarehouse.
Contribute to the development of event processing pipelines for near real-time analytics use cases.
Flawless communication within internal teams and across the organization.
Work independently on business deliverables and mentor team on near real-time data pipelines.
REQUIRES BACHELOR'S DEGREE IN COMPUTER SCIENCE OR RELATED FIELD PLUS 6 YEARS OF EXPERIENCE.
3 YEARS OF EXPERIENCE WORKING AS A DEVELOPER IN DATA ENGINEERING WITH DATA PIPELINES AND DATA PROCESSING.
ANY AMOUNT OF HANDS-ON EXPERIENCE WITH TECHNOLOGIES LIKE APACHE SPARK, PYSPARK, AND OTHER STREAM PROCESSING FRAMEWORKS.
ANY AMOUNT OF EXPERIENCE WORKING WITH DATA FORMATS SUCH AS APACHE AVRO, APACHE PARQUET, AND COMMON METHODS IN DATA TRANSFORMATION.
ANY AMOUNT OF EXPERIENCE WITH AWS BIG DATA TECHNOLOGY STACK.
Must also have authority to work permanently in the U.S.
","['aw', 'pyspark', 'spark']","['commun', 'pipelin', 'big data']",1,"['aw', 'pyspark', 'spark', 'commun', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'aw', 'amount', 'comput', 'big', 'relat', 'common', 'engin']","['aw', 'pyspark', 'spark', 'commun', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'aw', 'amount', 'comput', 'big', 'relat', 'common', 'engin']"
DE,"Position Summary
The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills and Qualifications
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB
","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python']","['pipelin', 'dashboard', 'visual', 'optim', 'big data']",999,"['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data']","['pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'big']","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data', 'pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'big']"
DE,"Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market (https://goo.gl/ERiQgY).
During 2018, Hiretual achieves 500% growth with minimal focus on sales to date (https://goo.gl/ZTA1D9).
As a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS.
You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.
The core technical skills you should have:
• Strong computer science fundamentals: algorithms, data structures, and object-oriented programming
• Strong coding capability with Python
• must have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETL
• Proficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.
• Proficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMR
• Writing scalable REST APIs for web services
Benefits
• Unlimited growth/promotion space
• Competitive salary and options
• 401k matching program
• Free and nice food
• Comprehensive medical, dental, and life insurance
• PTO policy
• Commuter benefits
• Fun, collaborative, and energetic team environment with nice office environment
• Fun events for family!
","['spark', 'elasticsearch', 'cassandra', 'hadoop', 'python', 'hive', 'mysql', 'kafka']","['research', 'graph', 'pipelin', 'clean', 'big data', 'etl']",999,"['spark', 'elasticsearch', 'cassandra', 'hadoop', 'python', 'hive', 'sql', 'kafka', 'research', 'graph', 'pipelin', 'clean', 'big data', 'etl']","['program', 'spark', 'pipelin', 'power', 'hadoop', 'infrastructur', 'python', 'comput', 'big', 'etl', 'engin']","['spark', 'elasticsearch', 'cassandra', 'hadoop', 'python', 'hive', 'sql', 'kafka', 'research', 'graph', 'pipelin', 'clean', 'big data', 'etl', 'program', 'spark', 'pipelin', 'power', 'hadoop', 'infrastructur', 'python', 'comput', 'big', 'etl', 'engin']"
DE,"Commitment Level: 40 hours per week, on-site, contract through December 2020
Overview:
- Real-time Data processing using Kafka, XML, Python and SQLAlchemy
- ETL Data processing module developed in Python and SQL – Postgres
- Rest API module developed in Java
What You’ll Do:
● Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements
● Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging
● Writing reusable and efficient code in Java, Python and SQL
● Develop Rest APIs using java libraries
● Write unit, functional, regression tests for enhanced feature, maintain engineering documentation
● Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture
Must Haves:
● 5+ years of SQL experience having hands-on experience in writing, debugging and optimizing SQL queries, functions and stored procedures
● 3+ years in writing Java-based code, debugging and knowledge of REST API
● 2+ years of experience with hands-on experience in writing, debugging Python code on Linux
● Basic understanding of regular Linux commands and usage
● Strong knowledge of Computer Science fundamentals - OOPS, Data Structures and Algorithms.
● Experience writing Java and python applications that interact with ORM (Object Relational Mapper) libraries
● Able to integrate multiple data sources and databases
● Strong knowledge in XML and JSON parsing with unit test and debugging skills
● Knowledge in Kafka, EMS queues or any messaging platform gateway application
● Willingness and ability to learn new tools/languages as needed
● Process-oriented with excellent verbal and written communication skill with a desire for customer service
● Experience and knowledge of ETL’s and basics of Data warehousing
Education: Bachelor's Degree required
M-F, 40 hours/week.
Health Benefits: Medical, Dental, Vision, Life (including spouse & child), 401k, STD/LTD, AD&D, and Commuter Benefits program.
","['sql', 'linux', 'java', 'python', 'postgr', 'kafka', 'excel']","['pipelin', 'regress', 'data warehousing', 'etl', 'commun']",1,"['sql', 'linux', 'java', 'python', 'kafka', 'excel', 'pipelin', 'regress', 'data warehousing', 'etl', 'commun']","['program', 'pipelin', 'relat', 'python', 'algorithm', 'comput', 'etl', 'sourc', 'engin', 'integr']","['sql', 'linux', 'java', 'python', 'kafka', 'excel', 'pipelin', 'regress', 'data warehousing', 'etl', 'commun', 'program', 'pipelin', 'relat', 'python', 'algorithm', 'comput', 'etl', 'sourc', 'engin', 'integr']"
DE,"· Expert Data Engineering skills
· Expert Python skills
· Experience with web development technologies such as HTML and CSS + JavaScript a plus
· Python scripting skills for performance and efficiency
· Knowledge of SQL, setup and tuning of databases
· Linux knowledge and shell scripting.
Job Types: Full-time, Temporary, Contract
Experience:
Data Engineering: 5 years (Required)
sql: 5 years (Required)
Python Scripting: 3 years (Required)
Work Remotely:
Temporarily due to COVID-19
","['sql', 'javascript', 'linux', 'python']",['tune'],999,"['sql', 'javascript', 'linux', 'python', 'tune']","['python', 'engin']","['sql', 'javascript', 'linux', 'python', 'tune', 'python', 'engin']"
DE,", 2020
Role Number:
200175306
Imagine what you could do here.
Key Qualifications
Strong programming skills in C++, Java, Scala, or Python
Deep understanding of basic data structures and algorithms
Experience with scaling data platforms to hundreds of terabytes or petabytes using Spark or Hadoop
Proficient at schema design and data modeling
Familiarity with modern machine learning techniques
Creative, collaborative, & product focused
Curious about new technologies and passionate about exploring new use cases
Passion for customer privacy
You will have strong engineering and communication skills, as well as a belief that data driven processes lead to great products.
Education & Experience
BS, MS, or PhD in Computer Science, Computer Engineering, Statistics, Bioinformatics, Applied Mathematics, or equivalent experience
Additional Requirements
","['spark', 'scala', 'hadoop', 'java', 'python']","['bioinformat', 'data modeling', 'machine learning', 'statist', 'commun']",1,"['spark', 'scala', 'hadoop', 'java', 'python', 'bioinformat', 'data modeling', 'machine learning', 'statist', 'commun']","['machin', 'techniqu', 'spark', 'hadoop', 'python', 'comput', 'statist', 'appli', 'engin', 'algorithm']","['spark', 'scala', 'hadoop', 'java', 'python', 'bioinformat', 'data modeling', 'machine learning', 'statist', 'commun', 'machin', 'techniqu', 'spark', 'hadoop', 'python', 'comput', 'statist', 'appli', 'engin', 'algorithm']"
DE,"Â
JD:
10+ years of experience
Experience in Advance SQL, Python, ETL, Data Modelling, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
Should be in BI Engineer
Python (Nice to have)
Please share resumes to bpolice@esharpedge.com
","['sql', 'bi', 'tableau', 'python']","['etl', 'pipelin']",999,"['sql', 'powerbi', 'tableau', 'python', 'etl', 'pipelin']","['pipelin', 'bi', 'python', 'â', 'etl', 'engin']","['sql', 'powerbi', 'tableau', 'python', 'etl', 'pipelin', 'pipelin', 'bi', 'python', 'â', 'etl', 'engin']"
DE,"Job Title:
Data Engineer
Address:
5918 STONERIDGE MALL RD, PLEASANTON, California 94588
The Data Engineering team at Albertsons Companies is looking for an experienced Data Engineer to work for the most transformational food and drug retailers in the United States.
The Information Technology Department has an opening for a Data Engineer.
This position is located in Pleasanton, California.
Position Purpose
Data Engineering at Albertsons is inspired to build best in class customer experience and revolutionize the food and drug retail industry.
You will enjoy working with one of the richest data sets in the world, cutting edge technology, and the ability to see your insights turned into business impacts on regular basis.
The candidate will have a background in computer science or a related technical field with experiences working with large data sets, analytical platforms, and a passion towards enabling data-driven decision making.
A successful candidate will be both technically strong and business savvy, with a passion to make an impact through creative storytelling and timely actions.
You are a self-starter, smart yet humble, with a bias for action.
Key Responsibilities include, but are not limited to:
Design and build highly scalable data pipelines and analytical platforms using new generation tools and technologies like Spark, Kafka, and other cloud technologies to ingest and process real time and near real time data from various systems
Build and optimize data pipelines using SQL and other programming languages to process and store data following complex business requirements
Show strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency
Implement, communicate, and enforce best practices and standards
Work with cross functional teams across the organization in varying roles
Provide production support as needed
Qualifications:
BS in Computer Science, Information Systems, Engineering or Mathematics
5+ years of experience in developing, tuning, and supporting data pipelines in a big data environment, preferably in the Cloud; Azure and Snowflake experience is a plus
5+ years of experience in creating scalable pipelines for both real time and near real time data for analytical and operational use cases
5+ years of experience with Spark, Spark Streaming, Kafka, or related technologies
5+ years of experience with at least one programming language like Python or Java
Experience with Azure Databricks, Datafactory or related technologies
Experience in working with - flat files, XML, JSON, Avro files and databases
Experience with workflow management and scheduling tools like Airflow
Experience in handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities
Knowledge of Jenkins for continuous integration and End-to-End automation
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements
Diversity is fundamental at Albertsons Companies.
A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.
The Albertsons Companies policy is to provide employment, training, compensation, promotion and other conditions of employment without regard to race, color, religion, sexual orientation, gender identity, national origin, sex, age, disability, veteran status, medical condition, marital status or any other legally protected status.
AN EQUAL OPPORTUNITY EMPLOYER
","['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka']","['pipelin', 'tune', 'big data', 'information technology', 'commun']",1,"['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka', 'pipelin', 'tune', 'big data', 'information technology', 'commun']","['basi', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'azur', 'set', 'relat', 'divers', 'provid', 'python', 'comput', 'action', 'big', 'integr', 'engin']","['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka', 'pipelin', 'tune', 'big data', 'information technology', 'commun', 'basi', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'azur', 'set', 'relat', 'divers', 'provid', 'python', 'comput', 'action', 'big', 'integr', 'engin']"
DE,"""Any intelligent fool can make things bigger, more complex, and more violent.
It takes a touch of genius -- and a lot of courage -- to move in the opposite direction.""
- Einstein
Responsibilities:
• Design and develop highly scalable and available real time analytics platform using Spark, Kafka, Cassandra, and Elasticsearch for large data input streams
• Work closely with data science and UI teams to define and implement various analytics features related to product
• Configure, monitor, and optimize Spark and related infrastructure
Requirements:
• Strong desire to work for an early stage startup and be a part of its success
• Strong in Map-Reduce, parallelizing computations, and identifying bottleneck computations
• Strong in Scala and Python
• Experience in configuring and tuning Spark and Kafka systems
• Good understanding on Spark UI to extract useful information on application stages, and identify bottlenecks
• B.S.
or higher degree in Computer Science or equivalent
Pluses:
• Experience with Cassandra, Elasticsearch, Mongo
• Experience with Ganglia and able to correlate information from various UIs to diagnose efficiency issues
• Experience working with AWS
• Experience with Spray to build REST endpoints
","['spark', 'einstein', 'elasticsearch', 'cassandra', 'scala', 'aw', 'python', 'kafka']",[None],1,"['spark', 'einstein', 'elasticsearch', 'cassandra', 'scala', 'aw', 'python', 'kafka']","['analyt', 'stream', 'spark', 'input', 'infrastructur', 'aw', 'avail', 'python', 'comput', 'relat']","['spark', 'einstein', 'elasticsearch', 'cassandra', 'scala', 'aw', 'python', 'kafka', 'analyt', 'stream', 'spark', 'input', 'infrastructur', 'aw', 'avail', 'python', 'comput', 'relat']"
DE,"When applying for a job you are required to create an account, if you have already created an account - click Sign In.
Creating an account will allow you to follow the progress of your applications.
Please Capitalize first letter of your First and Last Name.
Please avoid using fully capitalized text for your First and/or Last Name.
NOTE: If your name is hyphenated or has multiple capitalization, please use the same format as your government ID.
Its vision is to “Provide Enterprise Product Data at lower cost and better quality”.
This position is focused on delivering Core Data solutions using modern technology to serve the various needs of the business.
The scope of the organization is global, and its data platforms serve a wide array of functions including Merchant, Partner, Operations, and Compliance business operations.
In this position you will also have the opportunity to work with stakeholders and users to understand their needs and partner with engineering to deliver the solution.
This position requires an individual who is comfortable working in cross-functional teams with a very high degree of analytical and technical skills.
In this role, the individual will be part of the engineering team in Payment OPS and will be responsible for.
Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.
Planning the execution of the project in an effective and efficient manner.
Approaching the problem, taking into account all possibilities.
Creativity and out of the box thinking is required.
Proactively anticipating problems and appropriately communicating to the team and management in a timely manner.
Being flexible and being able to support all functions of product life cycle when required.
Ability to work in a fast-paced environment
Ability to deliver from coarse grained requirements
Be a Mentor for the junior members in the organization.
Required Skills
8+ years of experience in the IT industry, experience in Data Technology space is preferred.
Advanced Shell or Perl scripting experience or proficiency in any programming language like C, C++ or CORE Java
Working experience in any MPP systems, should have strong SQL programming skills
Knowledge of data warehousing concepts
Working knowledge on any ETL tool (i.e.
Informatica/ Ab Initio) is preferable.
Knowledge of Scheduling Tools is a plus
Excellent written and oral communication skills
Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions
Expertise in database programming and performance tuning techniques
Familiar with data movement techniques and best practices to handle large volumes of data
Experience with data warehousing architecture and data modeling best practices
Experience with File Systems, server architectures, and distributed systems
Strong communication skills and willingness to take initiative to contribute beyond basic responsibilities
Working experience in an Agile methodology is highly preferred
Knowledge of Hadoop, HBase and Hive is highly preferred
Required Skills
Knowledge in MPP Databases/ Distributed systems
Knowledge on Big data Environments is a plus (Hadoop etc).
Exposure to Data Quality and Profiling tools is a plus.
Exposure to BI tools desired, but not required (Micro strategy, Business Objects).
Basic Qualifications
Bachelor/Master’s in Engineering/ Science degree with Mathematics or equivalent experience
8+ years of experience in IT
Subsidiary:
Travel Percent:
0
San Jose, California, United States of America
Additional Locations:
","['sql', 'perl', 'hadoop', 'bi', 'java', 'hive', 'c', 'hbase', 'excel']","['data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'account']",1,"['sql', 'perl', 'hadoop', 'powerbi', 'java', 'hive', 'c', 'hbase', 'excel', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'account']","['analyt', 'program', 'techniqu', 'hadoop', 'bi', 'big', 'etl', 'engin']","['sql', 'perl', 'hadoop', 'powerbi', 'java', 'hive', 'c', 'hbase', 'excel', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'account', 'analyt', 'program', 'techniqu', 'hadoop', 'bi', 'big', 'etl', 'engin']"
DE,"Experience in Advance SQL, Python, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['bi', 'python', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'bi', 'python', 'pipelin']"
DE,"Job Details:
Job Title: Data Engineer
Duration: 9-12 Months Contract
• 6-8 years of overall experience
• Excellent SQL and data querying skills.
• Experience with SQL Tuning using good sql coding practices
• Experience with Data Warehousing development, preferably Teradata is a plus
• Experience with Cloud Data Warehouse, preferably Snowflake is a plus.
• Background to ANSI SQL
• Sound knowledge of Database concepts and database architecture
Thanks & Regards
Ram Kishor
Phone: 732-452-1006*238
ram.kishor@diverselynx.com
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['sql', 'cloud', 'snowflak', 'excel']","['data warehousing', 'tune']",999,"['sql', 'cloud', 'snowflak', 'excel', 'data warehousing', 'tune']","['basi', 'warehous', 'engin', 'evalu']","['sql', 'cloud', 'snowflak', 'excel', 'data warehousing', 'tune', 'basi', 'warehous', 'engin', 'evalu']"
DE,")* Has experience with different data storages (e.g., relational and NoSQL)* Has experience with containerization and other modern software development workflows* Takes initiative and ownership in a fast-paced environmentNice to have:* Expertise with multiple modern programming languages (Python, C++, Go, etc.
","['python', 'nosql']",[None],999,"['python', 'nosql']","['relat', 'program', 'python']","['python', 'nosql', 'relat', 'program', 'python']"
DE,"Job Title: Data Engineer
Duration: 12 Months
RESPONSIBILITIES
Partner with Product and Engineering teams to solve problems and identify trends and opportunities
MINIMUM QUALIFICATIONS
BA/BS in Computer Science, Math, Physics, Engineering, Statistics or other technical field
Experience in SQL or other programming languages
Development experience in any scripting language (PHP, Python, Perl, etc.)
Experience communicating the results of analyses with product and leadership teams to influence the strategy of the product
Knowledge of statistics (e.g.
hypothesis testing, regressions)
Experience manipulating data sets through statistical software (ex.
R, SAS) or other methods
","['sql', 'perl', 'sa', 'php', 'python', 'r']","['regress', 'statist', 'math']",1,"['sql', 'perl', 'sa', 'php', 'python', 'r', 'regress', 'statist', 'math']","['program', 'sa', 'python', 'comput', 'statist', 'set', 'engin']","['sql', 'perl', 'sa', 'php', 'python', 'r', 'regress', 'statist', 'math', 'program', 'sa', 'python', 'comput', 'statist', 'set', 'engin']"
DE,"Key Qualifications
Experience working with big data, such as Hadoop ecosystem, Spark, Kafka, etc.
Development experience in Java, Scala, or Python
Solid understanding of database fundamentals, with proficiency in SQL
Experiencebuilding and optimizingETL data pipelines
Enthusiastic, highly motivated, and able to work collaboratively
Broad understanding ofvarious technologies and frameworkswith ability tocombine intopracticalsolutions
Familiarity withprovisioning, installing, configuring, and maintaining data platform solutions
Education & Experience
BS degree in Computer Science or related field, 4+ years of industry experience
MS degree in Computer Science or related field, 2+ years of industry experience
Were doing work that matters.
","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka']","['pipelin', 'big data']",1,"['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka', 'pipelin', 'big data']","['spark', 'pipelin', 'hadoop', 'python', 'comput', 'big', 'relat']","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'kafka', 'pipelin', 'big data', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'big', 'relat']"
DE,"Should have an analytical approach and have good programming skills.
Provide business insights, while leveraging internal tools and systems, databases and industry data Minimum of 5+ yearsrsquo experience.
Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter Ability to document requirements, data lineage, subject matter in both business and technical terminology.
Guide and learn from other team members.
Demonstrated ability to transform business requirements to code, specific analytical reports and tools This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
Must Have Strong analytical background Self-starter Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights Technical Requirements Knowledgeexperience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport) Advanced SQL (preferably Teradata) Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data Experience in UnixLinux shell scripting or similar programmingscripting knowledge Experience in ETL processes Real time data ingestion (Kafka) Nice to Have Development experience with Java, Scala, Flume, Python Cassandra Automic scheduler RR studio, SAS experience a plus Presto Hbase Tableau or similar reportingdash boarding tool Modeling and Data Science background Retail industry background Education BS degree in specific technical fields like computer science, math, statistics preferred
","['sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'sa', 'kafka', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'unixlinux', 'excel']","['optim', 'statist', 'big data', 'etl', 'commun', 'math']",1,"['sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'sa', 'kafka', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'unixlinux', 'excel', 'optim', 'statist', 'big data', 'etl', 'commun', 'math']","['analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']","['sql', 'mapreduc', 'spark', 'cassandra', 'scala', 'pig', 'sa', 'kafka', 'hadoop', 'tableau', 'python', 'hive', 'java', 'hbase', 'unixlinux', 'excel', 'optim', 'statist', 'big data', 'etl', 'commun', 'math', 'analyt', 'program', 'spark', 'set', 'sa', 'hadoop', 'provid', 'optim', 'python', 'comput', 'statist', 'big', 'etl', 'engin']"
DE,"Position Role/Tile: Data Engineer
Python programming skills.
Must have used python in ETL process.
Pull data from API and move data to Snowflake
Good in SQL
Suite #214
Newark, CA 94560
","['sql', 'python']",['etl'],999,"['sql', 'python', 'etl']","['etl', 'python', 'engin']","['sql', 'python', 'etl', 'etl', 'python', 'engin']"
DE,"middot Expert Data Engineering skills middot Expert Python skills middot Experience with web development technologies such as HTML and CSS + JavaScript a plus middot Python scripting skills for performance and efficiency middot Knowledge of SQL, setup and tuning of databases middot Linux knowledge and shell scripting.
","['sql', 'javascript', 'linux', 'python']",['tune'],999,"['sql', 'javascript', 'linux', 'python', 'tune']","['python', 'engin']","['sql', 'javascript', 'linux', 'python', 'tune', 'python', 'engin']"
DE,"Jobs Itility-US
Data Engineer
Working in teams (consisting of Hadoop data engineers, Hadoop data warehouse engineers, and platform engineers) that are building and managing Hadoop stacks.
The teams install, configure and manage Hadoop ecosystem components.
As Hadoop data engineer, you are responsible for the functional part of provisioning data – e.g.
building data ingestion pipelines and data connectors.
You work closely with the data scientists and business intelligence engineers who are using this data to create analytical models.
After taking inventory of an application, all found servers, storage, network and database configurations will be transferred to the new data center.
You are well acquainted with the complete Hadoop stack.
In addition, you have practical experience of being part of a DevOps team.
Further requirements:
Bachelor of Science / master’s degree in Computer Science, System Administration, or any other IT infrastructure or software related study with a passion for the automation side of IT infrastructure
Minimum two to three years of relevant work experience
Capable of building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets of structured, semi-structured and unstructured data
Experience in building data products incrementally and integrating and managing data sets from multiple sources
Data quality oriented
Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture
Hortonworks Certified Hadoop Developer and/or Cloudera Certified Hadoop Developer and/or Certified Hadoop Administrator
Knowledge of continuous integration & delivery tooling: e.g.
Jira, Git, Jenkins, Bamboo
Coding proficiency in at least one modern programming language (Python, Ruby, Java)
Strong verbal and written communication skills
Good documenting capabilities
You have a hands-on mindset, a strong customer focus, a problem-solving orientation and can show fast results
You have a clear focus on results and quality.
Willingness to travel to the Netherlands if required for training or project work
Bachelor of Science / master’s degree
Minimum 3-5 years of relevant work experience within an enterprise environment
Advanced knowledge of RHEL 6 & 7
Advanced knowledge of VMWare 5 & 6; VCAP5-DCD, VCDX5-DCV preferred
Experience with Cisco UCS manager and NetApp FAS / ScaleIO storage solutions
Experience with databases (MSSQL, Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
Willingness to travel to the Netherlands if required for training or project work
Do you like to go above and beyond?
Do you want to work with passion for what you do, in a team of people fueled by the same passion?
You believe in
Build efficient and highly reliable data ingestion pipelines for the Hadoop stack
Own data quality and data knowledge around all data that you touch
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production
Understand the entire life cycle of data that flows through any systems for which you are responsible
Pay constant attention and effort to the reliability of your pipelines
San Jose, CA.
Contact person
Share:
Share on linkedin
Share on twitter
Share on facebook
Share on whatsapp
Share on email
","['git', 'jira', 'hadoop', 'java', 'python', 'oracl', 'rubi']","['commun', 'pipelin']",1,"['git', 'jira', 'hadoop', 'java', 'python', 'oracl', 'rubi', 'commun', 'pipelin']","['analyt', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin']","['git', 'jira', 'hadoop', 'java', 'python', 'oracl', 'rubi', 'commun', 'pipelin', 'analyt', 'pipelin', 'set', 'relat', 'hadoop', 'infrastructur', 'python', 'avail', 'scientist', 'comput', 'warehous', 'integr', 'sourc', 'engin']"
DE,"The hire will be responsible for the development, architecture, administration, testing, support, project management, and governance of data integrations and APIs.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will define and build data pipelines that will enable faster, better, data-informed decision-making within the business.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Responsibilities
Contribute in each phase of enabling data analytics e.g.
collection, processing, governance, etc.
of the data.
Design, develop and support ETL mappings using tools like Oracle Data Integrator and Mulesoft for the business integration needs.
Build and deliver high quality data architecture to support business analysis and customer reporting needs.
Interface with other teams to extract, transform, and load data from a wide variety of data sources.
Write optimized SQL statements for data extraction, performance optimization and integration.
Basic Qualifications
Demonstrated strength in data modeling, ETL development, and Data warehousing.
Hands-on experience orchestrating integration use cases involving multiple systems and complex business logic binding in the system, processes, and RESTful APIs.
Ability to work in multiple programming languages such as Python, Java, SQL etc.
Proven success in communicating with users, other technical teams, and senior management to collect requirements and describe data modeling decisions and data engineering strategy.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
Project management and organizational skills.
Nice to Have
Experience with big data tools and distributed systems such as Hadoop, Spark, Kafka, etc.
Experience with Cloud implementations with major providers (GDC, AWS, Azure, etc.).
Experience of Business Intelligence (BI) tools such as Tableau, Power BI or QlikView.
Exposure to and/or interest in machine learning and data science specifically to help solve day-to-day problems and reach objectives in an innovative way.
Comfortable working in a Linux environment.
","['sql', 'linux', 'spark', 'azur', 'hadoop', 'bi', 'aw', 'python', 'java', 'tableau', 'cloud', 'kafka', 'oracl', 'power bi']","['pipelin', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']",999,"['sql', 'linux', 'spark', 'azur', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'kafka', 'oracl', 'pipelin', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']","['day', 'program', 'provid', 'python', 'etl', 'integr', 'sourc', 'collect', 'analyt', 'azur', 'hadoop', 'optim', 'learn', 'bi', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'power']","['sql', 'linux', 'spark', 'azur', 'hadoop', 'powerbi', 'aw', 'python', 'java', 'tableau', 'cloud', 'kafka', 'oracl', 'pipelin', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl', 'day', 'program', 'provid', 'python', 'etl', 'integr', 'sourc', 'collect', 'analyt', 'azur', 'hadoop', 'optim', 'learn', 'bi', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'power']"
DE,"eHealthInsurance has many exciting career opportunities in a number of locations, across various functions.
Data Engineer
This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.
Responsibilities:
Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Work with data infrastructure team to triage issues and support issue resolution.
Minimum Qualifications:
Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
2+ years of experience with designing, implementing and maintaining scalable and reliable data pipelines
Mastery of SQL in writing complex and high performance queries
Working experience with MPP systems (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Production coding experience with Python, Scala or Java and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Knowledge of AWS data tools.
Excellent communication skills.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
Nice to Have:
Experience collaborating with data science team.
Strong experience in designing and implementing data APIs.
Product familiarity with Adobe Analytics, Cisco systems, Snowflake.
Familiarity with workflow management tools (Airflow).
Working experience with data warehousing.
Ability to create beautiful data visualizations using D3, Tableau, or similar tools.
Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys.
Experience in building healthcare data pipelines would be a big plus.
Knowledge of healthcare insurance industry, products, systems, business strategies, and products.
Experience working with call center operations.
","['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho']","['healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun']",1,"['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho', 'healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun']","['visual', 'python', 'etl', 'quantit', 'integr', 'analyt', 'statist', 'learn', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'pipelin', '3rd', 'scientist', 'comput', 'relat']","['mongodb', 'sql', 'spark', 'airflow', 'scala', 'excel', 'd3', 'unix', 'git', 'aw', 'snowflak', 'python', 'hive', 'java', 'nosql', 'tableau', 'pentaho', 'healthcar', 'pipelin', 'visual', 'data modeling', 'machine learning', 'data warehousing', 'statist', 'etl', 'commun', 'visual', 'python', 'etl', 'quantit', 'integr', 'analyt', 'statist', 'learn', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'pipelin', '3rd', 'scientist', 'comput', 'relat']"
DE,"Your primary role will be to design and build data pipelines.
You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive.
In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies.
If you believe you have these skills please email your resume to info@springml.com.
Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR
","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker']","['clean', 'visual', 'pipelin', 'big data']",1,"['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data']","['program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data', 'program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']"
DE,"The Information Technology Department has an opening for a Data Engineer.
This position is located in Pleasanton, California.
Position Purpose
You will enjoy working with one of the richest data sets in the world, cutting edge technology, and the ability to see your insights turned into business impacts on regular basis.
The candidate will have a background in computer science or a related technical field with experiences working with large data sets, analytical platforms, and a passion towards enabling data-driven decision making.
A successful candidate will be both technically strong and business savvy, with a passion to make an impact through creative storytelling and timely actions.
You are a self-starter, smart yet humble, with a bias for action.
Key Responsibilities include, but are not limited to:
Design and build highly scalable data pipelines and analytical platforms using new generation tools and technologies like Spark, Kafka, and other cloud technologies to ingest and process real time and near real time data from various systems
Build and optimize data pipelines using SQL and other programming languages to process and store data following complex business requirements
Show strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency
Implement, communicate, and enforce best practices and standards
Work with cross functional teams across the organization in varying roles
Provide production support as needed
Qualifications:
BS in Computer Science, Information Systems, Engineering or Mathematics
5+ years of experience in developing, tuning, and supporting data pipelines in a big data environment, preferably in the Cloud; Azure and Snowflake experience is a plus
5+ years of experience in creating scalable pipelines for both real time and near real time data for analytical and operational use cases
5+ years of experience with Spark, Spark Streaming, Kafka, or related technologies
5+ years of experience with at least one programming language like Python or Java
Experience with Azure Databricks, Datafactory or related technologies
Experience in working with - flat files, XML, JSON, Avro files and databases
Experience with workflow management and scheduling tools like Airflow
Experience in handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities
Knowledge of Jenkins for continuous integration and End-to-End automation
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements
A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.
AN EQUAL OPPORTUNITY EMPLOYER
","['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka']","['pipelin', 'tune', 'big data', 'information technology', 'commun']",1,"['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka', 'pipelin', 'tune', 'big data', 'information technology', 'commun']","['basi', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'azur', 'set', 'relat', 'divers', 'provid', 'python', 'comput', 'action', 'big', 'integr', 'engin']","['sql', 'spark', 'airflow', 'azur', 'cloud', 'snowflak', 'python', 'java', 'kafka', 'pipelin', 'tune', 'big data', 'information technology', 'commun', 'basi', 'analyt', 'program', 'stream', 'spark', 'pipelin', 'azur', 'set', 'relat', 'divers', 'provid', 'python', 'comput', 'action', 'big', 'integr', 'engin']"
DE,"Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!
Position Summary
Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies.
Shape provides industry-leading web and mobile security services designed to protect organizations against automated attacks that evade traditional security defenses.
The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB
Upbeat, positive teammate.
Qualifications
Bachelors degree in Computer science or similar relevant field with 5+years of experience; Master’s degree with 3+ years; or equivalent experience.
Equal Employment Opportunity
This policy applies to all aspects of employment, including, but not limited to, hiring, job assignment, compensation, promotion, benefits, training, discipline, and termination.
","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python']","['pipelin', 'dashboard', 'visual', 'optim', 'big data']",1,"['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data']","['pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'comput', 'big']","['mongodb', 'gcp', 'bigqueri', 'bigtabl', 'aw', 'cloud', 'java', 'python', 'pipelin', 'dashboard', 'visual', 'optim', 'big data', 'pipelin', 'visual', 'primari', 'aw', 'optim', 'python', 'comput', 'big']"
DE,"10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['bi', 'python', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'bi', 'python', 'pipelin']"
DE,"Start: Immediate
Duration: 12 Months+
Â
This is a highly technical requirement.
Must be senior level.
Candidate must have:
Very Strong SQL, Python + Hive
Familiarity with Hadoop
Know how to transform Data & put in Hadoop
Must be able to translate the business req into technical specs
Should have strong Data Analyst skills
Work closely with IT Team.
Â
","['sql', 'python', 'hive', 'hadoop']",[None],999,"['sql', 'python', 'hive', 'hadoop']","['python', 'â', 'hadoop']","['sql', 'python', 'hive', 'hadoop', 'python', 'â', 'hadoop']"
DE,"2+ years of experience in developing production quality code in Python/C/C++
1+ years of experience in data warehousing and SQL
Experience in running applications in the cloud (AWS preferred) is a plus
Track record of taking on open ended problems and implementing robust solutions
Experience in SSD/NAND Flash development and validation a plus
Experience in USB, Serial port, PCIe knowledge is a plus
BS/MS in Engineering or related technical discipline
","['sql', 'aw', 'cloud', 'python', 'c']",['data warehousing'],1,"['sql', 'aw', 'cloud', 'python', 'c', 'data warehousing']","['aw', 'relat', 'python', 'engin']","['sql', 'aw', 'cloud', 'python', 'c', 'data warehousing', 'aw', 'relat', 'python', 'engin']"
DE,"The Business
GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.
The Role
You are the Scotty Pippin to the Michael Jordans.
You are the Xavi to the Messis.
You'll do things like:
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience
Non negotiables:
SQL
Python
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract
Salary: $100,000.00 /year
Work Remotely:
Yes
","['sql', 'tableau', 'python']","['commun', 'pipelin']",2,"['sql', 'tableau', 'python', 'commun', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']","['sql', 'tableau', 'python', 'commun', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']"
DE,"If you thrive on working with big data in high performance teams then this is the place for you.
You would work on data and build some of the tools that are critical to moving & transforming this data into valuable and insightful information.
Creating reliable, scalable, and high performance products requires exceptional technical expertise and practical experience working with large-scale distributed systems.
Finally, you will tackle challenging issues of scale, reliability and security while delivering a delightful, simple user experience to a global user base.
RESPONSIBILITIES
You will manage data warehouse plans for a product or a group of products.
You will interface with engineers, product managers and product analysts to understand data needs.
In addition, you will design, build and launch new data extraction, transformation and loading processes in production.
You will work with data infrastructure to triage infra issues and drive to resolution.
You will use your expert coding skills across a number of languages from Python, Scala, Java and PHP and work across multiple teams in high visibility roles.
REQUIREMENTS
2+ years of Scala and/or Python development experience is necessary
2+ years of SQL (Oracle, Vertica, Hive, etc) experience is required
2+ years of experience in custom or structured (ie.
Informatica/Talend/Pentaho) ETL design, implementation and maintenance
2+ years or experience applying statistical data analysis to real-life problems
Experience working with either a Map Reduce or a MPP system on any size/scale
BS or MS degree in Computer Science or a related technical field
Previous experience with Data ingestion and IR (information retrieval) is highly desirable
Industry experience as a Data Engineer or related specialty
Powered by JazzHR
","['sql', 'scala', 'java', 'python', 'hive', 'php', 'pentaho', 'oracl']","['etl', 'statist', 'big data']",1,"['sql', 'scala', 'java', 'python', 'hive', 'php', 'pentaho', 'oracl', 'etl', 'statist', 'big data']","['relat', 'power', 'infrastructur', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin']","['sql', 'scala', 'java', 'python', 'hive', 'php', 'pentaho', 'oracl', 'etl', 'statist', 'big data', 'relat', 'power', 'infrastructur', 'python', 'warehous', 'statist', 'comput', 'big', 'etl', 'engin']"
DE,"Senior Data Engineer with 5+ years of data engineering experience.
Key skills required SQL Python Pyspark AWS
","['sql', 'aw', 'pyspark', 'python']",[None],999,"['sql', 'aw', 'pyspark', 'python']","['aw', 'python', 'engin']","['sql', 'aw', 'pyspark', 'python', 'aw', 'python', 'engin']"
DE,"Should be very strong in writing advanced SQL queries.
Should have experience with complex data types shorting manipulationclubbingordering.
Thanks and Regards, Sumit Kumar skumarbraintreeus.com mailtoskumarbraintreeus.com httpwww.braintreeus.com httpwww.braintreeus.com
",['sql'],[None],999,['sql'],[None],"['sql', None]"
DE,"Duties Interacting with business users and analysts to understand business processes and creating analytic requirements and technical specs Designing data mart dimensions and facts to satisfy the business needs Implementing transformation logic to populate dimensions and facts Designingdeveloping ETL jobs across multiple platforms and tools including S3, Hadoop, Vertica in order to pull in source data needed for the data marts Roughly 60-70 hands-on coding Act in a technical leadership capacity Mentoring junior engineers, new team members, and applying technical expertise to challenging data and design problems Resolve defectsbugs during QA testing, pre-production, production, and post-release patches Work cross-functionally with various teams Product Management, Project Management, Data Architects, Data Scientists, Data Analysts, Software Engineers, and other Data Engineers Contribute to the design and architecture of project across the data landscape REQUIREMENTS BSMS in Computer Science or equivalent work experience 4-6 years of experience designing and implementing data marts using star schema design Strong expertise in data warehousedata mart architecture Advanced experience in writing complex SQL, SQL tuning is a must have Strong experience in one of the Columnar and MPP database (Vertica, Teradata, Netezza, etc.)
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
","['sql', 's3', 'hadoop']","['etl', 'tune']",999,"['sql', 's3', 'hadoop', 'etl', 'tune']","['analyt', 'hadoop', 'comput', 'scientist', 'releas', 'etl', 'sourc', 'engin']","['sql', 's3', 'hadoop', 'etl', 'tune', 'analyt', 'hadoop', 'comput', 'scientist', 'releas', 'etl', 'sourc', 'engin']"
DE,"If you like mastering a domain and going deep, this role is for you.
If you love the benefits of process and methodical improvement, you will love it here.
Programming skills (e.g.
Python, JavaScript and Java.
Python, etc).
Proficiency in SQL and understanding of database/data warehousing concepts
Strong critical thinking, communication and collaboration skills
Organized, passionate about the details, and a self starter
It's great, but not required, if you have:
Knowledge and experience of reporting and BI tools (e.g.
Tableau, R, SAS, etc)
Hands-on forecasting and financial modeling skills in Excel, with experience in SaaS/recurring/subscription revenue models
Experience working on Amazon Web Services (in particular using EMR, Kinesis, RDS and the like).
Additional Information
All your information will be kept confidential according to EEO guidelines.
","['sql', 'sa', 'javascript', 'bi', 'java', 'python', 'tableau', 'r', 'excel', 'amazon web services']","['data warehousing', 'forecast', 'commun', 'financial modeling']",999,"['sql', 'sa', 'javascript', 'powerbi', 'java', 'python', 'tableau', 'r', 'excel', 'aw', 'data warehousing', 'forecast', 'commun', 'financial modeling']","['bi', 'sa', 'python']","['sql', 'sa', 'javascript', 'powerbi', 'java', 'python', 'tableau', 'r', 'excel', 'aw', 'data warehousing', 'forecast', 'commun', 'financial modeling', 'bi', 'sa', 'python']"
DE,"Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture Skills Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new toolslanguages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity.
","['sql', 'linux', 'python', 'excel']","['etl', 'commun', 'regress']",999,"['sql', 'linux', 'python', 'excel', 'etl', 'commun', 'regress']","['relat', 'comput', 'python', 'etl', 'engin', 'algorithm']","['sql', 'linux', 'python', 'excel', 'etl', 'commun', 'regress', 'relat', 'comput', 'python', 'etl', 'engin', 'algorithm']"
DE,"Should be comfortable working with complex SQL queries , fine tuning them , using functions like maps and arrays Should have good knowledge in Python and Hives Experience working on Data pipelines Must have exposure working on SFDC and familiar with table structures (deals, quotes,service orders, subscription billing etc etc) Should be comfortable in creating tables ( preferably in dimensional modeling ) , should also know normalization Good to know Tableau or any other reporting tool.
Requirement gatherings and co-coordinating with multiple clients and cross functional teams
","['sql', 'tableau', 'python', 'hive']",['pipelin'],999,"['sql', 'tableau', 'python', 'hive', 'pipelin']","['python', 'pipelin']","['sql', 'tableau', 'python', 'hive', 'pipelin', 'python', 'pipelin']"
DE,"What your role and responsibilities will be
• Collaborate with Integration team to build ETL processes to ingest data into BI stores.
• Work with the internal teams in understanding the client requirements and convert
them into technical solutions.
• Be a team player in performing development work during the production life cycle.
• Experience with building stream and batch data processing systems.
• Gather and process complex raw data at scale (including writing scripts, calling APIs,
write SQL queries, etc.).
• Design and develop data processing solutions that support high performing and scalable
analytic solutions.
What youll need to succeed
3+ years in a data engineering role.
Advanced knowledge of SQL and SQL queries performance tuning.
Good experience with RDBMS (Potgres, MS SQL Server, Oracle, DB2 ... etc)
Good experience with REST APIs
Good Experience with NOSQL databases (HBase, Mongo DB etc)
Experience with Impala, Hive & Presto is an asset.
Good knowledge of the Spark/Hadoop ecosystem.
Good knowledge of Scala/Java is an asset
Good knowledge of Python and Shell scripting.
Familiarity with micro-services and lambda architecture is an asset.
","['sql', 'spark', 'scala', 'hadoop', 'bi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl']",['etl'],999,"['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl', 'etl']","['analyt', 'asset', 'stream', 'spark', 'hadoop', 'bi', 'python', 'etl', 'engin', 'integr']","['sql', 'spark', 'scala', 'hadoop', 'powerbi', 'lambda', 'java', 'python', 'hbase', 'nosql', 'hive', 'db', 'oracl', 'etl', 'analyt', 'asset', 'stream', 'spark', 'hadoop', 'bi', 'python', 'etl', 'engin', 'integr']"
DE,"5+ years of hands-on server-side programming
Experience in JVM languages (Java, Scala, Groovy)
Experience in designing and developing scalable RESTful
services
Experience with concurrency and data structures
Experience in data-modelling for RDBMS/NoSQL databases
Experience in developing scalable event driven data pipelines
using Kafka, Spark
Experience with containers such as Apache/Tomcat, Jboss
Passion to excel in fast-paced, dynamic environment
Solid understanding of GitHub and open source development culture
Exposure to A/B testing
Open source/Stack Overflow contributions!
Ecommerce!
","['spark', 'scala', 'java', 'nosql', 'github', 'kafka']",['pipelin'],2,"['spark', 'scala', 'java', 'nosql', 'github', 'kafka', 'pipelin']","['spark', 'sourc', 'pipelin']","['spark', 'scala', 'java', 'nosql', 'github', 'kafka', 'pipelin', 'spark', 'sourc', 'pipelin']"
DE,"Work Authorization Those authorized to work in the United states are encouraged to apply.We are able to sponsor H1-B at this time.
Design, develop, maintain and support of Enterprise Data Warehouse amp BI platform within using various data amp BI tools, this position offers a unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture.
Responsibilities Work in a time constrained environment to analyse, design, develop and deliver Enterprise Data Warehouse solutions for Sales, Delivery and Logistics Teams Create ETL pipelines using Python, Airflow Create real time data streaming and processing using Open source technologies like Kafka , Spark etc Required Skills 1.
Strong in Python 2.
Good hold on Data Modelling and Data Warehousing 3.
Self-Starter If you are interested and meet the above job requirements, please submit your resume.
","['spark', 'airflow', 'bi', 'python', 'kafka']","['data warehousing', 'logist', 'pipelin', 'etl']",999,"['spark', 'airflow', 'powerbi', 'python', 'kafka', 'data warehousing', 'logist', 'pipelin', 'etl']","['stream', 'spark', 'pipelin', 'bi', 'python', 'warehous', 'etl', 'sourc']","['spark', 'airflow', 'powerbi', 'python', 'kafka', 'data warehousing', 'logist', 'pipelin', 'etl', 'stream', 'spark', 'pipelin', 'bi', 'python', 'warehous', 'etl', 'sourc']"
DE,"(6 - 12) Months Contract.
Requirements:
5+ years of data engineering experience
Proficiency with REST APIs, Cassandra, Python, MongoDB, Postgres, and querying.
Proficiency with both relational and non-relational databases and how to combine them
Proficiency with distributed computing engines,frameworks like Hadoop v2, Spark
Responsibilities:
Experience with building stream-processing systems, (e.g., using solutions such as Spark-
Streaming or Storm)
Previous Experience with various messaging systems, such as Kafka or Amazon Kinesis(
for example)
Coding Experience with any of the programming languages - Python,Java, Scala
Powered by JazzHR
","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'java', 'python', 'postgr', 'kafka']",[None],999,"['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'java', 'python', 'sql', 'kafka']","['program', 'stream', 'spark', 'power', 'hadoop', 'python', 'relat', 'engin']","['mongodb', 'spark', 'cassandra', 'scala', 'hadoop', 'java', 'python', 'sql', 'kafka', 'program', 'stream', 'spark', 'power', 'hadoop', 'python', 'relat', 'engin']"
DE,"Duration: 6 Months +
Roles and Responsibilities
Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
Design and development of data warehouse using T-SQL, SQL, and python
Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Implement data structures using best practices in data modelling, processes, and technologies.
Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
Writing analytics programs (transformations/calculations) in T-SQL, R, Python or comparable
Knowledge and understanding of Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Knowledge and functional understanding of Finance, Global Supply Chain business processes
Development with one or more data visualization/reporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI)
Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
Continually recommend, develop, and implement process improvements and tools to collect and analyse data, and visualize/present insights.
Skill/Job Requirements:
Bachelors degree in Business, MIS or related area.
Masters degree a plus.
8+ years Business Intelligence / Data Warehouse development experience
3+ years of experience in ETL development tools, preferably with knowledge of Microsoft Integration Services 2005 or greater (SSIS), SAP Data Services, SAP SLT and Python.
5+ years of experience in design and development using Microsoft SQL Server, SAP HANA and Snowflake databases.
Strong experience in full life cycle development, implementation, management and performance tuning of the Enterprise Data Warehouse
Experience in database development (T-SQL, PLSQL, and/or SQL scripts)
Experience in building data pipelines using python, C# and JSON
Experience in Microsoft BI development in Integration Services (SSIS), Analysis Services (SSAS) or Reporting Services (SSRS)
Experience building and managing data flows to and from cloud applications
Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses
Experience visualizing data in business intelligence tools such as Tableau, Business Objects or Hana Analytics
Experience and functional understanding with Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Strong experience with performance and scalability design and testing
Experience creating test plans, testing and resolving data discrepancies
Must be a self-motivated, energetic, detail-oriented team player passionate about producing high quality BI & Analytics deliverables
Strong sense of customer service for internal customers
Medical robotics has unique characteristics that will require immersion in clinical and technical training and he or she must come up to speed quickly an interest and desire to learn are critical
Health
Vision
Dental
Paid time off
Sick Leave
Short-Term Disability
Life Insurance
Wellness & Discount Programs
","['sql', 'spss', 'ssr', 'salesforc', 'bi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi']","['pipelin', 'visual', 'tune', 'financ', 'etl']",1,"['sql', 'spss', 'ssr', 'salesforc', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'pipelin', 'visual', 'tune', 'financ', 'etl']","['analyt', 'program', 'pipelin', 'relat', 'visual', 'bi', 'python', 'warehous', 'clinic', 'etl', 'integr']","['sql', 'spss', 'ssr', 'salesforc', 'powerbi', 'snowflak', 'python', 'c', 'tableau', 'r', 'microsoft', 'cloud', 'hana', 'powerbi', 'pipelin', 'visual', 'tune', 'financ', 'etl', 'analyt', 'program', 'pipelin', 'relat', 'visual', 'bi', 'python', 'warehous', 'clinic', 'etl', 'integr']"
DE,"Required Skills:Bachelors degree in Computer Science or equivalent education/training4- 5 years of Software development and testing experience.3+ years of Working experience on tools like Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce, etc.3+ years of programming experience in Scala, Java or PythonExperience with development and automated testing in a CI/CD environment.
Knowledge of GIT/Jenkins and pipeline automation is must.Experience with developing and testing real-time data-processing and Analytics Application System.Strong knowledge in SQL development on Database and/or BI/DWStrong knowledge in shell scriptingExperience in Web Services API development and testing.A solid understanding of common software development practices and tools.Strong analytical skills with a methodical approach to problem solving applied to Big Data domainGood organizational skills and strong written and verbal communication skills.Desired Skills:Working experience on large migration Projects is a big plus.Development and Testing experience of Machine Learning Applications is a plus.Experience in load and performance testing, familiarity with testing tools such as JMeter.Job Requirements:
","['mapreduc', 'sql', 'spark', 'scala', 'git', 'bi', 'java', 'hive', 'hbase', 'kafka']","['pipelin', 'machine learning', 'problem solving', 'big data', 'commun']",1,"['mapreduc', 'sql', 'spark', 'scala', 'git', 'powerbi', 'java', 'hive', 'hbase', 'kafka', 'pipelin', 'machine learning', 'problem solving', 'big data', 'commun']","['analyt', 'machin', 'learn', 'spark', 'pipelin', 'bi', 'comput', 'appli', 'big', 'common']","['mapreduc', 'sql', 'spark', 'scala', 'git', 'powerbi', 'java', 'hive', 'hbase', 'kafka', 'pipelin', 'machine learning', 'problem solving', 'big data', 'commun', 'analyt', 'machin', 'learn', 'spark', 'pipelin', 'bi', 'comput', 'appli', 'big', 'common']"
DE,"Ref ID: 00420-9502602834Classification: Data Engineer
Compensation: DOE
Ability to Clean/transform data from raw data inputs
Ability to deep dive into the data to meet stakeholders requirements Outstanding communication skills with the ability to influence decision makers and build consensus with teams
Development and execution of data movement tools using scripts in languages such as SQL, HQL, SAS
Please send resumes to Trupti Deshpande.
Job Requirements:
5+ years relevant experience Advanced SQL skills to get the data you need from a warehouse (Vertica, Hive, SparkSQL, etc) Advanced SAS skills Experience with AWS EMR Strong ETL experience Experience using Github and Tidal
All applicants applying for U.S. job openings must be authorized to work in the United States.
All applicants applying for Canadian job openings must be authorized to work in Canada.
An Equal Opportunity Employer M/F/Disability/Veterans.
","['sql', 'sa', 'aw', 'hive', 'github']","['etl', 'commun']",999,"['sql', 'sa', 'aw', 'hive', 'github', 'etl', 'commun']","['input', 'sa', 'aw', 'warehous', 'etl', 'engin']","['sql', 'sa', 'aw', 'hive', 'github', 'etl', 'commun', 'input', 'sa', 'aw', 'warehous', 'etl', 'engin']"
DE,"Duration: 12+ Months Contact/Long Term
Rate: $DOE
Responsibilities:
â Responsible for developing and translating computer algorithms into prototype code and maintaining, organizing, and identifying trends in large data sets.
â Proficiency in SQL database design, proficiency in creating process documentation, strong written and verbal communication skills, and the ability to work independently and on teams.Â
â Familiarity with the computer coding languages python, java, Kafka, hive or storm may be required in order to oversee real-time business metric aggregation, data warehousing and querying, schema and data management, and related duties.Â
â Should have knowledge of algorithms, data structures, and performance optimism and experience with processing and interpreting data sets.Â
â Develop technical solutions to improve access to data and data usage.
â Aggregate and analyze various data sets to provide actionable insight.Â
â Develop reports, dashboards, and tools for business-users
Required Skills:
â Experience designing and deploying data systems on AWS.
â Hands-on experience with AWS technologies like S3, Redshift, Dynamo DB, EMR/EC2, HiveÂ
â Hands-on experience building scalable and reliable data pipelines based on Big Data processing technologies like Hadoop, MapReduce, Spark, Python.Â
â Hands-on experience in ETL tools and working with large data sets in the cloud Capability of building data marts and data solutions
Thanks and look forward to working with you,
","['sql', 'mapreduc', 'spark', 'ec2', 'hadoop', 'aw', 'db', 'redshift', 's3', 'hive', 'java', 'cloud', 'kafka']","['pipelin', 'dashboard', 'optim', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'mapreduc', 'spark', 'ec2', 'hadoop', 'aw', 'db', 'redshift', 's3', 'hive', 'java', 'cloud', 'kafka', 'pipelin', 'dashboard', 'optim', 'data warehousing', 'big data', 'etl', 'commun']","['spark', 'pipelin', 'set', 'relat', 'hadoop', 'optim', 'aw', 'comput', 'action', 'big', 'etl', 'â', 'algorithm']","['sql', 'mapreduc', 'spark', 'ec2', 'hadoop', 'aw', 'db', 'redshift', 's3', 'hive', 'java', 'cloud', 'kafka', 'pipelin', 'dashboard', 'optim', 'data warehousing', 'big data', 'etl', 'commun', 'spark', 'pipelin', 'set', 'relat', 'hadoop', 'optim', 'aw', 'comput', 'action', 'big', 'etl', 'â', 'algorithm']"
DE,"Data Engineer - Data Platform
Mountain ViewR&DExperienced
Responsibilities
As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world.
You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem.
Responsibility:
Design and build data transformations efficiently and reliably for different purposes (e.g.
reporting, growth analysis, and multi-dimensional analysis)
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Qualifications
1.
BS or MS degree in Computer Science or related technical field or equivalent practical experience
Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)
Experience with performing data analysis, data ingestion and data integration
Solid communication and collaboration skills
Experience with ETL (Extraction, Transformation & Loading) and architecting data systems
Experience with schema design and data modeling
Experience in writing, analyzing and debugging SQL queries
Deep understanding of various Big Data technologies
Passionate and self-motivated about technologies in the Big Data area
Share to
","['sql', 'spark', 'hadoop', 'r', 'hive', 'kafka']","['analyz', 'data modeling', 'optim', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'hadoop', 'r', 'hive', 'kafka', 'analyz', 'data modeling', 'optim', 'big data', 'etl', 'commun']","['spark', 'relat', 'hadoop', 'optim', 'comput', 'big', 'etl', 'engin', 'integr']","['sql', 'spark', 'hadoop', 'r', 'hive', 'kafka', 'analyz', 'data modeling', 'optim', 'big data', 'etl', 'commun', 'spark', 'relat', 'hadoop', 'optim', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
Design and development of data warehouse using T-SQL, SQL, and python
Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Implement data structures using best practices in data modeling, processes, and technologies.
Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
Writing analytics programs (transformations/calculations) in T-SQL,R, Python or comparable
Knowledge and understanding of Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Knowledge and functional understanding of Finance, Global Supply Chain business processes
Development with one or more data visualization/reporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI)
Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
Continually recommend, develop, and implement process improvements and tools to collect and analyze data, and visualize/present insights.
Job Posting Type
Agency Recruited Worker Required
Worker Legal Name (For Manager Sourced Only)
(No Value)
","['sql', 'salesforc', 'snowflak', 'python', 'tableau', 'r', 'microsoft', 'hana', 'powerbi']","['data modeling', 'visual', 'pipelin', 'financ']",999,"['sql', 'salesforc', 'snowflak', 'python', 'tableau', 'r', 'microsoft', 'hana', 'powerbi', 'data modeling', 'visual', 'pipelin', 'financ']","['analyt', 'program', 'pipelin', 'visual', 'python', 'sourc']","['sql', 'salesforc', 'snowflak', 'python', 'tableau', 'r', 'microsoft', 'hana', 'powerbi', 'data modeling', 'visual', 'pipelin', 'financ', 'analyt', 'program', 'pipelin', 'visual', 'python', 'sourc']"
DE,"Data Engineer
Share3
Job ID: FA-0100-560
Open Since: 2019-12-17
City: Sunnyvale
State: California
Country: United States of America
Job Skills:
Perform data quality analytics on streaming data with business rules setup and modified during streaming to assess the quality.
He would need Big data lead at onsite and team at offshore to supplement.
Must Skills - Spark, Spark streaming, neo4j, Java / Scala, Kafka
Minimum Experience: 8 Yrs
Education:
Must have a Bachelor's degree, preferably in Computer Science or Engineering
","['scala', 'kafka', 'java', 'spark']",['big data'],1,"['scala', 'kafka', 'java', 'spark', 'big data']","['analyt', 'spark', 'comput', 'big', 'engin']","['scala', 'kafka', 'java', 'spark', 'big data', 'analyt', 'spark', 'comput', 'big', 'engin']"
DE,"Want in?
A Lot About You
You get people.
You get data.
You have a thirst for knowledge and insight.
You thrive and strive to present data in ways that product, design, engineering, marketing, and executive teams understand and act upon.
Your data is 100% accurate and credible.
Your reports are always clear and actionable.
You get growth.
You are a consumer-focused, data-driven, and growth-enabling analyst who has supported Growth strategies, roadmaps, scrums, and final product rollouts, across the analytics/insights, acquisition/referrals,activation/onboarding, and adoption/retention loop.
You get mobile/digital.
You have significant industry experience – and a strong understanding of the mobile/digital ecosystem – from apps to advertising and analytics.
You have successfully applied the latest mobile/digital tools to help drive reach, retention, and revenue growth.
You get it done.
You have successfully worked with product, design, engineering, marketing, and executive teams to understand requirements, translate business needs into data requests, develop methodologies/plans, analyze data, and present findings that are embraced/enacted.
Your Day
Understand the marketplace trends and help answer revenue trends
Analyze supply as well as demand patterns and find revenue opportunities, explain model behaviors, suggest improvements etc.
Gain insights on what drives performance in terms of reach and revenue growth
Create dashboards and reports that provide analysis and commentary, explaining product, sales, and business trends for Executive reporting
Work closely with product and inform and update stakeholders on product performance, plans, and progress towards metrics
Define data testing plans and create methodologies that help teams to iterate fast and release new features for testing and, if successful, rollout to all users globally
Generate and go deep on consumer insights and competitive intelligence to help teams drive product innovation and iteration
Build strong partnerships with product, sales, engineering, and marketing teams and enable them to launch new Growth initiatives for testing/iteration
Provide feedback to product, sales and engineering teams on impact of product launches: target launch metrics, A|B testing, post-launch metrics
Investigate data and monitor data quality – partner closely with and provide requirements to the Data Engineering teams that can be clearly acted upon
Frame business problems into questions that can be answered through data analysis, and translate business needs into requirements
You Must Have
BS/MS in highly-quantitative field (Analytics, Computer Science, Mathematics) is preferred
Data analysis, generating insights for consumer-focused products
Experience with big data technologies such as Hive, Hadoop, MapReduce, Spark, PIG etc.
Experience with programming languages such as Perl/Python/R is good to have
Familiarity with Unix/Linux environment highly recommended
Significant experience, proficiency in, and passion for Mobile and/or Web products
Track record of proactively establishing and following through on commitments
Demonstrated use of analytics, metrics, and benchmarking to drive decisions
Excellent interpersonal, organizational, creative, and communications skills
Team player in driving growth results combined with a positive attitude
Strong work ethic and strong core values (honesty, integrity, creativity)
Problem solver who never stops thinking about ways to improve
All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category.
If you need accessibility assistance and/or a reasonable accommodation due to a disability, please submit a request via the Accommodation Request Form (https://www.verizonmedia.com/careers/contact-us.html) or call 408-336-1409.
Requests and calls received for non-disability related issues, such as following up on an application, will not receive a response.
","['mapreduc', 'linux', 'spark', 'perl', 'pig', 'unix', 'hadoop', 'python', 'hive', 'r', 'excel']","['recommend', 'dashboard', 'commun', 'big data']",1,"['mapreduc', 'linux', 'spark', 'perl', 'pig', 'unix', 'hadoop', 'python', 'hive', 'r', 'excel', 'recommend', 'dashboard', 'commun', 'big data']","['day', 'analyt', 'digit', 'spark', 'relat', 'hadoop', 'provid', 'python', 'comput', 'appli', 'action', 'big', 'quantit', 'engin', 'integr']","['mapreduc', 'linux', 'spark', 'perl', 'pig', 'unix', 'hadoop', 'python', 'hive', 'r', 'excel', 'recommend', 'dashboard', 'commun', 'big data', 'day', 'analyt', 'digit', 'spark', 'relat', 'hadoop', 'provid', 'python', 'comput', 'appli', 'action', 'big', 'quantit', 'engin', 'integr']"
DE,"Must have skills SQL, Python, Tableau 10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['bi', 'python', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'bi', 'python', 'pipelin']"
DE,"When you come across a sea of data, is your first instinct to put on your software diving suit and go deep?
Requirements
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing
Advanced knowledge of system architecture and database design
Proficiency in Big Data tools: Spark, Hadoop, Kafka, etc.
Advanced experience with SQL and NoSQL database architecture and implementation (hands-on experience with PostgreSQL, Elasticsearch, and Cassandra a plus)
Demonstrable experience designing, developing, and implementing ETL processes
Experience working with private cloud infrastructure
Demonstrable experience building and optimizing Big Data pipelines and architecture
Proficiency in Python, Java, C++
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Ability to collaborate well with others
Strong communication skills
Visit www.entefy.com and www.blog.entefy.com
","['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka']","['etl', 'commun', 'pipelin', 'big data']",999,"['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka', 'etl', 'commun', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'python', 'big', 'etl']","['sql', 'spark', 'elasticsearch', 'cassandra', 'hadoop', 'cloud', 'java', 'python', 'nosql', 'postgresql', 'kafka', 'etl', 'commun', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'hadoop', 'infrastructur', 'python', 'big', 'etl']"
DE,"The ticker symbol for the combined public entity will remain NYSE: RUBI.
Why You'll Be Excited
Having a large stake and impact on the product and business direction and bottom-line
Collaborating with innovative and goal-focused engineering and business teams
Working with data scientists, data analysts, and product managers to identify and use the data that is most relevant to the problem at hand
Building systems that can effectively stream, store, and crunch vast amounts of data to help inform customers and power business analytics
Solving complex problems revolving around real-time strategic decision-making and large data systems
Developing, deploying, and maintaining robust and high-performance systems and features
You have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams
You are passionate about learning different technologies, exploring engineering challenges, and working in a dynamic and collaborative environment
You have working experience and skills designing and coding in Java/Scala and/or Python
You are proficient in writing efficient and well-structured SQL queries and have experience with database schemas and design
You have experience with big data technologies (Spark, Presto, Druid, etc.)
You have knowledge of UNIX/Linux and scripting with Perl, Shell, etc.
Degree in Computer Science or a related field
Bonus: Experience working in a data science / machine learning environment
Bonus: Experience working with AWS Services (Redshift, Kinesis, Glue, etc.)
Perks and Benefits:
open vacation policy!
), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more!
All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.
","['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi']","['machine learning', 'commun', 'big data']",1,"['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi', 'machine learning', 'commun', 'big data']","['analyt', 'machin', 'program', 'spark', 'challeng', 'relat', 'line', 'power', 'public', 'amount', 'aw', 'python', 'scientist', 'comput', 'big', 'employe', 'engin']","['sql', 'linux', 'spark', 'perl', 'scala', 'unix', 'aw', 'python', 'redshift', 'java', 'rubi', 'machine learning', 'commun', 'big data', 'analyt', 'machin', 'program', 'spark', 'challeng', 'relat', 'line', 'power', 'public', 'amount', 'aw', 'python', 'scientist', 'comput', 'big', 'employe', 'engin']"
DE,"Power the Possibilities
that will change the landscape for automotive dealers, original equipment manufacturers (OEMs) and the customers they serve.
Be Part of Something Bigger
It’s time you joined an evolving marketplace where research and development
investment is measured in the tens of billions.
It’s time you were a part of something bigger.
The possibilities for impact are endless.
Summary:
If you have worked on bringing a large software product to market, or have a desire to gain this experience, this role might be perfect for you.
Specific responsibilities include:
Evolve existing framework to support new scalability requirements as well as new functionality needed.
Work with the team to drive big data solutions.
Work with product owners to identify and iron out upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Experience:
Qualified candidates will generally have 3+ years of software development experience, including:
5+ years of software development experience or Masters + 2yrs in CS.
Experience programming in Python, Java, writing ETL applications
Experience with Amazon AWS services such as S3, Lambda, StepFunctions
Knowledge in SQL and understanding relational database
Experience designing robust scalable applications
Experience in application design and implementation using agile practices & TDD
Experience with highly scalable / distributed systems desired
Experience with Kafka, Hadoop, and NoSQL datastore is a plus
Enthusiasm for solving interesting and complicated problems.
Education:
A BS or MS in Computer Science or equivalent education/experience
You have family, friends, sporting events, and lots of things going on.
","['sql', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'nosql', 'kafka']","['research', 'etl', 'big data']",1,"['sql', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'nosql', 'kafka', 'research', 'etl', 'big data']","['program', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl']","['sql', 'hadoop', 'aw', 'lambda', 'java', 'python', 's3', 'nosql', 'kafka', 'research', 'etl', 'big data', 'program', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl']"
DE,"8+Years of overall IT Experience
Can program using Spark with Python and Scala
Understands Hive and Hadoop configuration
Understands different file formats in the Hadoop environment and how to organize data for query performance.
Supporting number of tools: Data Lake sync, SMFDB population, Metadata sync
Tuning of the cluster for performance optimization with Spark and Presto.
Understand the different formats parquet, Avro and snappy
","['spark', 'scala', 'hadoop', 'python', 'hive']","['tune', 'optim', 'cluster']",999,"['spark', 'scala', 'hadoop', 'python', 'hive', 'tune', 'optim', 'cluster']","['program', 'spark', 'hadoop', 'optim', 'python']","['spark', 'scala', 'hadoop', 'python', 'hive', 'tune', 'optim', 'cluster', 'program', 'spark', 'hadoop', 'optim', 'python']"
DE,"You are not only a strong software engineer, but you bring a passion for data and analytics to the table, and a track record of robust design experience.
You thrive working in cross-functional teams between product, engineering, ML, and analytics, and you are looking to make substantial impact in the next stage of your career.
What will you do?
Work with various internal teams and customers to gather requirements and deliver data solutions to address those requirements
Model data and metadata to support analytics and reporting for different use cases
Design and implement a data platform to process large, complex data sets
Implement best practices around data integrity, validation, and documentation for data processing, reporting, and analysis
Optimize data processing pipelines and storage performance
What do you bring to the table?
Strong coding and design expertise
Familiarity with latest data processing and warehousing technologies is required
Hands on experience working with different teams for their data requirements
Experience with some data stores, such as Postgres, MySQL, HBase, etc.
BS or higher in Computer Science or a related field
Nice-to-haves:
Experience with large-scale machine learning pipelines is a plus
","['mysql', 'hbase', 'postgr']","['machine learning', 'optim', 'pipelin']",1,"['sql', 'hbase', 'machine learning', 'optim', 'pipelin']","['analyt', 'machin', 'pipelin', 'ml', 'set', 'relat', 'optim', 'comput', 'integr', 'engin']","['sql', 'hbase', 'machine learning', 'optim', 'pipelin', 'analyt', 'machin', 'pipelin', 'ml', 'set', 'relat', 'optim', 'comput', 'integr', 'engin']"
DE,"Hi, I tried reaching you ndash Howrsquos everything?
Kindly look for same only.
10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
Phone 860.247.1400 x 160 Fax 860 256 8484 payal.deyvlinkinfo.com mailtopayal.deyvlinkinfo.com www.vlinkinfo.com httpwww.vlinkinfo.com
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['look', 'bi', 'python', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'look', 'bi', 'python', 'pipelin']"
DE,"Posted: Mar 26, 2020
Weekly Hours: 40
Role Number:
200159737
Bring passion and dedication to your job and there's no telling what you could accomplish!
How can they be improved?
These are the kinds of questions that the SWE Analytics Engineering team answers.
Do you want to work with the latest Big Data technology on one of the largest data sets in the world?
Collaborating with data analysts, device engineers and engineering teams, you will drive the development of data pipelines and services with high degree of ownership.
Key Qualifications
Deep experience developing large scale distributed computing systems in a large organization.
In-depth knowledge and experience in one or more of the following technologies: Hadoop ecosystem, Kafka, Samza, Flume, HBase, Cassandra, Redshift, Vertica, Spark.
Deep understanding of key algorithms and tools for developing high efficiency data processing systems
Validated software engineering experience and discipline in design, test, source code management and CI/CD practices
Experience in data modeling and developing SQL database solutions
Proficient in working with Linux or other Posix operating systems, shell scripting, and networking technologies
Strong software development, problem-solving and debugging skills with experience in one or more of the following languages: Java, Python, Scala, or Ruby
Ambitious, passionate about software development, especially in data technologies, you love working in a fast-paced and dynamic environment
Organized, detail oriented, and thorough in every undertaking.
You are able to multi-task and change focus quickly
Excellent interpersonal skills.
You will build self-service analytics tools to help engineering teams derive actionable metrics out of large volumes of raw data.
Education & Experience
B.S.
in Computer Science or equivalent.
","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'redshift', 'python', 'java', 'hbase', 'kafka', 'excel', 'rubi']","['data modeling', 'pipelin', 'big data']",1,"['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'redshift', 'python', 'java', 'hbase', 'kafka', 'excel', 'rubi', 'data modeling', 'pipelin', 'big data']","['analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'action', 'big', 'set', 'sourc', 'engin', 'algorithm']","['sql', 'linux', 'spark', 'cassandra', 'scala', 'hadoop', 'redshift', 'python', 'java', 'hbase', 'kafka', 'excel', 'rubi', 'data modeling', 'pipelin', 'big data', 'analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'action', 'big', 'set', 'sourc', 'engin', 'algorithm']"
DE,"Role: Data Engineer
Duration: 12 to 18 Months
6 Positions
10+ years of experience
Experience in Advance SQL, Python, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
As hacker rank test is must
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['bi', 'python', 'engin', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'bi', 'python', 'engin', 'pipelin']"
DE,"8+Years of overall IT Experience
Can program using Spark with Python and Scala
Understands Hive and Hadoop configuration
Understands different file formats in the Hadoop environment and how to organize data for query performance.
Supporting number of tools: Data Lake sync, SMFDB population, Metadata sync
Tuning of the cluster for performance optimization with Spark and Presto.
Understand the different formats parquet, Avro and snappy
","['spark', 'scala', 'hadoop', 'python', 'hive']","['tune', 'optim', 'cluster']",999,"['spark', 'scala', 'hadoop', 'python', 'hive', 'tune', 'optim', 'cluster']","['program', 'spark', 'hadoop', 'optim', 'python']","['spark', 'scala', 'hadoop', 'python', 'hive', 'tune', 'optim', 'cluster', 'program', 'spark', 'hadoop', 'optim', 'python']"
DE,"You will help drive and support the data requirements of multiple teams and systems.
What you will do:
Build high-performance data quality algorithms, predictive models, and/or prototypes.
Adhere to data security best practices.
Develop set processes for data mining, data modeling, and data production.
Research new uses for existing data.
Work with stakeholder teams to assist with data-related technical issues and support their data infrastructure needs.
Your experience should include:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Creating, testing, monitoring, and maintaining data pipelines.
Working knowledge of using Python or other languages for creating pipelines and data processing
Strong project management and using agile process within test driven development
Performing root cause analysis data and processes to answer specific business questions and identify opportunities for improvement.
Working with structured and unstructured datasets.
Build tested processes supporting data transformation, data structures, metadata, dependency and workload management.
Manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Experience supporting and working with cross-functional teams in a dynamic environment..
Big data ecosystem tools: GCP Big Query, AirFlow, Spark, or similar tools.
Cloud services: AWS, GCP, Azure
","['sql', 'gcp', 'spark', 'airflow', 'azur', 'aw', 'cloud', 'python']","['research', 'pipelin', 'data mining', 'predict', 'data modeling', 'big data']",999,"['sql', 'gcp', 'spark', 'airflow', 'azur', 'aw', 'cloud', 'python', 'research', 'pipelin', 'data mining', 'predict', 'data modeling', 'big data']","['stream', 'spark', 'pipelin', 'azur', 'relat', 'predict', 'infrastructur', 'aw', 'python', 'big', 'set']","['sql', 'gcp', 'spark', 'airflow', 'azur', 'aw', 'cloud', 'python', 'research', 'pipelin', 'data mining', 'predict', 'data modeling', 'big data', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'predict', 'infrastructur', 'aw', 'python', 'big', 'set']"
DE,"10+ years of experience
Experience in Advance SQL, Python, ETL, Data Modelling, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
Should be in BI Engineer
Python (Nice to have)
","['sql', 'bi', 'tableau', 'python']","['etl', 'pipelin']",999,"['sql', 'powerbi', 'tableau', 'python', 'etl', 'pipelin']","['pipelin', 'bi', 'python', 'etl', 'engin']","['sql', 'powerbi', 'tableau', 'python', 'etl', 'pipelin', 'pipelin', 'bi', 'python', 'etl', 'engin']"
DE,"**Please Read**
Local candidates only.
This opportunity does not provide Visa sponsorship.
No corp to corp applicants please.
Data Engineer
The work includes:
?
Refactoring existing and build new data pipelines
?
Migrating existing data sets into next-gen reporting frameworks and tools
?
Using existing data tools and frameworks to configure reports and metrics
?
?
Building and refactoring scalable data pipelines on top of Hive and Spark leveraging Airflow scheduler/executor framework
?
Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and drive effective product solutions
?
Experience designing and deploying high performance systems with robust monitoring and logging practices
?
Experience building high performance data pipelines
?
Nice to have: proven ability to think critically about team direction and use analysis to inform that
?
Experience using machine learning is a plus, but not required.
?
Excellent communication skills, both written and verbal
","['spark', 'excel', 'hive', 'airflow']","['machine learning', 'commun', 'pipelin']",999,"['spark', 'excel', 'hive', 'airflow', 'machine learning', 'commun', 'pipelin']","['machin', 'spark', 'learn', 'pipelin', 'set', 'engin']","['spark', 'excel', 'hive', 'airflow', 'machine learning', 'commun', 'pipelin', 'machin', 'spark', 'learn', 'pipelin', 'set', 'engin']"
DE,"Posted: May 22, 2020
Role Number:
200172180
These engineers build secure, end-to-end solutions.
A great engineer, and a visionary software tools architect who can review, plan and influence key innovations that will deliver the best-in-class video experiences for the users.
Key Qualifications
Building analytics systems and delivering at scale.
Working in Linux or Unix environments and ability to tune for performance (e.g.
disk or memory).
Usage of devops and automation tools (e.g.
chef, Jenkins).
Architecting or storing data in NoSQL, Relational or Big data technologies (e.g.
Hadoop, MongoDB/CouchDB, Spark, Redshift).
Working with data visualization tools like Tableau and/or Lookr.
Implementing and optimizing data pipeline for collecting, cleaning and aggregating data.
Dealing with data privacy and security issues related to user information.
Java/Scala/Go and run-time configurations.
Agile development with code tools like svn and git.
ML training to deploying models at scale.
Anomoly detection and root cause analytic systems.
Aptitude to independently network, learn, and influence multi-functional teams.
Good written and communication skills.
Ability to organize, plan and deliver against key landmarks.
Education & Experience
Bachelor’s degree in Computer Science and Engineering or related field with 7+ years of industry experience
","['mongodb', 'linux', 'spark', 'scala', 'unix', 'git', 'hadoop', 'tableau', 'redshift', 'java', 'nosql']","['pipelin', 'visual', 'clean', 'big data', 'commun']",1,"['mongodb', 'linux', 'spark', 'scala', 'unix', 'git', 'hadoop', 'tableau', 'redshift', 'java', 'nosql', 'pipelin', 'visual', 'clean', 'big data', 'commun']","['analyt', 'learn', 'spark', 'pipelin', 'ml', 'visual', 'hadoop', 'comput', 'big', 'relat', 'engin']","['mongodb', 'linux', 'spark', 'scala', 'unix', 'git', 'hadoop', 'tableau', 'redshift', 'java', 'nosql', 'pipelin', 'visual', 'clean', 'big data', 'commun', 'analyt', 'learn', 'spark', 'pipelin', 'ml', 'visual', 'hadoop', 'comput', 'big', 'relat', 'engin']"
DE,"Hi,
Â
Â
Â
Â
Â
Please share the resumes to bharat@yochana.com
Â
For more details, please reach me on 248-599-1102.
Â
Position Details
Job Title Sr. Data Engineer (Data Analyst, SQL)
Project Duration 6 months+
No of openings 1
Client Interview Needed for Selection (yes / No) Yes
Overall Responsibilities
Understand end-to-end Legal entity process and drive opportunities to increase efficiencies, scale and transparency
Assess current manual processes and systems/tools used to track and report Legal entity data/approvals; develop & recommend frameworks and solutions
Drive improvements in data integrity by creating and maintaining standardized documentation and processes to store Legal entity data
Own detailed day-to-day tasks around end to end LE tracking & data management while working as part of a projects with multiple moving, related parts
Work collaboratively cross-functionally with other teams across Product Areas, Tax, Treasury, Legal, business intelligence, product operations, vendor management, and finance
Â
Minimum Qualifications:
BA/BS degree or equivalent practical experience.
5+ years of financial or consulting experience; 3-4 years operational experience preferred.
Excellent communication skills, both written and in person, with a strong attention to detail
Excellent spreadsheet and data management skills; working knowledge of SQL and relational databases
Â
Preferred skills:
Comfort working in a geographically distributed, cross-functional environment.
Proven self-starter who sets priorities, understands the broader business context of the work, and works efficiently in a high-paced environment
Ability to plan, execute, and deliver on projects in a timely manner and the ability to multitask on varying projects and initiatives, all while dealing with ambiguity in an unstructured, ever-changing environment.
Experience with (i) Google PLX dashboard suite (ii) DataStudio or other database visualization tools (iii) building and maintaining compelling websites and other stakeholder engagement outreach outlets
Â
Â
Regards,
23000 Commerce Dr, Farmington hills, MI-48335
bharat@yochana.com|| www.yochana.com
Note: This is not an unsolicited mail.
Â
","['sql', 'excel']","['commun', 'dashboard', 'visual', 'financ']",1,"['sql', 'excel', 'commun', 'dashboard', 'visual', 'financ']","['day', 'engin', 'relat', 'visual', 'integr', 'â']","['sql', 'excel', 'commun', 'dashboard', 'visual', 'financ', 'day', 'engin', 'relat', 'visual', 'integr', 'â']"
DE,"RESPONSIBILITIES:
Duties:
Interacting with business users and analysts to understand business processes and creating analytic requirements and technical specs
Designing data mart dimensions and facts to satisfy the business needs
Implementing transformation logic to populate dimensions and facts
Designing/developing ETL jobs across multiple platforms and tools including S3, Hadoop, Vertica in order to pull in source data needed for the data marts
Roughly 60-70% hands-on coding
Act in a technical leadership capacity: Mentoring junior engineers, new team members, and applying technical expertise to challenging data and design problems
Resolve defects/bugs during QA testing, pre-production, production, and post-release patches
Work cross-functionally with various teams: Product Management, Project Management, Data Architects, Data Scientists, Data Analysts, Software Engineers, and other Data Engineers
Contribute to the design and architecture of project across the data landscape
REQUIREMENTS:
BS/MS in Computer Science or equivalent work experience
4-6 years of experience designing and implementing data marts using star schema design
Strong expertise in data warehouse/data mart architecture
Advanced experience in writing complex SQL, SQL tuning is a must have
Strong experience in one of the Columnar and MPP database (Vertica, Teradata, Netezza, etc.)
is a must have
Advanced experience with scripting language
Python or Shell is a must have
Excellent communication skills, both spoken and written; Ability to communicate effectively with engineers, analysts, business users and adjusting as needed
Experience with Agile Development, SCRUM, or Extreme Programming methodologies
Experience with ETL batch and streaming processes
Experience with Amazon analytics services including EMR & Redshift; Experience working with large data volumes; The ability to get code into production
SQL star schema design required; Not looking for someone who know transactional systems; SQL tuning
Python or Shell a must
MPP - Vertica experience
AWS is a nice to have
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
","['sql', 'hadoop', 'aw', 'redshift', 'python', 's3', 'excel']","['etl', 'commun', 'tune']",1,"['sql', 'hadoop', 'aw', 'redshift', 'python', 's3', 'excel', 'etl', 'commun', 'tune']","['analyt', 'program', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'releas', 'warehous', 'etl', 'sourc', 'engin']","['sql', 'hadoop', 'aw', 'redshift', 'python', 's3', 'excel', 'etl', 'commun', 'tune', 'analyt', 'program', 'hadoop', 'aw', 'python', 'scientist', 'comput', 'releas', 'warehous', 'etl', 'sourc', 'engin']"
DE,"The Infrastructure Supply Chain group is responsible for the strategic analysis to support and enable the continued growth critical to Facebooks infrastructure organization.
This is a partnership-heavy role.
As a member of Infrastructure Supply Chain Analytics team, you will belong to a centralized Analytics and Data Engineering team who partners closely with teams in Facebooks Infrastructure organization.
Projects include analytics, ML modeling, tooling, services and more.
Responsibilities:
Partner with leadership, engineers, program managers and data analyst to understand data needs.
Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
Communicate at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Use your data and analytics experience to see whats missing identifying and address data gaps, build monitor to detect data quality issues and partner to establish a self-serve environment.
Broad range of partners equates to a broad range of projects and deliverables including ML Models, datasets, measurements, services, tools and process.
Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, improve both business and data knowledge base.
Build data expertise and own data quality for your areas.
Mininum Qualifications:
5+ years of SQL experience.
4+ years of Python development experience.
3+ years of experience with workflow management engines (i.e.
Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e.
Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Preferred Qualifications:
Experience with more than one coding language.
Experience in designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and e2e process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Experience working with Supply Chain or Data Center Operation team.
","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive']","['pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun']",999,"['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive', 'pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun']","['analyt', 'program', 'spark', 'pipelin', 'ml', 'azur', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'big', 'etl', 'engin']","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'bigqueri', 'aw', 'cloud', 'python', 'redshift', 'hive', 'pipelin', 'anomali', 'dashboard', 'tune', 'data modeling', 'optim', 'outlier', 'big data', 'etl', 'commun', 'analyt', 'program', 'spark', 'pipelin', 'ml', 'azur', 'infrastructur', 'optim', 'aw', 'python', 'warehous', 'big', 'etl', 'engin']"
DE,"Menlo Park, CA Data Engineer 6 Positon 10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
Skill Matrix Sl.NO Skill Years of Exp 1 SQL Must Have 2 Python Must Have 3 Tableau Must Have
","['sql', 'bi', 'tableau', 'python']",['pipelin'],999,"['sql', 'powerbi', 'tableau', 'python', 'pipelin']","['bi', 'python', 'engin', 'pipelin']","['sql', 'powerbi', 'tableau', 'python', 'pipelin', 'bi', 'python', 'engin', 'pipelin']"
DE,"5+ years of DevOps experience is a must.
Cisco MDM platform experience of a minimum of 2 years is a must.
Solid development background in Oracle PL/SQL and Python is a must.
Expected to have good exposure to Hadoop and Snowflake.
Should have a good background in data analytics.
Should be able to support the ongoing roadmap of new MDM capabilities and platform transformation.
Should be able to translate business capabilities into technical capabilities.
","['sql', 'hadoop', 'snowflak', 'python', 'oracl']",[None],999,"['sql', 'hadoop', 'snowflak', 'python', 'oracl']","['analyt', 'python', 'hadoop']","['sql', 'hadoop', 'snowflak', 'python', 'oracl', 'analyt', 'python', 'hadoop']"
DE,"An ideal candidate brings curiosity, a passion for data, and a deep understanding of the technologies behind data pipelines, warehousing, big data and analytics.
Google BigQuery, Amazon RedShift or Athena) for data analysis* Understanding of using data to reconcile financial models* Collaborative, empathetic, cares about the team and the development and enablement of othersPreferred Qualifications:* Prior startup experience* Experience with running cloud-scheduled tasks using a platform like Amazon Lambda, Kubernetes (cron)* Experience with cloud storage like Google Cloud Storage or Amazon s3.
","['google cloud', 'bigqueri', 'cloud', 'lambda', 'redshift', 's3', 'kubernet']","['pipelin', 'big data']",999,"['google cloud', 'bigqueri', 'cloud', 'lambda', 'redshift', 's3', 'kubernet', 'pipelin', 'big data']","['analyt', 'big', 'pipelin']","['google cloud', 'bigqueri', 'cloud', 'lambda', 'redshift', 's3', 'kubernet', 'pipelin', 'big data', 'analyt', 'big', 'pipelin']"
DE,"Role : Data Engineer
Duration : Long Term Contract
6+ of Experience
1.
Hadoop, Teradata, Google Cloud platform, Programming(Java/Python), SQL Skills, Kafka, API Development
","['sql', 'google cloud', 'hadoop', 'cloud', 'java', 'python', 'kafka']",[None],999,"['sql', 'google cloud', 'hadoop', 'cloud', 'java', 'python', 'kafka']","['python', 'engin', 'hadoop']","['sql', 'google cloud', 'hadoop', 'cloud', 'java', 'python', 'kafka', 'python', 'engin', 'hadoop']"
DE,"NG - 017
Big Data Engineer
San Jose, California, USA
JOB TITLE
Big Data Engineer
Job Duties
• Complete spark and hive based ETL framework with Python/Java/Scala along with AWS services and Cloud.
• Write the Python Pyspark code for data analysis and data management and spark development for reports.
• Design AWS migration Python framework application which helps the Data engineering team data pipelines migration from On-prem to AWS cloud.
• Translate business requirements into flow charts and user stories as a baseline for development for Data pipelines with automation tools to eliminate human error and speed up production processes.
• Write SQL and NoSQL queries for different data requirements.
• Participate in designing and develop pipelines from different type resources.
• Deploy AWS EMR cluster and create data for reports using Spark and Hive.
• Write CloudFormation scripts to provision different AWS resources.
• Manage the AWS EMR clusters to complete data generation.
• Analyze data and create data pipeline for data modeling and data quality.
• Contribute and implement the continuous integration and continuous delivery pipeline
• Ensure data availability, data quality and data modeling smooth process.
• Act to find out the root cause of any issues, while generating the data using spark and Hadoop eco system components with AWS services and cloud services.
• Implement data management using Redshift, HBase, Kafka, spark, hive and sqoop.
• Create reports using AWS Glue, AWS Athena, Hadoop, Spark, AWS EMR, Hive and Databricks Platform.
• Ability to communicate with data analysts, Data scientists and Clients requirements for Data, to deliver the data from data pipelines.
Taking proactive identification & resolve the incidents in data delivery process.
• Review Design, code changes, test scenarios and test results to onboard the migration changes.
Job Requirements
Required Bachelors or foreign equivalent in CS, CA, CIS, IT, MIS, Engineering (Any), or any related field.
Must be able to travel/relocate to various client sites throughout the U.S.
San Jose, CA.
","['sql', 'pyspark', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'hive', 'nosql', 'java', 'hbase', 'kafka']","['pipelin', 'analyz', 'data modeling', 'big data', 'etl', 'cluster']",1,"['sql', 'pyspark', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'hive', 'nosql', 'java', 'hbase', 'kafka', 'pipelin', 'analyz', 'data modeling', 'big data', 'etl', 'cluster']","['spark', 'pipelin', 'relat', 'human', 'hadoop', 'aw', 'python', 'avail', 'scientist', 'big', 'etl', 'engin', 'particip', 'integr']","['sql', 'pyspark', 'spark', 'scala', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'hive', 'nosql', 'java', 'hbase', 'kafka', 'pipelin', 'analyz', 'data modeling', 'big data', 'etl', 'cluster', 'spark', 'pipelin', 'relat', 'human', 'hadoop', 'aw', 'python', 'avail', 'scientist', 'big', 'etl', 'engin', 'particip', 'integr']"
DE,"Nice to have (but not required) beginner or intermediate level java experience.
Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.
Responsibilities:
Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements.
Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements
Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture.
Skills:
Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new tools/languages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity.
","['sql', 'linux', 'java', 'python', 'excel']","['etl', 'commun', 'pipelin', 'regress']",999,"['sql', 'linux', 'java', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress']","['pipelin', 'relat', 'integr', 'primari', 'python', 'comput', 'etl', 'engin', 'algorithm']","['sql', 'linux', 'java', 'python', 'excel', 'etl', 'commun', 'pipelin', 'regress', 'pipelin', 'relat', 'integr', 'primari', 'python', 'comput', 'etl', 'engin', 'algorithm']"
DE,"Posted: Nov 2, 2018
Role Number:
200007010
The iTunes Store is looking for a top-notch Big Data engineer to develop an analytics infrastructure that will generate insights into customer experiences on products such as the iTunes Store, App Store, and iBookstore.
Key Qualifications
Language: Java or Scala
Working knowledge on the following distributed data processing platforms: Spark, Hadoop
Great if you also know: HBase, Kafka, Java Map Reduce
Algorithms: You will be working on developing new algorithms to process large scale data efficiently.
Map Reduce Algorithm
Great, but not required, if you also know about how to develop: graph, data classification and clustering algorithms in distributed environment
Good debugging, critical thinking, and communication skills
Knowledge in engineering machine learning, feature engineering systems is a plus.
Able to gather cross-functional requirements and translate them into practical engineering tasks
5+ years of programming experience
The iTunes Store Analytics team is responsible for collecting, analyzing, and reporting on customer experience data.
You will be working on a small team and will be responsible for processing large amounts of data and developing platforms to process, analyze and mine that data to extract intelligence.
Prepare data for visualization, ad-hoc exploration, reporting, and further analysis.
The ideal candidate pays close attention to details -- caring about the quality of the input data as well as how the processed data is ultimately interpreted and used.
You are also a team player -- ready to contribute during design sessions, and able to give and receive constructive code reviews.
Education & Experience
BS degree in Computer Science or a related field
• Build large scale data processing, mining and analysis projects and features, ensuring robust & maintainable solutions are implemented with special attention to data quality, performance and usability details.
• Effectively demonstrate feature prototypes to executives
• Develop, advocate for, and build consensus on, coding best practices.
• Ability to effectively work with cross functional teams to understand requirements and identify design and engineering impacts
• Experience with architecting big data and analytical applications that scale to petabytes highly preferred.
","['spark', 'scala', 'hadoop', 'java', 'hbase', 'kafka']","['classif', 'big data', 'graph', 'visual', 'machine learning', 'analyz', 'commun']",1,"['spark', 'scala', 'hadoop', 'java', 'hbase', 'kafka', 'classif', 'big data', 'graph', 'visual', 'machine learning', 'analyz', 'commun']","['analyt', 'machin', 'learn', 'spark', 'input', 'visual', 'hadoop', 'infrastructur', 'amount', 'interpret', 'comput', 'big', 'relat', 'engin', 'algorithm']","['spark', 'scala', 'hadoop', 'java', 'hbase', 'kafka', 'classif', 'big data', 'graph', 'visual', 'machine learning', 'analyz', 'commun', 'analyt', 'machin', 'learn', 'spark', 'input', 'visual', 'hadoop', 'infrastructur', 'amount', 'interpret', 'comput', 'big', 'relat', 'engin', 'algorithm']"
DE,"THE COMPANYB-Stock is the world's largest online marketplace for returned, excess, and other liquidation merchandise.
Much of it ends up being liquidated for pennies on the dollar; some of it is even destroyed or landfilled.
The B-Stock platform gives buyers a simple and direct way to buy valuable products, and offers sellers a trusted replacement for traditional liquidation and a critical boost in operational efficiency.Backed by top investors including Spectrum Equity, True Ventures, and Susquehanna Growth Equity, B-Stock runs lean, fast, and shows no signs of slowing down.
You will partner closely with cross-functional teams, including Data Science, Engineering, and Product / Business Technology, to build data infrastructure, processes, and tooling.ESSENTIAL JOB DUTIES AND RESPONSIBILITIES* Manage and optimize core data infrastructure* Build monitoring infrastructure to give visibility into the pipeline's status* Monitor all jobs for impact on cluster performance* Run maintenance routines regularly* Tune table schemas (i.e.
partitions, compression, distribution) to minimize costs and maximize performance* Develop custom data infrastructure not available off-the-shelf* Build and maintain custom ingestion pipelines* Support data team resources with design and performance optimization* Build non-SQL transformation pipelinesMINIMUM QUALIFICATIONS, JOB SKILLS, AND ABILITIESEDUCATION:* Bachelor's degree in a technical and/or quantitative field of study-e.g., computer science, math, physics or statistics, or equivalent and/or appropriate experienceEXPERIENCE:* 3+ years of experience working with distributed data technologies* Experience working with server-side concepts such as containers, micro-services, caching, performance monitoring, and API design* Experience with cloud technologies such as AWS, Azure, and Google Cloud* Experience using Python, preferred* Experience with databases such as MySQL, and PostgreSQL, preferred* Experience with highly scalable ETL/ELT/Data Lake technologies, nice to haveOUR VALUESBe honest.
Let's find new ways to grow B-Stock together.Humor.
Take whatever you are doing very seriously but do not take yourself too seriously.Teamwork.
Whether blatant or hidden, barriers to success have no place at B-Stock.US Work Authorization required.
","['sql', 'google cloud', 'azur', 'aw', 'cloud', 'python', 'postgresql', 'mysql']","['pipelin', 'cluster', 'boost', 'tune', 'optim', 'statist', 'etl', 'math']",1,"['sql', 'google cloud', 'azur', 'aw', 'cloud', 'python', 'postgresql', 'pipelin', 'cluster', 'boost', 'tune', 'optim', 'statist', 'etl', 'math']","['pipelin', 'azur', 'etl', 'infrastructur', 'optim', 'aw', 'python', 'avail', 'statist', 'comput', 'quantit', 'engin']","['sql', 'google cloud', 'azur', 'aw', 'cloud', 'python', 'postgresql', 'pipelin', 'cluster', 'boost', 'tune', 'optim', 'statist', 'etl', 'math', 'pipelin', 'azur', 'etl', 'infrastructur', 'optim', 'aw', 'python', 'avail', 'statist', 'comput', 'quantit', 'engin']"
DE,"Strategy and Analytics- Data Engineer with Map Reduce- Project Delivery Specialist
Are you an experienced, passionate pioneer in technology – a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel?
If so, consider an opportunity with Project Delivery Practice.
Work you’ll do
• Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
• Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
• Support in the development of technical solutions to business problems
The team
Analytics & Cognitive
In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.
The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making.
• Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
• Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
• Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Qualifications
Required
• Bachelor-level degree in engineering, information technology, data communications, telecommunications, computer science, or equivalent professional experience and/or qualifications
• 5+ years of hands-on experience as a Data Engineer or Big Data developer role
• 3+ years of experience in building scalable and high-performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
• 5+ years of experience in Core JAVA and SQL
• 3+ years or experience in Python & Unix Shell Scripting
• Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
• Hands on big data performance tuning and optimization experience in Map Reduce and Pig
• Strong SQL knowledge with ability to work with the latest database technologies.
• Strong data & logical analysis skills
• Limited immigration sponsorship may be available
Preferred
• Experience in Apache Spark & Scala is a plus
Additional Requirements
• Must be willing to live and work in the Greater San Jose, CA area.
How you’ll grow
Benefits
Corporate citizenship
Recruiter tips
Certain services may not be available to attest clients under the rules and regulations of public accounting.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.
Requisition code: E21SJOCSRCKR300-PDM
","['sql', 'spark', 'scala', 'pig', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'java']","['pipelin', 'cleans', 'tune', 'optim', 'big data', 'information technology', 'commun']",1,"['sql', 'spark', 'scala', 'pig', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'java', 'pipelin', 'cleans', 'tune', 'optim', 'big data', 'information technology', 'commun']","['basi', 'analyt', 'techniqu', 'spark', 'pipelin', 'corpor', 'power', 'public', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'action', 'big', 'integr', 'engin']","['sql', 'spark', 'scala', 'pig', 'unix', 'hadoop', 'cloud', 'python', 'hive', 'java', 'pipelin', 'cleans', 'tune', 'optim', 'big data', 'information technology', 'commun', 'basi', 'analyt', 'techniqu', 'spark', 'pipelin', 'corpor', 'power', 'public', 'hadoop', 'provid', 'optim', 'python', 'avail', 'comput', 'action', 'big', 'integr', 'engin']"
DE,"Job Title: ETL Data Engineer
Job Duration: 6 Months
Informatica ETL
Oracle SQL
Linux
Must be able to bring up new architecture/design for ETL processing
Minimum 8+ years' experience in Data/Warehousing
Python or Scala with good ETL/ELT skills.
Good exposure and knowledge on using HIVE and MPP databases (like REDSHIFT,Vertica etc..)
Candidate must be able to use Python/Scala with Spark and AWS for writing ETLs
Good exposure on writing ETL using Spark
Good SQL query writing skills
AWS Services like REDIS,EMR,EC2,Glue,s3,Cloudwatch etc.
#LI-FRESH
","['sql', 'linux', 'spark', 'scala', 'ec2', 'aw', 'python', 's3', 'redshift', 'hive', 'oracl']",['etl'],999,"['sql', 'linux', 'spark', 'scala', 'ec2', 'aw', 'python', 's3', 'redshift', 'hive', 'oracl', 'etl']","['spark', 'aw', 'python', 'etl', 'engin']","['sql', 'linux', 'spark', 'scala', 'ec2', 'aw', 'python', 's3', 'redshift', 'hive', 'oracl', 'etl', 'spark', 'aw', 'python', 'etl', 'engin']"
DE,"As a Product Data Engineer, you will craft and develop infrastructure to store, process, and analyze large data sets.
The product insights gained from these data sets will improve manufacturing and impact designs for future products.
What you’ll be doing:
Crafting and building a data platform to handle growth in data scale and analytics scope
Working with Product Engineers to understand analytics use cases and develop data pipeline processes to support them
Building tools for efficient product characterization during NPI
Supporting the training and release of machine learning algorithms in product manufacturing
Bachelors in Computer Science or equivalent experience
2 - 4 years of proven experience
Strong programming skills in Python, Java, Scala, or Go
Advanced understanding of SQL
Experience in building and maintaining data infrastructure to handle large datasets
Experience with relational databases MySQL or PostgreSQL and data stores such as Hadoop, HBase, or Cassandra
Background with compute frameworks and data warehouses like Hive, Redshift, Spark, and Dask
Experience with cloud providers such as AWS or Azure
Stellar partnership and interpersonal skills
Ways to stand out from the crowd:
Masters in CS or Computer Engineering
Experience with ETL frameworks like Airflow and Luigi
Familiarity with machine learning tools such as SVMs, Kernel Functions, gradient descent, back-propagation
Display familiarity with chip and board testing data
Experience with Tableau, Looker, Kibana or other data visualization platforms
Hardworking and passionate about the industry
Show willingness to collaborate with teams across the globe
","['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'mysql', 'spark', 'airflow', 'cloud', 'postgresql']","['pipelin', 'visual', 'machine learning', 'etl', 'svm']",1,"['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'spark', 'airflow', 'cloud', 'postgresql', 'pipelin', 'visual', 'machine learning', 'etl', 'svm']","['program', 'visual', 'provid', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'warehous', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'python', 'redshift', 'java', 'hbase', 'dask', 'azur', 'hadoop', 'hive', 'cassandra', 'scala', 'looker', 'aw', 'tableau', 'spark', 'airflow', 'cloud', 'postgresql', 'pipelin', 'visual', 'machine learning', 'etl', 'svm', 'program', 'visual', 'provid', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'releas', 'learn', 'infrastructur', 'aw', 'warehous', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Lead Data Engineer - (Elasticsearch, Logstash, Kibana)
Successful candidates would bring a data driven mindset to ensure customer success.
Ability to manage the ELK infrastructure using kubernetes on a global scale.
You will be responsible for architecture and driving roadmap execution in a matrix cross functional organization.
Have strong written and communication skills and a sense of ownership and drive towards business objectives.
Responsibilities:
• SLDC process involving scrum agile process from inception to product delivery.
• Elasticsearch Engineer will work closely with product manager, architects, engineers, and integrators to assess customer requirements and to design and support an Elasticsearch Stack solution to ensure data is useable for internal and external customers.
• Deploy and maintain ELK and Prometheus infrastructure on development and production environments.
• Testing data flows, troubleshooting issues, and monitoring the health of the solution and servers to maximize performance and minimize downtime
• Securing environments with TLS, certificates, SSO authentication
Minimum qualifications:
• 3+ years' experience in technical leadership leading a team of engineers.
• 3+ years' experience contributing as a team member using Scrumban/Kanban agile model
• 3+ years hands on experience in performance profiling and optimization in a distributed environment
• 5+ years hands on experience with Prometheus, Alertmanager, ELK, Grafana, and fluentd
• BS degree in Computer Science or equivalent
• 2-4 years of cloud engineering experience with at least one of the leading public cloud
•
Additional Qualifications:
• Working experience development using Python/Go
• Experience working with fault tolerant and highly-available distributed systems
• Experience with big data systems and/or database administration (e.g.
PostgresSQL, Cassandra, etc) a plus;
• Strong linux environment / OS performance and troubleshooting skills
","['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet']","['optim', 'commun', 'big data']",1,"['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet', 'optim', 'commun', 'big data']","['public', 'infrastructur', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']","['linux', 'cassandra', 'elasticsearch', 'postgressql', 'cloud', 'python', 'kubernet', 'optim', 'commun', 'big data', 'public', 'infrastructur', 'optim', 'python', 'avail', 'comput', 'big', 'integr', 'engin']"
DE,"Qualifications Skills and Experience At least 5+ years of experience as a Data Engineer in analyzing product, customer, marketing related data and building data pipelines.
Strong Communication skills Strong background in quantitative data analysis as well as prediction and regression techniques.
Experience performing AB testing on very large, multi-dimension datasets Proficient in SQL and Visualization (TablueauQlikview) Strong Proficiency in Microsoft Excel and standard analytic programs Strong business acumen and ability to manage conversations at multiple levels of the organization Ability to present data insights concisely to various stakeholders, especially business teams Entrepreneurial spirit and a passion for data, customer focused mindset, team player Responsibilities Demonstrate up-to-date expertise in handling data extractions from multiple data sources and capable of analyzing large volumes of data to address key business questions to enable growth Manage stakeholder relationships and expectations by developing a communication process to keep others up-to-date on project results and timeline expectations Lead or participate in multiple projects by completing and updating project documentation managing project scope adjusting schedules when necessary determining daily priorities ensuring efficient and on-time delivery of project tasks and milestones following proper escalation paths and managing customer and supplier relationships Provide decision makers and influencers with logical data-driven insights translating into hypotheses for lean testing, rapid prototyping and effective implementation Construction of critical business application tools to be used by managers and consultants to be tuned to the pulse of the business Identify new technical requirements, and testtroubleshoot data capture to ensure reporting and analytics systems conform to data quality standards
","['sql', 'excel', 'microsoft']","['pipelin', 'visual', 'tune', 'regress', 'predict', 'commun', 'ab testing']",999,"['sql', 'excel', 'microsoft', 'pipelin', 'visual', 'tune', 'regress', 'predict', 'commun', 'ab testing']","['analyt', 'program', 'techniqu', 'pipelin', 'relat', 'visual', 'predict', 'provid', 'quantit', 'sourc', 'engin']","['sql', 'excel', 'microsoft', 'pipelin', 'visual', 'tune', 'regress', 'predict', 'commun', 'ab testing', 'analyt', 'program', 'techniqu', 'pipelin', 'relat', 'visual', 'predict', 'provid', 'quantit', 'sourc', 'engin']"
DE,"--------------
are looking for a data engineer responsible for the development
and maintenance
of critical data processing and classification pipelines.
Your
primary focus
will be developing new systems and services while leveraging
cutting edge data
science and machine learning technologies.
You will be working
alongside other
engineers and developers working on different layers of the
infrastructure.
Therefore, a commitment to collaborative problem solving,
sophisticated design,
and the creation of quality products is essential.
Responsibilities
---------------
Design and build data processing systems and APIs
Ensure the performance, quality, and responsiveness of
applications
Collaborate with a team to define, design, and ship new
features
Identify and correct bottlenecks and fix bugs
Help maintain code quality, organization, and
automation
Skills
-----
Works with Python on a daily basis
Experience working with distributed systems
Working knowledge of database technologies like SQL or
MongoDB
Experience developing and scaling RESTful APIs
Familiarity with best practices for functional and unit
testing
Knack for benchmarking and optimization
Experience with source control using Git
Ability to review and identify issues in other developers
code
Familiarity with continuous integration
Knowledge of queueing and messaging systems such as Celery or
Kafka
Experience building web applications with Flask, Django, or
other
Python web server frameworks
","['mongodb', 'sql', 'django', 'git', 'python', 'flask', 'kafka']","['classif', 'pipelin', 'machine learning', 'optim', 'problem solving']",999,"['mongodb', 'sql', 'django', 'git', 'python', 'flask', 'kafka', 'classif', 'pipelin', 'machine learning', 'optim', 'problem solving']","['basi', 'essenti', 'machin', 'learn', 'pipelin', 'primari', 'infrastructur', 'optim', 'python', 'integr', 'sourc', 'engin']","['mongodb', 'sql', 'django', 'git', 'python', 'flask', 'kafka', 'classif', 'pipelin', 'machine learning', 'optim', 'problem solving', 'basi', 'essenti', 'machin', 'learn', 'pipelin', 'primari', 'infrastructur', 'optim', 'python', 'integr', 'sourc', 'engin']"
DE,"Duties:
The Business Intelligence (BI) team is looking to hire a Data Engineer to build robust, extensible, and scalable data and BI solutions for ***'s product org.
These solutions could be a combination of one or more of the following: source-of-truth datasets, daily/hourly pipelines, dashboards, visualization tools, and alerting.
This role will entail the following responsibilities:
Understand the data landscape of *** products
Design, architect, and implement new source of truth datasets, in partnership with analytics and business teams
Build the required data and reporting pipelines using ***'s internal ETL tools
Develop dashboards, web-based visualizations (using Tableau, D3, or Plotly), and web-based tools (node.js or flask apps)
Collaborate with other engineering teams to develop and integrate BI solutions
Skills:
Experience designing, architecting, and maintaining data warehouses that seamlessly stitches together data from production databases and clickstream event data
Hands-on experience with Hive query development and optimization, and, building workflows (preferably using Airflow)
Hands-on experience with building data pipelines in a programming language like Python
Hands-on experience with building and maintaining Tableau dashboards and/or Jupyter reports
Working understanding of Hadoop and Big data analytics
Ability to understand the needs of and collaborate with stakeholders from analytics and business teams
Powered by JazzHR
BldnFu5fX3
","['airflow', 'plotli', 'jupyt', 'd3', 'hadoop', 'bi', 'tableau', 'python', 'hive']","['pipelin', 'dashboard', 'visual', 'optim', 'big data', 'etl']",999,"['airflow', 'plotli', 'jupyt', 'd3', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'pipelin', 'dashboard', 'visual', 'optim', 'big data', 'etl']","['analyt', 'big', 'program', 'pipelin', 'visual', 'power', 'hadoop', 'optim', 'bi', 'python', 'warehous', 'etl', 'sourc', 'engin']","['airflow', 'plotli', 'jupyt', 'd3', 'hadoop', 'powerbi', 'tableau', 'python', 'hive', 'pipelin', 'dashboard', 'visual', 'optim', 'big data', 'etl', 'analyt', 'big', 'program', 'pipelin', 'visual', 'power', 'hadoop', 'optim', 'bi', 'python', 'warehous', 'etl', 'sourc', 'engin']"
DE,"Skills Required Experience range 8-14 years Java Development Experience Data Engineer.
Python Hadoop Stack Data Pipeline Using ETL or other tool spark - spark streaming, RDD SQL, noSQL, Cassandra Handling High Volume Data AWS Expeience Experience On Big Data Migration Experience is PLUS
","['sql', 'spark', 'cassandra', 'hadoop', 'aw', 'java', 'python']","['etl', 'pipelin', 'big data']",999,"['sql', 'spark', 'cassandra', 'hadoop', 'aw', 'java', 'python', 'etl', 'pipelin', 'big data']","['spark', 'pipelin', 'handl', 'hadoop', 'aw', 'python', 'big', 'etl', 'engin']","['sql', 'spark', 'cassandra', 'hadoop', 'aw', 'java', 'python', 'etl', 'pipelin', 'big data', 'spark', 'pipelin', 'handl', 'hadoop', 'aw', 'python', 'big', 'etl', 'engin']"
DE,"Job #: 1075497
Contract Length: 5 months
Pay Rate: Competitive
Duties:
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions.
Build data expertise and own data quality for allocated areas of ownership.
Design, build, optimize, launch and support new and existing data models and ETL processes in production.
Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.
Skills:
5+ years hands-on experience with Linux and shell scripting
2+ years hands-on experience in MySQL database administration, implementation and maintenance
5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred)
Data architecture, data modeling and schema design skills
Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution
Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus
Need to be able to travel to China periodically Mandarin language a plus
Education:
BE/BTECH/MCA degree (preferred) with a strong academic record
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions.
Build data expertise and own data quality for allocated areas of ownership.
Design, build, optimize, launch and support new and existing data models and ETL processes in production.
Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.
Skills:
5+ years hands-on experience with Linux and shell scripting 2+ years hands-on experience in MySQL database administration, implementation and maintenance 5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Data architecture, data modeling and schema design skills Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus Need to be able to travel to China periodically Mandarin language a plus
Education:
BE/BTECH/MCA degree (preferred) with a strong academic record
EEO Employer
","['sql', 'linux', 'python', 'mysql', 'excel']","['optim', 'etl', 'commun', 'data modeling']",1,"['sql', 'linux', 'python', 'excel', 'optim', 'etl', 'commun', 'data modeling']","['infrastructur', 'optim', 'python', 'interpret', 'etl', 'engin']","['sql', 'linux', 'python', 'excel', 'optim', 'etl', 'commun', 'data modeling', 'infrastructur', 'optim', 'python', 'interpret', 'etl', 'engin']"
DE,"The Job Details are as follows:
OVERVIEW
In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player
","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker']","['data warehousing', 'commun']",1,"['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun']","['day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']","['sql', 'spark', 'airflow', 'cloud', 'python', 'nosql', 'docker', 'data warehousing', 'commun', 'day', 'analyt', 'spark', 'infrastructur', 'provid', 'python', 'comput', 'relat', 'engin']"
DE,"Cedar Falls, IA
Austin, Texas
Social:
Scientific Games, Social is a global leader focused in providing an ever-expanding portfolio of robust iGaming and Social Casino solutions to the global gaming industry.
See more details below!
Scientific Games:
Scientific Games (SG) is a global leader focused on delivering an ever-expanding portfolio of lottery and gaming products and services to the world's government-regulated and government-sponsored entities.
Position Summary
Essential Job Functions:
Responsible for the availability of all Redshift game data
Resolves data ingestion issues and is proactive in recommending long-term solutions to prevent future issues
Works with individual team members and their managers to optimize queries and table schemas to meet demands
Run SQL explains on queries for query optimization
Responsible for maintaining documentation on new data sources, intermediate structures, and how best to access data
Education:
Bachelor’s degree or higher in Computer Science, MIS, or equivalent experience
Required Experience:
Minimum 2-5 years of experience with Big Data analytics and querying Big Data
Minimum 1 year of experience with Amazon AWS or similar cloud-based data storage solution
Preferred Experience:
Experience with Amazon AWS
Advanced SQL skills, preferably PostgreSQL or HiveQL
Expert on use and management of Redshift or PostgreSQL tables, schemas, and functions
Experience with developing, scheduling, and maintaining ETL processes, especially in an AWS environment
Clear and effective communication of complex ideas and analysis
Ability to work with little direction, self-directed
Active learner, motivated to learn without guidance
Solutions oriented attitude of customer service
Critical thinking and creative problem solving
Qualifications
Competitive salaries and annual bonuses alongside matching 401k, employee stock options, and other competitive benefits.
Strong commitment to work/life balance
Fully stocked kitchen
Brand new, shiny, and aesthetically pleasing 10,000 sq ft office located in the Arboretum area in Austin
The employee in this position may be requested to perform other job-related tasks and responsibilities than those stated above.
If you’d like more information about your equal employment opportunity rights as an applicant under the law, please click here EEOC Poster.
Scientific Games Corporation and its affiliates (collectively, “SG”) are engaged in highly regulated gaming and lottery businesses.
As a prerequisite to employment with SG (to the extent permitted by law), you shall be asked to consent to SG conducting a due diligence/background investigation on you.
The employee in this position may be requested to perform other job-related tasks and responsibilities than those stated above.
SG is an Equal Opportunity Employer and does not discriminate against applicants due to race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you’d like more information about your equal employment opportunity rights as an applicant under the law, please click here EEOC Poster.
","['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql']","['optim', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql', 'optim', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'analyt', 'essenti', 'corpor', 'relat', 'optim', 'aw', 'avail', 'employe', 'comput', 'texa', 'big', 'etl', 'sourc']","['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql', 'optim', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'analyt', 'essenti', 'corpor', 'relat', 'optim', 'aw', 'avail', 'employe', 'comput', 'texa', 'big', 'etl', 'sourc']"
DE,"Your primary role will be to design and build data pipelines.
You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive.
In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies.
If you believe you have these skills please email your resume to info@springml.com.
Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR
","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker']","['clean', 'visual', 'pipelin', 'big data']",1,"['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data']","['program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data', 'program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']"
DE,"Now Hiring a Data Engineer
Ability to maintain an Interim Secret Clearance is Required
Must be able to maintain an Interim Secret Clearance.
Must be able to commute to Austin, Texas.
Conduct preliminary data evaluation and database structure, optimize and standardize if necessary.
Work with SME and ATF EDW team to build the scripts for the CGI Dashboard view
Importing NESS data into EDW
Creating Reports in PowerBI that are high value for field or executives
NESS data visualized in the CGI Dashboard
NESS Data Validation with emphasis on data from other ATF databases
Work with NESS Dev team to identify best solutions for incorporating RMS data into NESS
Translate business needs into longterm architecture solutions
Review, simplify and optimize existing data import process from various sources such as NIBIN, etrace and various RMS imports
Review object and data models and the metadata repository to structure the data for better management and quicker access
Profile of Success
Bachelor's degree in an IT-related field and two years of experience
Proven experience developing Big Data solutions in the Azure space
Extensive experience with the Azure suite (Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Data Lake Analytics, HDInsight, Machine Learning, and Stream Analytics)
SQL Server 2014/2016 experience
Experience using Spark, Hive, Pig, and Scala
Comfortable with Microsoft Full Stack (SSAS/SSIS/SSRS)
In-depth knowledge of Data Warehousing and ETL/ELT
Proven ability to work with clients to understand requirements and to envision solutions
Possess DoD 8570 security certification
Desirable Skills
Background with Data Science tools such as R, Python, and SAS is a plus
Microsoft related certifications such as the MCSD/MCSE
Experience with Tableau
Experience working with Hadoop ecosystem
","['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi']","['dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']",1,"['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi', 'dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']","['stream', 'visual', 'python', 'etl', 'sourc', 'evalu', 'analyt', 'azur', 'sa', 'hadoop', 'optim', 'texa', 'learn', 'warehous', 'big', 'engin', 'machin', 'spark', 'relat']","['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi', 'dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl', 'stream', 'visual', 'python', 'etl', 'sourc', 'evalu', 'analyt', 'azur', 'sa', 'hadoop', 'optim', 'texa', 'learn', 'warehous', 'big', 'engin', 'machin', 'spark', 'relat']"
DE,"This position will be the Senior technical resource driving architecture for the integration of large 3rd party partner integrations with companies like Facebook, Google and Twitter to name a few.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
This position will require the ability to own and lead data initiatives on a cross-functional team.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
You should be able to not take yourself too seriously as well.
And most of all, you will enjoy working with great people who are changing the entire industry.
What you'll do:
Migrate existing data pipelines from on-prem regional data centers to AWS and GCP.
Architect a new modern event driven architecture with both batching and streaming
Adjust existing pipelines to fit the AWS processing model such as integration with S3, migrate to open source version of hadoop, adjustments to security model, etc...
Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Partner and work closely with the QA Engineers to develop automated tests
Participate in training and mentoring of junior team members
8+ years of experience designing and building data-intensive applications
5+ years architecting systems in a big data ecosystem using MapReduce, Spark, MPP Data Warehouses, and sql/nosql databases.
5+ years recent hands-on experience with object oriented languages (Java, Scala, Python)
5+ years Hands on experience building production level systems in a cloud environment (AWS or GCP)
Excellent interpersonal and communication skills in English
Proven experience leading the design and execution of event driven architectures for distributed systems
Experience designing systems for performance, scalability, and reliability
In-depth understanding of object oriented programming concepts
Low level working knowledge of collections, multi-threading, JVM memory model, etc.
Solid understanding of database fundamentals and SQL
Understanding the full software development life cycle, agile development and continuous integration
Ability to clearly communicate with team-members in a cross-matrix environment
What puts you over the top:
Built systems in a containerized environment with familiarity in Docker, ECS, Kubernetes
Exposure to Data Warehousing solutions like Snowflake and BigQuery
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker']","['pipelin', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun']","['engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun', 'engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Join the power behind Prime.
SCOT builds software systems to make the most products available to the most people for delivery as quickly as possible.
Watch this short video for more on SCOT: http://bit.ly/amazon-scot
You should have deep expertise in the design, creation, management, and business use of significantly large datasets.
You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions.
You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications.
You should be able to work with business customers in a fast paced environment understanding the business requirements and implementing reporting solutions.
Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.
Basic Qualifications
· Degree in Computer Science or related field
· 4+ years professional experience in database development, handling large data sets using SQL and databases in a business environment
· Must be proficient with Oracle/SQL Server/ Redshift/Tera data
· Familiar with ETL and DW processes
· Prior experience with Scala, Python or Java
· Strong troubleshooting and problem solving skills
Preferred Qualifications
· Previous experience with Linux
· Experience with multiple database platforms
· Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complex analysis
","['sql', 'linux', 'scala', 'redshift', 'python', 'java', 'excel', 'oracl']","['etl', 'problem solving', 'commun']",1,"['sql', 'linux', 'scala', 'redshift', 'python', 'java', 'excel', 'oracl', 'etl', 'problem solving', 'commun']","['set', 'relat', 'power', 'avail', 'python', 'comput', 'warehous', 'etl']","['sql', 'linux', 'scala', 'redshift', 'python', 'java', 'excel', 'oracl', 'etl', 'problem solving', 'commun', 'set', 'relat', 'power', 'avail', 'python', 'comput', 'warehous', 'etl']"
DE,"Posted: Feb 6, 2020
Role Number:
200148400
Bring passion and dedication to your job and there's no telling what you could accomplish.
Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis?
These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet services enabling business drivers to make critical decisions.
The team member will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.
Key Qualifications
Database development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop
Experience in designing and developing ETL data pipelines.
Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs
You will demonstrate excellent understanding of development processes and agile methodologies
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Experience with or advance courses on data science and machine learning is ideal
Work/project experience with Big Data and advanced programming languages is a plus
Experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus
You will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability.
Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems.
Translate complex business requirements into scalable technical solutions meeting data warehousing design standards.
Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency.
Build dashboards using Self-Service tools like Tableau and perform data analysis to support business.
Ability to communicate effectively, both written and verbal, with technical and non-technical multi-functional teams.
You will interact with many other group’s internal team to lead and deliver elite products in an exciting rapidly changing environment.
Education & Experience
Bachelors Degree
","['sql', 'spark', 'hadoop', 'tableau', 'java', 'hive', 'kafka', 'excel', 'oracl']","['big data', 'pipelin', 'dashboard', 'tune', 'machine learning', 'data warehousing', 'financ', 'etl']",1,"['sql', 'spark', 'hadoop', 'tableau', 'java', 'hive', 'kafka', 'excel', 'oracl', 'big data', 'pipelin', 'dashboard', 'tune', 'machine learning', 'data warehousing', 'financ', 'etl']","['day', 'basi', 'analyt', 'program', 'learn', 'machin', 'challeng', 'spark', 'pipelin', 'relat', 'hadoop', 'big', 'etl', 'integr']","['sql', 'spark', 'hadoop', 'tableau', 'java', 'hive', 'kafka', 'excel', 'oracl', 'big data', 'pipelin', 'dashboard', 'tune', 'machine learning', 'data warehousing', 'financ', 'etl', 'day', 'basi', 'analyt', 'program', 'learn', 'machin', 'challeng', 'spark', 'pipelin', 'relat', 'hadoop', 'big', 'etl', 'integr']"
DE,"Posted: Jun 18, 2020
Weekly Hours: 40
Role Number:
200175836
Imagine what you could do here.
Bring passion and dedication to your job and there's no telling what you could accomplish.
The Product Marketing Customer Analytics team is seeking a data engineer to support customer analytics with advanced, scalable and robust architecture, tools, data products, and critical data pipelines that are optimized for rapid business intelligence, data analysis, and data science.
Key Qualifications
Proficient in SQL and programming (Python preferred)
Experience with MPP databases preferred
6+ years of experience in data engineering and ETL pipeline development.
3+ years of Spark development.
6+ years of experience in Big Data Technologies (Hadoop, MapReduce, Hive etc…).
Spark experience preferred.
Experience on Kubernetes, Docker preferred.
Technical
Experience in designing, developing, and managing a highly optimized, flexible, and scalable data platform for customer analytics.
Experience building a connected low latency data platform (highly distributed, scalable with high availability), and stitching together various large and disparate data sources for data analysis.
Deep experience in Big Data, Cloud and programming.
Deep experience in developing custom ETL frameworks and developing robust, low latency and fault tolerant data pipelines dealing with very high volumes.
Deep experience with relational databases and data warehouses (preferably MPP system such as Teradata), and optimizing SQL statements on large data set.
Deploy inclusive data quality checks to ensure high quality of data
Problem Solving
Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs.
Project Management / Product Design
Significant experience managing data engineering projects through all phases, including requirements, ETL, data quality assessments, and data exploration.
Communication
Strong documentation and technical writing skills.
Attention to detail and effective verbal/written communication skills.
Environment / Culture
Can work effectively on sometimes ambiguous data and constructs within a fast changing environment, tight deadlines and priority changes.
Education & Experience
Prefer:
BS/MS in Computer Science Quantitative Finance, Math, Physics or a related Engineering degree
","['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker']","['big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math']",1,"['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker', 'big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math']","['analyt', 'spark', 'pipelin', 'set', 'relat', 'etl', 'hadoop', 'optim', 'python', 'avail', 'warehous', 'comput', 'big', 'quantit', 'sourc', 'engin']","['sql', 'mapreduc', 'spark', 'hadoop', 'cloud', 'python', 'hive', 'kubernet', 'docker', 'big data', 'pipelin', 'data modeling', 'optim', 'problem solving', 'financ', 'etl', 'commun', 'math', 'analyt', 'spark', 'pipelin', 'set', 'relat', 'etl', 'hadoop', 'optim', 'python', 'avail', 'warehous', 'comput', 'big', 'quantit', 'sourc', 'engin']"
DE,"Responsibilities:
Work closely with the engineering team to advise on system architecture and help guide engineering priorities
Build and lead a team of analysts and data scientists
A little more about you:
18+ months of NLP, Data Science, ML professional work
Experience specifying and building clean and functional data pipelines
Comfort manipulating and analyzing complex, unstructured, data from various sources
Ability to communicate complex quantitative analysis and approaches clearly
Nice-to-have experience:
Named Entity Recognition
Dependency Parsing
Semantic Role Labeling
Probabilistic String Matching
Elasticsearch, Apache Spark
Perks:
Small team = opportunity for big impact
Compensation includes equity and competitive salary
Excellent company-sponsored benefits
Stocked kitchen including all of the Topo Chico your heart desires
An Equal Opportunity Employer
","['spark', 'excel', 'elasticsearch']","['clean', 'nlp', 'pipelin']",999,"['spark', 'excel', 'elasticsearch', 'clean', 'nlp', 'pipelin']","['spark', 'pipelin', 'ml', 'scientist', 'big', 'quantit', 'sourc', 'engin']","['spark', 'excel', 'elasticsearch', 'clean', 'nlp', 'pipelin', 'spark', 'pipelin', 'ml', 'scientist', 'big', 'quantit', 'sourc', 'engin']"
DE,"Â
Title: Data Engineer/Software Engineer
Â
Key Qualifications:
Â
Â Strong programming skills in Java, knowledge of Scala is a plus
Â Experience with Big Data applications that use Spark, Hive, Kafka, Hadoop, and Oozie
Â Knowledge of build and test tools such as Maven, Gradle, SBT, and JUnit
Â Good understanding of relational and NoSQL databases
Â Experience writing and optimizing SQL queries
Â Experience in developing ETL data pipelines
Â Strong communication skills
Â Passion for excellence and commitment to continuous learning
Â
In addition, all colleagues are eligible for a number of rewards and recognition programs.
The Client retains the discretion to add or change the duties of the position at any time
","['sql', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka', 'excel']","['etl', 'commun', 'pipelin', 'big data']",999,"['sql', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka', 'excel', 'etl', 'commun', 'pipelin', 'big data']","['program', 'engin', 'spark', 'pipelin', 'relat', 'hadoop', 'big', 'etl', 'â']","['sql', 'spark', 'scala', 'hadoop', 'java', 'hive', 'nosql', 'kafka', 'excel', 'etl', 'commun', 'pipelin', 'big data', 'program', 'engin', 'spark', 'pipelin', 'relat', 'hadoop', 'big', 'etl', 'â']"
DE,"Data Engineer
ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow
Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle
Experience with cloud-based platforms and tools
Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps
Experience with integrating to both database systems and APIs
Experience with documenting technical requirements, designs and systems
Extensive experience building scalable and resilient data pipelines
Extensive experience writing SQL
Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R
Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
3-5+ years of experience in technology and/or consulting
Bachelor's Degree in CS, MIS, CIS, or a comparable technical degree
","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud']","['data warehousing', 'etl', 'pipelin', 'big data']",1,"['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']","['sql', 'jira', 'python', 's3', 'redshift', 'nosql', 'java', 'hbase', 'kafka', 'oracl', 'azur', 'bigqueri', 'git', 'hadoop', 'snowflak', 'hive', 'google cloud', 'cassandra', 'scala', 'r', 'microsoft', 'spark', 'airflow', 'cloud', 'data warehousing', 'etl', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'azur', 'hadoop', 'python', 'big', 'etl', 'engin']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers.
You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion.
You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Now Hiring a Senior Data Engineer
Ability to maintain an Interim Secret Clearance Required
An Interim Secret Clearance is required along with the ability to commute to Austin, Texas.
Work in a team using cutting edge technologies to solve challenging business problems and build solutions
Work in an agile environment with participation in daily stand-ups/scrum
Design, write, test, troubleshoot, and document application code
Provide mentorship to junior developers
Learn new technologies and be aware of industry standards, best practices, and trends.
Profile of Success
Bachelor's degree in a related field and 4 years of experience
Deep knowledge of data ingestion strategies and understanding of the V-dimensions of data (velocity, volume, variety, veracity)
Extensive experience with the Azure storage technologies (Azure Data Lake, Azure SQL Data Warehouse, Azure SQL Database)
Extensive experience with Azure data movement and transformation capabilities (Azure Data Factory, Data Lake Analytics, Data Bricks, Stream Analytics)
Proven experience developing Big Data solutions in the Azure space
SQL Server 2014+ experience.
Comfortable with Microsoft SQL data technologies (SSAS/SSIS/SSRS)
Proven ability to work with clients to understand requirements and envision data ingestion solutions
Possess DoD 8570 security certification
Desirable Skills
Microsoft related certifications such as the MCSD/MCSE
Experience with Hadoop-based technologies (HDInsight, Spark, Hive, Pig, Scala, etc.)
Experience with visualization tools such as Power BI or Tableau
","['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'bi', 'tableau', 'hive', 'microsoft', 'power bi']","['visual', 'big data']",1,"['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'powerbi', 'tableau', 'hive', 'microsoft', 'visual', 'big data']","['analyt', 'stream', 'spark', 'azur', 'visual', 'power', 'hadoop', 'provid', 'bi', 'warehous', 'texa', 'big', 'relat', 'engin', 'particip']","['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'powerbi', 'tableau', 'hive', 'microsoft', 'visual', 'big data', 'analyt', 'stream', 'spark', 'azur', 'visual', 'power', 'hadoop', 'provid', 'bi', 'warehous', 'texa', 'big', 'relat', 'engin', 'particip']"
DE,"Redefining an industry is hard work and it takes focused, dedicated team players.
Come get in the game.
As a Data Engineer, you will work with stakeholders and the internal development team to guide technical development of enterprise data solutions.
The Data Engineer is a combination of a business and technical customer facing role that will be accountable for the end-to-end customer data architecture, development, deployment and support.
The Data Engineer will be responsible for working with stakeholders to identify opportunities to leverage data to drive business value.
In addition, you will be responsible for mining and analyzing data to drive efficiency and optimization, develop custom data models and algorithms, and develop processes and tools to monitor production systems and data accuracy.
The ideal candidate will have experience in customer facing and development management roles and have led successful technical and economic value discussions with senior customer executives, driving decisions and implementation.
Requirements:
Austin, TX
Minimum 5 years related experience
AWS and\or Azure experience architecting solutions in Cloud Environments
Agile software development experience
Data background either in Analytics, Warehousing, Data Integration\API Dev, Visualization, etc
Technical Background
Can be one of following or multiple:
Data Engineer
Integration Engineer using SSIS, Talend, Pentaho
Data Warehousing
Data Modeling
Big Data technologies
Map/Reduce, Hadoop, Hive, Spark, Elasticsearch, etc
SQL Server, MySQL, Aurora primary experience (Oracle, Postgres, MongoDB secondary experience)
Data Scientist
Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
TensorFlow, Kubernetes
R, SQL, Python – Pandas, Scikit, Numpy
Data Visualization
SQL Server, MySQL, Aurora primary experience (Oracle, Postgres, MongoDB secondary experience)
Map/Reduce, Hadoop, Hive, Spark, etc
It features a powerful method to integrate internal, proprietary data and third-party data into a common platform that will transform analytics into insights.
Additionally, no other business intelligence tool uses real-time animation to show time-series data trends with the ability to customize that data on the fly.
This can be a fully remote or work-from-home position, but those based in Austin have the option to work in an office.
It features a powerful method to integrate internal, proprietary data and third-party data into a common platform that will transform analytics into insights.
Additionally, no other business intelligence tool uses real-time animation to show time-series data trends with the ability to customize that data on the fly.
","['sql', 'python', 'tensorflow', 'scikit', 'oracl', 'azur', 'hadoop', 'hive', 'postgr', 'kubernet', 'mongodb', 'elasticsearch', 'panda', 'aw', 'r', 'mysql', 'pentaho', 'spark', 'numpi', 'cloud']","['glm', 'data mining', 'visual', 'boost', 'neural network', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'statist', 'random forest', 'big data', 'decision tree', 'regress', 'econom', 'account']",999,"['sql', 'python', 'tensorflow', 'scikit', 'oracl', 'azur', 'hadoop', 'hive', 'kubernet', 'mongodb', 'elasticsearch', 'panda', 'aw', 'r', 'pentaho', 'spark', 'numpi', 'cloud', 'glm', 'data mining', 'visual', 'boost', 'nn', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'statist', 'random forest', 'big data', 'decision tree', 'regress', 'econom', 'account']","['techniqu', 'visual', 'python', 'integr', 'common', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'statist', 'primari', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'power', 'scientist', 'relat']","['sql', 'python', 'tensorflow', 'scikit', 'oracl', 'azur', 'hadoop', 'hive', 'kubernet', 'mongodb', 'elasticsearch', 'panda', 'aw', 'r', 'pentaho', 'spark', 'numpi', 'cloud', 'glm', 'data mining', 'visual', 'boost', 'nn', 'data modeling', 'machine learning', 'optim', 'data warehousing', 'statist', 'random forest', 'big data', 'decision tree', 'regress', 'econom', 'account', 'techniqu', 'visual', 'python', 'integr', 'common', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'statist', 'primari', 'aw', 'parti', 'big', 'engin', 'machin', 'spark', 'power', 'scientist', 'relat']"
DE,"Responsibilities:
Author and monitor directed acyclic graphs (DAGs) in Apache Airflow to ingest and transform data
Build and maintain internal Python packages to streamline ingest processes and add connections for new types of data
Manage Kubernetes infrastructure and PostgreSQL databases on Google Cloud Platform
Participate in Agile / Kanban processes on a daily basis
Comply with change management policies and code reviews to ensure data integrity and system stability
Requirements
BS/MS in a STEM field and 2+ years of industry experience programming and working with data
Exceptional understanding of data architecture and software engineering best practices
2+ years experience with Python (Python 3 preferred)
2+ years experience with SQL (PostgreSQL preferred)
2+ years of experience with Docker
1+ years experience with cloud infrastructure (GCP preferred)
1+ years of server orchestration (Kubernetes preferred)
Experience using Apache Airflow or similar data pipeline systems
Experience using Git or other DVCS
Knowledge of Agile / Kanban processes
Entrepreneurial spirit and highly self-motivated
Benefits
Competitive base salary with ability to earn bonuses
Professional development and entrepreneurial opportunities
Paid time off
401(k)
Medical and dental plans
","['sql', 'google cloud', 'gcp', 'airflow', 'git', 'cloud', 'python', 'postgresql', 'kubernet', 'docker']","['graph', 'pipelin']",1,"['sql', 'google cloud', 'gcp', 'airflow', 'git', 'cloud', 'python', 'postgresql', 'kubernet', 'docker', 'graph', 'pipelin']","['basi', 'program', 'pipelin', 'infrastructur', 'python', 'packag', 'integr', 'engin', 'particip']","['sql', 'google cloud', 'gcp', 'airflow', 'git', 'cloud', 'python', 'postgresql', 'kubernet', 'docker', 'graph', 'pipelin', 'basi', 'program', 'pipelin', 'infrastructur', 'python', 'packag', 'integr', 'engin', 'particip']"
DE,"Your primary role will be to design and build data pipelines.
You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive.
In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies.
If you believe you have these skills please email your resume to info@springml.com.
Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR
","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker']","['clean', 'visual', 'pipelin', 'big data']",1,"['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data']","['program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']","['google cloud', 'gcp', 'spark', 'perl', 'pig', 'bigqueri', 'unix', 'hadoop', 'javascript', 'cloud', 'python', 'hive', 'java', 'c', 'kubernet', 'docker', 'clean', 'visual', 'pipelin', 'big data', 'program', 'spark', 'challeng', 'pipelin', 'visual', 'power', 'hadoop', 'primari', 'python', 'comput', 'big']"
DE,"Now Hiring a Data Engineer
Ability to maintain an Interim Secret Clearance is Required
Must be able to maintain an Interim Secret Clearance.
Must be able to commute to Austin, Texas.
Conduct preliminary data evaluation and database structure, optimize and standardize if necessary.
Work with SME and ATF EDW team to build the scripts for the CGI Dashboard view
Importing NESS data into EDW
Creating Reports in PowerBI that are high value for field or executives
NESS data visualized in the CGI Dashboard
NESS Data Validation with emphasis on data from other ATF databases
Work with NESS Dev team to identify best solutions for incorporating RMS data into NESS
Translate business needs into longterm architecture solutions
Review, simplify and optimize existing data import process from various sources such as NIBIN, etrace and various RMS imports
Review object and data models and the metadata repository to structure the data for better management and quicker access
Profile of Success
Bachelor's degree in an IT-related field and two years of experience
Proven experience developing Big Data solutions in the Azure space
Extensive experience with the Azure suite (Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Data Lake Analytics, HDInsight, Machine Learning, and Stream Analytics)
SQL Server 2014/2016 experience
Experience using Spark, Hive, Pig, and Scala
Comfortable with Microsoft Full Stack (SSAS/SSIS/SSRS)
In-depth knowledge of Data Warehousing and ETL/ELT
Proven ability to work with clients to understand requirements and to envision solutions
Possess DoD 8570 security certification
Desirable Skills
Background with Data Science tools such as R, Python, and SAS is a plus
Microsoft related certifications such as the MCSD/MCSE
Experience with Tableau
Experience working with Hadoop ecosystem
","['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi']","['dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']",1,"['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi', 'dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl']","['stream', 'visual', 'python', 'etl', 'sourc', 'evalu', 'analyt', 'azur', 'sa', 'hadoop', 'optim', 'texa', 'learn', 'warehous', 'big', 'engin', 'machin', 'spark', 'relat']","['sql', 'spark', 'azur', 'scala', 'pig', 'sa', 'ssr', 'hadoop', 'tableau', 'python', 'hive', 'r', 'microsoft', 'powerbi', 'dashboard', 'visual', 'machine learning', 'optim', 'data warehousing', 'big data', 'etl', 'stream', 'visual', 'python', 'etl', 'sourc', 'evalu', 'analyt', 'azur', 'sa', 'hadoop', 'optim', 'texa', 'learn', 'warehous', 'big', 'engin', 'machin', 'spark', 'relat']"
DE,"For over a decade, weve helped Fortune 5000 businesses throughout the mobile app lifecycle with data-backed decisions at every step.
Everything You Need to Succeed on Mobile Transforming Digital Human Experience
Phunwares Software Development Kits (SDKs) include location-based services, mobile engagement, content management, messaging, advertising, loyalty (PhunCoin & Phun) and analytics, as well as a mobile application framework of pre-integrated iOS and Android software modules for building in-house or channel-based mobile application and vertical solutions.
Job Summary:
The ideal candidate will bring strong technical skills and be proactive, responsive and very comfortable dealing with ambiguity.
He or she will also bring good experience with Big Data systems/technologies and have a strong track record of deployment, maintenance, and optimization of production code.
The ideal candidate is someone who combines an understanding of business processes with knowledge of both client and server-side technical requirements in mobile software projects.
They will put the customer first, quickly build strong relationships, learn rapidly, and enjoy autonomy and problem-solving.
They must be a gifted leader with a genuine passion for working with high-performance teams, extraordinarily organized., and have a strong work ethic.
Additionally, the position may require travel both domestically and internationally.
What Youll Do:
Create robust, high-volume production systems/architectures, and develop prototypes quickly
Work with development teams to design maintenance and support strategies
Create optimized workflows using relevant technologies (Spark, Elastic Search, Kafka, Oozie, Hadoop)
Create architectural workflows, diagrams, and specification documents to help define platform features/functionality
Perform experiments and analyze results to improve the performance and quality of algorithms
Work with product management and executive stakeholders to take detailed requirements and implement them using Agile Test Driven techniques
Work in an organized team-oriented environment with shared responsibilities
What Youll Bring:
Bachelors Degree or higher in Computer Science or Computer Engineering; Masters Degree preferred
Have previously worked in Big Data technologies and deployed in production environment
Strong experience in building highly scalable, available and responsive systems using open-source software tools and technologies
5-10 years of professional software development
5-8 years strong Java development experience
Good experience with REST API frameworks
Strong SQL skills
1+ years of professional software development experience with some of the big data technologies including: Spark, Map Reduce, Hive, HBase, Hadoop, Kafka, Impala, Cassandra
Experience in Elastic Search is highly desirable
Some experience with one or more of the following will be an added advantage: statistical analysis, machine learning, natural language processing, predictive modeling
Domain experience in one or more of the following:
Outstanding skills for interacting with people
Responsible, organized and hardworking with excellent communication skills
Must be living in the Irvine, CA or Austin, TX area or be able to immediately relocate
Desirable:
NoSQL or similar DB design/implementation experience with large number of records (i.e.
1 Billion+)
Experience with information retrieval, network programming and/or developing large software systems
Experience with cloud delivery platforms, ideally Amazon
Experience doing Test Driven Development (TDD), Continuous Integration (CI) and test automation
Open-source software contributions
Track record of success in a start-up or high-growth environment
Compensation and Benefits:
Fun, casual, fast-paced work environment filled with talented colleagues
Flexible paid time off
Competitive salary
Restricted Stock Units
Full range of benefits, including 401(k), medical, dental and vision coverage
","['sql', 'spark', 'cassandra', 'hadoop', 'cloud', 'db', 'java', 'hive', 'hbase', 'nosql', 'kafka', 'excel']","['natural language processing', 'big data', 'predict', 'machine learning', 'optim', 'statist', 'analyz', 'commun']",1,"['sql', 'spark', 'cassandra', 'hadoop', 'cloud', 'db', 'java', 'hive', 'hbase', 'nosql', 'kafka', 'excel', 'nlp', 'big data', 'predict', 'machine learning', 'optim', 'statist', 'analyz', 'commun']","['analyt', 'digit', 'machin', 'program', 'techniqu', 'spark', 'human', 'predict', 'hadoop', 'optim', 'avail', 'comput', 'statist', 'big', 'integr', 'sourc', 'engin', 'algorithm']","['sql', 'spark', 'cassandra', 'hadoop', 'cloud', 'db', 'java', 'hive', 'hbase', 'nosql', 'kafka', 'excel', 'nlp', 'big data', 'predict', 'machine learning', 'optim', 'statist', 'analyz', 'commun', 'analyt', 'digit', 'machin', 'program', 'techniqu', 'spark', 'human', 'predict', 'hadoop', 'optim', 'avail', 'comput', 'statist', 'big', 'integr', 'sourc', 'engin', 'algorithm']"
DE,"Social:
See more details below!
Position Summary
Essential Job Functions:
Responsible for the availability of all Redshift game data
Resolves data ingestion issues and is proactive in recommending long-term solutions to prevent future issues
Works with individual team members and their managers to optimize queries and table schemas to meet demands
Run SQL explains on queries for query optimization
Responsible for maintaining documentation on new data sources, intermediate structures, and how best to access data
Education:
Bachelor’s degree or higher in Computer Science, MIS, or equivalent experience
Required Experience:
Minimum 2-5 years of experience with Big Data analytics and querying Big Data
Minimum 1 year of experience with Amazon AWS or similar cloud-based data storage solution
Preferred Experience:
Experience with Amazon AWS
Advanced SQL skills, preferably PostgreSQL or HiveQL
Expert on use and management of Redshift or PostgreSQL tables, schemas, and functions
Experience with developing, scheduling, and maintaining ETL processes, especially in an AWS environment
Clear and effective communication of complex ideas and analysis
Ability to work with little direction, self-directed
Active learner, motivated to learn without guidance
Solutions oriented attitude of customer service
Critical thinking and creative problem solving
Qualifications
Competitive salaries and annual bonuses alongside matching 401k, employee stock options, and other competitive benefits.
Strong commitment to work/life balance
Fully stocked kitchen
Brand new, shiny, and aesthetically pleasing 10,000 sq ft office located in the Arboretum area in Austin
SciPlay Corporation and its affiliates (collectively, SciPlay) are committed to creating a workforce of credibility and dependability.
As a prerequisite to employment with SciPlay (to the extent permitted by law), you shall be asked to consent to SciPlay conducting a due diligence/background investigation on you.
The employee in this position may be requested to perform other job-related tasks and responsibilities than those stated above.
SciPlay is an Equal Opportunity Employer and does not discriminate against applicants due to race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you’d like more information about your equal employment opportunity rights as an applicant under the law, please click here EEOC Poster.
The employee in this position may be requested to perform other job-related tasks and responsibilities than those stated above.
If you’d like more information about your equal employment opportunity rights as an applicant under the law, please click here EEOC Poster.
","['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql']","['optim', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql', 'optim', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'analyt', 'essenti', 'corpor', 'relat', 'optim', 'aw', 'avail', 'employe', 'comput', 'big', 'etl', 'sourc']","['sql', 'shini', 'aw', 'cloud', 'redshift', 'postgresql', 'optim', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'analyt', 'essenti', 'corpor', 'relat', 'optim', 'aw', 'avail', 'employe', 'comput', 'big', 'etl', 'sourc']"
DE,"The best candidate will be someone that can understand data issues, perform profiling and analysis using SQL and stored procedures.This is mostly a back end data position but does require some interaction with business users.
Most of the work will be performed remotely.
Qualifications Minimum six (6) years experience writing SQL code preferable SQL Server or MySQL.
Knowledge with Python is required.
Ability to work with business users and management to understand the business need for data exploration, data quality and profiling.
Experience working with relational databases, data modeling and writing ETL scripts to move data from source systems to an Enterprise Datawarehouse.
Understanding of Data Warehousing concepts and experience in creating data warehouse schemas with Kimball methodology.
Knowledge working with data in the cloud (AWS) desired.
Solid understanding of Software Development Lifecycle (SDLC) and versionsource control disciplines.
Be able to work effectively in an Agile Project Development team Must be able to define functional and technical docs based on data availability, quality and profiling on source data systems.
The ability to work effectively within a team environment Understand complex logic and solve data issues by coming up with sound technical solutions.
Soft Skills The ability to work effectively with minimal direction and supervision.
To think creativity and come up with solutions that improve process and efficiencies.
Ability to understand existing process and requirements.
Understand complex logic and solve data issues by coming up with sound technical solutions.
Must have solid written and oral communication skills.
Can prepare and maintain technical documentation.
Education Bachelors Degree in Computer Science, Finance or Data Analytics preferred.
SQL andor Python related certifications preferred Work references will be requested.
Must be authorized to work in the U.S.
","['sql', 'aw', 'cloud', 'python', 'mysql']","['data modeling', 'data warehousing', 'financ', 'supervis', 'etl', 'commun']",1,"['sql', 'aw', 'cloud', 'python', 'data modeling', 'data warehousing', 'financ', 'supervis', 'etl', 'commun']","['analyt', 'relat', 'aw', 'avail', 'python', 'comput', 'warehous', 'etl', 'sourc']","['sql', 'aw', 'cloud', 'python', 'data modeling', 'data warehousing', 'financ', 'supervis', 'etl', 'commun', 'analyt', 'relat', 'aw', 'avail', 'python', 'comput', 'warehous', 'etl', 'sourc']"
DE,"This position is located in Austin, TX
The ideal candidate will have direct, applied experience with one or more of the following areas:
- Develop data structures and systems to support the generation of business insights
- Knowledge and experience in overall ETL processes
- Maintain data infrastructure and develop scripts for regular processes
- Define, design, and develop data flow diagrams, data dictionaries, and logical and physical models
- Define data requirements, document data elements, and capture and maintain metadeta
- Identify and clean incomplete, incorrect, inaccurate or irrelevant data
- Identify new opportunities to use data to improve business performance
- Communicate and present data by developing reports using Tableau or Business Intelligence tools
- Adhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.)
is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
Secret or Top Secret clearance is preferred.
","['mongodb', 'sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'tableau', 'redshift', 'python', 'java', 'nosql', 'postgr', 'cloud', 'kafka']","['clean', 'statist', 'big data', 'information technology', 'etl', 'commun']",1,"['mongodb', 'sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'tableau', 'redshift', 'python', 'java', 'nosql', 'cloud', 'kafka', 'clean', 'statist', 'big data', 'information technology', 'etl', 'commun']","['spark', 'relat', 'hadoop', 'etl', 'infrastructur', 'aw', 'python', 'appli', 'statist', 'big', 'quantit', 'engin']","['mongodb', 'sql', 'spark', 'cassandra', 'scala', 'ec2', 'hadoop', 'aw', 'tableau', 'redshift', 'python', 'java', 'nosql', 'cloud', 'kafka', 'clean', 'statist', 'big data', 'information technology', 'etl', 'commun', 'spark', 'relat', 'hadoop', 'etl', 'infrastructur', 'aw', 'python', 'appli', 'statist', 'big', 'quantit', 'engin']"
DE,"As a successful Data Engineer, you have a strong background in cloud infrastructure, particularly AWS and Google Cloud Platform.
You strive to excel at everything you do while being able to prioritize between the must-haves and nice-to-haves.
If you're intrinsically motivated and ready to roll up your sleeves and dive in—we'd love to hear from you!
What you'll do:
Create ETL (Extract, Transform & Load) pipelines to deliver sanctioned data to stakeholders, while maintaining high accuracy and reliability
Tune and monitor data infrastructure Performance to support a growing organization
Brainstorm data product ideas and partner closely with Data Scientists, Product Management and Operations teams to develop, test, deploy, and operate high-quality software
Develop data infrastructure that ingests and transform data from different sources and customers at scale.
Partner end-to-end with Business Managers, Product Managers, and Data Scientists to understand customer requirements and design prototypes and bring ideas to production
Work with internal business leaders to ingest data to enrich their data modeling and work products.
Participate in conversations with teams about business-impacting topics and brainstorm innovative ways to transform data into information and knowledge that drives revenue and reduces cost
BS or MS in Computer Science or equivalent
5+ years of data warehousing or data engineering experience with a distinguished track record on technically demanding projects
Deep knowledge of SQL databases (preferably PostgreSQL)
Strong experience working with Python, particularly for ETL or Data Science related tasks
Experience working in a data lake architecture, separating compute from storage
Passion for creating new products and services, including being comfortable with the ambiguity associated with designing new products
Experience working with REST APIs to ingest and enrich data sets
Experience with Apache Airflow for workflow management is preferred
is preferred
Data Science/Machine Learning background is preferred
Familiarity with the construction industry is preferred
Perks & Benefits
You are a person with dreams, goals, and ambitions—both personally and professionally.
","['sql', 'google cloud', 'airflow', 'aw', 'cloud', 'python', 'postgresql']","['pipelin', 'tune', 'data modeling', 'machine learning', 'data warehousing', 'etl']",1,"['sql', 'google cloud', 'airflow', 'aw', 'cloud', 'python', 'postgresql', 'pipelin', 'tune', 'data modeling', 'machine learning', 'data warehousing', 'etl']","['machin', 'learn', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin', 'particip']","['sql', 'google cloud', 'airflow', 'aw', 'cloud', 'python', 'postgresql', 'pipelin', 'tune', 'data modeling', 'machine learning', 'data warehousing', 'etl', 'machin', 'learn', 'pipelin', 'set', 'relat', 'infrastructur', 'aw', 'python', 'scientist', 'comput', 'etl', 'sourc', 'engin', 'particip']"
DE,"Complex SQL is used in BigData or DWH
",['sql'],[None],999,['sql'],[None],"['sql', None]"
DE,"A successful candidate should possess a passion for data, an innate curiosity, excellent problem-solving skills, and strong technical knowledge of Microsoft SQL Server and Microsoft data tools.
In addition, you should have strong verbal and written communication skills and work well with others across many departments and groups.
Key Responsibilities
Build, document, test and launch new data extract, transform and load (ETL) process for one or more core data sets.
Collaborate with Business Intelligence team and other stakeholders for data set requirements
Collaborate with Data Management team to provide quality data to stakeholders.
General Requirements
Highly collaborative and empathetic approach to problem solving
Naturally curious with the drive to learn what is needed to identify root cause
Capable of inputting high-level, sometimes ambiguous requirements into operational tools/dashboards
Demonstrated organizational skills, attention to detail and ability to work independently while staying integrated with the broader Data Engineering team as well as stakeholders
A strategist with sound business and technical acumen.
Proven ability to influence at various levels of leadership, without direct authority
Strong problem-solving skills (critical, strategic, and evaluative thinking)
Ability to synthesize complex data concepts into easily comprehended messages
Required Education and Skills
Bachelor's degree required in related field
2 - 4 years of SSMS & T-SQL experience a MUST
2 - 4 years of ETL Automation (SSIS Packages)
Relational Data Base Architecture experience
SSAS / Tabular Modelling preferred
","['sql', 'excel', 'microsoft']","['dashboard', 'etl', 'problem solving', 'commun']",1,"['sql', 'excel', 'microsoft', 'dashboard', 'etl', 'problem solving', 'commun']","['relat', 'etl', 'evalu', 'packag', 'set', 'engin', 'integr']","['sql', 'excel', 'microsoft', 'dashboard', 'etl', 'problem solving', 'commun', 'relat', 'etl', 'evalu', 'packag', 'set', 'engin', 'integr']"
DE,"You…
As a Data Engineer you will work with the application and data science teams to support the development of custom data solutions.
Us…
Program Mission…
The project you will be working on is an existing cloud-based information technology infrastructure to host mission systems, applications, services, and data.
While the current environment exists as an enterprise enabling platform, the end state of the environment is to enable Army leaders at every echelon to make fully informed, data driven decisions, based on authoritative and/or production data sources.
Additionally, modernize current applications by breaking them down into loosely coupled micro-services, and leveraging a continuous integration / continuous delivery pipeline to enable an agile DevOps Strategy.
Capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets.
Experience with traditional, modern, and cloud native database solutions.
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file.
Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.
Current IAT I Certification (A+ CE, Network+, CCNA, SSCP).
Desired Skills:
Works with considerable freedom to make decisions on the techniques and approaches to be used.
Prepares recommendations for system improvement for management and user consideration.
Years of Experience: 3 years of experience or more
Education: Bachelor’s Degree in Computer Science, Information Systems, or other closely related discipline
Clearance: Active Secret Security Clearance Required
",['cloud'],"['recommend', 'information technology', 'pipelin']",1,"['cloud', 'recommend', 'information technology', 'pipelin']","['program', 'techniqu', 'pipelin', 'set', 'relat', 'infrastructur', 'comput', 'integr', 'sourc', 'engin']","['cloud', 'recommend', 'information technology', 'pipelin', 'program', 'techniqu', 'pipelin', 'set', 'relat', 'infrastructur', 'comput', 'integr', 'sourc', 'engin']"
DE,"WorldQuants success is built on a culture that pairs academic sensibility with accountability for results.
Great ideas come from anyone, anywhere.
Thats a key ingredient in remaining a leader in any industry.
The Role:
Design and implement software to facilitate data integration with trading and simulating systems
Adopt new technologies to improve existing frameworks of data flow and monitoring
Implement and maintain software that interface with external vendors to bring in new data sets
Implement the rules and procedures that ensure integrity in data sets
Provide second level support to production support team regarding market data issues
Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes
Design and implement systems that track and manage data availability, access and usage
What Youll Bring:
Degree in a quantitative or technical discipline from a top university and strong academic scores
Interest in applying technology to real situations, comfortable working in a fast-paced environment, detail-oriented and capable of performing tasks under pressure
Demonstrated experience with C++ or other object oriented languages
Experience with scripting languages such as Perl, Python, and shell scripting; Interface with database (such as MySQL)
Possess strong trouble shooting and problem solving skills
Ability to work independently and as member of a team
Strong verbal and written communication skills
Have experience working under a Linux environment, familiar with Vim or Emacs for editing files under the command line
All Rights Reserved.
","['mysql', 'linux', 'python', 'perl']","['problem solving', 'statist', 'analyz', 'commun', 'account']",1,"['sql', 'linux', 'python', 'perl', 'problem solving', 'statist', 'analyz', 'commun', 'account']","['set', 'line', 'provid', 'python', 'avail', 'statist', 'quantit', 'collect', 'integr']","['sql', 'linux', 'python', 'perl', 'problem solving', 'statist', 'analyz', 'commun', 'account', 'set', 'line', 'provid', 'python', 'avail', 'statist', 'quantit', 'collect', 'integr']"
DE,"Overview
You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.
What You'll Be Doing
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
It's time to rethink the possible.
Are you ready?
","['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'mysql', 'kafka', 'excel', 'oracl']","['pipelin', 'etl', 'commun', 'math']",1,"['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'kafka', 'excel', 'oracl', 'pipelin', 'etl', 'commun', 'math']","['analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'etl', 'engin']","['sql', 'mongodb', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'r', 'kafka', 'excel', 'oracl', 'pipelin', 'etl', 'commun', 'math', 'analyt', 'spark', 'pipelin', 'hadoop', 'python', 'comput', 'etl', 'engin']"
DE,"Senior Data Engineer
Career opportunity:
Join an industry with massive socio, economic, and political importance in the 21st century
Work alongside some of the best and the brightest minds in the security industry
Be recognized, internally and publicly, for your contributions in a high profile position
Core responsibilities:
Create tools to scour the internet to find important security information and ingest it into their infrastructure
Work with data scientists to create and maintain data ontologies for security
Mentor junior data engineers and teach them how to use data engineering techniques to solve real world problems
Communicate complex concepts to team members
Accountable for:
Creation of data engineering pipelines to find and ingest security vulnerabilities
Creation of data engineering tools to help label and validate data
Required qualifications:
At least 8 years experience designing and building data processing/ETL pipelines
At least 8 years experience in Python and Spark or similar technologies
At least 8 years experience with SQL and relational databases
At least 8 years experience parsing flat files
8+ years development experience
Bachelor's degree or equivalent practical experience
Desired qualifications:
Experience working with Google Tensorflow
Experience with modern technology stacks
Experience with micro-services architectures
Experience with agile/scrum development practices
Experience with test driven development, continuous integration, continuous deployment
Experience with Git, JIRA, Confluence
Experience with Google Compute, Firebase, and GKE
Experience with Docker
Desired behaviors:
Relentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems
Determination to always learn and get better and never rest on ones laurels
Personable individual who enjoys working in a team-oriented environment
Ability to work within constraints and to challenge the status quo
Ability to self-direct work and truly own the position in a hyper-growth environment
Compensation package:
Competitive compensation
Ownership opportunity through employee stock option plan
Health, dental, and vision insurance
","['sql', 'spark', 'firebas', 'jira', 'git', 'python', 'tensorflow', 'docker']","['pipelin', 'etl', 'commun', 'econom', 'account']",1,"['sql', 'spark', 'firebas', 'jira', 'git', 'python', 'tensorflow', 'docker', 'pipelin', 'etl', 'commun', 'econom', 'account']","['techniqu', 'spark', 'pipelin', 'relat', 'infrastructur', 'employe', 'python', 'scientist', 'packag', 'comput', 'etl', 'engin', 'integr']","['sql', 'spark', 'firebas', 'jira', 'git', 'python', 'tensorflow', 'docker', 'pipelin', 'etl', 'commun', 'econom', 'account', 'techniqu', 'spark', 'pipelin', 'relat', 'infrastructur', 'employe', 'python', 'scientist', 'packag', 'comput', 'etl', 'engin', 'integr']"
DE,"OverviewOBXtek Inc. is an established, award-winning business providing information technology and professional management services to the federal government.
In this AFC, uses the Modernization Application & Data Environment (MADE) in a managed service provider style model to enable subordinate elements throughout AFC to access commercial cloud services.
AFC DDSD has primary responsibility for the provisioning and operating the shared common services, maintaining approved desired state configurations for Infrastructure as a Service (IaaS) deployments, operating and maintaining a central data warehouse capability, and maintaining the Risk Management Framework (RMF) documentation for the commons services and environmental level accreditation in a manner that maximizes inheritance.The Data Engineer will:* Work with application and data science teams to support development of custom data solutions.
* Support rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets.
* Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.
* Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file.
* Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.Potential CONUS Travel of 25%QualificationsActive Secret ClearanceRequires a Bachelor's degree in Computer Science, Information Systems, Engineering, or other Scientific/Technical discipline* Experience with traditional, modern, and cloud native database solutions.
* 3 years of related data engineering work experience.
",['cloud'],"['information technology', 'risk']",1,"['cloud', 'information technology', 'risk']","['relat', 'primari', 'infrastructur', 'provid', 'comput', 'warehous', 'set', 'common', 'engin']","['cloud', 'information technology', 'risk', 'relat', 'primari', 'infrastructur', 'provid', 'comput', 'warehous', 'set', 'common', 'engin']"
DE,"The National Research Center for College and University Admissions™ (NRCCUA®), now a part of ACT®, is a membership organization that links colleges and universities to the nation’s largest college and career planning program for students seeking post-secondary guidance.
In addition, members can receive exclusive access to Encoura™ Data Lab—an educational data science, analytics, and research platform.
All Eduventures Research is now available in the Encoura Data Lab platform.
The Senior Database Engineer is a hands-on technical position for a senior-level professional.
The role involves an advanced, experienced skill set to design, develop and implement database objects, procedures and processes using the SQL Server platform to support business objectives throughout the organization.
This is a hands-on role, working with other engineers, writing code, testing, and deploying the finished apps and libraries.
Responsibilities and Deliverables
In partnership with your co-workers, design and develop database infrastructure (tables and views) to support a complex and rapidly changing data environment.
Create working, maintainable, and fast Python scripts and stored procedures using best practices and current organizational standards to support data-driven applications, both internal and client-facing.
Use indexing and other techniques to optimize new and existing objects and processes.
Develop processes for the ETL of data throughout the entire organization.
Generate data to support reporting (ad-hoc and standardized).
Follow and help develop database team standards and methodologies; use source control and build management procedures to ensure stable development, staging and production database environments.
Enhance, refactor, and continuously improve the database schemas and related code.
Communicate effectively with technical and non-technical people.
Solve business needs with short-term deliverables, while constantly improving and moving towards long-term architectural goals.
Generate new ideas, never say or think ""that's not my job.""
Be proactive in keeping your skills fresh
Qualifications and Experience
5+ years of T-SQL development experience in SQL Server.
3+ years of experience developing ETL processes with SQL Server Integration Services, Pentaho, or other tools.
Bonus points for having developed ETL processes with Python and AWS.
2+ years of experience in development with Python 3.
1+ years of experience developing on AWS and Linux.
Mastery of advanced database design methodologies and experience with database modeling tools, dimensional modeling and statistical analysis
Clear understanding of SQL Server best practices for development of stored procedures, views, tables, security objects, indexes, etc.
Experience with any of the following is a plus: Postgres, SQL Server Reporting Services, MongoDB, Redis, Exasol
An appreciation for pragmatism and simplicity in code.
A strong code and architecture design sensibility.
Customized mathematical skills as determined by the requirements of the job
All applicants must be eligible to work in the U.S.
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability and need an alternative method for applying, please contact the People Team.
","['mongodb', 'sql', 'linux', 'aw', 'python', 'postgr', 'pentaho']","['research', 'etl', 'commun', 'statist']",999,"['mongodb', 'sql', 'linux', 'aw', 'python', 'pentaho', 'research', 'etl', 'commun', 'statist']","['analyt', 'program', 'techniqu', 'set', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'statist', 'etl', 'sourc', 'engin', 'integr']","['mongodb', 'sql', 'linux', 'aw', 'python', 'pentaho', 'research', 'etl', 'commun', 'statist', 'analyt', 'program', 'techniqu', 'set', 'relat', 'infrastructur', 'aw', 'python', 'avail', 'statist', 'etl', 'sourc', 'engin', 'integr']"
DE,"The role is visible and critical as part of a high performing team - it will appeal to you if you have an effective combination of domain knowledge, relevant experience and the ability to execute on the details.
You will bring cutting edge software and full stack development skills with advanced knowledge of cloud and data lake experience while working with massive data volume.
DevOps)Here's what sets you apart:* 5+ years of data warehousing/data lake development experience * Strong data modeling and data integration experience* Strong SQL and higher-level programming languages* Solid knowledge of data mining, machine learning algorithms and tools* Good understanding of data warehouse/data lake design patterns and best practices* Solid understanding of data ingestion (i.e.
streaming platforms like Kafka)* Strong experience with data integration tools - ETL/ELT tools (i.e.
Do not include any medical or health information in this email.
The Reasonable Accommodations team will respond to your email promptly.PDN-MCR-108348-1
","['sql', 'cloud', 'kafka']","['data mining', 'data modeling', 'machine learning', 'data warehousing', 'etl']",999,"['sql', 'cloud', 'kafka', 'data mining', 'data modeling', 'machine learning', 'data warehousing', 'etl']","['machin', 'learn', 'etl', 'warehous', 'integr', 'algorithm']","['sql', 'cloud', 'kafka', 'data mining', 'data modeling', 'machine learning', 'data warehousing', 'etl', 'machin', 'learn', 'etl', 'warehous', 'integr', 'algorithm']"
DE,"You will participate and effectively contribute to the design, development, and implementation of complex applications, often using new technologies.
You will provide technical expertise and systems design for individual initiatives.
Education & Experience:
Candidate should have recent work experience with US-based customers
Minimum 5 years of relevant experience is required
Technical Skill Requirements:
4+ years of data engineer experience with large data lake platforms, ideally google or hadoop.
Familiarity with cloud, specifically GCP.
Experience utilizing tools like Spark and understand how to move data, build code-based data pipelines, data transformation, etc.
Development experience with Python and/or Go-lang
Plus: Hotspots and composure experience- how to break queries out—Saves time, performance, and money.
Retail/E-commerce industry would be a plus.
Experience with very large (multi-petabyte size) data lakes.
Experience with Unix (Shell, Scripting) is very helpful.
Development experience with building APIs – ex: REST, etc.—Python and/or Go-Lang
Experience with BigQuery and Composer.
Operational Experience managing a large data lake
Experience with code-based pipelines (ex Spark).
Some exposure to Java, Scala, and/or Python is a plus
","['gcp', 'spark', 'scala', 'bigqueri', 'unix', 'cloud', 'java', 'python']",['pipelin'],999,"['gcp', 'spark', 'scala', 'bigqueri', 'unix', 'cloud', 'java', 'python', 'pipelin']","['python', 'spark', 'engin', 'pipelin']","['gcp', 'spark', 'scala', 'bigqueri', 'unix', 'cloud', 'java', 'python', 'pipelin', 'python', 'spark', 'engin', 'pipelin']"
DE,"Job Overview
Responsibilities and Duties
Re-designing infrastructure for greater scalability and stability, etc.
Qualification and Experience
A bachelor’s or higher degree in Computer Science, Physics, Statistics, Informatics, Information Systems or another quantitative field.
3+ years of work experience in software design and development
Advanced working SQL and CQL knowledge and experience working with relational databases and Cassandra, query authoring (SQL, CQL, KSQL, SparkSQL) as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object development languages: Python, Java, C++, Scala, etc.
Start-up experience is ideal
Salary based on experience - $100K +
","['sql', 'spark', 'cassandra', 'scala', 'python', 'java', 'postgr', 'nosql']",['statist'],1,"['sql', 'spark', 'cassandra', 'scala', 'python', 'java', 'nosql', 'statist']","['analyt', 'stream', 'spark', 'relat', 'infrastructur', 'python', 'comput', 'statist', 'quantit']","['sql', 'spark', 'cassandra', 'scala', 'python', 'java', 'nosql', 'statist', 'analyt', 'stream', 'spark', 'relat', 'infrastructur', 'python', 'comput', 'statist', 'quantit']"
DE,"At a glance:
Are you an experienced engineer passionate about supporting data operations by gathering and processing raw data at scale?
Can you commit to a position measuring the quality, integrity, accuracy and completeness of data used in company-wide solutions?
Do you desire a competitive salary with lucrative benefits and a focus on professional development?
Highlights:
As a Data Engineer II, you enhance operations for reporting, applications and data science by gathering and processing raw data at scale.
You focus on maintaining scalable, reliable, consistent and repeatable systems that efficiently provide data for company-wide operations.
You excel at profiling data to measure quality, integrity, accuracy and completeness.
You deliver effective data solutions through the development, testing and implementation of code and scripts.
You maximize the department by developing data set processes for data modeling, mining and consumption.
You have cultivated a keen ability for identifying roadblocks and overcoming obstacles to see your projects completed on time and within budget.
You report to the Manager, Network Engineering for goals, guidance and assistance.
Position benefits:
Competitive Salary with Bonus
Health, Vision and Dental Insurance
Education Assistance
Pretax Child Care Spending Account
Paid Holidays, Vacation Days, Personal Days and Sick Days
What you will do:
Drive company-wide excellence through active and consistent support of all efforts to simplify and enhance the client experience.
Support data operations for reporting, analytics, applications and data science by creating and maintaining scalable, reliable, consistent and repeatable systems.
Drive data operations processes through the collection and processing of raw data at scale, including writing scripts, web scraping, calling APIs and writing SQL queries.
Maintain, improve, clean and manipulate data using extract, transform and load (ETL) processes.
Profile data to measure quality, integrity, accuracy and completeness.
Improve department processes by implementing tools, scripts, queries and applications for ETL and data operations.
Deliver solutions by developing, testing and implementing code and scripts.
Produce reports and uphold data delivery schedules for senior leadership to leverage.
Maximize the department through the management of multiple data source lifecycles.
Increase delivery speed of data by implementing workflow automation solutions.
Perform additional duties related to the position as assigned.
Required keys for success:
One or more years of experience as a Linux, Unix or CentOS system administrator
Two or more years of hands-on experience with RDMNS, SQL, scripting and coding
Intermediate knowledge of coding and scripting using Python, R or shell scripts
Extensive experience with SQL, Tableau and ETL techniques
Background in Linus and CentOS installation and administration
Knowledge of data storage that demonstrates the correct use of a file system, relational database or NoSQL variant
Track record of effectively using Spark and Hadoop
Detail-oriented with the ability to effectively prioritize and execute multiple tasks
Passion for learning new techniques, methods and concepts regarding data
Interest in learning programming languages
Effective written and spoken English communication skills with all levels of an organization
How you will stand out from the crowd:
Familiar with JavaScript API, Rest API or Data Extract APIs
Experience with data workflow or data preparation platforms, such as Infomatica, Pentaho or Talend
Knowledge of best practices and IT operations in an always-up, always-available service
History of working with data virtualization concepts and software, such as Denodo, Teiid and JBoss
Track record of receiving, converting and cleansing data
Able to work with visualization or BI tools, such as Tableau
Your education:
Bachelors Degree in engineering, computer science or a related field (required)
","['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'bi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho']","['visual', 'data modeling', 'clean', 'etl', 'commun', 'account']",1,"['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'powerbi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho', 'visual', 'data modeling', 'clean', 'etl', 'commun', 'account']","['day', 'analyt', 'techniqu', 'engin', 'spark', 'set', 'relat', 'visual', 'hadoop', 'bi', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'integr']","['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'powerbi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho', 'visual', 'data modeling', 'clean', 'etl', 'commun', 'account', 'day', 'analyt', 'techniqu', 'engin', 'spark', 'set', 'relat', 'visual', 'hadoop', 'bi', 'python', 'avail', 'comput', 'etl', 'sourc', 'collect', 'integr']"
DE,"Work with application and data science teams to support development of custom data solutions
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file
Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development
Be capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets
Experience with traditional, modern, and cloud native database solutions
Must meet IAT-I requirements as specified in DoD 8570.01-M (A+, Network +, etc.)
Active Secret clearance
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, disability, or status as a protected veteran.”
",['cloud'],[None],2,['cloud'],"['set', 'engin']","['cloud', 'set', 'engin']"
DE,"The ideal candidate is experienced in building data pipelines, data services, and distributed/concurrent systems.
What you'll do
Build services that extract, transform, and load (ETL) data from a variety of data sources using cloud-based, ""big data"" technologies (e.g., Spark, Hadoop)
Building services that leverage queuing and stream processing to deliver near-real-time data services
Create and maintain optimal data pipeline architecture
Generate test data sets based on established internal and external schemas
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build data analytics services that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Make heavy use of Google Cloud offerings to provide timely, accurate, and comprehensive data services to large research universities for analytics and reporting
Participate in a production support rotation with other engineers
What makes you a great fit
You love building automated data pipelines for batch and event data processing
You love wrangling, processing, and analyzing data in streamlined data systems
You love building systems from the ground up
Required skills
Experience in building concurrent, distributed systems
Expert-level knowledge of streaming and batch ETL techniques
Expert-level knowledge of SQL
Expert-level knowledge of relational and NoSQL databases (e.g.
Postgres, BigQuery)
Experience with distributed query systems (e.g., Hive, Spark SQL, Presto)
Python, Ansible, Docker, Kubernetes, Git
Skills that will set you apart
Experience building catalog and inventory systems
Experience with RStudio, Jupyter
Experience in translating analytical models in R to production implementations
Apache Airflow
Experience with GitfFlow & GitOps
Benefits
Great team
Cool technology
Mission of high social value*
Flexibility to implement new, custom, and better solutions
Competitive salary
Excellent medical, dental, and optical plans
403b with matching
Open PTO
Free parking
Loaded kitchen
Flexible hours
Powered by JazzHR
9UZqd62Hza
","['sql', 'google cloud', 'spark', 'airflow', 'jupyt', 'bigqueri', 'git', 'hadoop', 'cloud', 'rstudio', 'python', 'hive', 'postgr', 'nosql', 'r', 'kubernet', 'excel', 'docker']","['research', 'pipelin', 'optim', 'big data', 'etl']",999,"['sql', 'google cloud', 'spark', 'airflow', 'jupyt', 'bigqueri', 'git', 'hadoop', 'cloud', 'r', 'python', 'hive', 'nosql', 'r', 'kubernet', 'excel', 'docker', 'research', 'pipelin', 'optim', 'big data', 'etl']","['analyt', 'techniqu', 'stream', 'spark', 'pipelin', 'set', 'relat', 'power', 'hadoop', 'infrastructur', 'optim', 'python', 'action', 'big', 'etl', 'sourc', 'engin', 'particip']","['sql', 'google cloud', 'spark', 'airflow', 'jupyt', 'bigqueri', 'git', 'hadoop', 'cloud', 'r', 'python', 'hive', 'nosql', 'r', 'kubernet', 'excel', 'docker', 'research', 'pipelin', 'optim', 'big data', 'etl', 'analyt', 'techniqu', 'stream', 'spark', 'pipelin', 'set', 'relat', 'power', 'hadoop', 'infrastructur', 'optim', 'python', 'action', 'big', 'etl', 'sourc', 'engin', 'particip']"
DE,"null
",[None],[None],999,[],[None],[None]
DE,"Note to applicants: Resumes are required as part of your application.
When applying from a mobile device or tablet, you may not be able to attach a resume.
As a successful candidate, you demonstrate strong technical and analytical ability as well as are detail oriented and passionate about building data pipelines.
Also, you enjoy optimizing data systems and building them from the ground up.The level of seniority for this position is negotiable based on experience.Responsibilities* Formulate, design, develop, test, and deliver data technology solutions with a balanced focus on speed and quality.
* Collaborate with business analysts, product owners, and project managers to develop user stories, estimates, and work plans.
Advise business clients and IT management of technology capabilities and recommend strategies to maximize the benefits of new technologies.
* Identify, design, and implement changes to data pipelines at various stages including data ingestion, data validation, and quality control, data integration, storage, management, and data delivery.
* Write unit/integration tests, contribute to engineering wiki, and write detailed documentation* Build and enhance CI/CD pipelines and develop supportable solutions.
* Participate in code and design reviews.
This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.
",[None],['pipelin'],999,['pipelin'],"['analyt', 'program', 'pipelin', 'integr', 'engin', 'particip']","['pipelin', 'analyt', 'program', 'pipelin', 'integr', 'engin', 'particip']"
DE,"Jr. Data Engineer
Austin, TX
Smart people with a passion for technology
Ability to solve challenging business problems
Self-directed professionals
Hunger to continually learn and grow
Responsibilities:
Work with application and data science teams to support development of custom data solutions
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file
Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development
Requirements:
Bachelors Degree in Computer Science, Information Systems, Engineering, or other Scientific/Technical discipline and 3 years of related work experience
Be capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets
Experience with traditional, modern, and cloud native database solutions
Must meet IAT-I requirements as specified in DoD 8570.01-M (A+, Network +, etc.)
Active Secret clearance
This job posting sets forth the authorities and responsibilities of this position, which may be changed from time to time as shall be determined.
#LI-NP1
#clearance
#dice
",['cloud'],[None],1,['cloud'],"['learn', 'relat', 'comput', 'set', 'engin']","['cloud', 'learn', 'relat', 'comput', 'set', 'engin']"
DE,"This position will be the Senior technical resource driving architecture for the integration of large 3rd party partner integrations with companies like Facebook, Google and Twitter to name a few.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
This position will require the ability to own and lead data initiatives on a cross-functional team.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
You should be able to not take yourself too seriously as well.
And most of all, you will enjoy working with great people who are changing the entire industry.
What you'll do:
Migrate existing data pipelines from on-prem regional data centers to AWS and GCP.
Architect a new modern event driven architecture with both batching and streaming
Adjust existing pipelines to fit the AWS processing model such as integration with S3, migrate to open source version of hadoop, adjustments to security model, etc...
Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Partner and work closely with the QA Engineers to develop automated tests
Participate in training and mentoring of junior team members
8+ years of experience designing and building data-intensive applications
5+ years architecting systems in a big data ecosystem using MapReduce, Spark, MPP Data Warehouses, and sql/nosql databases.
5+ years recent hands-on experience with object oriented languages (Java, Scala, Python)
5+ years Hands on experience building production level systems in a cloud environment (AWS or GCP)
Excellent interpersonal and communication skills in English
Proven experience leading the design and execution of event driven architectures for distributed systems
Experience designing systems for performance, scalability, and reliability
In-depth understanding of object oriented programming concepts
Low level working knowledge of collections, multi-threading, JVM memory model, etc.
Solid understanding of database fundamentals and SQL
Understanding the full software development life cycle, agile development and continuous integration
Ability to clearly communicate with team-members in a cross-matrix environment
What puts you over the top:
Built systems in a containerized environment with familiarity in Docker, ECS, Kubernetes
Exposure to Data Warehousing solutions like Snowflake and BigQuery
Equal Opportunity Employer:
California Applicant Pre-Collection Notice:
","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker']","['pipelin', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun']","['engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']","['sql', 'mapreduc', 'gcp', 'spark', 'scala', 'bigqueri', 'hadoop', 'aw', 'snowflak', 'python', 's3', 'java', 'nosql', 'cloud', 'kafka', 'excel', 'docker', 'pipelin', 'data warehousing', 'big data', 'etl', 'commun', 'engin', 'spark', 'pipelin', 'hadoop', 'aw', 'python', 'parti', 'warehous', '3rd', 'big', 'etl', 'sourc', 'collect', 'particip', 'integr']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.
You will be recognized as an established contributor by your team.
You will contribute design and implementation components for multiple projects.
You will work mostly independently with limited oversight.
You will also participate in client-facing discussions in areas of expertise.
Pathway to Success
Your success comes from your enthusiasm, insight, and positive impact.
You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'appli', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"This individual must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
* Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and big data technologies* Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
* Work with stakeholders including the product management, data and design teams to assist with data-related technical issues and support their data infrastructure needs.
* Experience building and optimizing 'big data' data pipelines, architectures and data sets.
* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
* Strong analytic skills related to working with unstructured datasets.
* Build processes supporting data transformation, data structures, metadata, dependency and workload management.
* A successful history of manipulating, processing and extracting value from large disconnected datasets.
* Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
* Good organizational skills.
* Experience supporting and working with cross-functional teams in a dynamic environment.
* Experience with big data tools: Hadoop, Spark, Kafka, etc.
* Experience with relational SQL and NoSQL databases, including Postgres, Cassandra, MongoDB.
* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
* Experience with cloud services: such as EC2, EMR, RDS, Redshift, Azure services* Experience with stream-processing systems: Storm, Spark-Streaming, etc.
","['mongodb', 'sql', 'spark', 'airflow', 'cassandra', 'azur', 'ec2', 'hadoop', 'cloud', 'redshift', 'postgr', 'nosql', 'kafka']","['optim', 'pipelin', 'big data']",999,"['mongodb', 'sql', 'spark', 'airflow', 'cassandra', 'azur', 'ec2', 'hadoop', 'cloud', 'redshift', 'nosql', 'kafka', 'optim', 'pipelin', 'big data']","['analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'infrastructur', 'optim', 'action', 'big', 'set', 'sourc']","['mongodb', 'sql', 'spark', 'airflow', 'cassandra', 'azur', 'ec2', 'hadoop', 'cloud', 'redshift', 'nosql', 'kafka', 'optim', 'pipelin', 'big data', 'analyt', 'stream', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'infrastructur', 'optim', 'action', 'big', 'set', 'sourc']"
DE,"This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python.
Preferably you know that countMinSketch is not a children's game.
You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR.
You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential.
You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products.
When someone says, ""my Spark job failed"", your first question is ""what's the skew?"".
Responsibilities:
Design, test, and implement storage solutions for various consumers of the data.
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods.
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics.
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines.
Collaborate on the design, implementation, and deployment of applications with the rest of software engineering.
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data.
Build and maintain tools for automation, deployment, monitoring, and operations.
Create test plans, test cases, and run tests with automated tools.
Requirements:
5+ years of experience with Python3, and 2+ years experience with Spark.
Scala experience is helpful.
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments.
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles.
Strong emphasis on unit testing and code quality.
Proficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc).
Preferred Experience:
Very strong Python and PySpark experience.
Very strong back end development experience.
Strong experience with cloud deployments and CI/CD.
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN).
Experience with NoSQL Non-Relational databases (AWS DynamoDB).
Education:
MS or BS in Computer Science or a related field, or equivalent work experience required.
Perks:
A career path with opportunities to grow
Discretionary Paid Time Off policy to promote a healthy work/life balance + world-class benefits
And many, many more perks!
To check out what it's like to be a Bloxer click here.
#LI-AB1
","['sql', 'pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'nosql', 'postgresql', 'mysql', 'kubernet', 'docker']","['pipelin', 'optim', 'statist', 'big data', 'etl']",1,"['sql', 'pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'nosql', 'postgresql', 'kubernet', 'docker', 'pipelin', 'optim', 'statist', 'big data', 'etl']","['analyt', 'spark', 'pipelin', 'relat', 'optim', 'aw', 'python', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin', 'algorithm']","['sql', 'pyspark', 'spark', 'scala', 'ec2', 'aw', 'lambda', 'python', 's3', 'cloud', 'nosql', 'postgresql', 'kubernet', 'docker', 'pipelin', 'optim', 'statist', 'big data', 'etl', 'analyt', 'spark', 'pipelin', 'relat', 'optim', 'aw', 'python', 'scientist', 'statist', 'comput', 'big', 'etl', 'sourc', 'engin', 'algorithm']"
DE,"They will leverage PaaS and IaaS cloud offerings to build services that support data management, infrastructure and industry interoperability.
They should contribute to projects and development efforts using agile methodologies.
YOUR RESPONSIBILITIES
Leverage Cloud PaaS and IaaS offerings to build services in support of data liquidity, data management and storage, data pipelines including both traditional ETL and streaming platforms, master data management and data stewardship
Write traditional code and server-less functions using the language best suited for the task, which typically include C#, T-SQL and PowerShell
Provide subject matter expertise on performance tuning and query optimization to full-stack peers and data analysts
Participate in building and owning a culture of DevOps and Quality Assurance
Continuously document your code, framework standards, and team processes
EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE
Five (5) or more years of experience in an enterprise or commercial software development environment.
Healthcare IT background is highly preferred.
Extensive experience developing data-intensive solutions in a Cloud environment.
Highly skilled in writing SQL queries, DML, DDL, CDC/change tracking, index and performance tuning.
Enterprise development experience coding in at least one, but preferably more than one, procedural/OO language, including C#, Java, Python, PowerShell.
Enterprise experience developing solutions that use event sourcing and/or Big Data architectures.
Experience with at least once traditional ETL platform including SSIS, Informatics and Talend, or with cloud ETL platforms like Azure Data Factory.
PROFESSIONAL COMPETENCIES
Team player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures
Good communicator - both written and verbal - with high emotional intelligence
Ability to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt
Together.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
","['sql', 'azur', 'cloud', 'java', 'python', 'c']","['healthcar', 'pipelin', 'risk', 'tune', 'optim', 'big data', 'etl', 'commun']",2,"['sql', 'azur', 'cloud', 'java', 'python', 'c', 'healthcar', 'pipelin', 'risk', 'tune', 'optim', 'big data', 'etl', 'commun']","['pipelin', 'azur', 'infrastructur', 'provid', 'optim', 'python', 'big', 'etl', 'sourc', 'particip']","['sql', 'azur', 'cloud', 'java', 'python', 'c', 'healthcar', 'pipelin', 'risk', 'tune', 'optim', 'big data', 'etl', 'commun', 'pipelin', 'azur', 'infrastructur', 'provid', 'optim', 'python', 'big', 'etl', 'sourc', 'particip']"
DE,"Overview
ProSphere is seeking an experienced Data Engineer to assist in strategizing, designing, continually improving and operating an existing Department of Army Government client’s cloud-based information technology infrastructure to host mission systems, applications, services, and data.
This is full-time position that can be located in Austin, TX, National Capital (DC) Region, Redstone, AL, Detroit, MI, Natick, MA, Orlando, FL along with the possibility of telework options.
Responsibilities
• Works with application and data science teams to support development of custom data solutions.
• Capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets.
• Experience with traditional, modern, and cloud native database solutions.
• Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.
• Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file.
• Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.
Qualifications
• Requires a Bachelor’s degree in Computer Science, Information Systems, Engineering, or other Scientific/Technical discipline and • 3 years of related data engineering work experience.
• Must meet IAT-I requirements as specified in DoD 8570.01-M (A+ CE, CCNA-Security, CND, Network+ CE, SSCP)• U.S.
Citizenship (Government Requirement)• Must have an active Secret Clearance
Qualifications Highly Desired• U.S. Army or Army Future Command experience • Former Military
Knowledge/Skills/Abilities• Well organized• Superior attention to detail• Exceptional multi-tasking skills• Demonstrated and verifiable ability to recruit, develop, support, and maintain high performing teams
Physical Demands
• Ability to sit in an office environment for long periods of time • Typical office environment.
Ability to sit and stand for extended periods of time; ability to lift 5-20 lbs
All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law.
",['cloud'],['information technology'],1,"['cloud', 'information technology']","['relat', 'infrastructur', 'comput', 'set', 'engin']","['cloud', 'information technology', 'relat', 'infrastructur', 'comput', 'set', 'engin']"
DE,"The data practice is still very young within the organization and this individual will be tasked with helping develop it into a more robust environment.
The management and organization of data is highly technical and requires advanced skills with computers and proficiency with data-oriented computer languages such as Python, SQL, and XML.
A Senior Data Engineer possesses superior analytical skills and is detail-oriented.
This Senior Data Engineer will be required to communicate effectively with C-Level and needs to explain complex technical concepts to non-technical staff.
Since development of data models and logical workflows is common, a Senior Data Engineer must also exhibit advanced visualization skills, as well as creative problem-solving.
Responsibilities Plans, architects, designs, analyses, develops, codes, tests, debugs and documents data analytics platforms to satisfy business requirements for large, complex Data ReservoirData Warehouse, Reporting Analytics development Lead and perform database level tuning and optimization in support of application development teams on an ad-hoc basis.
Analyses business and data requirements to support the implementation of an applicationrsquos full functionality Contributes to high level functional design used across all Reporting Analytics applications based on system build and knowledge of business needs Collaborates with fellow team members and keeps the team and other key stakeholders well informed of progress of application business features being developed Create data architecture strategies for each subject area of the enterprise data model.
Communicate plans, status and issues to higher management levels.
Collaborate with the business and other IT organizations to plan a data strategy.
Qualifications Bachelor or Masterrsquos degree in computer science or similar 5+ years of experience with demonstrated knowledge in the design development of data warehouses andor data reservoirdata lakedata mart platform.
4+ years of advanced analytics tools methodologies (Python, R etc.)
4+ years of experience in data ingestion tools and techniques including ETL ELT methodologies.
3+ years experience in Tableau is preferred but other visualization tools such as PowerBI will also be considered.
Working proficiency in a selection of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of software.
Strong communication skills with demonstrated experience coordinating development cycles and project management.
Corporate httpswww.rekruiters.com httpswww.google.comurl?qhttpswww.rekruiters.
ndash Main Site rekruiters.com ndash Twitter httpswww.facebook.comrekruiters httpswww.google.comurl?qhttpswww.facebook.-hlw ndash Facebook ------------------------------------------------------------------------ JOB ID - 6581
","['sql', 'tableau', 'python', 'c', 'r', 'powerbi']","['visual', 'tune', 'optim', 'etl', 'commun']",1,"['sql', 'tableau', 'python', 'c', 'r', 'powerbi', 'visual', 'tune', 'optim', 'etl', 'commun']","['basi', 'analyt', 'techniqu', 'corpor', 'visual', 'optim', 'python', 'comput', 'warehous', 'etl', 'common', 'engin']","['sql', 'tableau', 'python', 'c', 'r', 'powerbi', 'visual', 'tune', 'optim', 'etl', 'commun', 'basi', 'analyt', 'techniqu', 'corpor', 'visual', 'optim', 'python', 'comput', 'warehous', 'etl', 'common', 'engin']"
DE,"H-E-B Digital is seeking new team members
(Partners)!
customers’ digital experience, reinventing how they find inspiration from food,
how they make food decisions, and how they ultimately get food into their
homes.
the stack: front-end web and mobile, full-stack, and backend engineering.
using the best available technologies to deliver modern, engaging, reliable,
solutions are growing in popularity and adoption—like Curbside and Home
Delivery—so you’ll get the opportunity to define the user experience for
millions of customers and hundreds of thousands of Partners.
who enjoys taking on new challenges, working in a rapidly changing environment,
learning new skills, and applying it all to solve large and impactful business
In the Sr. Data Engineer job, that means
you have a…
HEART FOR PEOPLE… you can organize multiple engineers, negotiate
solutions, and provide upward communication
HEAD FOR BUSINESS… you consistently demonstrate and uphold the
standards of codding, infrastructure, and process
in multiple technical domains
What you’ll do
provide data solutions for ecommerce, supply chain, store operations,
finance, and marketing reporting and analytics platforms
Contribute to existing data
platforms and implement new technologies
Develop a deep understanding of
Ensure data is distributed in a
timely and accurate manner
Make data discoverable and
accessible to business users
Who You
Are
4 years of
data engineering experience
Proficient
with data technologies (e.g.
Spark, Kinesis, Kafka, Airflow, Oracle,
PostgreSQL, Redshift, Presto, etc.)
Experienced
with designing and developing ETL data pipelines using tools such as
Airflow, Nifi, or Kafka.
Strong
understanding of SQL and data modeling
Understanding
of Linux, Amazon Web Services (or other cloud platforms), Python, Docker,
and Kubernetes
Experienced
with common software engineering tools (e.g., Git, JIRA, Confluence, or
similar)
Bachelor's
degree in computer science or comparable field or equivalent experience
A proven
understanding and application of computer science fundamentals: data
structures, algorithms, design patterns, and data modeling
What are the Perks?
A robust Benefits
plan with coverage starting Day One
Dental, vision, life,
and other insurance plans; flexible spending accounts; short term / long term
disability coverage
Partner Care Team,
for any time you have healthcare or coverage questions
Telehealth offers
24/7 access to board-certified doctors by phone
Partner Guidance
allows free counselor visits
Funeral leave, jury
duty, and military pay (subject to applicable law)
Maternal / paternal
leave for new parents, including adoptions
10"" off H-E-B brand
products in-store and online
Eligibility to
participate in 401(k)
Opportunity to become
a “Partner-Owner” after 12 months
Who
H-E-B is one of the largest, independently owned food
retailers in the nation, operating over 400 stores throughout Texas and Mexico,
with annual sales generating over $25 billion
people (109,000 Partners), and give them autonomy to be creative in how they
impact the business
and Inclusion as core values, and support them with thriving company-wide
programs
communities every day
resources—People—drive the innovation, growth, and success that make H-E-B The
04-2019
DASO3232
","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'amazon web services']","['healthcar', 'pipelin', 'data modeling', 'financ', 'etl', 'commun', 'account']",3,"['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'financ', 'etl', 'commun', 'account']","['day', 'analyt', 'digit', 'program', 'spark', 'challeng', 'pipelin', 'infrastructur', 'python', 'avail', 'comput', 'texa', 'etl', 'common', 'engin']","['sql', 'linux', 'spark', 'airflow', 'jira', 'git', 'cloud', 'python', 'redshift', 'postgresql', 'kubernet', 'kafka', 'docker', 'oracl', 'aw', 'healthcar', 'pipelin', 'data modeling', 'financ', 'etl', 'commun', 'account', 'day', 'analyt', 'digit', 'program', 'spark', 'challeng', 'pipelin', 'infrastructur', 'python', 'avail', 'comput', 'texa', 'etl', 'common', 'engin']"
DE,"Hypori is a Virtual Mobile Infrastructure platform that enables a secure BYOD system for clients.
You should have the experience to manage and build enterprise class software that can be deployed around the world.
You can learn more about the Hypori product at: https://hypori.com/
Investigate and recommend data platforms and tools to support Hypori’s analytics needs
Build data pipelines and transformation workflows
Mentor junior Data Engineers on data flow and quality
Advise on data stewardship and ETL/ELT quality assurance best practices
5+ years of data pipeline and ETL work and tuning data management environments
3+ years of data platform architecture
Experience applying common, data warehousing concepts (Kimball, Inmon, etc)
Experience with reporting and visualization for unstructured and structured data.
Experience with data cleansing and data quality functions.
Nice to Have
Professional Data Engineer Certification.
Experience embedding analytics in automated processes.
The perks of an Austin startup such as free snacks, soda, coffee, and a relaxed office
The opportunity to work on cool technology with brilliant people
Breakfast Taco Fridays
A Veteran Friendly Organization
#IW
#Hypori
",[None],"['data warehousing', 'etl', 'visual', 'pipelin']",999,"['data warehousing', 'etl', 'visual', 'pipelin']","['analyt', 'pipelin', 'visual', 'infrastructur', 'etl', 'common', 'engin']","['data warehousing', 'etl', 'visual', 'pipelin', 'analyt', 'pipelin', 'visual', 'infrastructur', 'etl', 'common', 'engin']"
DE,"Summary
***
What if there was a different way of looking at money?
This is a great opportunity to join a Fortune 500 and not-for-profit organization that gives you a great sense of purpose!
As a full-time Data Engineer you will work closely with the Data Architect as they provide guidance and vision so you can develop, construct and maintain the data architecture for the enterprise data professionals.
You will work with large scale data processing systems that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
Communication is key in this role as you will work with users of all levels across the organization.
Job Duties and Responsibilities:
Oversee and direct efforts to identify information and technology solutions that enable business needs and strategies.
Own and define DevOps pipelines and release management for data engineering
Lead efforts to analyze IT industry and market trends and determine potential impacts.
Develop concepts and constructs necessary to create technology-enabled business systems.
Influence technology direction and provide thought leadership and execution to large complex efforts.
Utilize breadth of technical understanding and dive deep when necessary.
Consult on and manage initiatives to ensure alignment across multiple business and IT areas.
Proactively mitigate risks across multiple assets, information domains, technologies and platforms.
Provide leadership, mentoring and technical guidance to others to drive initiatives.
Facilitate communications that involve obtaining cooperation and agreement on issues that may be complex or controversial.
Utilize negotiation and persuasion to come to agreement and to effectively form partnerships.
Act as a change agent to continuously improve and move the organization forward.
Accountable to successfully deliver the right results on initiatives in a timely and effective manner.
Direct the work of others to lead initiatives that cross multiple assets, technologies, platforms, departments and vendors.
Ability to work within a diverse team of skillsets and experience levels to deliver results.
Required Job Qualifications:
Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business, or related field.
10+ years of experience in Technology related field including 3+ years prior lead experience.
Expert knowledge of predictive analytics, statistical modeling, advanced mathematics, data integration concepts, business intelligence and data warehousing and implementing large systems
Implement and configure data platforms including but not limited to Hadoop, Spark, Kafka and batch integration is preferred.
Working Experience developing data processes with Java, Python, R or other scripting languages preferred.
","['spark', 'hadoop', 'java', 'python', 'r', 'kafka']","['pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']",1,"['spark', 'hadoop', 'java', 'python', 'r', 'kafka', 'pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']","['analyt', 'asset', 'techniqu', 'spark', 'pipelin', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'python', 'comput', 'statist', 'integr', 'sourc', 'engin']","['spark', 'hadoop', 'java', 'python', 'r', 'kafka', 'pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account', 'analyt', 'asset', 'techniqu', 'spark', 'pipelin', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'python', 'comput', 'statist', 'integr', 'sourc', 'engin']"
DE,"Your Mission
Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers.
You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion.
You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.
Expectations
Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly.
Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close.
You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule.
Details of the timeline can be shared.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools.
Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions.
Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing
Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.
The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.
","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'bi', 'cloud', 'python', 'hive', 'java', 'nosql']","['pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']",999,"['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun']","['day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']","['google cloud', 'spark', 'airflow', 'scala', 'bigqueri', 'bigtabl', 'hadoop', 'powerbi', 'cloud', 'python', 'hive', 'java', 'nosql', 'pipelin', 'machine learning', 'statist', 'big data', 'etl', 'commun', 'day', 'basi', 'program', 'techniqu', 'python', 'etl', 'common', 'analyt', 'hadoop', 'statist', 'infrastructur', 'bi', 'warehous', 'big', 'engin', 'machin', 'spark', 'pipelin', 'divers', 'relat']"
DE,"Job Overview
You will need to understand the business cases of the various products and build tools as well as data sets and access methodologies to support all of them in a scalable way.
Responsibilities:
Individual Contributor (expected to be hands-on coder 50%-80% of the time)
Lead the design and delivery of complex, cross-team systems built & deployed on AWS
Interview top talent for Engineering's staffing needs
Mentor highly talented engineers within the org
Required skills:
12+ years of overall relevant Software Engineering experience
7+ years of Data Engineering development experience including familiarity in multiple batch and streaming data processing technologies (Hadoop, Spark, Kafka, MapReduce, Hive, etc.)
3+ years Architecting, Designing and Building large-scale, multi-use Big Data Systems on Public cloud hosting providers
Strong communication skills (including influencing skills across multiple teams)
Extensively worked in a range of database technologies including SQL and NoSQL DB
Nice to Have:
3+ years in AWS environment
Ecommerce industry experience
Experience influencing Product and other stakeholders
Influenster is a digital destination where consumers discover products and reviews that enable them to make well-informed purchase decisions with over 6 million members who have written over 38 million product reviews.
For more information, visit
.
Last year, 135K reviews were submitted each day.
Commitment to diversity and inclusion
","['sql', 'mapreduc', 'spark', 'hadoop', 'aw', 'db', 'hive', 'nosql', 'kafka']","['commun', 'big data']",999,"['sql', 'mapreduc', 'spark', 'hadoop', 'aw', 'db', 'hive', 'nosql', 'kafka', 'commun', 'big data']","['day', 'digit', 'spark', 'divers', 'public', 'hadoop', 'provid', 'aw', 'big', 'set', 'engin']","['sql', 'mapreduc', 'spark', 'hadoop', 'aw', 'db', 'hive', 'nosql', 'kafka', 'commun', 'big data', 'day', 'digit', 'spark', 'divers', 'public', 'hadoop', 'provid', 'aw', 'big', 'set', 'engin']"
DE,"Career opportunity:
Join an industry with massive socio, economic, and political importance in the 21st century
Work alongside some of the best and the brightest minds in the security industry
Be recognized, internally and publicly, for your contributions in a high profile position
Core responsibilities:
Work with data scientists to create and maintain data ontologies for security
Mentor junior data engineers and teach them how to use data engineering techniques to solve real world problems
Communicate complex concepts to team members
Accountable for:
Creation of data engineering pipelines to find and ingest security vulnerabilities
Creation of data engineering tools to help label and validate data
Required qualifications:
At least 8 years experience designing and building data processing/ETL pipelines
At least 8 years experience in Python and Spark or similar technologies
At least 8 years experience with SQL and relational databases
At least 8 years experience parsing flat files
8+ years development experience
Bachelor's degree or equivalent practical experience
Desired qualifications:
Experience working with Google Tensorflow
Experience with modern technology stacks
Experience with micro-services architectures
Experience with cloud platforms and SaaS solutions
Experience with agile/scrum development practices
Experience with test driven development, continuous integration, continuous deployment
Experience with Git, JIRA, Confluence
Experience with Google Compute, Firebase, and GKE
Experience with Docker
Desired behaviors:
Relentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems
Determination to always learn and get better and never rest on ones laurels
Personable individual who enjoys working in a team-oriented environment
Ability to work within constraints and to challenge the status quo
Ability to self-direct work and truly own the position in a hyper-growth environment
Compensation package:
Competitive compensation
Ownership opportunity through employee stock option plan
Health, dental, and vision insurance
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'spark', 'firebas', 'jira', 'git', 'cloud', 'python', 'tensorflow', 'docker']","['pipelin', 'etl', 'commun', 'econom', 'account']",1,"['sql', 'spark', 'firebas', 'jira', 'git', 'cloud', 'python', 'tensorflow', 'docker', 'pipelin', 'etl', 'commun', 'econom', 'account']","['techniqu', 'spark', 'pipelin', 'relat', 'employe', 'python', 'scientist', 'packag', 'comput', 'etl', 'engin', 'integr']","['sql', 'spark', 'firebas', 'jira', 'git', 'cloud', 'python', 'tensorflow', 'docker', 'pipelin', 'etl', 'commun', 'econom', 'account', 'techniqu', 'spark', 'pipelin', 'relat', 'employe', 'python', 'scientist', 'packag', 'comput', 'etl', 'engin', 'integr']"
DE,"You'll be part of the final stages of transforming a service-oriented organization into a world-class product team.
Responsibilities:
Provide leadership and management to a team of data engineers.
Youll split your time between unblocking your team, vetting their designs, and coaching them to realize their full potential, while also building things yourself.
Proactively drive the vision for data engineering across the Finance Products pillar, and define and execute on a plan to achieve that vision.
Proactively mentor and grow the team, identify the required skills, hire the best talent, actively manage their performance and help them to grow to their fullest potential.
Define the processes needed to achieve operational excellence in all areas, including project management and system reliability.
Build cross-functional relationships with Data Scientists, Product Managers, Software Engineers, and leaders in Facebooks Finance business to understand data needs and deliver on those needs.
Drive the design, building, and launching of new data models and data pipelines in production.
Manage development of data resources and support new product launches.
Drive data quality across financial products and related business areas.
Manage the delivery of high impact dashboards and data visualizations.
Define and manage SLAs for all data sets and processes running in production.
Mininum Qualifications:
6+ years of experience in Data Engineering, BI, or Data Warehousing.
Experience scaling and managing 5+ person teams.
Communication and leadership experience and experience initiating and driving projects.
Project management experience.
Data architecture experience.
Experience in SQL or similar languages and development experience in at least one scripting language (Python, Perl, etc.).
BA/BS in Computer Science, Math, Physics, or other technical fields.
Preferred Qualifications:
Experience working in Financial Technology companies.
Experience partnering with leaders of Financial Planning & Analysis, Financial Operations, and/or Accounting teams.
Experience with Hive or Presto, data visualization tools (e.g., Tableau), and/or relational database management systems.
","['sql', 'perl', 'bi', 'tableau', 'python', 'hive', 'excel']","['pipelin', 'dashboard', 'visual', 'data warehousing', 'financ', 'commun', 'math', 'account']",1,"['sql', 'perl', 'powerbi', 'tableau', 'python', 'hive', 'excel', 'pipelin', 'dashboard', 'visual', 'data warehousing', 'financ', 'commun', 'math', 'account']","['pipelin', 'relat', 'visual', 'bi', 'provid', 'python', 'scientist', 'comput', 'set', 'engin']","['sql', 'perl', 'powerbi', 'tableau', 'python', 'hive', 'excel', 'pipelin', 'dashboard', 'visual', 'data warehousing', 'financ', 'commun', 'math', 'account', 'pipelin', 'relat', 'visual', 'bi', 'provid', 'python', 'scientist', 'comput', 'set', 'engin']"
DE,"Introduction:
What you will do:
Diagnose and resolve deficiencies in data quality and systems performance
Guide and assist team efforts for data collection, data cleansing, and data sharing
Perform exploratory analysis and provide ad-hoc reports
Track, evaluate, and contribute to technology advances across academic, open source, and commercial forums
About you:
Advanced skills in database programming (every aspect of SQL, plus PL/pgSQL or PL/SQL or T-SQL)
Basic skills in database administration (PostgreSQL preferred, but Oracle or MySQL okay)
Practical experience with database tuning and performance optimization
Solid understanding of RDBMS principles and modern practices
Solid programming skills (especially Java, Python, and/or JavaScript)
Practical experience with a variety of application data workloads (OLTP, OLAP, etc.)
Familiar with a variety of data integration and data warehouse approaches
Familiar with a variety of middleware approaches and tools (ORM, MQ, GraphQL, REST, webhook, etc)
Committed to leveraging data to elect Democrats and empower progressive organizations
Additional Qualifications (desired but not required):
Experience with cloud computing environments (Google Cloud preferred, but AWS or Azure okay)
Experience with ETL and data pipeline systems (AirFlow, Spark, Nifi, Stitch, Talend, or others)
Experience with data visualization, dashboard, and/or reporting tools (Tableau, Jasper, or others)
Experience with mapping/spatial/GIS data (including tools such as PostGIS)Experience with graph data (such as social influence networks)
Knowledge of machine learning (ML) and artificial intelligence (AI) methods
Advanced knowledge of scaling and high-availability techniques for data architectures
Advanced knowledge of cryptography, authentication, authorization, and/or data privacy methods
Advanced knowledge of USA elections administration and/or campaign operations
Advanced knowledge of statistics (including tools such as R)Familiar with web application frameworks (especially Angular, but VueJS or React okay)
Political campaigns are expensive and inefficient, frequently spending their limited resources reaching out to the same people over and over because the existing software solutions don’t talk to each other.
Classification, Salary, and Benefits: Full-time, competitive salary.
","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'angular', 'javascript', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'mysql', 'cloud', 'react', 'oracl']","['classif', 'graph', 'pipelin', 'cleans', 'dashboard', 'visual', 'gi', 'tune', 'machine learning', 'optim', 'statist', 'exploratori', 'etl']",999,"['sql', 'google cloud', 'spark', 'airflow', 'azur', 'angular', 'javascript', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'cloud', 'react', 'oracl', 'classif', 'graph', 'pipelin', 'cleans', 'dashboard', 'visual', 'gi', 'tune', 'machine learning', 'optim', 'statist', 'exploratori', 'etl']","['program', 'techniqu', 'visual', 'python', 'integr', 'etl', 'sourc', 'collect', 'evalu', 'ml', 'azur', 'optim', 'avail', 'statist', 'aw', 'warehous', 'machin', 'spark', 'pipelin']","['sql', 'google cloud', 'spark', 'airflow', 'azur', 'angular', 'javascript', 'aw', 'python', 'java', 'tableau', 'r', 'postgresql', 'cloud', 'react', 'oracl', 'classif', 'graph', 'pipelin', 'cleans', 'dashboard', 'visual', 'gi', 'tune', 'machine learning', 'optim', 'statist', 'exploratori', 'etl', 'program', 'techniqu', 'visual', 'python', 'integr', 'etl', 'sourc', 'collect', 'evalu', 'ml', 'azur', 'optim', 'avail', 'statist', 'aw', 'warehous', 'machin', 'spark', 'pipelin']"
DE,"The role is visible and critical as part of a high performing team - it will appeal to you if you have an effective combination of domain knowledge, relevant experience and the ability to execute on the details.
You will bring cutting edge software and full stack development skills and data lake experience while working with massive data volume.
DevOps)Here's what sets you apart:* 3+ years of data warehousing/data lake development experience * Strong data modeling and data integration experience* Strong SQL and higher-level programming languages* Good understanding of data warehouse/data lake design patterns and best practices* Solid understanding of data ingestion (i.e.
streaming platforms like Kafka)* Strong experience with data integration tools - ETL/ELT tools (i.e.
Do not include any medical or health information in this email.
The Reasonable Accommodations team will respond to your email promptly.PDN-MCR-108739-1
","['sql', 'kafka']","['data warehousing', 'etl', 'data modeling']",999,"['sql', 'kafka', 'data warehousing', 'etl', 'data modeling']","['warehous', 'integr', 'program', 'etl']","['sql', 'kafka', 'data warehousing', 'etl', 'data modeling', 'warehous', 'integr', 'program', 'etl']"
DE,"Mandatory Skills:
3 years of experience with Big Data technologies such as Hadoop, MapReduce, Kafka, Spark, or Flink.
At least 2 year's experience in JavaScript, Python, or Shell scripting.
At least 2 years of experience in Java programming.
At least 2 years' experience with Hortonworks Data Platform or Cloudera.
At least 2 years' experience with Elastic stack.
At least 2 years' experience with Ansible or Chef.
At least 2 years' experience with Full-stack Development.
At least 1-year experience with Java frameworks (e.g.
spring, hibernate, struts).
At least 1-year experience with XML and REST API integration.
At least 1-year experience with Nginx or other web application servers.
At least 1 year experience with Relational database design/development (e.g.
DB2, MySQL).
At least 1-year experience with NoSQL database design/development (e.g.
HBase, MongoDB, Cloudant).
1year experience integrating SAML / Single Sign-On (SSO) / LDAP authentication for applications Ability and interest to learn new tools and technologies.
1-year experience with Agile development / DevOps methodologies and tools (e.g.
GitHub | Travis | Jira) is preferred.
Technical and Professional Experience:
Experience with implementing ServiceNow Knowledge and hands-on experience with Cybersecurity technologies.
Experience with Machine Learning and User Behavioral Analytics (UBA).
Experience with a major SIEM (QRadar, Splunk, ArcSight).
Experience integrating with Resilient and/or Remedy platform.
","['mongodb', 'mapreduc', 'spark', 'jira', 'hadoop', 'javascript', 'python', 'java', 'hbase', 'nosql', 'mysql', 'github', 'kafka', 'splunk']","['machine learning', 'big data']",999,"['mongodb', 'mapreduc', 'spark', 'jira', 'hadoop', 'javascript', 'python', 'java', 'hbase', 'nosql', 'sql', 'github', 'kafka', 'splunk', 'machine learning', 'big data']","['analyt', 'machin', 'learn', 'spark', 'relat', 'hadoop', 'python', 'big', 'integr']","['mongodb', 'mapreduc', 'spark', 'jira', 'hadoop', 'javascript', 'python', 'java', 'hbase', 'nosql', 'sql', 'github', 'kafka', 'splunk', 'machine learning', 'big data', 'analyt', 'machin', 'learn', 'spark', 'relat', 'hadoop', 'python', 'big', 'integr']"
DE,"But please consider not applying.
Key Responsibilities:
Have 1:1 meetings with your team members each week
Manage and optimize existing Cloud SQL databases (schema design, indexing, security)
Drive strategies for improving data architecture and adopting new tooling in the service of solving problems and adding business value
Design solutions to data-centric issues like performance, scalability and manageability
Requirements:
Expertise with MySQL / Google Cloud SQL databases and SQL query syntax
Hands-on experience with ETLs or other data pipeline technologies
Experience with data modeling, data warehousing / BI and scalable data design and architecture
5+ years of professional software development experience
1+ years of engineering leadership experience
Excellent communication skills
Nice-to-have skills:
Hands-on experience with Google Cloud Platform and Cloud SQL
Experience with large-scale databases like BigQuery, Redshift, Snowflake, etc
If you are interested in learning more and think you have what it takes to win, please respond to this job posting with:
Your résumé
Your GitHub username, StackOverflow profile and/or links to any projects or other resources that provide the full picture of your experience and skill set, if available.
Physical Demands & Work Environment: The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Some travel may be required.
Job Type: Full-time
Pay: $100,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental Insurance
Disability Insurance
Flexible Schedule
Flexible Spending Account
Health Insurance
Life Insurance
Paid Time Off
Parental Leave
Referral Program
Vision Insurance
Schedule:
Monday to Friday
Experience:
Engineering Leadership: 1 year (Preferred)
Data Engineering: 5 years (Required)
Visa Sponsorship Potentially Available:
No: Not providing sponsorship for this job
www.auntbertha.com
Work Remotely:
Temporarily due to COVID-19
","['sql', 'google cloud', 'bigqueri', 'bi', 'snowflak', 'redshift', 'cloud', 'mysql', 'github', 'excel']","['pipelin', 'data modeling', 'data warehousing', 'etl', 'commun', 'account']",999,"['sql', 'google cloud', 'bigqueri', 'powerbi', 'snowflak', 'redshift', 'cloud', 'github', 'excel', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun', 'account']","['essenti', 'program', 'pipelin', 'set', 'bi', 'employe', 'avail', 'etl', 'engin']","['sql', 'google cloud', 'bigqueri', 'powerbi', 'snowflak', 'redshift', 'cloud', 'github', 'excel', 'pipelin', 'data modeling', 'data warehousing', 'etl', 'commun', 'account', 'essenti', 'program', 'pipelin', 'set', 'bi', 'employe', 'avail', 'etl', 'engin']"
DE,"About The Client
They help Fortune 500 companies become more process-efficient and improve their end-customer experience through the use of RPA, AI & ML powered solutions.
Position Summary
Exposure to Supply chain function would be a plus
Role & Responsibilities (Data Engineering)
Set up and maintain data models/structures that would be the basis for analysis and work closely with client SME’s to generate analytics reports.
Set data dictionary and maintain data governance on the created structure
Collaborate with IT system experts to ensure the IT systems are set up correctly to gather all relevant information and support the most effective data structures
Maintain and extend existing Celonis data processing pipelines
Role & Responsibilities (Analyst)
Analyze data from Celonis, other dashboards and IT systems
Create engaging and interactive visualizations utilizing Celonis to locate and define new process improvement opportunities
Understand inefficiencies in manual steps and rework task list
Discover the root causes for specific delays and prioritize the issues that impact the delivery the most
Create actions and real-time alerts to avert late deliveries to high-value customers
Publish dashboard and reports to Management with summary of issues and suggestive improvement.
Requirements
Must be certified in Celonis
Minimum 3-8 years experience
Strong analytical skills, especially in applying technology solutions to core business problems
",[None],"['dashboard', 'visual', 'pipelin', 'analyz']",999,"['dashboard', 'visual', 'pipelin', 'analyz']","['basi', 'analyt', 'pipelin', 'ml', 'visual', 'power', 'action', 'set', 'engin']","['dashboard', 'visual', 'pipelin', 'analyz', 'basi', 'analyt', 'pipelin', 'ml', 'visual', 'power', 'action', 'set', 'engin']"
DE,"Data Engineer (Mid-Level)
If you are a Data Engineer (Mid-Level) with Python experience, please read on!
What You Need for this Position
- Bachelor's Degree in STEM (Master's or PhD, even better!)
- Python (Other languages also accepted)
- ETL (Data Modeling, Data Analysis, etc.)
- SQL
- RESTful Web Services
Nice to Have (NOT Required)
- Petroleum Engineering
What's In It for You
- Competitive Salary
- Equity
- Catered Lunches
- Vacation/PTO
- Medical
- Dental
- Vision
- 401k
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'python']","['etl', 'data modeling']",1,"['sql', 'python', 'etl', 'data modeling']","['etl', 'python', 'engin']","['sql', 'python', 'etl', 'data modeling', 'etl', 'python', 'engin']"
DE,"Now Hiring a Senior Data Engineer
Ability to maintain an Interim Secret Clearance Required
An Interim Secret Clearance is required along with the ability to commute to Austin, Texas.
Work in a team using cutting edge technologies to solve challenging business problems and build solutions
Work in an agile environment with participation in daily stand-ups/scrum
Design, write, test, troubleshoot, and document application code
Provide mentorship to junior developers
Learn new technologies and be aware of industry standards, best practices, and trends.
Profile of Success
Bachelor's degree in a related field and 4 years of experience
Deep knowledge of data ingestion strategies and understanding of the V-dimensions of data (velocity, volume, variety, veracity)
Extensive experience with the Azure storage technologies (Azure Data Lake, Azure SQL Data Warehouse, Azure SQL Database)
Extensive experience with Azure data movement and transformation capabilities (Azure Data Factory, Data Lake Analytics, Data Bricks, Stream Analytics)
Proven experience developing Big Data solutions in the Azure space
SQL Server 2014+ experience.
Comfortable with Microsoft SQL data technologies (SSAS/SSIS/SSRS)
Proven ability to work with clients to understand requirements and envision data ingestion solutions
Possess DoD 8570 security certification
Desirable Skills
Microsoft related certifications such as the MCSD/MCSE
Experience with Hadoop-based technologies (HDInsight, Spark, Hive, Pig, Scala, etc.)
Experience with visualization tools such as Power BI or Tableau
","['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'bi', 'tableau', 'hive', 'microsoft', 'power bi']","['visual', 'big data']",1,"['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'powerbi', 'tableau', 'hive', 'microsoft', 'visual', 'big data']","['analyt', 'stream', 'spark', 'azur', 'visual', 'power', 'hadoop', 'provid', 'bi', 'warehous', 'texa', 'big', 'relat', 'engin', 'particip']","['sql', 'spark', 'azur', 'scala', 'pig', 'ssr', 'hadoop', 'powerbi', 'tableau', 'hive', 'microsoft', 'visual', 'big data', 'analyt', 'stream', 'spark', 'azur', 'visual', 'power', 'hadoop', 'provid', 'bi', 'warehous', 'texa', 'big', 'relat', 'engin', 'particip']"
DE,"The Data Engineer will be responsible for supporting the development of a data factory pipeline within a cloud environment.
The Data Engineer will work closely with Software Engineers, DevOps, and Data Analysts.
They must be comfortable working both independently and as part of the larger data factory team in a complex, fluid environment.
Requirements:
• Data Factory Experience
• Data Pipeline experience
• CI/CD experience
• ETL
• Big Data experience
• SQL
• Cloud Services
• Java or Scala
Day-to-day responsibilities:
• Responsibilities for Data Engineer Develop and maintain optimal data pipeline architecture - including development related to data acquisition and monitoring, data quality, integration, normalization, and analytics development.
• Adhere to good code practices within an agile environment with a DevOps approach to development and implementation.
• Develop/design appropriate orchestration and structures supporting data transformation, metadata, code dependency and workload management.
In order to fast-track your opportunity, please reach out to me ASAP to talk more about this role!
Please note, this is a permanent role.
My contact information can be found below.
Email: e.riley@nigelfrank.com
LinkedIn: https://www.linkedin.com/in/emma-riley-72028917a/
Job Requirements:
Azure, SQL, ETL, Data
","['sql', 'azur', 'scala', 'cloud', 'java']","['pipelin', 'normal', 'optim', 'big data', 'etl']",999,"['sql', 'azur', 'scala', 'cloud', 'java', 'pipelin', 'normal', 'optim', 'big data', 'etl']","['day', 'analyt', 'pipelin', 'azur', 'relat', 'optim', 'big', 'etl', 'engin', 'integr']","['sql', 'azur', 'scala', 'cloud', 'java', 'pipelin', 'normal', 'optim', 'big data', 'etl', 'day', 'analyt', 'pipelin', 'azur', 'relat', 'optim', 'big', 'etl', 'engin', 'integr']"
DE,"Skills AWS Data EngineerLocation Sunnyvale, CA Austin, TX12+ Months middot 10+ years of experience middot Experience in AWS data engineering middot Experience in Big Data Technologies middot Experience in building Big Data Data Lake solutions on AWS S3 middot Experience in Python andor Scala middot Good communication skill
","['scala', 'aw', 'python', 's3']","['commun', 'big data']",999,"['scala', 'aw', 'python', 's3', 'commun', 'big data']","['big', 'aw', 'python', 'engin']","['scala', 'aw', 'python', 's3', 'commun', 'big data', 'big', 'aw', 'python', 'engin']"
DE,"You will work with some of the brightest people helping drive infrastructure and insights to disrupt one of the oldest and largest industries in the country.
Duties and Responsibilities:
Build realtime data pipelines to ingest structured and unstructured data into data warehouse
Build ETL pipelines using Airflow to drive analytics, reporting and machine learning
Build and maintain data governance, classification and dictionary
Constantly improve A/B test framework and reporting
Support leadership and every single team with research on key business initiatives and challenges
Qualifications and Skills:
5+ years of experience with Python
5+ years of solid software engineering experience
5+ years of experience with ETL and Data Pipelines
5+ years of experience with Airflow, Hive, Presto, AWS
Comfortable with navigating complex topics and using data to make decisions
Excellent communication, presentation, and interpersonal skills
","['airflow', 'aw', 'python', 'hive', 'excel']","['research', 'classif', 'pipelin', 'machine learning', 'etl', 'commun']",999,"['airflow', 'aw', 'python', 'hive', 'excel', 'research', 'classif', 'pipelin', 'machine learning', 'etl', 'commun']","['analyt', 'machin', 'learn', 'challeng', 'pipelin', 'infrastructur', 'aw', 'python', 'etl', 'engin']","['airflow', 'aw', 'python', 'hive', 'excel', 'research', 'classif', 'pipelin', 'machine learning', 'etl', 'commun', 'analyt', 'machin', 'learn', 'challeng', 'pipelin', 'infrastructur', 'aw', 'python', 'etl', 'engin']"
DE,"The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
***Advanced Python Engineering Experience Required.
***
Desired Experience:
5+ years of Advanced Python Engineering Experience Preferred
3+ years of data pipeline engineering experience
1+ years of experience with Spark
Experience with Snowflake, Redshift, or other columnar databases
Experience with Pandas, Anaconda, or similar libraries
Experience with Airflow or Prefect
Experience with Docker
Experience scaling technology solutions to hundreds of thousands active users
Experience mentoring other developers
Deep understanding of common API methodologies
Strong experience with unit testing and test-driven development
Startup experience
Advanced degree in Computer Science
Powered by JazzHR
4oRLdwjRUG
","['spark', 'airflow', 'panda', 'snowflak', 'python', 'redshift', 'docker']",['pipelin'],2,"['spark', 'airflow', 'panda', 'snowflak', 'python', 'redshift', 'docker', 'pipelin']","['spark', 'pipelin', 'power', 'python', 'comput', 'common', 'engin']","['spark', 'airflow', 'panda', 'snowflak', 'python', 'redshift', 'docker', 'pipelin', 'spark', 'pipelin', 'power', 'python', 'comput', 'common', 'engin']"
DE,"Duration: 9 Months
Looking for local candidates only.
Project: Modeling Environment
Data Acquisition (Hadoop, Spark, SQL, GreenPlum etc.)
Data Engineering & Preparation
Modeling & Machine learning
Source & Version Control
Microservice-enablement of models & delivery in Pivotal Cloud Foundry
Lab Operations (logging, SRs, uptime, dependency management)
Must Have (skillsets Data engineer with CS background.
BS required Master’s degree ideal):
All data scientists are:
Using the same general tools (while different languages)
Expert in applied Python and R
Knowledge in Java, JavaScript
Can easily share, reuse and operationalize the predictive models & results
Have shared repositories, version control and security features
Assumptions/Considerations: (like to have):
Collaborate with the Data Science lab on tools and technologies
Experienced with Big data sets Structured and Unstructured
Regards,
Vikas
Vikasy@apninc.com
","['sql', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'java', 'r']","['machine learning', 'big data', 'predict']",1,"['sql', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'java', 'r', 'machine learning', 'big data', 'predict']","['machin', 'spark', 'hadoop', 'look', 'predict', 'python', 'scientist', 'appli', 'big', 'set', 'sourc', 'engin']","['sql', 'spark', 'hadoop', 'javascript', 'cloud', 'python', 'java', 'r', 'machine learning', 'big data', 'predict', 'machin', 'spark', 'hadoop', 'look', 'predict', 'python', 'scientist', 'appli', 'big', 'set', 'sourc', 'engin']"
DE,"Teradata Data Engineer ( 8-12 yrs ) _ Austin, TX
- Clear understanding of Teradata architecture
- Good Experience in scripting (preferably Python)
- Very strong in Teradata programming and optimization techniques
- Hands on experience in Stored Procedures, Bteq, Teradata utilities like TPTExport / TPTLoad / fastLoad etc
- Strong Data warehouse concepts
- Experience creating technical designs
- Good unit testing and integration test strategies
- Familiarity with Hadoop and enthusiam to learn new technologies
- Excellent communication skill in terms of both verbal and written
- Experience in leading small to medium projects
","['python', 'excel', 'hadoop']","['optim', 'commun']",999,"['python', 'excel', 'hadoop', 'optim', 'commun']","['program', 'techniqu', 'hadoop', 'optim', 'python', 'warehous', 'integr', 'engin']","['python', 'excel', 'hadoop', 'optim', 'commun', 'program', 'techniqu', 'hadoop', 'optim', 'python', 'warehous', 'integr', 'engin']"
DE,"Why Join Ascension?
Ascension Technologies leverages technology to create collaborative solutions that improve everyday health decisions.
It is used to provide insightful use of automation and data-drive improvements to enhance the provider, patient and consumer experience as well as keeping cybersecruity with a strong posture to protect data and other valuable assets.
Ascension is a faith-based healthcare organization dedicated to transformation through innovation across the continuum of care.
As one of the leading non-profit and Catholic health systems in the U.S., Ascension is committed to delivering compassionate, personalized care to all, especially to those most in need.
In FY2018, Ascension provided nearly $2 billion in care of persons living in poverty and other community benefit programs.
Ascension Information Services is one of the nation’s largest healthcare information technology services organizations.
Support rapid and effective clinical decision making
Improve efficiency and care transitions
Foster information sharing across the continuum of care
Make knowledge and data actionable, leading to improved patient outcomes
What You Will Do
Responsibilities:
Responsible for construction and development of ""large-scale cloud data processing systems"" The Data Engineer must have considerable expertise in data warehousing and job requires proven coding expertise with Python, Java, SQL, and Spark languages.
Must be able to implement enterprise cloud data architecture designs, and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design and implement large scale data solutions, structured and unstructured, public and proprietary data.
The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
Develops partnerships with senior users to understand their business needs and define future application requirements.
Evaluates the applicability of leading edge technologies and uses this information to significantly influence future business strategies.
Analyzes complex business and competitive issues and discerns the implications for systems support.
Designs, directs and performs analyses to resolve complex first-time project issues, including analysis of the technical and economic feasibility of proposed system solutions.
Designs projects with broad implication for the business and/or the future architecture, successfully addressing cross-technology and cross-platform issues.
Balances and negotiates the needs of multiple users and communicates the business advantages of various technical solutions.
Manages customer expectations and ensures prompt and complete customer service.
Customizes presentations to the interests of the audience.
Develops expert understanding of applications development processes, and in-depth knowledge of leading edge technologies to create plans for future technology use.
Required Work Experience:
Four to seven years of experience.
Minimum number years of relevant experience: 2 Years
Some of the minimum experience requirement may be met with Masters or other advanced degree
Cloud Experience Required
Coding experience with Python, Java, Spark, and SQL
Strong Linux/Unix background and hands on knowledge.
Past experience with big data technologies like HDFS, Spark, Impala, Hive,
Experience with Shell scripting and bash.
Experience with version control platform github
Experience unit testing code.
Experience with development ecosystem such as Jenkins, Artifactory, CI/CD, and Terraform.
Works on problems of diverse scope and complexity ranging from moderate to substantial
Assists senior professionals in determining methods and procedures for new tasks
Leads basic or moderately complex projects/activities on semi-regular basis Must possess excellent written and verbal communication skills
Ability to understand and analyze complex data sets
Exercises independent judgment on basic or moderately complex issues regarding job and related tasks
Makes recommendations to management on new processes, tools and techniques, or development of new products and services
Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues
Decisions have a moderate impact on operations within a department Works under minimal supervision, uses independent judgment requiring analysis of variable factors
Collaborates with senior professionals in the development of methods, techniques and analytical approach
Ability to advise management on approaches to optimize for data platform success.
Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.
Consistently communicate on status of project deliverables
Consistently provide work effort estimates to management to assist in setting priorities
Deliver timely work in accordance with estimates Solve problems as they arise and communicate potential roadblocks to manage expectations
Adhere strictly to all security policies
Desired Work Experience:
Proficient in multiple programming languages, frameworks, domains, and tools.
Coding skills in Scala
Experience with GCP platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.
Strong Linux/Unix background and hands on knowledge.
Knowledge in Hadoop and cloud platforms and surrounding ecosystems.
Experience with web services and APIs as in RESTful and SOAP.
Ability to document designs and concepts
API Orchestration and Choreography for consumer apps
Well rounded technical expertise in Apache packages and Hybrid cloud architectures
Pipeline creation and automation for Data Acquisition Metadata extraction pipeline design and creation between raw and finally transformed datasets
Quality control metrics data collection on data acquisition pipelines
Able to collaborate with scrum team including scrum master, product owner, data analysts,
Quality Assurance, business owners, and data architecture to produce the best possible end products
Experience contributing to and leveraging Jira and confluence.
Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.
Ability to work with different file formats like Avro, Parquet, and JSON.
Managing and scheduling batch jobs.
Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).
Master level technology degree preferred
Bachelor's Level degree preferred
Technology certifications preferred
What You Will Need
Education:
High school diploma/GED with 2 years of experience, or Associate's degree, or Bachelor's degree required.
Work Experience:
1 year of experience required.
4 years of experience preferred.
2 years of leadership or management experience preferred.
Equal Employment Opportunity
Ascension Technologies is an EEO/AA Employer M/F/Disability/Vet.
Please click the link below for more information.
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
EEO is the Law Poster Supplement
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/ofccp_eeo_supplement_final_jrf_qa_508c.pdf
E-Verify Statement
Ascension Technologies participates in the Electronic Employment Verification Program.
Please click the E-Verify link below for more information.
E-Verify (link to E-verify site)
","['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel']","['recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom']",1,"['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel', 'recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom']","['basi', 'program', 'techniqu', 'provid', 'python', 'packag', 'collect', 'evalu', 'analyt', 'judgment', 'hadoop', 'action', 'clinic', 'asset', 'public', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat', 'school']","['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel', 'recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom', 'basi', 'program', 'techniqu', 'provid', 'python', 'packag', 'collect', 'evalu', 'analyt', 'judgment', 'hadoop', 'action', 'clinic', 'asset', 'public', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat', 'school']"
DE,"Job Title :Data Engineer
Position Type :Contract
Candidates would need to have 10 years of experience or higher.
Here's the technology stack - Greenplum, Hadoop (for their Data Lake) and Teradata (for their Datawarehouse).
Spark is used for Data transformations.
Experience with MongoDB and any type of Graph databases(Neo4j, Amazon Neptune, Cassandra) is a plus.
","['mongodb', 'spark', 'cassandra', 'hadoop']",['graph'],999,"['mongodb', 'spark', 'cassandra', 'hadoop', 'graph']","['spark', 'engin', 'hadoop']","['mongodb', 'spark', 'cassandra', 'hadoop', 'graph', 'spark', 'engin', 'hadoop']"
DE,"Data Engineer - Full-Time Day - Austin, TX, St. Louis, MO, Chicago, IL, Nashville, TN or Indianapolis, IN
Why Join Ascension?
Ascension Information Services is one of the nation’s largest healthcare information technology services organizations.
Support rapid and effective clinical decision making
Improve efficiency and care transitions • Foster information sharing across the continuum of care
Make knowledge and data actionable, leading to improved patient outcomes
Ascension is a faith-based healthcare organization dedicated to transformation through innovation across the continuum of care.
As one of the leading non-profit and Catholic health systems in the U.S., Ascension is committed to delivering compassionate, personalized care to all, especially to those most in need.
In FY2018, Ascension provided nearly $2 billion in care of persons living in poverty and other community benefit programs.
What You Will Do
Ascension Technologies leverages technology to create collaborative solutions that improve everyday health decisions.
It is used to provide insightful use of automation and data-drive improvements to enhance the provider, patient and consumer experience as well as keeping cybersecruity with a strong posture to protect data and other valuable assets.
Responsibilities:
Responsible for construction and development of ""large-scale cloud data processing systems"" The Data Engineer must have considerable expertise in data warehousing and job requires proven coding expertise with Python, Java, SQL, and Spark languages.
Must be able to implement enterprise cloud data architecture designs and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design, and implement large scale data solutions, structured and unstructured, public and proprietary data.
The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
Develops partnerships with senior users to understand their business needs and define future application requirements.
Evaluates the applicability of leading edge technologies and uses this information to significantly influence future business strategies.
Analyzes complex business and competitive issues and discerns the implications for systems support.
Designs, directs and performs analyses to resolve complex first-time project issues, including analysis of the technical and economic feasibility of proposed system solutions.
Designs projects with broad implication for the business and/or the future architecture, successfully addressing cross-technology and cross-platform issues.
Balances and negotiates the needs of multiple users and communicates the business advantages of various technical solutions.
Manages customer expectations and ensures prompt and complete customer service.
Customizes presentations to the interests of the audience.
Develops expert understanding of applications development processes, and in-depth knowledge of leading edge technologies to create plans for future technology use.
Required Work Experience:
Four to seven years of experience.
Minimum number years of relevant experience: 2 Years
Some of the minimum experience requirement may be met with Masters or other advanced degree
Cloud Experience Required
Coding experience with Python, Java, Spark, and SQL
Strong Linux/Unix background and hands on knowledge.
Past experience with big data technologies like HDFS, Spark, Impala, Hive,
Experience with Shell scripting and bash.
Experience with version control platform github
Experience unit testing code.
Experience with development ecosystem such as Jenkins, Artifactory, CI/CD, and Terraform.
Works on problems of diverse scope and complexity ranging from moderate to substantial
Assists senior professionals in determining methods and procedures for new tasks
Leads basic or moderately complex projects/activities on semi-regular basis Must possess excellent written and verbal communication skills
Ability to understand and analyze complex data sets
Exercises independent judgment on basic or moderately complex issues regarding job and related tasks
Makes recommendations to management on new processes, tools and techniques, or development of new products and services
Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues
Decisions have a moderate impact on operations within a department Works under minimal supervision, uses independent judgment requiring analysis of variable factors
Collaborates with senior professionals in the development of methods, techniques and analytical approach
Ability to advise management on approaches to optimize for data platform success.
Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.
Consistently communicate on status of project deliverables
Consistently provide work effort estimates to management to assist in setting priorities
Deliver timely work in accordance with estimates Solve problems as they arise and communicate potential roadblocks to manage expectations
Adhere strictly to all security policies
Desired Work Experience:
Proficient in multiple programming languages, frameworks, domains, and tools.
Coding skills in Scala
Experience with GCP platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.
Strong Linux/Unix background and hands on knowledge.
Knowledge in Hadoop and cloud platforms and surrounding ecosystems.
Experience with web services and APIs as in RESTful and SOAP.
Ability to document designs and concepts
API Orchestration and Choreography for consumer apps
Well rounded technical expertise in Apache packages and Hybrid cloud architectures
Pipeline creation and automation for Data Acquisition Metadata extraction pipeline design and creation between raw and finally transformed datasets
Quality control metrics data collection on data acquisition pipelines
Able to collaborate with scrum team including scrum master, product owner, data analysts,
Quality Assurance, business owners, and data architecture to produce the best possible end products
Experience contributing to and leveraging Jira and confluence.
Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.
Ability to work with different file formats like Avro, Parquet, and JSON.
Managing and scheduling batch jobs.
Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).
Master level technology degree preferred
Bachelor's Level degree preferred
Technology certifications preferred
What You Will Need
Education:
High school diploma/GED with 2 years of experience, or Associate's degree, or Bachelor's degree required.
Work Experience:
1 year of experience required.
4 years of experience preferred.
2 years of leadership or management experience preferred.
Equal Employment Opportunity
Ascension Technologies is an EEO/AA Employer M/F/Disability/Vet.
Please click the link below for more information.
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
EEO is the Law Poster Supplement
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/ofccp_eeo_supplement_final_jrf_qa_508c.pdf
E-Verify Statement
Ascension Technologies participates in the Electronic Employment Verification Program.
Please click the E-Verify link below for more information.
E-Verify (link to E-verify site)
","['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel']","['recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom']",1,"['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel', 'recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom']","['day', 'basi', 'program', 'techniqu', 'provid', 'python', 'packag', 'collect', 'evalu', 'analyt', 'judgment', 'hadoop', 'action', 'clinic', 'asset', 'public', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat', 'school']","['sql', 'linux', 'gcp', 'spark', 'airflow', 'scala', 'unix', 'jira', 'hadoop', 'cloud', 'python', 'hive', 'java', 'github', 'kafka', 'excel', 'recommend', 'healthcar', 'big data', 'pipelin', 'data warehousing', 'analyz', 'information technology', 'supervis', 'commun', 'econom', 'day', 'basi', 'program', 'techniqu', 'provid', 'python', 'packag', 'collect', 'evalu', 'analyt', 'judgment', 'hadoop', 'action', 'clinic', 'asset', 'public', 'big', 'set', 'engin', 'spark', 'pipelin', 'divers', 'relat', 'school']"
DE,"You must have Strong knowledge Good knowledge on Java/Python/Big Data
You must have good hand on experience in Full Stack Java Engineer with knowledge on Bigdata like Spark, Scala, Hadoop Good hands-on of Java programming in Hadoop, Spark and Scala, SQL.
Knowledge in data warehouse and basics of data analytics
Having extensive experience Build back-end ETL components and solutions using Hive and spark sql,Thorough analysis of documents and propose design approach,Working Proficiency and Expertise on Hive is required.
Strong in any programming language Python/Java/Scala
Good understanding of Hadoop/spark architecture.Strong experience with Data Warehousing concepts and standards.
Working experience in handling huge volume of data.
Good knowledge in Working with the team to build, manage, optimize and customize ETL products and solutions applying best practices, Proficiency in SQL
","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'hive']","['optim', 'etl', 'data warehousing', 'big data']",999,"['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'hive', 'optim', 'etl', 'data warehousing', 'big data']","['analyt', 'big', 'program', 'spark', 'hadoop', 'optim', 'python', 'warehous', 'etl', 'engin']","['sql', 'spark', 'scala', 'hadoop', 'java', 'python', 'hive', 'optim', 'etl', 'data warehousing', 'big data', 'analyt', 'big', 'program', 'spark', 'hadoop', 'optim', 'python', 'warehous', 'etl', 'engin']"
DE,"Data Engineer (Mid-Level)
If you are a Data Engineer (Mid-Level) with Python experience, please read on!
What You Need for this Position
- Bachelor's Degree in STEM (Master's or PhD, even better!)
- Python (Other languages also accepted)
- ETL (Data Modeling, Data Analysis, etc.)
- SQL
- RESTful Web Services
Nice to Have (NOT Required)
- Petroleum Engineering
What's In It for You
- Competitive Salary
- Equity
- Catered Lunches
- Vacation/PTO
- Medical
- Dental
- Vision
- 401k
-
Applicants must be authorized to work in the U.S.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
","['sql', 'python']","['etl', 'data modeling']",1,"['sql', 'python', 'etl', 'data modeling']","['etl', 'python', 'engin']","['sql', 'python', 'etl', 'data modeling', 'etl', 'python', 'engin']"
DE,"What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built?
Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous.
You'll help lead strategic initiatives across the organization while also conducting a range of system management work including managing uptime, event integrity, disaster recovery, self-healing and alerting, and event reporting.
Remote candidates will be considered based on experience.
What you'll do:
Work closely with Site Reliability, Security, and Development teams to ensure best practices are followed and architecture patterns are aligned with the requirements
Leverage subject matter and technical expertise to provide leadership, mentoring, and strategic influence across the organization
Provide hands-on support for all event-based systems including incident triage and root cause analysis
Support on-call participation, planning and change management, detailed runbooks, and documentation
Effectively prioritize and organize tasks and deliver business-driven decisions under pressure
Efficiently execute daily operational tasks including administration, performance monitoring, triage, ad-hoc walk-ups, and project milestones
Assist in designing event-based architecture, holding design reviews, building and troubleshooting databases, as well as digging deep into the root of an issue and providing solutions
Provide configuration and maintenance support for production and development environments
Assist Application Developers with optimization and tuning
Generate and maintain scripts to automate common operational and maintenance tasks
Identify performance trends and opportunities, and deploy proactive corrective measures to minimize downtime
BS degree in Computer Science or equivalent practical experience
4+ years of experience working within a Cloud infrastructure environment, on-premise or remote
Development experience in any language
Production infrastructure and operations background
Possess a strong understanding that automation is essential, and proactively push boundaries in areas like self-healing conditions and alerts
Strong Experience with Kafka is desired
Experience with ETL, data warehouse, data lake, message-brokering, and event-based architecture
Strong technical hands-on experience in delivering projects
Possess a natural wonder for technology including AWS and docker and automation
Experience using the following tools is desirable:
AWS, GCP, Azure
Linux Experience
AWS MSK, Kafka, RabbitMQ
Containers and Container Management (Docker, Kubernetes)
Config Management
Log aggregation tools ( SumoLogic, Splunk, Kibana )
Perks & Benefits
You are a person with dreams, goals, and ambitions—both personally and professionally.
","['linux', 'gcp', 'azur', 'aw', 'cloud', 'kubernet', 'kafka', 'splunk', 'docker']","['optim', 'etl', 'commun']",1,"['linux', 'gcp', 'azur', 'aw', 'cloud', 'kubernet', 'kafka', 'splunk', 'docker', 'optim', 'etl', 'commun']","['digit', 'essenti', 'azur', 'infrastructur', 'provid', 'optim', 'aw', 'comput', 'warehous', 'etl', 'common', 'school', 'particip', 'integr']","['linux', 'gcp', 'azur', 'aw', 'cloud', 'kubernet', 'kafka', 'splunk', 'docker', 'optim', 'etl', 'commun', 'digit', 'essenti', 'azur', 'infrastructur', 'provid', 'optim', 'aw', 'comput', 'warehous', 'etl', 'common', 'school', 'particip', 'integr']"
DE,"Date posted 07/01/2020
Requisition Number: BR
Area of Interest: Network Engineering
Position Type: Full Time
At a glance:
Are you an experienced engineer passionate about supporting data operations by gathering and processing raw data at scale?
Can you commit to a position measuring the quality, integrity, accuracy and completeness of data used in company-wide solutions?
Do you desire a competitive salary with lucrative benefits and a focus on professional development?
Spectrum Enterprise, a part of Charter Communications, Inc., is a national provider of scalable, fiber technology solutions serving America's largest businesses and communications service providers.
The broad Spectrum Enterprise portfolio includes networking and managed services solutions: Internet access, Ethernet access and networks, Voice and TV solutions.
Spectrum Enterprise's industry-leading team of experts works closely with clients to achieve greater business success by providing solutions designed to meet their evolving needs.
More information about Spectrum Enterprise can be found at .
Highlights:
As a Data Engineer II, you enhance operations for reporting, applications and data science by gathering and processing raw data at scale.
You focus on maintaining scalable, reliable, consistent and repeatable systems that efficiently provide data for company-wide operations.
You excel at profiling data to measure quality, integrity, accuracy and completeness.
You deliver effective data solutions through the development, testing and implementation of code and scripts.
You maximize the department by developing data set processes for data modeling, mining and consumption.
You have cultivated a keen ability for identifying roadblocks and overcoming obstacles to see your projects completed on time and within budget.
You flourish in an office environment collaborating with teams across Spectrum Enterprise.
You report to the Manager, Network Engineering for goals, guidance and assistance.
Position benefits:
Competitive Salary with Bonus
Health, Vision and Dental Insurance
Education Assistance
Pretax Child Care Spending Account
Paid Holidays, Vacation Days, Personal Days and Sick Days
Employee Discount on Spectrum Services Where AvailableWhat you will do:
Drive company-wide excellence through active and consistent support of all efforts to simplify and enhance the client experience.
Support data operations for reporting, analytics, applications and data science by creating and maintaining scalable, reliable, consistent and repeatable systems.
Drive data operations processes through the collection and processing of raw data at scale, including writing scripts, web scraping, calling APIs and writing SQL queries.
Maintain, improve, clean and manipulate data using extract, transform and load (ETL) processes.
Profile data to measure quality, integrity, accuracy and completeness.
Improve department processes by implementing tools, scripts, queries and applications for ETL and data operations.
Deliver solutions by developing, testing and implementing code and scripts.
Produce reports and uphold data delivery schedules for senior leadership to leverage.
Maximize the department through the management of multiple data source lifecycles.
Increase delivery speed of data by implementing workflow automation solutions.
Perform additional duties related to the position as assigned.
Required keys for success:
One or more years of experience as a Linux, Unix or CentOS system administrator
Two or more years of hands-on experience with RDMNS, SQL, scripting and coding
Intermediate knowledge of coding and scripting using Python, R or shell scripts
Extensive experience with SQL, Tableau and ETL techniques
Background in Linus and CentOS installation and administration
Knowledge of data storage that demonstrates the correct use of a file system, relational database or NoSQL variant
Track record of effectively using Spark and Hadoop
Detail-oriented with the ability to effectively prioritize and execute multiple tasks
Passion for learning new techniques, methods and concepts regarding data
Interest in learning programming languages
Effective written and spoken English communication skills with all levels of an organizationHow you will stand out from the crowd:
Familiar with JavaScript API, Rest API or Data Extract APIs
Experience with data workflow or data preparation platforms, such as Infomatica, Pentaho or Talend
Knowledge of best practices and IT operations in an always-up, always-available service
History of working with data virtualization concepts and software, such as Denodo, Teiid and JBoss
Track record of receiving, converting and cleansing data
Able to work with visualization or BI tools, such as TableauYour education:
Bachelor's Degree in engineering, computer science or a related field (required)
The Spectrum brands (including Spectrum Networks, Spectrum Enterprise and Spectrum Reach) are powered and innovated by Charter Communications.
Individuals will be considered for positions for which they meet the minimum qualifications and are able to perform without regard to race, color, gender, age, religion, disability, national origin, veteran status, sexual orientation, gender identity, or any other basis protected by federal, state or local laws.
FCC Unit: 13551
Business Unit: Spectrum Enterprise
","['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'bi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho']","['visual', 'data modeling', 'clean', 'etl', 'commun', 'account']",1,"['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'powerbi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho', 'visual', 'data modeling', 'clean', 'etl', 'commun', 'account']","['day', 'basi', 'techniqu', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'collect', 'analyt', 'hadoop', 'avail', 'bi', 'set', 'employe', 'engin', 'spark', 'power', 'comput', 'relat']","['sql', 'linux', 'spark', 'excel', 'unix', 'hadoop', 'powerbi', 'javascript', 'python', 'tableau', 'r', 'nosql', 'pentaho', 'visual', 'data modeling', 'clean', 'etl', 'commun', 'account', 'day', 'basi', 'techniqu', 'visual', 'provid', 'python', 'etl', 'integr', 'sourc', 'collect', 'analyt', 'hadoop', 'avail', 'bi', 'set', 'employe', 'engin', 'spark', 'power', 'comput', 'relat']"
DE,"Summary
***
You will be responsible for working closely with Data Architect’s and Big Data Engineers and their vision to help design, build and provide governance of large-scale streaming architecture solution that delivers business value across the organization.
You will have the opportunity to both lead and execute on projects and always consider the bigger picture proactively with anticipating any performance issues, troubleshooting, monitoring, quality, etc… This role will require experience with hybrid platforms, cloud migration, publishing, development with newer technologies such as Spring Boot, KSQL/Stream Processing, data connectors such as Kafka, performance tuning, data quality and data visualization knowledge.
You and will report to the Director of Information Delivery.
Job Duties and Responsibilities
Lead the implementation, execution, and maintenance of Streaming Architecture technology solutions
Lead work to advance and support streaming architecture practices for business processes, applications and technology that underpin the EIM discipline
Provide leadership for Data Engineer tasks supporting projects
Revenue generated
Budget responsibilities
Leads the delivery, support and maintenance of solutions with one or more business and technology areas.
Organizational impact results from mid-large sized projects
Required Job Qualifications
Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field
5+ years of experience in Technology related field including prior lead experience
Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools
Strong organizational, analytical, critical thinking and leadership skills
Demonstrated leadership on mid-large-scale project impacting strategic partners
",['kafka'],"['big data', 'visual', 'statist', 'predict']",1,"['kafka', 'big data', 'visual', 'statist', 'predict']","['analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']","['kafka', 'big data', 'visual', 'statist', 'predict', 'analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']"
DE,"Essential Responsibilities:
Are you ready to be part of something new and unique?
Here you will have the opportunity to invent and create data driven solutions that connect customers with their home and build mobile and web applications supporting end users and service providers.
Come and join the Resideo software engineering team in Austin.
This is an exciting environment where you'll have the opportunity to develop your skills solving interesting problems.
Work with other smart people and be challenged in your work embracing new technologies and approaches.
The ideal candidate is an experienced data pipeline builder and data engineer who enjoys optimizing data systems and building them from the ground up.
Job Duties:
Design, develop, and maintain data pipelines to ingest data from external APIs for analysis, store in data stores that are scalable, secured, and accessible by other services in the organization
Work with Security to implement and maintain data privacy and integrity to stay in compliant within the legal data framework
Work with Data Scientists including the Product, Data and Design teams to assist with data-related technical issues and support their data mobility & infrastructure needs
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Ensure up-time and disaster recovery with latest technologies
YOU MUST HAVE:
Extensive experience in software development, micro service architecture, and design driven development
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience building processes supporting data transformation, data structures, metadata, dependency and workload management
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Experience with big data tools: Hadoop, Teradata, Spark, Kafka, Streamsets, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Mongo.
Experience with AWS/Azure cloud services
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
BSc/BA in Computer Science or equivalent.
Experience working independently with little supervision and lead a team
Excellent organization and problem-solving skills
Creative Thinking
New and innovative ideas
Collaboration and teamwork
Attention to detail
High quality work product
Strong communication skills
PERKS:
Resideo offers a wide selection of benefits, including medical, dental, vison, disability, and life insurance.
Free Food and Beverages
Flexible Hours
Casual Attire
Really good coffee!
Are you ready to be part of something new and unique?
Here you will have the opportunity to invent and create data driven solutions that connect customers with their home and build mobile and web applications supporting end users and service providers.
","['sql', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'postgr', 'kafka', 'excel']","['pipelin', 'end user', 'big data', 'supervis', 'commun']",1,"['sql', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'kafka', 'excel', 'pipelin', 'end user', 'big data', 'supervis', 'commun']","['stream', 'provid', 'python', 'integr', 'analyt', 'essenti', 'challeng', 'azur', 'hadoop', 'action', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['sql', 'spark', 'azur', 'scala', 'ec2', 'hadoop', 'aw', 'cloud', 'redshift', 'python', 'java', 'nosql', 'kafka', 'excel', 'pipelin', 'end user', 'big data', 'supervis', 'commun', 'stream', 'provid', 'python', 'integr', 'analyt', 'essenti', 'challeng', 'azur', 'hadoop', 'action', 'infrastructur', 'aw', 'big', 'set', 'engin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"This position will be the Senior technical resource driving architecture for the integration of large 3rd party partner integrations with companies like Facebook, Google and Twitter to name a few.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
This position will require the ability to own and lead data initiatives on a cross-functional team.
The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.
You should be able to not take yourself too seriously as well.
And most of all, you will enjoy working with great people who are changing the entire industry.What you'll do:* Migrate existing data pipelines from on-prem regional data centers to AWS and GCP.
* 5+ years recent hands-on experience with object oriented languages (Java, Scala, Python)* 5+ years Hands on experience building production level systems in a cloud environment (AWS or GCP)* Excellent interpersonal and communication skills in English* Proven experience leading the design and execution of event driven architectures for distributed systems* Experience designing systems for performance, scalability, and reliability* In-depth understanding of object oriented programming concepts* Low level working knowledge of collections, multi-threading, JVM memory model, etc.
IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes.
Founded in 2009, IAS is headquartered in New York with global operations in 18 offices across 13 countries.
IAS is part of the Vista Equity Partners portfolio of software companies.
","['gcp', 'scala', 'aw', 'cloud', 'java', 'python', 'excel']","['commun', 'pipelin']",999,"['gcp', 'scala', 'aw', 'cloud', 'java', 'python', 'excel', 'commun', 'pipelin']","['pipelin', 'aw', 'python', 'parti', '3rd', 'integr', 'collect']","['gcp', 'scala', 'aw', 'cloud', 'java', 'python', 'excel', 'commun', 'pipelin', 'pipelin', 'aw', 'python', 'parti', '3rd', 'integr', 'collect']"
DE,"* Active Secret clearance This job posting sets forth the authorities and responsibilities of this position, which may be changed from time to time as shall be determined.
#LI-NP1 #clearance #dice
",[None],[None],999,[],['set'],['set']
DE,"Multidisciplinary work supporting data pipelines, data warehouses and develop data movements using Snowflake capabilities like Snow SQL, Tasks, Streams and stored procedures.
Much of the work with be performed remotely working with others on the team who are working remotely and connecting using collaboration tools such as Microsoft Teams andor Zoom.
This is a minimum of a four-month engagement with possible extensions.
Fun Things You Will Do Be an active member of an EDW team focused in building best in class data solution on a large data warehouse and analytics project Effectively communicate objectives, plans, status, issues and risks in a timely manner to team members, stakeholders and management.
Work with analysts, developers and management to scope projects, and ensure designs are implemented and tested to specifications.
Create solutions utilizing industry-standard dimensional models and data architecture to support business users that include data scientists, analysts and non-technical functional users Communicates clearly in a timely manner in both verbal and written communication with other Software Database Development Engineers.
Collaborate with business analysts, subject matter experts, and other team members to determine data extraction and transformation requirements.
Technical Skills Architect, design and implement data solutions using Snowflake as a cloud service (virtual warehouses, staging etc).
Experience with optimizing security models around data access and authorization with Snowflake Cloud DW and AWS In depth knowledge of Snowflake architecture to create performance tuning and optimized data pipelines, query and storage solutions.
Hands-on experience with Snowflake utilities such as SnowSQL, SnowPipe, Python, Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing and stored procedures.
Understanding relational and big data models to both store and access data from data visualization and other query tools.
A total of 4+ years relevant experience in a multi-platform environment, including, but not limited to database development.
Knowledge and experience in a data warehouse environment with knowledge of data architecture patterns.
Experience with Jira and source control environments like Git, GitHub etc.
Minimum 7 years of experience in developing software applications including analysis, design, coding, testing, deploying and supporting of applications.
","['sql', 'git', 'jira', 'aw', 'snowflak', 'python', 'cloud', 'microsoft', 'github']","['pipelin', 'visual', 'risk', 'tune', 'optim', 'big data', 'commun']",999,"['sql', 'git', 'jira', 'aw', 'snowflak', 'python', 'cloud', 'microsoft', 'github', 'pipelin', 'visual', 'risk', 'tune', 'optim', 'big data', 'commun']","['analyt', 'stream', 'pipelin', 'visual', 'optim', 'aw', 'python', 'warehous', 'scientist', 'big', 'relat', 'sourc', 'engin']","['sql', 'git', 'jira', 'aw', 'snowflak', 'python', 'cloud', 'microsoft', 'github', 'pipelin', 'visual', 'risk', 'tune', 'optim', 'big data', 'commun', 'analyt', 'stream', 'pipelin', 'visual', 'optim', 'aw', 'python', 'warehous', 'scientist', 'big', 'relat', 'sourc', 'engin']"
DE,"Teradata Data Engineer ( 8-12 yrs ) _ Austin, TX
Clear understanding of Teradata architecture
Good Experience in scripting (preferably Python)
Very strong in Teradata programming and optimization techniques
Hands on experience in Stored Procedures, Bteq, Teradata utilities like TPTExport / TPTLoad / fastLoad etc
Strong Data warehouse concepts
Experience creating technical designs
Good unit testing and integration test strategies
Familiarity with Hadoop and enthusiam to learn new technologies
Excellent communication skill in terms of both verbal and written
Experience in leading small to medium projects
","['python', 'excel', 'hadoop']","['optim', 'commun']",999,"['python', 'excel', 'hadoop', 'optim', 'commun']","['program', 'techniqu', 'hadoop', 'optim', 'python', 'warehous', 'integr', 'engin']","['python', 'excel', 'hadoop', 'optim', 'commun', 'program', 'techniqu', 'hadoop', 'optim', 'python', 'warehous', 'integr', 'engin']"
DE,"The home services industry remains vast ($700b+) but untouched by technology and unencumbered by a dominant competitor.
The 3M+ mobile businesses in home services are facing dramatic change as the world shifts from offline to online.
The role:
You will actively work in a multi-disciplinary fast-paced environment, drive innovation and satisfy the analytics needs of your stakeholder.
This role requires a broad range of skills and abilities; you will be the functional lead, drive innovation, manage staff and do the work.
Data Warehouse (Snowflake)
ETL system & data pipelines (Segment, Stitchdata, Airflow, Python)
BI system (Tableau Online, Amplitude)
(Current analytics stack in parentheses)
Skills:
You have planned, built & managed data infrastructures in a public cloud.
You have 6+ years of experience as a Data Engineer/Data Architect and have architected distributed data platforms.
You can estimate, design, build and own a business initiative end to end.
You have in-depth experience with MySQL databases and Snowflake's data warehouse
You have managed a business intelligence system
You are proficient in at least one programming languages like Python, Scala and Java
You have familiarity with big data technologies like Hadoop, Spark, Hive
You are comfortable with setting and meeting SLAs for data availability and quality
You have an understanding of Machine Learning / AI principles in data engineering
You have consistently engaged in mentoring, training and reviewing other data engineers on the team.
You've worked in an Agile environment.
You thrive on scrum.
You make opportunities to bring value sooner rather than later.
You value data-driven decisions.
You are always looking for opportunities to quickly produce the right data to make decisions quickly.
You keep cool under pressure.
","['spark', 'airflow', 'scala', 'hadoop', 'bi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'mysql']","['pipelin', 'segment', 'machine learning', 'big data', 'etl']",999,"['spark', 'airflow', 'scala', 'hadoop', 'powerbi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'sql', 'pipelin', 'segment', 'machine learning', 'big data', 'etl']","['analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'public', 'hadoop', 'infrastructur', 'bi', 'python', 'avail', 'warehous', 'big', 'etl', 'engin']","['spark', 'airflow', 'scala', 'hadoop', 'powerbi', 'snowflak', 'python', 'hive', 'java', 'tableau', 'cloud', 'sql', 'pipelin', 'segment', 'machine learning', 'big data', 'etl', 'analyt', 'machin', 'program', 'learn', 'spark', 'pipelin', 'public', 'hadoop', 'infrastructur', 'bi', 'python', 'avail', 'warehous', 'big', 'etl', 'engin']"
DE,"Experience with big data tools Hadoop, Spark (preferably Databricks), Kinesis, etc.
Experience with relational SQL databases, including Snowflake and Postgres.
Experience with stream-processing systems Spark-Streaming, etc.
Experience with object-orientedobject function scripting languages Scala, Python etc.
Experience with data pipeline and workflow management tools Jenkins, Airflow, etc.
Experience with AWS cloud services EC2, RDS, etc.
Experience building and optimizing lsquobig datarsquo pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large datasets.
Working knowledge of message queuing, stream processing, and highly scalable lsquobig datarsquo stores.
","['sql', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 'cloud', 'postgr']","['pipelin', 'big data']",999,"['sql', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 'cloud', 'pipelin', 'big data']","['stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set']","['sql', 'spark', 'airflow', 'scala', 'ec2', 'hadoop', 'aw', 'snowflak', 'python', 'cloud', 'pipelin', 'big data', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'big', 'set']"
DE,"They must also demonstrate advanced analytical skills, technical and business knowledge and have a strong understanding of how to leverage industry standard tools and methods to solve problems.
The Data Engineer will work closely with Software Engineers by providing data mapping and wrangling expertise and Data Scientists by helping to determine and provide data sets needed for analysis.
They often wrestle with problems associated with database integration and messy, unstructured data sets.
Their ultimate aim is to provide clean, usable data to whomever may require it.
Must have: SQL, scripting languages, ETL tools and Data workflow tools Responsibilities:
Job Requirements:
• KEY RESPONSIBILITIES:
Research opportunities for data acquisition and new uses for existing data
Develop data set processes for data modeling, mining and production
Employ a variety of languages and tools (e.g.
scripting languages) to merge data together
Recommend ways to improve data reliability, efficiency and quality
Aggregate and analyze various data sets to provide actionable insight
Develop reports, dashboards, and tools for business-users
Perform detailed analysis of Customer data source
Write complex SQL queries across multiple data sources Job Requirements QUALIFICATIONS:
Must have 5+ years within a data management role performing implementation, integration and/or technical development, with a heavy focus on SQL and relational databases
A nice to have is prior use of Data Governance tools and processes
A nice to have background would involve knowledge and experience with healthcare data exchange platforms and data aggregation tools and healthcare interoperability and messaging standards, including but not limited to HL7 2.x, HL7 3.x, HL7 FHIR, IHE integration profiles
A nice to have background would be an understanding of general medical terminology and healthcare clinical code sets such as LOINC, CPT, ICD, RxNorm, etc.
A nice to have background would be a demonstrated advanced knowledge in Healthcare data, HL7 scripting and two or more programming languages, Healthcare operations, process improvement, and application of technology to improve patient outcomes.
A nice to have background would be as a highly skilled and proficient knowledge of and experience with build tools of the electronic medical record, and other clinical systems.
Self-starter, self-motivated, high level of initiative within a fast-paced, constantly evolving data management environment
Result focused, ability to solve complex problems and resolve conflicts in a timely manner
Required: Bachelors degree in Data Informatics, Computer Science, Business or related field.
",['sql'],"['recommend', 'research', 'healthcar', 'dashboard', 'data modeling', 'clean', 'etl']",1,"['sql', 'recommend', 'research', 'healthcar', 'dashboard', 'data modeling', 'clean', 'etl']","['analyt', 'program', 'set', 'relat', 'comput', 'scientist', 'action', 'clinic', 'etl', 'sourc', 'engin', 'integr']","['sql', 'recommend', 'research', 'healthcar', 'dashboard', 'data modeling', 'clean', 'etl', 'analyt', 'program', 'set', 'relat', 'comput', 'scientist', 'action', 'clinic', 'etl', 'sourc', 'engin', 'integr']"
DE,"Enables the core infrastructure needed by the Business Analysts and Data Scientists to perform analytics.
Partners with the enterprise IT to fill the data and tool gaps.
Ability to work in a fast paced environment and adapt to changing requirements.
Strong troubleshooting skills, able to debug production code with limited guidance.
Self-starter, able to work with limited supervision and see tasks through to completion.
Proficiency to create and support database technical environments.
Excellent customer service skills that build high levels of customer satisfaction for internal and external customers.
Excellent verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors).
Willingly shares relevant technical andor industry knowledge and expertise to other resources.
Excellent decision-making, problem-solving, team, and time management skills.
Is resourceful and proactive in gathering information and sharing ideas.
Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed.
3-5 years' experience with MS SQL.
2 years SSIS experience.
Responsibilities Ability to build, update, and scale large datasets.
Write queries to automate production tasks.
Responsible for health and hygiene of non-IT managed data.
Consume business requirements and develops the data, database specifications, tables and element attributes to support.
Models databases and develops tables, stored procedures, views, and other database objects.
Maintains database dictionaries, monitors overall database standards and procedures, and integrates systems through database design.
Works closely with other developers to integrate databases with other applications.
May provide leadership andor guidance to other technical professionals.
Performs other related duties as assigned.
","['sql', 'excel']","['supervis', 'commun']",999,"['sql', 'excel', 'supervis', 'commun']","['analyt', 'relat', 'infrastructur', 'scientist', 'integr']","['sql', 'excel', 'supervis', 'commun', 'analyt', 'relat', 'infrastructur', 'scientist', 'integr']"
DE,"The Business
GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.
The Role
You are the Scotty Pippin to the Michael Jordans.
You are the Xavi to the Messis.
You'll do things like:
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience
Non negotiables:
SQL
Python
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract
Salary: $100,000.00 /year
Work Remotely:
Yes
","['sql', 'tableau', 'python']","['commun', 'pipelin']",2,"['sql', 'tableau', 'python', 'commun', 'pipelin']","['analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']","['sql', 'tableau', 'python', 'commun', 'pipelin', 'analyt', 'techniqu', 'stream', 'pipelin', 'provid', 'python', 'parti', 'set']"
DE,"Data Engineer- Guidewire Data HubJacksonville, Florida- Contract to Hire
Overview:
This includes utilizing SAP BODS for loading SQL Server tables for down-stream processing as well as star schemas for Cognos reporting
Job Requirements:
Knowledge of the estimation process is required.
Position Qualifications:
• 8+ years of ETL coding experience (3+ years in SAP BODS)
• 8+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role
• 8+ years in SQL (T-SQL / PL SQL)
• 8+ years in RDBMS's (MS SQL Server / Oracle)
• 8+ years in BI Tools (Cognos/Business Objects or equivalent) (setting up the framework, star-schemas, admin functions)
• 8+ years in ETL Tools (3+ Years in SAP BODS or equivalent)
• 5+ years of Guidewire Insurance experience (PC / BC / CC)
• 2+ years of Guidewire Data Hub / Info Center experience
• Education equivalent to a college degree in information technology-related discipline, supplemented by insurance or computer-related courses/knowledge, or the equivalent in related work experience.
Preferred:
• 10+ years of ETL coding experience with 5+ years in SAP BODS
• Guidewire Insurance Suite experience
• Guidewire Data Hub / Info Center experience a huge plus
• 10+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role
• 10+ years of ETL coding experience with 3+ years in a lead role
• 3+ years' experience in Cognos
• 7 + years' experience in SAP BODS
• Experience in Cloud computing
• Some experience with Java concepts
Would you like to know more?
","['sql', 'cogno', 'bi', 'cloud', 'java', 'oracl']","['information technology', 'etl']",1,"['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl']","['stream', 'relat', 'bi', 'comput', 'etl', 'engin']","['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'stream', 'relat', 'bi', 'comput', 'etl', 'engin']"
DE,"BI reporting frameworks are critical to enabling controllership for the business on performance management, compliance and financial operations.
",['bi'],[None],999,['powerbi'],['bi'],"['powerbi', 'bi']"
DE,"Data Engineer- Guidewire Data Hub
Jacksonville, Florida- Contract to Hire
Overview:
This position must be experienced in and be a lead member of a team to drive technical development efforts supporting a variety of ETL-based technology solutions including the ETL processes to support the data flows into and out of the Guidewire Data Hub and Info Center.
This includes utilizing SAP BODS for loading SQL Server tables for down-stream processing as well as star schemas for Cognos reporting
In this role, you will be responsible for providing a leadership level of technical support during the development, implementation, and production support phases.
This includes analysis, design and code reviews, coordinating and developing code, debugging, and unit/regression/integration testing.
Ability to build, understand, tweak, and utilize technical specifications, design documents, functional designs, and requirements are required.
Knowledge of the estimation process is required.
Involved throughout the entire development lifecycle (e.g., initial concept, requirement gathering, development support, testing, and final implementation and post-production support).
Position Qualifications:
8+ years of ETL coding experience (3+ years in SAP BODS)
8+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role
8+ years in SQL (T-SQL / PL SQL)
8+ years in RDBMS's (MS SQL Server / Oracle)
8+ years in BI Tools (Cognos/Business Objects or equivalent) (setting up the framework, star-schemas, admin functions)
8+ years in ETL Tools (3+ Years in SAP BODS or equivalent)
5+ years of Guidewire Insurance experience (PC / BC / CC)
2+ years of Guidewire Data Hub / Info Center experience
Must have the desire to learn new technologies and technical tools
Education equivalent to a college degree in information technology-related discipline, supplemented by insurance or computer-related courses/knowledge, or the equivalent in related work experience.
Preferred:
10+ years of ETL coding experience with 5+ years in SAP BODS
Guidewire Insurance Suite experience
Guidewire Data Hub / Info Center experience a huge plus
10+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role
10+ years of ETL coding experience with 3+ years in a lead role
3+ years' experience in Cognos
7 + years' experience in SAP BODS
Experience in Cloud computing
Some experience with Java concepts
Would you like to know more?
","['sql', 'cogno', 'bi', 'cloud', 'java', 'oracl']","['information technology', 'etl', 'regress']",1,"['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'regress']","['stream', 'relat', 'bi', 'comput', 'etl', 'engin', 'integr']","['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'regress', 'stream', 'relat', 'bi', 'comput', 'etl', 'engin', 'integr']"
DE,"This position must be experienced in and be a lead member of a team to drive technical development efforts supporting a variety of ETL-based technology solutions including the ETL processes to support the data flows into and out of the Guidewire Data Hub and Info Center.
This includes utilizing SAP BODS for loading SQL Server tables for down-stream processing as well as star schemas for Cognos reportingIn this role, you will be responsible for providing a leadership level of technical support during the development, implementation and production support phases.
This includes analysis, design and code reviews, coordinating and developing code, debugging and unit/regression/integration testing.
Ability to build, understand, tweak and utilize technical specifications, design documents, functional designs and requirements are required.
Knowledge of estimation process is required.Involved throughout the entire development lifecycle (e.g., initial concept, requirement gathering, development support, testing, and final implementation and post production support).Position Qualifications:* 8+ years of ETL coding experience (3+ years in SAP BODS)* 8+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role* 8+ years in SQL (T-SQL / PL SQL)* 8+ years in RDBMS's (MS SQL Server / Oracle)* 8+ years in BI Tools (Cognos/Business Objects or equivalent) (setting up framework, star-schemas, admin functions)* 8+ years in ETL Tools (3+ Years in SAP BODS or equivalent)* 5+ years of Guidewire Insurance experience (PC / BC / CC)* 2+ years of Guidewire Data Hub / Info Center experience* Must have the desire to learn new technologies and technical tools* Education equivalent to a college degree in an information technology related discipline, supplemented by insurance or computer-related courses/knowledge, or the equivalent in related work experience.Preferred:* 10+ years of ETL coding experience with 5+ years in SAP BODS* Property and Casualty insurance industry experience* Guidewire Insurance Suite experience* Guidewire Data Hub / Info Center experience a huge plus* 10+ years of DW/EDW, Data Hub/Data Lake experience in a lead development role* 10+ years of ETL coding experience with 3+ years in a lead role* 3+ years experience in Cognos* 7 + years experience in SAP BODS* Experience in Cloud computing* Some experience with Java concepts
","['sql', 'cogno', 'bi', 'cloud', 'java', 'oracl']","['information technology', 'etl', 'regress']",1,"['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'regress']","['stream', 'relat', 'bi', 'comput', 'etl', 'integr']","['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'regress', 'stream', 'relat', 'bi', 'comput', 'etl', 'integr']"
DE,"This includes utilizing SAP BODS for loading SQL Server tables for down-stream processing as well as star schemas for Cognos reporting All candidates MUST have direct GUIDEWIRE datahub experience.
If you do not have this experience you will NOT BE considered.
You must have PERMANENT residence status to be considered for this opportunity.
There is NO C2C available for this opportunity.
In this role, you will be responsible for providing a leadership level of technical support during the development, implementation and production support phases.
This includes analysis, design and code reviews, coordinating and developing code, debugging and unitregressionintegration testing.
Ability to build, understand, tweak and utilize technical specifications, design documents, functional designs and requirements are required.
Knowledge of estimation process is required.
Involved throughout the entire development lifecycle (e.g., initial concept, requirement gathering, development support, testing, and final implementation and post production support).
Position Qualifications 8+ years of ETL coding experience (3+ years in SAP BODS) 8+ years of DWEDW, Data HubData Lake experience in a lead development role 8+ years in SQL (T-SQL PL SQL) 8+ years in RDBMSrsquos (MS SQL Server Oracle) 8+ years in BI Tools (CognosBusiness Objects or equivalent) (setting up framework, star-schemas, admin functions) 8+ years in ETL Tools (3+ Years in SAP BODS or equivalent) 5+ years of Guidewire Insurance experience (PC BC CC) 2+ years of Guidewire Data Hub Info Center experience Must have the desire to learn new technologies and technical tools bull Education equivalent to a college degree in an information technology related discipline, supplemented by insurance or computer-related coursesknowledge, or the equivalent in related work experience.
Preferred 10+ years of ETL coding experience with 5+ years in SAP BODS Property and Casualty insurance industry experience Guidewire Insurance Suite experience Guidewire Data Hub Info Center experience a huge plus 10+ years of DWEDW, Data HubData Lake experience in a lead development role 10+ years of ETL coding experience with 3+ years in a lead role 3+ years experience in Cognos 7 + years experience in SAP BODS Experience in Cloud computing Some experience with Java concepts
","['sql', 'cogno', 'bi', 'cloud', 'java', 'oracl']","['information technology', 'etl']",1,"['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl']","['stream', 'relat', 'bi', 'avail', 'comput', 'etl']","['sql', 'cogno', 'powerbi', 'cloud', 'java', 'oracl', 'information technology', 'etl', 'stream', 'relat', 'bi', 'avail', 'comput', 'etl']"
DE,"Position Title: Senior Data Engineer - C3.ai
Job Type: Full time Role with Benefits
Work Auth: ANY
Salary Method: W2
Salary: 250-350K
Data Engineers partner with data scientists, application developers, and business subject matter experts to understand business and technical requirements, build data ingestion pipelines and create time aligned object models for the algorithms to execute against.
As a member of the Artificial Intelligence team you will need to thrive in a fast pace and innovative environment.
Collaboration, creativity, and an intense focus on attaining positive business results will be necessary skills.
The successful candidate will possess the following:
Strong collaboration and communication skills and ability to work within cross functional teams
Strong economic thinking skills and business acumen
Self-motivation and a drive to create innovative solutions and see them applied
An entrepreneur's mindset and the persistence to create value through work process transformation
A Day In The Life Typically Includes:
Continuously improve data pipelines including solutions for data management, security and performance
Develop and implement solutions for data quality validation
Build APIs for data consumption
Work closely with Data Scientists to re-engineer model code with new features and deploy models to production
Monitor, resolve, and escalate data pipeline production issues as appropriate
Stay current on technologies and best practices, ensuring that best knowledge is leveraged to achieve success
What You Will Need:
Basic Qualifications:
Bachelor's Degree
10+ years proven professional experience with object-oriented programming in a technology focused role (including but not limited to IT Roles such as: Software Developer, Data Engineer, DevOps/Cloud Engineer, Data Scientist)
5+ years professional coding experience with Python language
5+ years' experience building Data Integrations and/or Machine Learning models for analytics
What Will Put You Ahead?
Preferred Qualifications:
Bachelor's Degree with an Engineering or Technology focus
Experience transforming datasets using Python Architecture skills including designing data pipelines, ETL workflows and integrations
Experience working with SQL and database programming
Hackathon Championships
Experience working or leading Agile Product Development Teams
Experience with cloud platforms like AWS/GCP/Azure
Salary and benefits commensurate with experience.
Equal Opportunity Employer.
Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test.
Salary 250-350K
","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python']","['pipelin', 'machine learning', 'etl', 'commun', 'econom']",1,"['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom']","['day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']","['sql', 'gcp', 'azur', 'aw', 'cloud', 'python', 'pipelin', 'machine learning', 'etl', 'commun', 'econom', 'day', 'analyt', 'machin', 'program', 'learn', 'pipelin', 'azur', 'aw', 'python', 'algorithm', 'scientist', 'etl', 'engin', 'integr']"
DE,"Your Opportunity
They help Marketing, Finance, Risk and various P&Ls make fact-based decisions by integrating and analyzing data as well as operationally leverage data for competitive advantage.
The team delivers innovative client experience capability and rich business insight through robust enterprise data-driven capabilities.
Dev Engineering team within GDT focuses on building new frameworks and enhancing existing ones to improve overall project delivery efficiency and maintain coding standards.
What you are good at
Diagnose / fix highly complex technical issues independently
Communicate individual and project-level development statuses, issues, risks, and concerns to technical leadership and management
Create documentation and training related to technology stacks and standards within assigned team
Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed
Experience collaborating with business and technology partners and offshore development teams
Working with product owners and technical directors to lead technical discussions and resolve technical issues
Strong skills in design, development and delivery of data solutions on Teradata, Big Data and cloud data platforms.
Developing and maintaining code for data ingestion and curation using Informatica, Talend, Scoop, Hive etc
Managing day-to-day development activities for new data solutions and troubleshooting existing implementations.
Applying best practices of data integration for data quality and automation
Reviewing data models and data architecture for Hadoop and HBase environments
Documenting design solutions and supporting documentation.
Working with business analysts to understand business requirements and use cases
What you have
Bachelor's degree in Computer Science or related discipline
Experience with a structured application development methodology, using any industry standard Software Development Lifecycle, in particular Agile Methodologies is required
6+ years of overall experience in I.T.
with strong understanding of best practices for building and designing ETL code
5+ years of experience in ETL tools.
Specific expertise in implementing Informatica / Talend in an Enterprise environment.
Very good experience/understanding on Building Enterprise Data Lake using Talend, Scoop, Hive, Mongo DB, etc
Design, build and support data processing pipelines to transform data in Big Data, Teradata platforms, Cloud Platforms (GCP, AWS)
Experience in real time data ingestion into Hadoop is required
Understanding Hadoop file format and compressions is required
Understanding of best practices for building Data Lake and analytical architecture on Hadoop is required
Familiarity with MapR distribution of Hadoop is preferred
Experience in or deep understanding of cloud based data technology GCP/AWS is preferred
Experience with change data capture tools (CDC) preferred such as Attunity
Scripting / programming with UNIX, Java, Python, Scala etc.
is preferred
Hands-on experience in Java object oriented programming (At least 2 years)
Hands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE (At least 3 years)
Experience in Active Batch Scheduling , control M preferred
Experience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred
Good verbal and written communication skills
Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills
Ability to thrive in a flexible and fast-paced environment across multiple time zones and locations
Experience in Financial Services industry a plus.
","['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka']","['big data', 'pipelin', 'risk', 'financ', 'etl', 'commun']",1,"['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka', 'big data', 'pipelin', 'risk', 'financ', 'etl', 'commun']","['day', 'analyt', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']","['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka', 'big data', 'pipelin', 'risk', 'financ', 'etl', 'commun', 'day', 'analyt', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data.
This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose.
Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.
Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions.
Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions.
Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve.
This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics.
This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.
The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e.
Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel']","['recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']",2,"['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account']","['basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']","['sql', 'perl', 'hadoop', 'java', 'python', 'c', 'r', 'excel', 'recommend', 'research', 'hardwar', 'predict', 'optim', 'statist', 'big data', 'etl', 'commun', 'account', 'basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'optim', 'avail', 'statist', 'learn', 'public', 'big', 'employe', 'engin', 'comput', 'relat']"
DE,"POSITION SUMMARY
The Cloud Data Engineer relishes working with large volumes of data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics.
He/she is an expert with modern data architectures, data modeling, ETL design, business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure create significant business impact.
He/she needs to possess exceptional technical expertise in large scale data warehouse and BI systems.
RESPONSIBILITIES
Design, implement, and support a product data infrastructure providing ad-hoc access to large datasets and computing power.
Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena
Support and supplement current ETL activities based on Microsoft technologies including SQL Server, SSIS.
Analyze and facilitate the transition from on-prem database solutions to cloud solutions.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
Continual research of the latest big data, elastic search, and visualization technologies to provide new capabilities and increase efficiency
Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
Help continually improve ongoing reporting and analysis processes, automating or simplifying the process.
QUALIFICATIONS
Basic Qualifications
7+ years of industry experience in software development, data engineering, business intelligence, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in coding T-SQL, PowerShell, Python, PySpark and MySQL
Demonstrated strength in data modeling, ETL development, and data warehousing in AWS tools (EMR, Glue, Kinesis, Redshift, Spectrum and Athena etc.)
Demonstrated strength in utilizing Microsoft BI Stack including SSIS, SSAS, SSRS.
Experience using business intelligence reporting tools (Tableau, Power BI, Cognos etc.)
Knowledge of big data technologies (Hadoop, Hive, Hbase, Spark etc.)
Preferred Qualifications
Degree in computer science, engineering, mathematics, or a related technical discipline.
Experience working with AWS big data technologies (Redshift, S3, EMR)
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of data set
Certification on AWS tools and technologies
HD Vest, Inc is an equal opportunity employer and does not unlawfully discriminate on the basis of race, sex, age, color, religion, national origin, marital status, sexual orientation, veteran status, disability status or any other basis prohibited by federal, state or local law.
HD Vest, Inc considers information gathered in the hiring process, including information on this application, confidential, and only shares it on a need-to know basis or as required by law.
","['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'bi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'mysql', 'hbase', 'cloud', 'power bi']","['research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl']",1,"['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'powerbi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'hbase', 'cloud', 'research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl']","['basi', 'visual', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', 'power', 'comput', 'relat']","['sql', 'pyspark', 'spark', 'cogno', 'microsoft', 'ssr', 'hadoop', 'powerbi', 'aw', 'python', 's3', 'redshift', 'hive', 'tableau', 'hbase', 'cloud', 'research', 'big data', 'pipelin', 'visual', 'risk', 'data modeling', 'data warehousing', 'analyz', 'etl', 'basi', 'visual', 'python', 'etl', 'sourc', 'analyt', 'challeng', 'hadoop', 'avail', 'infrastructur', 'bi', 'aw', 'warehous', 'set', 'big', 'engin', 'spark', 'pipelin', 'power', 'comput', 'relat']"
DE,"Further, you will be responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
You will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
Senior skill level 4+ years of data engineering, schema design, dimensional data modeling, and or data management experience Proficient with data management tools, such as Python, SQL, Java, and use of Git Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data Active SECRET security clearance is required.
Preferred Experience with graph databases and event sourcing models Familiarity with machine learning, artificial intelligence, and or geospatial data analysis Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations Notes Candidates must have strong experience in SQL, Python and Data Modeling Must have secret clearance Warehousing would be preferred Thanks Regards Chris Martin cmartinamvotech.com
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'optim', 'python', 'comput', 'set', 'sourc', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'optim', 'python', 'comput', 'set', 'sourc', 'engin', 'evalu']"
DE,"Responsibilities Data Extraction, Transformation, Loading are main responsibilities considering enterprise data management for Data Lakes and Data Warehouses.
Build Azure Data Factory Pipelines.
Implement Azure Cloud Data Warehouses.
Work with the Application Development team to Implement Data Strategies, Build Data Flows and Develop Conceptual Data Models.
Required skills Experience Experience in Building Data Applications using Azure Data Factory Experience running ETLELT Projects from end to end, understanding required Experience in Azure Data Bricks with Python, Spark mandatory Strong SQL Skills Good Understanding of Azure SQL Data WarehouseDB Excellent in Data Analytical Skills Strong understanding of Data Integration (Validation and Cleaning), familiarity with Complex Data Structures Good to know Data Visualization tool Power BI
","['sql', 'spark', 'azur', 'bi', 'cloud', 'python', 'excel', 'power bi']","['clean', 'visual', 'pipelin']",999,"['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'excel', 'clean', 'visual', 'pipelin']","['analyt', 'spark', 'pipelin', 'azur', 'visual', 'power', 'bi', 'python', 'warehous', 'integr']","['sql', 'spark', 'azur', 'powerbi', 'cloud', 'python', 'excel', 'clean', 'visual', 'pipelin', 'analyt', 'spark', 'pipelin', 'azur', 'visual', 'power', 'bi', 'python', 'warehous', 'integr']"
DE,"Sr. Data Engineer
FT. Worth TX
MUST
Active Secret or TS required
Experienced Sr. Data Engineer
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience required
Proficient with data management tools, such as Python, SQL, Java, and use of Git required
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets required
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data required
Experience with graph databases and event sourcing models required
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis required
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations required
DUTIES:
Will be performing as part of, or in support of a SCRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.
Has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data.
Responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
Will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']"
DE,"The Data Engineer joins a team of engineers, subject matter experts, and data scientists to jointly provide experienced, high-quality Information Technology (IT) engineering solutions.
Your efforts will be integral to the success of these initiatives.
Performing as part of, or in support of a SCRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.
Specifically, the Data Engineer has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data.
Further, you will be responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
You will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
Senior skill level
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience
Proficient with data management tools, such as Python, SQL, Java, and use of Git
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data
Active SECRET security clearance is required.
Preferred:
Experience with graph databases and event sourcing models
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations
#CJ
#JT
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'information technology', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'information technology', 'geospati']","['machin', 'techniqu', 'set', 'optim', 'python', 'scientist', 'comput', 'integr', 'sourc', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'information technology', 'geospati', 'machin', 'techniqu', 'set', 'optim', 'python', 'scientist', 'comput', 'integr', 'sourc', 'engin', 'evalu']"
DE,"DATA ENGINEER
Simpli.fi is hiring talented and experienced software engineers to join its Data Engineering team.
At Simpli.fi, you will have the opportunity to truly work with Big Data.
The Data Engineering team is responsible for moving and transforming massive datasets into valuable and insightful information.
A career at Simpli.fi offers countless ways to make an impact in a fast-growing organization.
Â
Responsibilities
Design and develop new systems and tools to enable clients to optimize and track advertising campaigns (Vertica, Looker, Spark)
Use your expert skills across a number of platforms and tools such as Python, Ruby, SQL, Linux shell scripting, Git, and Chef
Work across multiple teams in high visibility roles and own the solution end-to-end
Requirements
Proficiency building and supporting applications on Linux topology.
Familiarity with OO and FP methodologies and philosophies.
Moderate experience in Big Data ecosystem (Hadoop, Spark, Kafka, etc.)
Proficiency in Ruby or Python development.
Familiarity with column-oriented Big Data systems such as Vertica or Cassandra.
Familiarity with profiling and tuning a SQL execution plan
Familiarity with the JVM.
Scala is a definite plus.
Excellent communication skills including the ability to identify and communicate data driven insights.
BS or MS degree in Computer Science, Software Engineering, or a related technical field.
","['sql', 'linux', 'spark', 'cassandra', 'scala', 'looker', 'git', 'hadoop', 'python', 'kafka', 'excel', 'rubi']","['commun', 'big data']",1,"['sql', 'linux', 'spark', 'cassandra', 'scala', 'looker', 'git', 'hadoop', 'python', 'kafka', 'excel', 'rubi', 'commun', 'big data']","['spark', 'hadoop', 'python', 'comput', 'â', 'big', 'relat', 'engin']","['sql', 'linux', 'spark', 'cassandra', 'scala', 'looker', 'git', 'hadoop', 'python', 'kafka', 'excel', 'rubi', 'commun', 'big data', 'spark', 'hadoop', 'python', 'comput', 'â', 'big', 'relat', 'engin']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
Key Responsibilities
Involved in the design, prototyping and delivery of software solutions within the big data eco-system
Work independently on big data projects and/or serving as analytics SME to provide new or enhanced data to the business
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, learns, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e.
Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required.
Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
At least 2 years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow is a plus
Working knowledge of Tableau a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka']","['recommend', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']"
DE,"Your efforts will be integral to the success of these initiatives.
As team member, you will be expected to be solutions-oriented, innovative, collaborative and agile.
Performing as part of, or in support of a S CRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.
Responsibilities Specifically, the Data Engineer has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data.
Further, you will be responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
You will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
Qualifications Senior skill level 4+ years of data engineering, schema design, dimensional data modeling, and or data management experience Proficient with data management tools, such as Python, SQL, Java, and use of Git Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data Active SECRET security clearance is required.
Preferred Experience with graph databases and event sourcing models Familiarity with machine learning, artificial intelligence, and or geospatial data analysis Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'techniqu', 'set', 'optim', 'python', 'comput', 'integr', 'sourc', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'techniqu', 'set', 'optim', 'python', 'comput', 'integr', 'sourc', 'engin', 'evalu']"
DE,"Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered.
",[None],[None],999,[],[None],[None]
DE,"Summary
***
What if there was a different way of looking at money?
This is a great opportunity to join a Fortune 500 and not-for-profit organization that gives you a great sense of purpose!
As a full-time Data Engineer you will work closely with the Data Architect as they provide guidance and vision so you can develop, construct and maintain the data architecture for the enterprise data professionals.
You will work with large scale data processing systems that collects data from a variety of structured and unstructured data sources, stores data in a scale-out data lake and prepare the data using ELT techniques in preparation for the data science data exploration and analytic modeling.
Communication is key in this role as you will work with users of all levels across the organization.
Job Duties and Responsibilities:
Oversee and direct efforts to identify information and technology solutions that enable business needs and strategies.
Own and define DevOps pipelines and release management for data engineering
Lead efforts to analyze IT industry and market trends and determine potential impacts.
Develop concepts and constructs necessary to create technology-enabled business systems.
Influence technology direction and provide thought leadership and execution to large complex efforts.
Utilize breadth of technical understanding and dive deep when necessary.
Consult on and manage initiatives to ensure alignment across multiple business and IT areas.
Proactively mitigate risks across multiple assets, information domains, technologies and platforms.
Provide leadership, mentoring and technical guidance to others to drive initiatives.
Facilitate communications that involve obtaining cooperation and agreement on issues that may be complex or controversial.
Utilize negotiation and persuasion to come to agreement and to effectively form partnerships.
Act as a change agent to continuously improve and move the organization forward.
Accountable to successfully deliver the right results on initiatives in a timely and effective manner.
Direct the work of others to lead initiatives that cross multiple assets, technologies, platforms, departments and vendors.
Ability to work within a diverse team of skillsets and experience levels to deliver results.
Required Job Qualifications:
Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business, or related field.
10+ years of experience in Technology related field including 3+ years prior lead experience.
Expert knowledge of predictive analytics, statistical modeling, advanced mathematics, data integration concepts, business intelligence and data warehousing and implementing large systems
Implement and configure data platforms including but not limited to Hadoop, Spark, Kafka and batch integration is preferred.
Working Experience developing data processes with Java, Python, R or other scripting languages preferred.
","['spark', 'hadoop', 'java', 'python', 'r', 'kafka']","['pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']",1,"['spark', 'hadoop', 'java', 'python', 'r', 'kafka', 'pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account']","['analyt', 'asset', 'techniqu', 'spark', 'pipelin', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'python', 'comput', 'statist', 'integr', 'sourc', 'engin']","['spark', 'hadoop', 'java', 'python', 'r', 'kafka', 'pipelin', 'risk', 'predict', 'data warehousing', 'statist', 'commun', 'account', 'analyt', 'asset', 'techniqu', 'spark', 'pipelin', 'relat', 'divers', 'predict', 'hadoop', 'provid', 'python', 'comput', 'statist', 'integr', 'sourc', 'engin']"
DE,"Data Engineer Ideal candidates should have experience with Data Ingestion and Consumption.
That is transforming from source raw data, cleansing missing data and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST.
In depth knowledge of preparing large scale data analytics for consumption's.
Key knowledge on HIVE and query optimization in HIVE Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']"
DE,"Role: Hadoop Data Engineer
Duration: 3 - 6 + Months
Responsibilities
â Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.
â Support/Enhance data pipelines and ETL using heterogeneous sources
â Transform data using data mapping and data processing capabilities like Spark, Spark SQL, Impala etc.
â Expands and grows data platform capabilities to solve new data problems and challenges
â Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project
Basic Skill Requirements
â 7+ years Enterprise Development
â 2+ Years Design with Big Data Strategies.
Other Skills
â Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, Hive, Impala, Spark,Â
â Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types (Parquet, Avro, ORC) for specific use cases etc.
â Experience with Query languages â SQL, Hive, Impala, Drill etc.
â Solid hands on experience of development/automation in Unix OS
â Experience in agile(scrum) development methodology
â Exposure to Data ingestion methodologies such as Kafka, Sqoop, Storm, Web Scrapping, etc.
â Experience with development/automation skills.
Must be very comfortable with reading and writing Python/Shell script and SQL code
â Experience with Hadoop open source distributions - Cloudera
In addition, all colleagues are eligible for a number of rewards and recognition programs.
The Client retains the discretion to add or change the duties of the position at any time.
","['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka']","['etl', 'pipelin', 'big data']",999,"['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka', 'etl', 'pipelin', 'big data']","['program', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'etl', 'sourc', 'â']","['sql', 'mapreduc', 'spark', 'unix', 'hadoop', 'python', 'hive', 'kafka', 'etl', 'pipelin', 'big data', 'program', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'etl', 'sourc', 'â']"
DE,"• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry* Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster* Proven Experience in handling variety of data formats* Experience in building large scale Data Lake Environment* Troubleshooting Hive Performance issues and developing HQL queries* Experience with Spark and PySpark* Experience in implementing CI/CD Process and Job Automation through Autosys* Experience in Hadoop Cluster Administration is a big plus* Experience with integration of data from multiple data sources* Assist Analytics and Data Scientist team and Business Users* Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations* The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow* Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus* Strong situational analysis and decision making abilitiesLI-AG1
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc']"
DE,"Req ID: 90378
Duties:
Required Skills:
2 years' experience with NoSQL databases, such as Cassandra, MongoDB, or CouchDB
2 years of Python
2 years' experience using frameworks/libs like TensorFlow, Keras, or Spacy
2 years' experience using Hadoop, Kafka, or HDFS
2 years' experience building stream-processing systems, using solutions such as Storm or Spark-Streaming
2 years' experience using Spark
Preferred Skills:
Experience with various messaging systems, such as MQTT or RabbitMQ
Experience working in public cloud environments like AWS, or Azure.
Good understanding of Lambda Architecture, along with its advantages and drawbacks
A strong team-oriented mindset
Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered.
Visit nttdataservices.com to learn more.
","['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka']",[None],999,"['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka']","['stream', 'spark', 'azur', 'public', 'hadoop', 'aw', 'python']","['kera', 'mongodb', 'spark', 'spaci', 'azur', 'cassandra', 'hadoop', 'aw', 'lambda', 'python', 'cloud', 'nosql', 'tensorflow', 'kafka', 'stream', 'spark', 'azur', 'public', 'hadoop', 'aw', 'python']"
DE,"Position Summary:
Bi-Lingual (Korean).
Under limited supervision, has the ability of coordinating and managing service engineering product requirements.
In addition, this individual will coordinate any additional technical service issue with the HQ Engineering Team to ensure a successful product launch while meeting all deliverables.
Responsibilities also include the establishment of effective working relationships with SEA Product Management Teams, Sales, and field testing personnel in order to drive product development and the product schedule successes.
Interface with major customer and end users to ensure customer satisfaction.
Analyze return rates and trends and plan future requirements.
Develop test and repair strategies for new products.
Auditing and training of out-sourced repair product to third party vendors.
Essential Duties And Responsibilities include the following: Other duties may be assigned.
In this position, either directly or through others, the incumbent will:
• Develops product inspection, tests, diagnostics, and repair procedures.
Identifies and initiates improvements in product testing techniques, tools and repair processes.
Provides product test and repair standards for new products.
Works closely with HQ engineering to resolve technical issues and to develop product test and repair modifications.
• Designs and implements production floor plans to maximize production output.
Establishes and maintains test equipment within calibration to ensure quality test and repair performance within specification standards
• Ability to determine necessary resources, and project timelines while communicating project status to all levels of management.
• Gathers repair service data, analyzes repair data for repair rates and trends.
Prepares and presents statistical reports and information.
Projects and plans future requirements, budgets and resources.
• Responsible for the establishment of new processes and improve existing processes to improve overall reverse logistics performance.
• Provides guidance, direction and decision control of RB-MES systems and application enhancements for the repair department.
Ensures and recommends enhancements that improve operation efficiency, provide availability and integrity of critical repair service information and product performance/failure data.
Manages systems and information projects against plans and performance expectations.
Background/Experience to qualify for this position, the following minimal background and skill levels are required:
• Bachelors Degree in Engineering, from an accredited college or university, or equivalent work experience in the telecommunications industry.
• Should have a minimum of 3-5 years experience in a project development with 1-3 years experience in the wireless telecommunications position requiring engineering direct project development experience and long-term customer contact and relationship building.
• Experience researching and resolving customer technical complaints and issues.
• Previous experience with Mobile or Network Field.
• Comprehensive use of Microsoft Office applications operating in a Windows XP or NT LAN/WAN environment.
Necessary Skills/Attributes for this position the following skills and abilities must be demonstrated at a proficient level:
• Plan, organize and prioritize multiple complex assignments and projects.
• Read and interpret detailed and complex customer requirements.
• Demonstrated competency in both oral and written communication modes for both internal and external personnel at various levels, especially in logistical or financial areas of clients, prospects, and SEA.
• Work independently and in a team environment, in order to achieve personal and team goals and complete assignments within established time frames.
• Easily takes direction and follows up as needed to ensure project completion.
• Use of MS Office (Word, Excel, PowerPoint and Access)
• Bi-Lingual Korean Speaking in order to communicate effectively with HQ on a regular basis.
Physical/Mental Demands and Working Conditions: The position requires the ability to perform the essential duties and responsibilities in the following environment:
• Operate a computer keyboard and view a video display terminal more than 25% of work time.
• Lift, move, or adjust general office equipment, boxes of marketing presentation or media materials using proper materials handling equipment and procedures.
• Physically make product marketing presentations or demonstrations to customers, internal and external groups using verbal and graphics communication modes.
• Travel to customer locations, trade shows, etc.
requires maximum 30% of the time.
• Occasionally work additional hours beyond normal schedule
","['microsoft', 'bi', 'ms office', 'powerpoint', 'excel']","['research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun']",1,"['microsoft', 'powerbi', 'powerpoint', 'excel', 'research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun']","['basi', 'essenti', 'techniqu', 'provid', 'bi', 'avail', 'parti', 'statist', 'comput', 'integr', 'sourc', 'engin']","['microsoft', 'powerbi', 'powerpoint', 'excel', 'research', 'end user', 'normal', 'statist', 'analyz', 'supervis', 'logist', 'commun', 'basi', 'essenti', 'techniqu', 'provid', 'bi', 'avail', 'parti', 'statist', 'comput', 'integr', 'sourc', 'engin']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
Key Responsibilities
Involved in the design, prototyping and delivery of software solutions within the big data eco-system
Work independently on big data projects and/or serving as analytics SME to provide new or enhanced data to the business
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, learns, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e.
Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required.
Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
At least 2 years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow is a plus
Working knowledge of Tableau a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka']","['recommend', 'problem solving', 'big data', 'etl', 'commun']",1,"['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun']","['basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']","['sql', 'spark', 'airflow', 'scala', 'unix', 'jira', 'git', 'hadoop', 'tableau', 'python', 'hive', 'java', 'r', 'github', 'kafka', 'recommend', 'problem solving', 'big data', 'etl', 'commun', 'basi', 'program', 'python', 'packag', 'etl', 'quantit', 'sourc', 'integr', 'analyt', 'hadoop', 'avail', 'action', 'learn', 'public', 'big', 'employe', 'engin', 'particip', 'spark', 'comput', 'relat']"
DE,"Where good people build rewarding careers.
Think that working in the insurance field cant be exciting, rewarding and challenging?
Think again.
And youll have fun doing it.
The Big Data Engineer - Senior Consultant II is accountable for leading a technical team of developers on the Enterprise Data Quality team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data.
This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose.
Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.
Key Responsibilities
Serve as a subject matter expert and an advocate of the data quality strategic vision and enterprise data strategy
Design and drive the implementation of end-to-end data quality solutions for multiple data ecosystems that leverage non-relational data
Consistently contribute to process improvements and help lead the team to deliver working solutions and data quality standards
Participate and review documentation for self-service data quality practices, the process framework, and internal guides
Collaborate with various business and technical teams to gather requirements around data quality rules and propose the optimization of these rules if applicable, then design and develop these rules in Informatica Data Quality, Informatica Data Engineering Quality (DEQ), or other home-grown technologies
Perform thorough data profiling with multiple usage patterns, root cause analysis and data cleansing and develop scorecards utilizing Informatica, Excel and other data quality tools
Configure and manage connection process
Develop ""matching"" plans, help determine best matching algorithm, configure identity matching and analyze duplicates
Build workflows scorecards in Informatica Data Engineering Quality (DEQ) to support data remediation
Run data quality jobs (address standardization and validation, email cleanups, name cleanup, parsing, etc.)
utilizing DEQ and other ETL tools
Serve as the primary resource to team members and data stewards for training, problem resolution, data profiling etc.
Analyze and provide data metrics to target personas to provide insight into data quality and help prioritize remediation efforts
Identify and resolve issues, bugs, and impediments
Job Qualifications
Education and Experience
Bachelors Degree in Math, Statistics, Finance, Business, Economics, IT or related field, or equivalent experience
5-7+ years of data and/or analytics experience
Required Skills
Experience with various data types (i.e.
relational, unstructured, semi-structured, hierarchical, linked graph data, streaming, biometrics)
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations
Job Qualifications
Nice-to-Have
Experience working within agile methodologies
Working knowledge of cloud infrastructure (AWS/Azure/Google)
Intermediate technical knowledge related to various data platforms
Administration experience on Hadoop, HDFS, YARN, Spark, Sentry/Ranger, HBase and Zookeeper
Experience using Informatica Data Quality, Informatica Data Engineering Quality, Informatica Analyst
Experience in configuring & troubleshooting the components in the Hadoop ecosystem Spark, Solr, Scala, Kafka etc.
Ability to communicate and present advanced technical topics to general audiences
Ability to code in languages such as Python, Perl, Java, C, R, SQL, XSLT
Understanding of analytics techniques such as predictive modeling, machine learning, neural networks
Certificates, Licenses, Registrations
Preferred: Certified Data Management Professional Certification
Preferred: Informatica Data Quality or Data Engineering Quality (or other Informatica tool) Certification
Preferred: non-relational data store certification
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work.
Good Life.
Good Hands®.
Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the EEO is the Law poster click here.
This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click here.
It is the Companys policy to employ the best qualified individuals available for all jobs.
This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.
","['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel']","['recommend', 'big data', 'graph', 'cleans', 'analyz', 'neural network', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account']",1,"['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel', 'recommend', 'big data', 'graph', 'cleans', 'analyz', 'nn', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account']","['basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'avail', 'statist', 'public', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'particip', 'machin', 'spark', 'relat']","['sql', 'spark', 'perl', 'azur', 'scala', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'java', 'c', 'hbase', 'r', 'kafka', 'excel', 'recommend', 'big data', 'graph', 'cleans', 'analyz', 'nn', 'predict', 'machine learning', 'optim', 'statist', 'financ', 'etl', 'commun', 'econom', 'math', 'account', 'basi', 'program', 'techniqu', 'predict', 'python', 'etl', 'algorithm', 'analyt', 'azur', 'hadoop', 'optim', 'avail', 'statist', 'public', 'primari', 'infrastructur', 'aw', 'big', 'employe', 'engin', 'particip', 'machin', 'spark', 'relat']"
DE,"Sr. Data Engineer
FT. Worth TX
MUST
Active Secret or TS required
Experienced Sr. Data Engineer
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience required
Proficient with data management tools, such as Python, SQL, Java, and use of Git required
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets required
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data required
Experience with graph databases and event sourcing models required
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis required
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations required
DUTIES:
Will be performing as part of, or in support of a SCRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.
Has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data.
Responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
Will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']"
DE,"As a part of a dynamic team working in the Agile methodology you will have the opportunity to contribute to multiple phases of the solution lifecycle.Essential Functions* Gather requirements, perform data analysis, and profile data to design appropriate solutions* Develop detailed data models and data architecture for enterprise data warehouse projects* Create and maintain processes to load the data lake utilizing DataBricks Notebooks and Python* Create and maintain processes to build Analysis Services DAX cubes from the data lake tables* Design and maintain Power BI and SSRS reports that meet business requirements* Performance tuning to ensure reports meet SLA targets* Troubleshoot issues and create solutions in a timely mannerJob Requirements* Bachelors degree in Computer Science or Information Systems or equivalent experience required* 8-10 years of general technology experience (required)* 5-7 years of Analytics/Business Intelligence solution delivery experience* Power BI visualization, Power Query data integration, and DAX experience required* DataBricks and Python experience required* Understanding of dimensional modeling concepts* Strong T-SQL skills required* SQL Server Analysis Services Tabular model experience required* SQL Server Reporting Service experience required* Azure SQL Data Warehouse or other MPP data warehouse experience preferred* Experience with Microsoft Azure-based solutions preferred* Must have advanced skills with the ETL Tools, BI & Analytics Tools and Cloud software utilized to perform their daily duties, e.g., Azure ADF, Informatica ICS, Python, Analysis Services, Power BI, SSRS,* Demonstrable experience with development lifecycle (development, testing, deployment, source control, etc.
PlainsCapital Bank is a leading commercial bank with locations throughout Texas.
PrimeLending is a national mortgage provider focused on purchase mortgage originations.
HilltopSecurities provides financial advisory, clearing, retail brokerage, and other investment banking services.
To learn more, please visit www.hilltop-holdings.com.
","['sql', 'azur', 'ssr', 'bi', 'cloud', 'python', 'microsoft', 'power bi']","['etl', 'visual']",1,"['sql', 'azur', 'ssr', 'powerbi', 'cloud', 'python', 'microsoft', 'etl', 'visual']","['analyt', 'azur', 'visual', 'power', 'provid', 'bi', 'python', 'comput', 'texa', 'warehous', 'etl', 'sourc', 'integr']","['sql', 'azur', 'ssr', 'powerbi', 'cloud', 'python', 'microsoft', 'etl', 'visual', 'analyt', 'azur', 'visual', 'power', 'provid', 'bi', 'python', 'comput', 'texa', 'warehous', 'etl', 'sourc', 'integr']"
DE,"Analyze and understand data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Design and Develop SQL , Hive queries, APIs to extract data from the store
Work closely with data Scientists to ensure the source data is aggregated and cleansed
Work with product managers to understand the business objectives
Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
Work with DevOps to build automated data pipelines
Total Experience Required
4 years <10>
The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
Big Data-Hadoop, NoSQL, Hive, Apache Spark
Python
Java & REST
GIT and Version Control
Desirable Technical Skills
Familiarity with HTTP and invoking web-APIs
Exposure to machine learning engineering
Exposure to NLP and text processing
Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
Experience working in SCRUM methodology
Proven sense of high accountability and self-drive to take on and see through big challenges
Confident, takes ownership, willingness to get the job done
Excellent verbal communications and cross group collaboration skills
Job Type: Contract
Salary: $55.00 per hour
Schedule:
Monday to Friday
Experience:
Hive: 2 years (Required)
Apache Spark: 2 years (Required)
REST: 1 year (Required)
NoSQL: 1 year (Required)
Java: 1 year (Required)
Git: 1 year (Required)
Hadoop: 4 years (Required)
SQL: 1 year (Required)
Python: 1 year (Required)
API: 1 year (Required)
Contract Length:
5 - 6 months
Work Remotely:
Temporarily due to COVID-19
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Summary
***
You will be responsible for working closely with Data Architect’s and Big Data Engineers and their vision to help design, build and provide governance of large-scale streaming architecture solution that delivers business value across the organization.
You will have the opportunity to both lead and execute on projects and always consider the bigger picture proactively with anticipating any performance issues, troubleshooting, monitoring, quality, etc… This role will require experience with hybrid platforms, cloud migration, publishing, development with newer technologies such as Spring Boot, KSQL/Stream Processing, data connectors such as Kafka, performance tuning, data quality and data visualization knowledge.
You and will report to the Director of Information Delivery.
Job Duties and Responsibilities
Lead the implementation, execution, and maintenance of Streaming Architecture technology solutions
Lead work to advance and support streaming architecture practices for business processes, applications and technology that underpin the EIM discipline
Provide leadership for Data Engineer tasks supporting projects
Revenue generated
Budget responsibilities
Leads the delivery, support and maintenance of solutions with one or more business and technology areas.
Organizational impact results from mid-large sized projects
Required Job Qualifications
Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field
5+ years of experience in Technology related field including prior lead experience
Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools
Strong organizational, analytical, critical thinking and leadership skills
Demonstrated leadership on mid-large-scale project impacting strategic partners
",['kafka'],"['big data', 'visual', 'statist', 'predict']",1,"['kafka', 'big data', 'visual', 'statist', 'predict']","['analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']","['kafka', 'big data', 'visual', 'statist', 'predict', 'analyt', 'stream', 'relat', 'visual', 'predict', 'provid', 'comput', 'statist', 'big', 'integr', 'engin']"
DE,"Level-row"">Level
Experienced
Irving, TX - , TX
Position Type
Full Time
Education Level
Graduate Degree
Salary Range
Undisclosed
Travel Percentage
Negligible
Job Shift
Undisclosed
Job Category
Science
Title: Data Engineer
Position Summary
The successful candidate will design, implement, and maintain data storage and data flow solutions for structured and non-structured multi-model data in support of data science and machine learning pipelines.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Must know and adhere to best practices and possess knowledge of current state of the art data platforms and solutions.
Job Responsibilities
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create data tools for analytics and data scientist team members that assist them in building and optimizing data science products
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Requirements
Masters in Computer Science, Engineering or a related field with exposure to cancer biology
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience with relational SQL and NoSQL databases, including MongoDB, Cassandra, etc.
Strong analytic skills related to working with unstructured datasets
Experience with microservices architecture
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Proficiency in Python, Pandas, PySpark, Dask, Ray, etc.
Experience writing RESTful APIs
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Proficient verbal and written communication skills to explain complex technical details in clear language
Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team
Demonstrate a focus on listening to and understanding user needs and then delighting the customer by exceeding service and quality expectations
#LI-DG1
Qualifications
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""First Name"" minlength=""1"" required=""required"" value="""">First Name*
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""Last Name"" minlength=""1"" required=""required"" value="""">Last Name*
"" class="" input--dark-grey"" data-parsley-type=""email"" aria-label=""Email"" minlength=""1"" required=""required"" value="""">Email*
"" data-parsley-pattern=""/^\d*$/"" data-parsley-pattern-message=""Only numbers are allowed.""
aria-label=""Phone"" minlength=""1"" required=""required"" value="""">Phone*
"" id=""quick-apply-desktop__application-form__resume__input"" accept="".bmp,.accdb,.xls,.xlsx,.gif,.html,.jpe,.jpg,.jpeg,.pdf,.png,.ppt,.pptx,.rtf,.tiff,.txt,.csv,.doc,.docx,.zip"" required=""required"" style=""width: 0; height: 0; opacity: 0; overflow: hidden; position: absolute; z-index: -1; "" data-parsley-errors-container=""#quick-apply-desktop__application-form__resume__errors-container"">
Attach Resume*
Submit
chevron_left
Sign In
Welcome !
It looks like you have an account with this email.
Please enter your password to complete your application.
""
class="" input--dark-grey"" minlength=""8"" required=""required"" value="""">Password*
Sign In
Forgot password?
Thanks!
An email has been sent to you with a password to access your account.
Ready for the next step?
Add your availability!
View More Listings
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""First Name"" minlength=""1"" required=""required"" value="""">First Name*
"" class="" input--dark-grey"" data-parsley-pattern=""/^[ a-z0-9\-\.
]*$/i"" data-parsley-pattern-message=""Only A-Z, 0-9, -, .
and space characters are allowed.""
aria-label=""Last Name"" minlength=""1"" required=""required"" value="""">Last Name*
"" class="" input--dark-grey"" data-parsley-type=""email"" aria-label=""Email"" minlength=""1"" required=""required"" value="""">Email*
"" data-parsley-pattern=""/^\d*$/"" data-parsley-pattern-message=""Only numbers are allowed.""
aria-label=""Phone"" minlength=""1"" required=""required"" value="""">Phone*
"" id=""quick-apply-mobile__application-form__resume__input"" accept="".bmp,.accdb,.xls,.xlsx,.gif,.html,.jpe,.jpg,.jpeg,.pdf,.png,.ppt,.pptx,.rtf,.tiff,.txt,.csv,.doc,.docx,.zip"" required=""required"" style=""width: 0; height: 0; opacity: 0; overflow: hidden; position: absolute; z-index: -1; "" data-parsley-errors-container=""#quick-apply-mobile__application-form__resume__errors-container"">
Attach Resume*
Submit
chevron_left
Sign In
Welcome !
It looks like you have an account with this email.
Please enter your password to complete your application.
""
class="" input--dark-grey"" minlength=""8"" required=""required"" value="""">Password*
Sign In
Forgot password?
Thanks!
An email has been sent to you with a password to access your account.
Ready for the next step?
Add your availability!
View More Listings
Terms of Use |
Paycom Privacy Policy |
© 2020 Paycom | All Rights Reserved.
This website uses cookies to customize and improve your experience.
If you are a California resident, you may be entitled to certain rights regarding your personal information.
Accept Cookies
","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka']","['pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account']",2,"['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account']","['python', 'analyt', 'input', 'hadoop', 'optim', 'avail', 'appli', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['mongodb', 'sql', 'pyspark', 'spark', 'airflow', 'dask', 'cassandra', 'scala', 'panda', 'ec2', 'hadoop', 'aw', 'cloud', 'python', 'redshift', 'java', 'nosql', 'kafka', 'pipelin', 'machine learning', 'optim', 'big data', 'commun', 'account', 'python', 'analyt', 'input', 'hadoop', 'optim', 'avail', 'appli', 'learn', 'infrastructur', 'aw', 'big', 'set', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"•
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala,
Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera
Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through
Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data
Scientist team and Business Users
• Exceptional communication skills and the ability to communicate
appropriately at all levels of the organization; this includes written and
verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business
to IT and data constraints to the business; applies equal conveyance
regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization
with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
#LI-AG1
Job Function
TECHNOLOGY
Role
Developer
Job Id
158552
Desired Skills
Big Data
Desired Candidate Profile
Qualifications :
BACHELOR OF ENGINEERING
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster', 'big data']",1,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']"
DE,"You will be responsible for developing and deploying novel algorithms along with optimizing existing machine learning systems to maximize their business value and increase consumer satisfaction at every brand touchpoint.
This role is ideal for someone looking to extend their algorithm design and software engineering skills into a part mentor, part IC, part thought partner role.
You will serve a large role in driving the client’s transformation into a data-driven enterprise.
Job Responsibilities
Architect, build, maintain, and improve new and existing suite of algorithms and their underlying systems.
Implement end-to-end solutions for batch and real-time algorithms along with requisite tooling around monitoring, logging, automated testing, performance testing and A/B testing.
Utilize your entrepreneurial spirit to identify new opportunities to optimize business processes and improve consumer experiences, and prototype solutions to demonstrate value with a crawl, walk, run mindset.
Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation
Write efficient and well-organized software to ship products in an iterative, continual-release environment
Contribute to and promote good software engineering practices across the team
Mentor and educate team members to adopt best practices in writing and maintaining production machine learning code
Communicate clearly and effectively to technical and non-technical audiences equally well
Actively contribute to and re-use community best practices
Required Skills
University or advanced degree in engineering, computer science, mathematics, or a related field
5+ years’ experience developing and deploying machine learning systems into production
Strong experience working with a variety of relational SQL and NoSQL databases
Strong experience working with big data tools: Hadoop, Spark, Kafka, etc.
Experience with at least one cloud provider solution (AWS, GCP, Azure)
Strong experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Ability to work in a Linux environment
Industry experience building and productionizing innovative end-to-end Machine Learning systems
Ability to quickly prototype ideas and solve complex problems by adapting creative approaches
Experience working with distributed systems, service oriented architectures and designing APIs
Strong knowledge of data pipeline and workflow management tools
Expertise in standard software engineering methodology, e.g.
unit testing, test automation, continuous integration, code reviews, design documentation
Relevant working experience with Docker and Kubernetes is a big plus
Additional Key Metrics
Preference will be given to candidates either based in, or willing to relocate to, Dallas/Ft.
Worth or San Francisco area.
Client has ability to transfer H1B’s and sponsor.
Start date is ASAP and compensation is negotiable contingent on experience and qualifications.
Job Type: Full-time
Pay: $160,000.00 - $180,000.00 per year
Benefits:
401(k)
Dental Insurance
Employee Discount
Health Insurance
Paid Time Off
Vision Insurance
Supplemental Pay:
Signing Bonus
Experience:
AWS, Azure or GCP: 1 year (Preferred)
Scala: 1 year (Preferred)
Kafka: 1 year (Preferred)
Java: 1 year (Preferred)
SQL: 3 years (Required)
Python: 1 year (Required)
Machine Learning: 1 year (Required)
NoSQL: 1 year (Preferred)
Spark: 1 year (Preferred)
Education:
Bachelor's (Required)
Work authorization:
United States (Required)
Additional Compensation:
Bonuses
Store Discounts
Innovative -- innovative and risk-taking
Stable -- traditional, stable, strong processes
People-oriented -- supportive and fairness-focused
Schedule:
Monday to Friday
Day shift
Work Remotely:
Temporarily due to COVID-19
","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker']","['pipelin', 'risk', 'machine learning', 'big data', 'commun']",1,"['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'risk', 'machine learning', 'big data', 'commun']","['day', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'provid', 'aw', 'python', 'employe', 'comput', 'releas', 'big', 'integr', 'engin', 'algorithm']","['sql', 'linux', 'gcp', 'spark', 'azur', 'scala', 'hadoop', 'aw', 'cloud', 'python', 'java', 'nosql', 'kubernet', 'kafka', 'docker', 'pipelin', 'risk', 'machine learning', 'big data', 'commun', 'day', 'machin', 'learn', 'spark', 'pipelin', 'azur', 'relat', 'hadoop', 'provid', 'aw', 'python', 'employe', 'comput', 'releas', 'big', 'integr', 'engin', 'algorithm']"
DE,"Secret Security Clearance Required - No Sponsorships
This is a full time position supporting a super cool project for the DoD.
QUALIFICATIONS
Senior skill level
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience
Proficient with data management tools, such as Python, SQL, Java, and use of Git
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data
Active SECRET security clearance is required.
Preferred:
Experience with graph databases and event sourcing models
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'optim', 'python', 'comput', 'set', 'sourc', 'engin']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'optim', 'python', 'comput', 'set', 'sourc', 'engin']"
DE,"JD: Python Lambda, building API integration on AWS platform (with Python as the language) and hands-on developer.
Disclaimer: The above statements are not intended to be a complete statement of job content, rather act as a guide to the essential functions performed by the employee assigned to this classification.
Management retains the discretion to add or change the duties of the position at any time
","['aw', 'lambda', 'python']",['classif'],999,"['aw', 'lambda', 'python', 'classif']","['essenti', 'aw', 'employe', 'python', 'integr']","['aw', 'lambda', 'python', 'classif', 'essenti', 'aw', 'employe', 'python', 'integr']"
DE,"Big Data Software Engineer
Proficiency in, at least, one modern programming language such as Java, C++, C, Python, or Scala
Strong problem solving skills; adaptability, proactivity and willingness to take ownership and deal with ambiguity
Strong verbal and written communication skills - with both technical and non-technical individuals
Knowledge of Software Design Patterns
Experience analyzing large data sets and developing data driven statistical model.
Experience with Apache Hadoop, Hadoop Distributed File System (HDFS) and Hadoop MapReduce.
Experience with tools from the Hadoop ecosystem such as Spark
Experience with Splunk or similar analytic framework, e.g.
ELK
Experience with MongoDB and GridFS..
Experience with RESTful Web Services (Jersey, RESTEasy, or similar).
Experience with natural language processing techniques and text analytics.
Ability to pick up new technologies, quickly review and integrate new technologies
They include:
*Identifying the~RIGHT PEOPLE~and developing them to their full capabilities**
","['mapreduc', 'mongodb', 'spark', 'scala', 'hadoop', 'c', 'python', 'java', 'splunk']","['natural language processing', 'text analytics', 'problem solving', 'statist', 'big data', 'commun']",999,"['mapreduc', 'mongodb', 'spark', 'scala', 'hadoop', 'c', 'python', 'java', 'splunk', 'nlp', 'text analytics', 'problem solving', 'statist', 'big data', 'commun']","['analyt', 'techniqu', 'spark', 'hadoop', 'python', 'statist', 'big', 'set', 'engin']","['mapreduc', 'mongodb', 'spark', 'scala', 'hadoop', 'c', 'python', 'java', 'splunk', 'nlp', 'text analytics', 'problem solving', 'statist', 'big data', 'commun', 'analyt', 'techniqu', 'spark', 'hadoop', 'python', 'statist', 'big', 'set', 'engin']"
DE,"The Applications Development Senior Programmer Analyst is an intermediate level position responsible for participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team.
The overall objective of this role is to contribute to applications systems analysis and programming activities.
Responsibilities: Conduct tasks related to feasibility studies, time and cost estimates, IT planning, risk technology, applications development, model development, and establish and implement new or revised applications systems and programs to meet specific business needs or user areas Monitor and control all phases of development process and analysis, design, construction, testing, and implementation as well as provide user and operational support on applications to business users Utilize in-depth specialty knowledge of applications development to analyze complex problems/issues, provide evaluation of business process, system process, and industry standards, and make evaluative judgement Recommend and develop security measures in post implementation analysis of business usage to ensure successful system design and functionality Consult with users/clients and other technology groups on issues, recommend advanced programming solutions, and install and assist customer exposure systems Ensure essential procedures are followed and help define operating standards and processes Serve as advisor or coach to new or lower level analysts Has the ability to operate with a limited level of direct supervision.
Can exercise independence of judgement and autonomy.
Acts as SME to senior stakeholders and /or other team members.
Other job-related duties may be assigned as required.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
To view the ""EEO is the Law"" poster CLICK HERE .
To view the EEO is the Law Supplement CLICK HERE .
To view the EEO Policy Statement CLICK HERE .
To view the Pay Transparency Posting CLICK HERE .
",[None],"['recommend', 'supervis', 'risk']",2,"['recommend', 'supervis', 'risk']","['essenti', 'program', 'provid', 'relat', 'particip', 'evalu']","['recommend', 'supervis', 'risk', 'essenti', 'program', 'provid', 'relat', 'particip', 'evalu']"
DE,"Job Title: Big Data Engineer
Duration: Full Time
Technical & Functional Skills :
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
Proven Experience in handling variety of data formats
Experience in building large scale Data Lake Environment
Troubleshooting Hive Performance issues and developing HQL queries
Experience with Spark and PySpark
Experience in implementing CI/CD Process and Job Automation through Autosys
Experience in Hadoop Cluster Administration is a big plus
Experience with integration of data from multiple data sources
Assist Analytics and Data Scientist team and Business Users
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster', 'big data']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data']","['analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'big data', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'engin']"
DE,"bull Analyze and understand data sources APIs bull Design and Develop methods to connect collect data from different data sources bull Design and Develop methods to filtercleanse the data bull Design and Develop SQL , Hive queries, APIs to extract data from the store bull Work closely with data Scientists to ensure the source data is aggregated and cleansed bull Work with product managers to understand the business objectives bull Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows bull Work with DevOps to build automated data pipelines Total Experience Required bull 4 years 10 of relevant experience bull The candidate should have performed client facing roles and possess excellent communication skills Business Domain knowledge Finance banking systems, Fraud, Payments Required Technical Skills bull Big Data-Hadoop, NoSQL, Hive, Apache Spark bull Python bull Java REST bull GIT and Version Control Desirable Technical Skills bull Familiarity with HTTP and invoking web-APIs bull Exposure to machine learning engineering bull Exposure to NLP and text processing bull Experience with pipelines, job scheduling and workflow management
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun']","['machin', 'engin', 'spark', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'machin', 'engin', 'spark', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Hi,
Â
Please find the below details
Â
Azure Data Engineer
Â
Irving,TX
Â
Need 10+ years of experience
Â
Need experience with ADF V2,Azure Devops and CI/CD
Â
Need Azure Data Catalog,Azure event hub
Â
Need Azure Synapse & Snowflake
Â
Experience with Scrum/Safe and Azure Storage
Â
","['snowflak', 'azur']",[None],999,"['snowflak', 'azur']","['engin', 'â', 'azur']","['snowflak', 'azur', 'engin', 'â', 'azur']"
DE,"Sr. Data Engineer Irving, TX Ideal candidates should have experience with Data Ingestion and Consumptions.
That is transforming from source raw data, cleansing missing data, and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of the Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST.
Note that this requirement is 4 technologies stack in ONE PROJECT, not in multiple projects In-depth knowledge of preparing large scale data analytics for consumptions.
Candidate MUST HAVE EXPERIENCE with Large Fortune 500 Data Analytics Key knowledge on HIVE and query optimization in HIVE The candidate must have done data transformation, cleansing, matching and standardization as part of the ingestion Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']"
DE,"Sr. Data Engineer
FT. Worth TX
MUST
Active Secret or TS required
Experienced Sr. Data Engineer
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience required
Proficient with data management tools, such as Python, SQL, Java, and use of Git required
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets required
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data required
Experience with graph databases and event sourcing models required
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis required
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations required
DUTIES:
Will be performing as part of, or in support of a SCRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.
Has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data.
Responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives.
Will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.
","['sql', 'java', 'python', 'git']","['graph', 'data modeling', 'machine learning', 'optim', 'geospati']",999,"['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati']","['machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']","['sql', 'java', 'python', 'git', 'graph', 'data modeling', 'machine learning', 'optim', 'geospati', 'machin', 'techniqu', 'optim', 'python', 'comput', 'set', 'engin', 'evalu']"
DE,"Data Engineer/Analyst
Coppell, TX
3 Months (Potential to extend)
The ideal candidate will have experience connecting Excel to Oracle Database, and performing data analysis within Oracle and Excel, and development with PL/SQL
Top Skills
Oracle Development
PL/SQL
Excel
Develop data analysis solutions within Oracle, using SQL development skills (hands-on developer)
Deep expertise in Oracle pl/SQL, Excel, and Data Analysis
This person will join an on-going project, and Finance/Banking client experience is a big plus
Equal Employment Opportunity Commission:
Hiring decisions are solely based upon job related criteria regardless of race, ethnicity, color, religion, sex (including pregnancy and gender identity), marital status, disability, age, national origin, political affiliation, retaliation, parental status, military service, other non-merit factor or any other status protected by U.S. law.
With Regards,
Maninder Kaur
Global HQ Address 7250 Dallas Pkwy, Suite 825 Plano, Texas 75024
Office: (201) 340-8700 x 408 | Cell: 201-479-1137| Fax: (201) 221-8131| Email: mani@net2source.com
Inception in 2007, privately held, Debt free
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
Awards and Accolades:
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 - Spirit of Alliance Award by Agile1
2018 - 40 Under 40 Leadership Awards by Triangle Business Journal
2018 Fastest 50 by NJBIZ
2018 TechServe Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1
","['sql', 'excel', 'oracl']","['account', 'financ']",999,"['sql', 'excel', 'oracl', 'account', 'financ']","['big', 'relat', 'engin', 'texa']","['sql', 'excel', 'oracl', 'account', 'financ', 'big', 'relat', 'engin', 'texa']"
DE,"Client- City Bank
Location- Irving, TX
FullTime Role
• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data Scientist team and Business Users
• Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
All qualified applicants will receive due consideration for employment without any discrimination.
All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role.
","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka']","['commun', 'visual', 'cluster']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster']","['basi', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'evalu']","['pyspark', 'spark', 'scala', 'hadoop', 'python', 'hive', 'kafka', 'commun', 'visual', 'cluster', 'basi', 'analyt', 'spark', 'visual', 'hadoop', 'python', 'scientist', 'appli', 'big', 'integr', 'sourc', 'evalu']"
DE,"Position:Sr. Data Engineer/ Big Data Engineer
Duration: Long term
Big Data, Cloud, Azure, AWS .
Thanks & Regards
Sai Krishna | Sr.
Address: 4701 Patrick Henry Dr Ste 8, Santa Clara, CA 95054
Work : 805-500-4366 x 104
","['aw', 'cloud', 'azur']",['big data'],999,"['aw', 'cloud', 'azur', 'big data']","['big', 'aw', 'engin', 'azur']","['aw', 'cloud', 'azur', 'big data', 'big', 'aw', 'engin', 'azur']"
DE,"This customer is the world's largest manufacturer of wood and laminate lockers.
Examples of successful clients have been the Dallas Cowboys, Texas Rangers, Dallas Mavericks, New Orleans Saints, TCU, UNT and more!
The IT Data Engineer will work in tandem with the current App Integration Specialist to create analytics and automate reports for the 3 main business units: Sales, Operations and Manufacturing.
Top 3 skills:
) Power BI
) SQL DB (Optimization and performance tuning)
) Scripting using stored procedures
This customer offers a variety of perks including:
Benefits
Medical, Dental
401K
FREE COLLEGE tuition and books at North Lake College
Company-sponsored getaways (after three years of employment)
Monthly food trucks provide free lunch
Refreshments every day, including birthday cakes (everyone celebrates together!)
Company-sponsored activities: fantasy football league, K1 Racing Day, and more
Holiday celebrations and year-end Christmas party
This person will need to be a self-starter with a creative mindset.
This person will need to be able to take a few directions and work independently without hesitation.
#ZR
Your Name:
Email Address:
Phone Number:
Upload Resume File:
Upload Resume File: …
Attach a resume file.
Accepted file types are DOC, DOCX, PDF, HTML, and TXT.
information, and use it in the consideration of your fitness for the position,
People looking for jobs should not put anything here.
It may take few moments to read your resume.
Please wait!
","['sql', 'bi', 'db', 'power bi']",['optim'],999,"['sql', 'powerbi', 'db', 'optim']","['day', 'analyt', 'power', 'optim', 'bi', 'parti', 'texa', 'integr', 'engin']","['sql', 'powerbi', 'db', 'optim', 'day', 'analyt', 'power', 'optim', 'bi', 'parti', 'texa', 'integr', 'engin']"
DE,"Analyze and understand data sources & APIs• Design and Develop methods to connect & collect data from different data sources
• Design and Develop methods to filter/cleanse the data
• Design and Develop SQL , Hive queries, APIs to extract data from the store
• Work closely with data Scientists to ensure the source data is aggregated and cleansed
• Work with product managers to understand the business objectives
• Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
• Work with DevOps to build automated data pipelines
Total Experience Required
• 4
• The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
• Big Data-Hadoop, NoSQL, Hive, Apache Spark
• Python
• Java & REST
• GIT and Version Control
Desirable Technical Skills
• Familiarity with HTTP and invoking web-APIs
• Exposure to machine learning engineering
• Exposure to NLP and text processing
• Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
• Experience working in SCRUM methodology
• Proven sense of high accountability and self-drive to take on and see through big challenges
• Confident, takes ownership, willingness to get the job done
• Excellent verbal communications and cross group collaboration skills
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"Analyze and understand data sources & APIs
• Design and Develop methods to connect & collect data from different data sources
• Design and Develop methods to filter/cleanse the data
• Design and Develop SQL , Hive queries, APIs to extract data from the store
• Work closely with data Scientists to ensure the source data is aggregated and cleansed
• Work with product managers to understand the business objectives
• Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows
• Work with DevOps to build automated data pipelines
Total Experience Required
• 4
• The candidate should have performed client facing roles and possess excellent communication skills
Business Domain knowledge: Finance & banking systems, Fraud, Payments
Required Technical Skills
• Big Data-Hadoop, NoSQL, Hive, Apache Spark
• Python
• Java & REST
• GIT and Version Control
Desirable Technical Skills
• Familiarity with HTTP and invoking web-APIs
• Exposure to machine learning engineering
• Exposure to NLP and text processing
• Experience with pipelines, job scheduling and workflow management
Personal Skills
Experienced in managing work with distributed teams
• Experience working in SCRUM methodology
• Proven sense of high accountability and self-drive to take on and see through big challenges
• Confident, takes ownership, willingness to get the job done
• Excellent verbal communications and cross group collaboration skills
","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel']","['big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']",999,"['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account']","['machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']","['sql', 'spark', 'git', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'excel', 'big data', 'pipelin', 'cleans', 'analyz', 'machine learning', 'nlp', 'financ', 'commun', 'account', 'machin', 'engin', 'spark', 'challeng', 'pipelin', 'hadoop', 'python', 'scientist', 'big', 'sourc', 'collect']"
DE,"""Candidates should possess strong knowledge and interest across big data technologies and have a background in data engineering.
• Transform complex analytical models in scalable, production-ready solutions
• Provide support and enhancements for an advanced anomaly detection machine learning platform
• Develop cloud based applications from the ground up using a modern technology stack
• Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment
Responsibilities:
• Building data APIs and data delivery services to support critical operational and analytical applications
• Contributing to the design of robust systems with an eye on the long-term maintenance and support of the application
• Leveraging reusable code modules to solve problems across the team and organization
• Handling multiple functions and roles for the projects and Agile teams
• Being a technology thought leader and strategist
Required:
At least 4 years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least4 years of experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS
At least 4 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 2 years of experience with SQL and Shell Scripting experience
Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Apache Beam, Apache Airflow etc).
Experience with Spark programming (pyspark or scala or java).
Experience troubleshooting JVM-related issues.
Experience and strategies to deal with mutable data in Hadoop.
Familiarity with Spark Structure Streaming and/or Kafka Streams.
Familiarity with machine learning implementation using PySpark.
Experience in data visualization tools like Cognos, Arcadia, Tableau.
Experience in Ab Initio technologies including, but not limited to Ab Initio graph development, EME, Co-Op, BRE, Continuous flow)
Preferred:
• Angular.JS 4 Development and React.JS Development expertise in a up to date Java Development Environment with Cloud Technologies
• 1 years’ experience with Amazon Web Services (AWS), Google Compute or another public cloud service
• 2 years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
• 2 years of experience working with Dimensional Data Model and pipelines in relation with the same
• Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
• Hands on design experience with data pipelines, joining data between structured and unstructured data
• Familiarity of SAS programming will be a plus
• Experience implementing open source frameworks & exposure to various open source & package software architectures (AngularJS, ReactJS, Node, Elastic Search, Spark, Scala, Splunk, Apigee, and Jenkins etc.).
• Experience with various noSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j) will be a plus
Other:
• Successfully complete assessment tests offered in Pluralsight, Udemy, etc.
or complete certifications to demonstrate technical expertise on more than one development platform.
""
","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'amazon web services', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk']","['graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']",999,"['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']","['program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data', 'program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Title: Test Data Engineer position
Duration: 6 Months Contract
The Customer Experience Testing and Certification (CETC) team is looking for a highly energetic, motivated and talented individuals for a testing end to end stack of applications as a part of One Fiber Initiatives for Verizon Business Markets.
This role requires a thorough understanding of sales and service offerings to business customers of One Fiber on web & mobile platforms.
-Understanding sales & service requirements on One Fiber Initiatives.
-Participate in design, sprint planning sessions for scope & impact of new development.
-Understand Systems Data, XML and data exchange across systems architecture
-Create test data points/profiles satisfying 100% test coverage on initiatives.
-Develop Testdata scripts to input orders from salesforce front-end applications
-Proficient in SQL/TSQL scripts excel macros and reporting tools
-Enhance JIRA to host test accounts to users and maintain inventory.
-Position available in either Basking Ridge, New Jersey; Irving, Texas; or Cary, North Carolina.
You'll need to have:
-Bachelor's degree or four or more years of work experience.
-Six or more years of relevant work experience.
-Experience in testing mobile, web/e-commerce applications using Salesforce application.
-Experience And/Or Understanding in a lab setup, telecom products installation
-Excellent communication skills for a good teamwork and partnership.
Thank You,
Rohan Ghag
Recruitment Lead
3155 North Point Pkwy Bldg.
G
Suite 130, Alpharetta GA 30005
Phone: 678-935-7308
Rohan.g@avacend.com
Required Skills
-Position available in either Basking Ridge, New Jersey; Irving, Texas; or Cary, North Carolina.
Required Experience
Irving, US-TX
","['sql', 'salesforc', 'excel', 'jira']","['commun', 'account']",1,"['sql', 'salesforc', 'excel', 'jira', 'commun', 'account']","['avail', 'engin', 'particip', 'texa']","['sql', 'salesforc', 'excel', 'jira', 'commun', 'account', 'avail', 'engin', 'particip', 'texa']"
DE,"Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services) cloud environment.
Essential Responsibilities:
Design, implement, and test major subsystems of AWS cloud platform and core service offerings using the Scrum agile framework
Develop and follow best practices relative to design, implementation, and testing
Prototype new ideas or technologies to prove efficacy and usefulness in production
Develop SOAP (Simple Object Access Protocol) and REST (Representational State Transfer) APIs (Application Programming Interfaces) to integrate with partners and customers in real-time
Build a service structure on AWS capable of being deployed and scaled to run a variety of platform components dynamically
Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS cloud
Construct a state-of-the-art data lake on AWS using Amazon EMR, and Apache Spark, NiFi, Kafka, and Cassandra
Design & develop data pipelines for batch & streaming data sets using open source/ AWS tech stack
Mentor junior team members as a senior member of the Engineering team
Non-Essential Responsibilities:
Other duties as assigned
Knowledge, Skills and Abilities:
Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Basic knowledge for distributed computing
Knowledge of the challenges associated with “big data” and how to build systems that scale seamlessly
Ability to communicate well
Work Conditions and Physical Demands:
Primarily sedentary work in a general office environment
Ability to communicate and exchange information
Ability to comprehend and interpret documents and data
Requires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)
Requires manual dexterity to use computer, telephone and peripherals
May be required to work extended hours for special business needs
May be required to travel at least 10"" of time based on business needs
Minimum Education:
The knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required.
A combination of education and experience may be used in lieu of a diploma.
Minimum Related Work Experience:
6 years’ experience designing and delivering production software
5 years’ experience designing and implementing big data high performance operational systems
Proven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc
EOE including disability/veteran.
","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'amazon web services']","['pipelin', 'big data']",1,"['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'pipelin', 'big data']","['essenti', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'pipelin', 'big data', 'essenti', 'program', 'spark', 'challeng', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']"
DE,"The Principal Software Engineer's responsibilities include modernization, improving the functionality of existing software, and ensuring that the design, application, and maintenance of software meets the quality standards.
You should also be able to mentor, guide and train other engineers.
To be successful as a Principal Software Engineer, you should be able to evaluate the user's needs, time limitations and system limitations when developing software.
A stand-out Principal Software Engineer is one who is up to date on new technologies and software development practices.
Principal Software Engineer Responsibilities:
Designing, coding, and debugging software.
Improving the performance of existing software.
Creating new design patterns and modernizing existing software applications
Providing training to other engineers.
Maintaining and upgrading existing software.
Recommending new technologies that can help increase productivity.
Supervising and overseeing the technical aspects of projects.
Investigating software-related complaints and making necessary adjustments to ensure optimal software performance.
Regularly attending team meetings to discuss projects, brainstorm ideas, and put forward solutions to any issues.
Design and build the infrastructure for data extraction, preparation, and loading of data from a variety of sources using technology
Principal Software Engineer Requirements:
Bachelor's degree in Computer Engineering/Computer Science or related field.
Strong analytical skills.
Good communication skills.
Excellent organizational and leadership skills
Proven experience in software development methodologies.
Proven experience building complex systems.
Experience working with and extracting value from large, disconnected and/or unstructured datasets.
Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management
Strong interpersonal skills and ability to project manage and work with cross-functional teams
Advanced working SQL knowledge and experience working with NOSQL and relational
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with the following tools and technologies:
Hadoop, Spark batch, Hive, Presto, Kafka, Pivotal Cloud Foundry
Relational SQL and NoSQL databases
GCP cloud services such as DataProc, GCE, GKEetc.
Stream-processing systems such as Storm and Spark-Streaming
Object-oriented/object function scripting languages such as Python, Java, Scala.
","['sql', 'gcp', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka', 'excel']","['pipelin', 'optim', 'big data', 'supervis', 'commun']",1,"['sql', 'gcp', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka', 'excel', 'pipelin', 'optim', 'big data', 'supervis', 'commun']","['analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'python', 'comput', 'set', 'sourc', 'engin']","['sql', 'gcp', 'spark', 'scala', 'hadoop', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka', 'excel', 'pipelin', 'optim', 'big data', 'supervis', 'commun', 'analyt', 'stream', 'spark', 'pipelin', 'relat', 'hadoop', 'infrastructur', 'optim', 'python', 'comput', 'set', 'sourc', 'engin']"
DE,"Must have skills*
Minimum 8+ years of experience with 4+ years of experience with building Big Data Solutions and Machine Learning Models.
Key responsibilities*
Understand Machine Learning Models in SAS and R and Convert them to Spark Jobs
Enhance Machine Learning Models using Pyspark or Scala
Build ML Models based on Business Requirements and Follow ML Cycle to Deploy them all the way to Production Environment
Participate Feature Engineering, Training Models, Scoring and retraining
Architect Data Pipeline and Automate Data Ingestion and Model Jobs
Experience in Cloudera Hadoop Eco System.
Expertise in Spark Framework using Python/Scala
Experience with SAS Modeling and converting them to Pyspark models is big Plus
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities
","['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r']","['pipelin', 'visual', 'machine learning', 'big data', 'commun']",999,"['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r', 'pipelin', 'visual', 'machine learning', 'big data', 'commun']","['machin', 'learn', 'spark', 'pipelin', 'ml', 'sa', 'visual', 'hadoop', 'python', 'appli', 'big', 'engin', 'particip']","['pyspark', 'spark', 'scala', 'sa', 'hadoop', 'python', 'r', 'pipelin', 'visual', 'machine learning', 'big data', 'commun', 'machin', 'learn', 'spark', 'pipelin', 'ml', 'sa', 'visual', 'hadoop', 'python', 'appli', 'big', 'engin', 'particip']"
DE,"The ideal candidate must have deep experience delivering Enterprise Data Warehouse solutions, designing, coding and leading end to end solutions on Teradata, Big Data and cloud technologies.
Leading teams in design, development and delivery of data solutions on Teradata, Big Data and cloud data platforms for the Global Data Technology team.
Managing day-to-day development activities for new data solutions and troubleshooting existing implementations.
Working with product owners and technical directors to lead technical discussions and resolve technical issues
Applying best practices of data integration for data quality and automation
Reviewing data models and data architecture for Hadoop and HBase environments
Working with product vendors to identify and manage open product issues
Solving complex data integration problems
Working with project team and technology partners to develop high-level designs and cost and effort estimates for new work efforts.
Documenting design solutions and supporting documentation.
Developing and maintaining code for data ingestion and curation using Informatica, Talend, Scoop, Hive etc.
Working with business analysts to understand business requirements and use cases
Minimum of 7 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.
Experience as an ETL lead with a track record of delivering design solutions (> 5 years) and leading a small team of developers
Substantial experience in Data Warehousing / Information Management (> 5 years)
Hands-on experience with ETL tools and deep experience in any integration tool (e.g.
Informatica, Talend) (At least 5 years)
Understanding of data modeling (logical and/or physical) with familiarity of Teradata platform preferred (> 5 years)
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Hadoop experience – Understanding of Hadoop file format and programming (egg.
Spark, Hive, etc.)
Knowledge of Scripting (UNIX) and scheduling tools (Control-M, ESP)
Experience with change data capture tools (CDC) preferred such as Attunity
Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills
Excellent written/verbal communication skills
","['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel']","['tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel', 'tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']","['day', 'analyt', 'spark', 'hadoop', 'warehous', 'big', 'etl', 'integr']","['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel', 'tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'day', 'analyt', 'spark', 'hadoop', 'warehous', 'big', 'etl', 'integr']"
DE,"Big Data Engineer (Kafka)
Share
Job ID: FA-0100-474
Open Since: 2019-10-04
City: Irving
State: Texas
Country: United States of America
Experience as a developer who has used the Kafka API to build producer and consumer applications, along with expertise in implementing KStreams components.
Have developed KStreams pipelines, as well as deployed KStreams clusters.
Experience with developing KSQL queries and best practices of using KSQL vs KStreams
Strong knowledge of the Kafka Connect framework, with experience using several connector types: HTTP REST proxy, JMS, File, SFTP, JDBC, Splunk, Salesforce, and how to support wire-format translations.
Knowledge of connectors available from Confluent and the community
Implementing stream processing using Kafka Streams / KSQL / Spark Jobs along with Kafka and Databricks integration.
Hands-on experience in designing, writing, and operationalizing new Kafka Connectors using the framework
Define strategy and roadmap of the NextGen Stream Data Platform based on Apache Kafka
Accelerate adoption of the Kafka ecosystem by creating a framework for leveraging technologies such as Kafka Connect, KStreams/KSQL, Schema Registry, and other streaming-oriented technology
Seasoned messaging expert with extensive, well-rounded background in a diverse set of messaging middleware solutions (commercial, open source, in-house) with in-depth understanding of architectures of such solutions.
Examples: Kafka, RabbitMQ
Deep understanding of different messaging paradigms (pub/sub, queuing), as well as delivery models, quality-of-service, and fault-tolerance architectures
Knowledge of messaging protocols and associated APIs
Working knowledge of Splunk, how it integrates with Kafka, and using it effectively as a Kafka operational tool
Strong background in integration patterns
Design, develop, support, maintenance and implementation of a complex project module.
Utilize Enterprise Integration Patterns to develop data pipelines and the necessary datastructures
Good experience in application of standard software development principles.
Implementing best practices and making sure the coding standards
Write documentation for the code to be written
Debug production issues and create subsequent mitigation plans
Optimize the performance of existing implementations
Bring forward ideas to experiment and work in teams to transform ideas to reality
Prioritize tasks with the scrum master that leads the team to be successful
Architect data structures that meet the reporting timelines.
Work directly with engineering teams for design and build their development requirements
Maintain high standards of software quality by establishing good practices and habits within the development team while delivering solutions on time and on budget.
Facilitate the agile development process through daily scrum, sprint planning, sprint demo, and retrospective meetings.
Participate in peer-reviews of solution designs and related code.
Analyze and resolve technical and application problems.
Research and evaluate a variety of software products and development tools.
Proven communication skills, both written and oral
Job Skills:
Engineering/development resources who have implemented stream processing using Kafka Streams / KSQL / Spark Jobs along with having Kafka and Databricks integration experiences.
Should have experiences, best practices and knowledge in Fast-Data Pipeline with Kafka/Databricks architecture, compute and scalability requirements to implement streaming jobs which handle a million messages per second/hour.
Minimum Experience: 10 Yrs
Education:
Bachelor's Degree in Engineering
","['splunk', 'kafka', 'spark', 'salesforc']","['research', 'big data', 'pipelin', 'analyz', 'commun', 'cluster']",1,"['splunk', 'kafka', 'spark', 'salesforc', 'research', 'big data', 'pipelin', 'analyz', 'commun', 'cluster']","['stream', 'spark', 'pipelin', 'set', 'relat', 'divers', 'avail', 'comput', 'texa', 'big', 'integr', 'sourc', 'engin', 'particip']","['splunk', 'kafka', 'spark', 'salesforc', 'research', 'big data', 'pipelin', 'analyz', 'commun', 'cluster', 'stream', 'spark', 'pipelin', 'set', 'relat', 'divers', 'avail', 'comput', 'texa', 'big', 'integr', 'sourc', 'engin', 'particip']"
DE,"This role needs to be well versed with innovation and trends in the world of data processing which includes data management, data quality and data curation.
The Senior Pro Data Engineering is passionate about all things data, data processing as well as the desire to turn data processing into a competitive advantage.
The work activity includes processing complex data sets, leveraging technologies used to process these disparate data sets and understanding the correlations as well as patterns that exist between these different complex data sets.
Works closely with other teams/ members of the team to develop solutions to meet business requirements, perform assessments, POC's and establish an execution/ development plan.
Forecasts technical challenges and develop implementation strategies.
Identifies enabling technologies and has the skills to develop solutions based on identified technologies as required
Prepares and presents white papers and proposals.
Employs lateral thinking to develop innovative solutions to meet business needs
Ability to communicate complex technical ideas in a straightforward and compelling way
8+ years of experience in area of data management and/or data curation
Experienced with data normalization and denormalization techniques
Experienced in implementing largescale event based streaming architectures
Experienced in data encryption, data security, data transformation and data archiving techniques
Experience with Hadoop, Spark batch, Hive, Presto, Kafka, Pivotal Cloud Foundry
Experience with Relational SQL and NoSQL databases
Experience with GCP cloud services such as DataProc, GCE, GKEetc.
Experience with Stream-processing systems such as Storm and Spark-Streaming
Experience with Object-oriented/object function scripting languages such as Python, Java, Scala.
Experienced with Java, Angular JS, Spring Cloud/Spring Boot, Kafka, Spark, Hive
Knowledge of API and Microservice development
Working knowledge of cloud architectures on AWS or GCP
Experienced in Agile methodology and/or pair programming
Knowledge of AI/ML concepts and technologies
Strong communication skills
Strong writing and documentation skills
Experienced in working with cross functional teams, building alignment and collaboration
BA/BS in Computer Science or related field, or equivalent experience
","['sql', 'gcp', 'spark', 'scala', 'angular', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka']","['forecast', 'correl', 'commun', 'normal']",1,"['sql', 'gcp', 'spark', 'scala', 'angular', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka', 'forecast', 'correl', 'commun', 'normal']","['techniqu', 'stream', 'spark', 'challeng', 'ml', 'relat', 'hadoop', 'aw', 'python', 'comput', 'set', 'engin']","['sql', 'gcp', 'spark', 'scala', 'angular', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'kafka', 'forecast', 'correl', 'commun', 'normal', 'techniqu', 'stream', 'spark', 'challeng', 'ml', 'relat', 'hadoop', 'aw', 'python', 'comput', 'set', 'engin']"
DE,"Data Platform Engineer
o Apache Pinot and Apache Spark or Azure Databricks a plus
o Data modeling, rdbms/no-sql, postgres
o Being able to program and/or go through Java code is a good skill to have.
o Nice to Have - Cassandra, MongoDB
o Retail or Fuels experience plus
","['sql', 'mongodb', 'spark', 'cassandra', 'azur', 'java']",['data modeling'],2,"['sql', 'mongodb', 'spark', 'cassandra', 'azur', 'java', 'data modeling']","['program', 'spark', 'engin', 'azur']","['sql', 'mongodb', 'spark', 'cassandra', 'azur', 'java', 'data modeling', 'program', 'spark', 'engin', 'azur']"
DE,"The Applications Development Senior Programmer Analyst is an intermediate level position responsible for participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team.
The overall objective of this role is to contribute to applications systems analysis and programming activities.
Responsibilities: Conduct tasks related to feasibility studies, time and cost estimates, IT planning, risk technology, applications development, model development, and establish and implement new or revised applications systems and programs to meet specific business needs or user areas Monitor and control all phases of development process and analysis, design, construction, testing, and implementation as well as provide user and operational support on applications to business users Utilize in-depth specialty knowledge of applications development to analyze complex problems/issues, provide evaluation of business process, system process, and industry standards, and make evaluative judgement Recommend and develop security measures in post implementation analysis of business usage to ensure successful system design and functionality Consult with users/clients and other technology groups on issues, recommend advanced programming solutions, and install and assist customer exposure systems Ensure essential procedures are followed and help define operating standards and processes Serve as advisor or coach to new or lower level analysts Has the ability to operate with a limited level of direct supervision.
Can exercise independence of judgement and autonomy.
Acts as SME to senior stakeholders and /or other team members.
Other job-related duties may be assigned as required.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
To view the ""EEO is the Law"" poster CLICK HERE .
To view the EEO is the Law Supplement CLICK HERE .
To view the EEO Policy Statement CLICK HERE .
To view the Pay Transparency Posting CLICK HERE .
",[None],"['recommend', 'supervis', 'risk']",2,"['recommend', 'supervis', 'risk']","['essenti', 'program', 'provid', 'relat', 'particip', 'evalu']","['recommend', 'supervis', 'risk', 'essenti', 'program', 'provid', 'relat', 'particip', 'evalu']"
DE,"Your Opportunity
The ideal candidate must have deep experience delivering Enterprise Data Warehouse solutions, designing, coding and leading end to end solutions on Teradata, Big Data and cloud technologies.
What you are good at
Leading teams in design, development and delivery of data solutions on Teradata, Big Data and cloud data platforms for the Global Data Technology team.
Managing day-to-day development activities for new data solutions and troubleshooting existing implementations.
Working with product owners and technical directors to lead technical discussions and resolve technical issues
Applying best practices of data integration for data quality and automation
Reviewing data models and data architecture for Hadoop and HBase environments
Working with product vendors to identify and manage open product issues
Solving complex data integration problems
Working with project team and technology partners to develop high-level designs and cost and effort estimates for new work efforts.
Documenting design solutions and supporting documentation.
Developing and maintaining code for data ingestion and curation using Informatica, Talend, Scoop, Hive etc.
Working with business analysts to understand business requirements and use cases
What you have
Minimum of 7 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms.
Experience as an ETL lead with a track record of delivering design solutions (> 5 years) and leading a small team of developers
Substantial experience in Data Warehousing / Information Management (> 5 years)
Hands-on experience with ETL tools and deep experience in any integration tool (e.g.
Informatica, Talend) (At least 5 years)
Understanding of data modeling (logical and/or physical) with familiarity of Teradata platform preferred (> 5 years)
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Hadoop experience - Understanding of Hadoop file format and programming (egg.
Spark, Hive, etc.)
Knowledge of Scripting (UNIX) and scheduling tools (Control-M, ESP)
Experience with change data capture tools (CDC) preferred such as Attunity
Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills
Excellent written/verbal communication skills
Requisition #: 2020-63618
r1d4rh5eu
","['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel']","['tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']",999,"['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel', 'tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun']","['day', 'analyt', 'spark', 'hadoop', 'warehous', 'big', 'etl', 'integr']","['sql', 'spark', 'unix', 'hadoop', 'cloud', 'hive', 'hbase', 'excel', 'tune', 'data modeling', 'data warehousing', 'big data', 'etl', 'commun', 'day', 'analyt', 'spark', 'hadoop', 'warehous', 'big', 'etl', 'integr']"
DE,"KEY RESPONSIBILITIES
Support the COO, CFO, and Directors of Decision Science, Marketing, Marketing Analytics, and Credit Risk Management by enabling their departments’ analysts with trustworthy data and act as their liaison to the BI and AppDev teams for their data needs
Act as an evangelist and internal resource to all business analysts on data assets
Facilitate and accelerate iterative learning and innovation using data assets
Assist business teams in the finalization of business requirements for the purpose of providing to the BI team for productionalizing dashboards and reports
Translate business requirements into data harvesting prototypes (e.g.
develop code to connect to APIs of new data sources for preliminary analysis as requested by business users, explore innovative approaches to harvest data, harvest data into data lake, and establish processes to allow future requests to become more efficient
Create and maintain data catalog and dictionaries
Add new data sources to data lake
Design and develop prototype data structures that refine raw data for use by business analysts, e.g.
parse new data sources for preliminary analysis into a consumable format when requested by business analysts
Provide data management support to business teams, including simple ad hoc data queries structured for use by less-technical business leaders and analysts
REQUIREMENTS
BS in Computer Science or related field, an analytical field (e.g.
Statistics, Econometrics, Applied Mathematics), or a highly quantitative engineering (e.g.
Chemical Engineering) field
2 years of experience as a data analyst, report developer, or business analyst
Advanced user of SQL
Experience with ETL tools (SSIS / Talend / Informatica / Data Services)
Results-oriented self-starter who is confident in defending his/her critical thinking abilities
Proven track record of delivering successful projects on-time and with high-quality
Customer service and consultative mindset
Capitalistic mindset supported by a strong business acumen and work ethic
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to a non-technical audience
Must be currently authorized to work in the United States without sponsorship and not require sponsorship in the future
PREFERRED QUALIFICATIONS
Master’s degree
Microsoft Azure certifications in related areas, such as Azure Solutions Architect Expert, or Azure Data Engineer Associate
PowerBI developer-level experience
COMPENSATION
Annual salary of $72,200
BENEFITS
Medical, dental, and vision
Voluntary life/ AD&D
Short-term & long-term disability
Paid vacation, holidays, and sick time
Paid maternity, paternity, extended medical leave, and jury duty
Corporate discount program on personal cell phone accounts with select providers
Business casual work environment
ABOUT COTTONWOOD
Founded in 1996, Cottonwood Financial is one of the largest privately held retail consumer finance companies in the United States.
","['sql', 'azur', 'bi', 'microsoft', 'powerbi']","['dashboard', 'risk', 'econometr', 'statist', 'financ', 'etl', 'account']",1,"['sql', 'azur', 'powerbi', 'microsoft', 'powerbi', 'dashboard', 'risk', 'econometr', 'statist', 'financ', 'etl', 'account']","['analyt', 'asset', 'program', 'learn', 'azur', 'corpor', 'relat', 'credit', 'etl', 'provid', 'bi', 'comput', 'appli', 'statist', 'quantit', 'sourc', 'engin']","['sql', 'azur', 'powerbi', 'microsoft', 'powerbi', 'dashboard', 'risk', 'econometr', 'statist', 'financ', 'etl', 'account', 'analyt', 'asset', 'program', 'learn', 'azur', 'corpor', 'relat', 'credit', 'etl', 'provid', 'bi', 'comput', 'appli', 'statist', 'quantit', 'sourc', 'engin']"
DE,"This is a great opportunity for a talented data analyst to take his/her next career step by playing a leading role in the transformation to an Azure analytics environment and by deeply learning the companys data and processes to gain the essential knowledge needed to support a wide variety of high-visibility projects.
• Support the COO, CFO, and Directors of Decision Science, Marketing, Marketing Analytics, and Credit Risk Management by enabling their departments analysts with trustworthy data and act as their liaison to the BI and AppDev teams for their data needs
• Play a key role in the companys transformation to a new Cloud paradigm, including the implementation of Azure Data Lake Analytics, SQL Database, and Data Factory
• Assist in the development of standards for the companys analytics platform, data quality, naming conventions, and data formats, as well as coordinate across the business an agreement of definitions consistent with the consumer credit industry
• Act as an evangelist and internal resource to all business analysts on data assets
• Facilitate and accelerate iterative learning and innovation using data assets
• Assist business teams in the finalization of business requirements for the purpose of providing to the BI team for productionalizing dashboards and reports
• Translate business requirements into data harvesting prototypes (e.g.
develop code to connect to APIs of new data sources for preliminary analysis as requested by business users, explore innovative approaches to harvest data, harvest data into data lake, and establish processes to allow future requests to become more efficient
• Create and maintain data catalog and dictionaries
• Add new data sources to data lake
• Design and develop prototype data structures that refine raw data for use by business analysts, e.g.
parse new data sources for preliminary analysis into a consumable format when requested by business analysts
• Understand the processes of QA, design, and deployment used by BI and AppDev teams to ensure the resulting data meets the companys quality standards
• Provide data management support to business teams, including simple ad hoc data queries structured for use by less-technical business leaders and analysts
REQUIREMENTS
• BS in Computer Science or related field, an analytical field (e.g.
Statistics, Econometrics, Applied Mathematics), or a highly quantitative engineering (e.g.
Chemical Engineering) field
• 2 years of experience as a data analyst, report developer, or business analystAdvanced user of SQL
• Experience with ETL tools (SSIS / Talend / Informatica / Data Services)
• Results-oriented self-starter who is confident in defending his/her critical thinking abilities
• Proven track record of delivering successful projects on-time and with high-quality
• Customer service and consultative mindset
• Capitalistic mindset supported by a strong business acumen and work ethic
• Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to a non-technical audience
• Must be currently authorized to work in the United States without sponsorship and not require sponsorship in the future
PREFERRED QUALIFICATIONS
• Masters degree
• Microsoft Azure certifications in related areas, such as Azure Solutions Architect Expert, or Azure Data Engineer Associate
• PowerBI developer-level experience
COMPENSATION
• Annual salary of $72,200
BENEFITS
• Medical, dental, and vision
• Voluntary life/ AD&D
• Short-term & long-term disability
• Paid vacation, holidays, and sick time
• Paid maternity, paternity, extended medical leave, and jury duty
• Corporate discount program on personal cell phone accounts with select providers
• Business casual work environment
Job Requirements:
","['sql', 'azur', 'bi', 'cloud', 'microsoft', 'powerbi']","['dashboard', 'risk', 'econometr', 'statist', 'etl', 'account']",1,"['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'powerbi', 'dashboard', 'risk', 'econometr', 'statist', 'etl', 'account']","['analyt', 'asset', 'essenti', 'program', 'learn', 'azur', 'corpor', 'relat', 'credit', 'etl', 'provid', 'bi', 'comput', 'appli', 'statist', 'quantit', 'sourc', 'engin']","['sql', 'azur', 'powerbi', 'cloud', 'microsoft', 'powerbi', 'dashboard', 'risk', 'econometr', 'statist', 'etl', 'account', 'analyt', 'asset', 'essenti', 'program', 'learn', 'azur', 'corpor', 'relat', 'credit', 'etl', 'provid', 'bi', 'comput', 'appli', 'statist', 'quantit', 'sourc', 'engin']"
DE,"Ideal candidates should have experience with Data Ingestion and Consumptions.
That is transforming from source raw data, cleansing missing data, and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of the Kafka-Yarn-Spark-HDFSecosystem for ingestion IS A MUST.
Note that this requirement is 4 technologies stack in ONE PROJECT, not in multiple projects In-depth knowledge of preparing large scale data analytics for consumptions.
Candidate MUST HAVE EXPERIENCE with Large Fortune 500 Data Analytics Key knowledge on HIVE and query optimization in HIVE The candidate must have done data transformation, cleansing, matching and standardization as part of the ingestion Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc']"
DE,"Sr. Data Engineer Ideal candidates should have experience with Data Ingestion and Consumptions.
That is transforming from source raw data, cleansing missing data and outliers and preparing the data ready for analytics processing.
Key requirements Understanding of Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST.
In depth knowledge of preparing large scale data analytics for consumption.
Candidate MUST HAVE EXPERIENCE with Large Fortune 500 Data Analytics Key knowledge on HIVE and query optimization in HIVE Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark
","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka']","['outlier', 'optim']",999,"['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim']","['analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']","['pyspark', 'spark', 'scala', 'hadoop', 'hive', 'kafka', 'outlier', 'optim', 'analyt', 'spark', 'hadoop', 'optim', 'relat', 'sourc', 'engin']"
DE,"HMS makes the healthcare system work better for everyone.
At HMS, you will develop new skills and build your career in a dynamic industry while making a difference in the lives of others.
Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services) cloud environment.
*Essential Responsibilities:**
+ Design, implement, and test major subsystems of AWS cloud platform and core service offerings using the Scrum agile framework
+ Develop and follow best practices relative to design, implementation, and testing
+ Prototype new ideas or technologies to prove efficacy and usefulness in production
+ Develop SOAP (Simple Object Access Protocol) and REST (Representational State Transfer) APIs (Application Programming Interfaces) to integrate with partners and customers in real-time
+ Build a service structure on AWS capable of being deployed and scaled to run a variety of platform components dynamically
+ Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS cloud
+ Construct a state-of-the-art data lake on AWS using Amazon EMR, and Apache Spark, NiFi, Kafka, and Cassandra
+ Design & develop data pipelines for batch & streaming data sets using open source/ AWS tech stack
+ Mentor junior team members as a senior member of the Engineering team
*Non-Essential Responsibilities:**
+ Other duties as assigned
*Knowledge, Skills and Abilities:**
+ Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
+ Basic knowledge for distributed computing
+ Ability to communicate well
*Work Conditions and Physical Demands:**
+ Primarily sedentary work in a general office environment
+ Ability to communicate and exchange information
+ Ability to comprehend and interpret documents and data
+ Requires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)
+ Requires manual dexterity to use computer, telephone and peripherals
+ May be required to work extended hours for special business needs
+ May be required to travel at least 10% of time based on business needs
*Minimum Education:**
+ The knowledge typically acquired during the course of attaining a Bachelor's degree in Computer Science, Mathematics, or related discipline is required.
A combination of education and experience may be used in lieu of a diploma.
*Minimum Related Work Experience:**
+ 6 years' experience designing and delivering production software
+ Proven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc
_EOE including disability/veteran._
*Title:** _Senior Big Data Engineer_
*Requisition ID:** _190010NE_
EEO/Minorities/Females/Protected Veterans/Disabled.
","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'amazon web services']","['healthcar', 'pipelin', 'big data']",1,"['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'healthcar', 'pipelin', 'big data']","['essenti', 'program', 'spark', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']","['spark', 'cassandra', 'tf', 'aw', 'cloud', 'python', 'java', 'microsoft', 'github', 'kafka', 'healthcar', 'pipelin', 'big data', 'essenti', 'program', 'spark', 'pipelin', 'relat', 'aw', 'python', 'warehous', 'comput', 'big', 'set', 'sourc', 'engin']"
DE,"""Candidates should possess strong knowledge and interest across big data technologies and have a background in data engineering.
• Transform complex analytical models in scalable, production-ready solutions
• Provide support and enhancements for an advanced anomaly detection machine learning platform
• Develop cloud based applications from the ground up using a modern technology stack
• Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment
Responsibilities:
• Building data APIs and data delivery services to support critical operational and analytical applications
• Contributing to the design of robust systems with an eye on the long-term maintenance and support of the application
• Leveraging reusable code modules to solve problems across the team and organization
• Handling multiple functions and roles for the projects and Agile teams
• Being a technology thought leader and strategist
Required:
At least 4 years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least4 years of experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS
At least 4 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 2 years of experience with SQL and Shell Scripting experience
Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Apache Beam, Apache Airflow etc).
Experience with Spark programming (pyspark or scala or java).
Experience troubleshooting JVM-related issues.
Experience and strategies to deal with mutable data in Hadoop.
Familiarity with Spark Structure Streaming and/or Kafka Streams.
Familiarity with machine learning implementation using PySpark.
Experience in data visualization tools like Cognos, Arcadia, Tableau.
Experience in Ab Initio technologies including, but not limited to Ab Initio graph development, EME, Co-Op, BRE, Continuous flow)
Preferred:
• Angular.JS 4 Development and React.JS Development expertise in a up to date Java Development Environment with Cloud Technologies
• 1 years’ experience with Amazon Web Services (AWS), Google Compute or another public cloud service
• 2 years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
• 2 years of experience working with Dimensional Data Model and pipelines in relation with the same
• Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
• Hands on design experience with data pipelines, joining data between structured and unstructured data
• Familiarity of SAS programming will be a plus
• Experience implementing open source frameworks & exposure to various open source & package software architectures (AngularJS, ReactJS, Node, Elastic Search, Spark, Scala, Splunk, Apigee, and Jenkins etc.).
• Experience with various noSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j) will be a plus
Other:
• Successfully complete assessment tests offered in Pluralsight, Udemy, etc.
or complete certifications to demonstrate technical expertise on more than one development platform.
""
","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'amazon web services', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk']","['graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']",999,"['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data']","['program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']","['sql', 'cogno', 'python', 'java', 'hbase', 'nosql', 'kafka', 'perl', 'sa', 'hadoop', 'javascript', 'hive', 'aw', 'mongodb', 'pyspark', 'cassandra', 'scala', 'aw', 'tableau', 'spark', 'airflow', 'node', 'cloud', 'splunk', 'graph', 'pipelin', 'anomali', 'visual', 'machine learning', 'big data', 'program', 'stream', 'handl', 'visual', 'python', 'packag', 'sourc', 'analyt', 'sa', 'hadoop', 'public', 'aw', 'big', 'engin', 'machin', 'spark', 'pipelin', 'comput', 'relat']"
DE,"Organization: Accenture Federal Services
Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations.
Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
So, you can deliver what matters most.
Work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes to technology with business strategy and goals.
",[None],[None],999,[],['public'],['public']
DE,"Job Responsibilities
Write well designed, testable, efficient code.
Ensure designs are in compliance with specifications.
Support continuous improvement by investigating alternatives and technologies.
Perform all other duties as needed.
Recommends programming and development, standards and procedures for application teams.
Provide strategic and architectural direction to address unique business problems.
Researches and maintains knowledge in emerging technologies and possible application to the business.
Skills & Experience
Undergraduate Degree or Technical Certificate.
1 year of experience working on the Hadoop platform
1 year of Development experience Scala & SPARK
Expertise managing large datasets
SQL and UNIX scripting skills.
Exposure to cloud technology like Azure, Google
Nice to have skills:
Experience with IntelliJ
Experience with Mongo
python & pyspark
Implement tokenization and encryption solutions using various Protegrity protector components such as Application Protector, File Protector, Database Protector, Bigdata Protectors, etc.
Experience in Health Care Payer Domain
Experience Protegrity is added advantage
Understanding of payer domain business process and application (custom or packaged application) to support testing activity like enrollment, quote to card, claim processing and payment etc.
Hands-on experience with scripting languages like python
Basic Qualifications:
Undergraduate Degree or Technical Certificate.
1 year of experience working on the Hadoop platform
1 year of Development experience Scala & SPARK
Nearest Major Market: Plano
Nearest Secondary Market: Dallas
Job Segment:
Database, Consulting, SQL, Unix, Technology
","['sql', 'pyspark', 'spark', 'azur', 'scala', 'unix', 'hadoop', 'python']","['recommend', 'segment', 'research']",1,"['sql', 'pyspark', 'spark', 'azur', 'scala', 'unix', 'hadoop', 'python', 'recommend', 'segment', 'research']","['spark', 'azur', 'hadoop', 'provid', 'python', 'packag']","['sql', 'pyspark', 'spark', 'azur', 'scala', 'unix', 'hadoop', 'python', 'recommend', 'segment', 'research', 'spark', 'azur', 'hadoop', 'provid', 'python', 'packag']"
DE,"Your Opportunity
They help Marketing, Finance, Risk and various P&Ls make fact-based decisions by integrating and analyzing data as well as operationally leverage data for competitive advantage.
The team delivers innovative client experience capability and rich business insight through robust enterprise data-driven capabilities.
Dev Engineering team within GDT focuses on building new frameworks and enhancing existing ones to improve overall project delivery efficiency and maintain coding standards.
What you are good at
Diagnose / fix highly complex technical issues independently
Communicate individual and project-level development statuses, issues, risks, and concerns to technical leadership and management
Create documentation and training related to technology stacks and standards within assigned team
Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed
Experience collaborating with business and technology partners and offshore development teams
Working with product owners and technical directors to lead technical discussions and resolve technical issues
Strong skills in design, development and delivery of data solutions on Teradata, Big Data and cloud data platforms.
Developing and maintaining code for data ingestion and curation using Informatica, Talend, Scoop, Hive etc
Managing day-to-day development activities for new data solutions and troubleshooting existing implementations.
Applying best practices of data integration for data quality and automation
Reviewing data models and data architecture for Hadoop and HBase environments
Documenting design solutions and supporting documentation.
Working with business analysts to understand business requirements and use cases
What you have
Bachelor's degree in Computer Science or related discipline
Experience with a structured application development methodology, using any industry standard Software Development Lifecycle, in particular Agile Methodologies is required
6+ years of overall experience in I.T.
with strong understanding of best practices for building and designing ETL code
5+ years of experience in ETL tools.
Specific expertise in implementing Informatica / Talend in an Enterprise environment.
Very good experience/understanding on Building Enterprise Data Lake using Talend, Scoop, Hive, Mongo DB, etc
Design, build and support data processing pipelines to transform data in Big Data, Teradata platforms, Cloud Platforms (GCP, AWS)
Experience in real time data ingestion into Hadoop is required
Understanding Hadoop file format and compressions is required
Understanding of best practices for building Data Lake and analytical architecture on Hadoop is required
Familiarity with MapR distribution of Hadoop is preferred
Experience in or deep understanding of cloud based data technology GCP/AWS is preferred
Experience with change data capture tools (CDC) preferred such as Attunity
Scripting / programming with UNIX, Java, Python, Scala etc.
is preferred
Hands-on experience in Java object oriented programming (At least 2 years)
Hands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE (At least 3 years)
Experience in Active Batch Scheduling , control M preferred
Experience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred
Good verbal and written communication skills
Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills
Ability to thrive in a flexible and fast-paced environment across multiple time zones and locations
Experience in Financial Services industry a plus.
Requisition #: 2020-63908
r1d4rh5eu
","['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka']","['big data', 'pipelin', 'risk', 'financ', 'etl', 'commun']",1,"['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka', 'big data', 'pipelin', 'risk', 'financ', 'etl', 'commun']","['day', 'analyt', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']","['mapreduc', 'gcp', 'spark', 'scala', 'pig', 'unix', 'git', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'hbase', 'db', 'kafka', 'big data', 'pipelin', 'risk', 'financ', 'etl', 'commun', 'day', 'analyt', 'program', 'techniqu', 'spark', 'pipelin', 'relat', 'hadoop', 'aw', 'python', 'comput', 'big', 'etl', 'engin', 'integr']"
DE,"Digital SW Eng Lead/Data Engineer
Irving, TX
$50/hr for Mid- Sr level and
$70-80/hr for Architect
Citi Bank
Only GC/USC/H4/L2 EAD and E3
Job Title
Digital S/W Eng Lead Analyst
Job Code
Job Function
Job Family
Digital Software Engineering
C-Level
Manager or IC
PRIMARY RESPONSIBILITIES
Key Activities â Please list in order of importance/time spent (highest to lowest)
Â Define needs around maintainability, testability, performance, security, quality and usability for data platform
Â Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes
Â Work with the Business Analysts and Customers throughout the requirements process to properly understand the long term goals of the program and where they fit in the overall UI infrastructure
Â Communication of new technologies, best practices, etc.
to developers, testers, and managers.
Â Mentoring and peer review of designs and coded implementations
Â Work with technical specialists (Security Team, Performance Engineer, etc.)
to ensure that all parties understand the system that is being designed and built and that all major issues are understood and mitigated.
Â Expected to participate in several implementation phases of product development cycle â design, scoping, planning, implementation and test.
Â Responsible for moving all legacy workloads to cloud platform
Â Work with data scientist to build Client pipelines using heterogeneous sources and provide engineering services for data science applications
Â Ensure automation through CI/CD across platforms both in cloud and on-premises
Â Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation
Â Be the technical expert and mentor other team members on Big Data and Cloud Tech stacks
Some of the best practices supported include but are not limited to:
â Achieving 85% code coverage with use of TDD (Test Driven Development),
â Leveraging automated testing on 100% of API code
â Leveraging automated testing for Continuous Improvement Continuous Development (functional & performance)
â Ensuring frequent check in of code and supporting Peer Code Reviews
KNOWLEDGE, SKILLS & EXPERIENCE
Education level
and/or relevant experience(s)
Required:
Â BS/BA degree or equivalent combination of education/experience.
Â Intermediate to senior level experience in an Apps Development role.
Demonstrated strong execution capabilities.
Preferred:
Knowledge and skills
(general and technical)
Required:
Â 5+ years of experience with Hadoop (Cloudera) or Cloud Technologies Expert level building pipelines using Apache Beam or Spark Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms
Â Experience with all aspects of DevOps (source control, continuous integration, deployments, etc.)
Â Experience with containerization and related technologies (e.g.
Docker, Kubernetes)
Â Experience in other open-sources like Druid, Elastic Search, Logstash etc is a plus
Â Advanced knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)
Â Knowledge of agile(scrum) development methodology is a plus
Â Strong development/automation skills
Â Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.Â
Â System level understanding - Data structures, algorithms, distributed storage & compute
Â Can-do attitude on solving complex business problems, good interpersonal and teamwork skills
Support of a highly distributed, scalable ETL processes - Sourcing, Data Enrichments and data delivery using Ab Initio ETL tool.
Hands on ETL production support (data transformations and data movement) using Ab Initio, Ab Initio Express IT, EDQE (Data Quality) and Enterprise Metadata Hub as key tools
Preferred:
Angular.JS 4 Development and React.JS Development expertise in a up to date Java Development Environment with Cloud Technologies
Exposure and/or development experience in Microservices Architectures best practices, Java Spring Boot Framework (Preferred), Docker, Kubernetes
Exposure to other data pipeline
Experience around REST APIs, services, and API authentication schemes
Knowledge in RDBMS and NoSQL technologies
Exposure to multiple programming languages
Knowledge of modern CI/CD, TDD, Frequent Release Technologies and Processes (Docker, Kubernetes, Jenkins)
Exposure to mobile programming will be a plus.
Other Requirements (licenses, certifications, specialized training, physical or mental abilities required)
Other:
Â Successfully complete assessment tests offered in Pluralsight, Udemy, etc.
or complete certifications to demonstrate technical expertise on more than one development platform.
","['mapreduc', 'gcp', 'spark', 'azur', 'pig', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'c', 'kubernet', 'kafka', 'docker']","['research', 'pipelin', 'big data', 'etl', 'commun']",1,"['mapreduc', 'gcp', 'spark', 'azur', 'pig', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'c', 'kubernet', 'kafka', 'docker', 'research', 'pipelin', 'big data', 'etl', 'commun']","['program', 'provid', 'python', 'etl', 'integr', 'sourc', 'azur', 'hadoop', 'releas', 'â', 'primari', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']","['mapreduc', 'gcp', 'spark', 'azur', 'pig', 'solr', 'hadoop', 'aw', 'cloud', 'python', 'hive', 'java', 'nosql', 'c', 'kubernet', 'kafka', 'docker', 'research', 'pipelin', 'big data', 'etl', 'commun', 'program', 'provid', 'python', 'etl', 'integr', 'sourc', 'azur', 'hadoop', 'releas', 'â', 'primari', 'infrastructur', 'aw', 'parti', 'big', 'engin', 'digit', 'spark', 'pipelin', 'comput', 'scientist', 'relat']"
DE,"Develop data collections, labeling pipelines, and evaluation pipelines.
Design and implement software applications using Machine Learning and Artificial Intelligence for data verification, transformation, and analytics.
Dashboard UI design and deployment.
Optimize models for on-device and multi-modal intelligence.
What wersquore looking for... Yoursquoll need to have Bachelorrsquos degree or four or more years of work experience.
Six or more years of relevant work experience.
Hands-on experience with Restful APIs, OpenAPI, Swagger UI, POSTMAN, and ELK Stack (Elasticsearch, Logstash, Kibana).
Experience developing applications using machine learning and deep learning models for real world, large scale problems in computer networks.
Hands-on experience on machine learning algorithms, from supervised and unsupervised to reinforcement learning Excellent Java, CC++, andor Python programming skills.
Experience with one or more of the following artificial neural networks, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.
Experience in analyzing sophisticated and dynamic patterns.
Experience on with automation, Machine Learning, and Deep Learning tools and applications (TensorFlow, RPA, etc.)
","['elasticsearch', 'java', 'python', 'tensorflow', 'excel']","['recommend', 'supervis', 'classif', 'pipelin', 'dashboard', 'neural network', 'machine learning', 'optim', 'deep learning', 'unsupervis']",2,"['elasticsearch', 'java', 'python', 'tensorflow', 'excel', 'recommend', 'supervis', 'classif', 'pipelin', 'dashboard', 'nn', 'machine learning', 'optim', 'deep learning', 'unsupervis']","['analyt', 'machin', 'learn', 'pipelin', 'evalu', 'optim', 'python', 'comput', 'collect', 'algorithm']","['elasticsearch', 'java', 'python', 'tensorflow', 'excel', 'recommend', 'supervis', 'classif', 'pipelin', 'dashboard', 'nn', 'machine learning', 'optim', 'deep learning', 'unsupervis', 'analyt', 'machin', 'learn', 'pipelin', 'evalu', 'optim', 'python', 'comput', 'collect', 'algorithm']"
